{
  "hash": "69863550c67d463c3dee994d4e356dd4",
  "result": {
    "markdown": "# Data Transformations {#datatrans}\n\n\n::: {.cell}\n\n:::\n\n::: {.cell hash='DataTrans_cache/html/unnamed-chunk-2_aa7d04675f5673c053a7a2e2290188da'}\n<STYLE type='text/css' scoped>\nPRE.fansi SPAN {padding-top: .25em; padding-bottom: .25em};\n</STYLE>\n:::\n\n::: {.cell hash='DataTrans_cache/html/unnamed-chunk-3_dbda1e5adc188b71ee061588493643cb'}\n\n:::\n\n\n## Continuous variables\n\n### Standardization / Scaling & Centering with `scale()` {#zscore}\n\n**Scaling** of a numeric vector is achieved by elementwise division with the standard deviation. A scaled vector therefore has standard deviation equal to 1.\n\n**Centering** of a numeric vector is achieved by elementwise subtraction of its mean. A centered vector therefore has mean equal to 0.\n\n**Standardizing**, a.k.a. converting to Z-scores, involves scaling and centering. Scaling and centering is performed in R with the `scale()` function.\n\nDepending on your modeling needs / the algorithms you plan to use, it is often important to scale and/or center your data. Note that many functions, but not all, may automatically scale and center data internally if it is required by the algorithm. Check the function documentation to see if you should manually scale or not.\n\n`scale()` can be applied to a single vector or a matrix/data.frame.\nIn the case of a matrix or data.frame, scaling is applied on each column individually.\nBy default, both arguments `scale` and `center` are set to `TRUE`.\n\nScale a vector:\n\n\n::: {.cell hash='DataTrans_cache/html/unnamed-chunk-4_1e977f7f83e1d7f74b792cb3e7d92da8'}\n\n```{.r .cell-code}\nhead(iris$Sepal.Length)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.1 4.9 4.7 4.6 5.0 5.4\n```\n:::\n\n```{.r .cell-code}\nPetal.Length_scaled <- scale(iris$Petal.Length)\nhead(Petal.Length_scaled)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]\n[1,] -1.335752\n[2,] -1.335752\n[3,] -1.392399\n[4,] -1.279104\n[5,] -1.335752\n[6,] -1.165809\n```\n:::\n:::\n\n\nScale multiple columns of a matrix/data.frame:\n\n\n::: {.cell hash='DataTrans_cache/html/unnamed-chunk-5_d607751e075fb7c3e995161cf780d573'}\n\n```{.r .cell-code}\niris.scaled <- scale(iris[, -5])\nhead(iris.scaled)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Sepal.Length Sepal.Width Petal.Length Petal.Width\n[1,]   -0.8976739  1.01560199    -1.335752   -1.311052\n[2,]   -1.1392005 -0.13153881    -1.335752   -1.311052\n[3,]   -1.3807271  0.32731751    -1.392399   -1.311052\n[4,]   -1.5014904  0.09788935    -1.279104   -1.311052\n[5,]   -1.0184372  1.24503015    -1.335752   -1.311052\n[6,]   -0.5353840  1.93331463    -1.165809   -1.048667\n```\n:::\n:::\n\n\nFirst, let's verify that `scale()` did what we wanted:  \n\n\n::: {.cell hash='DataTrans_cache/html/unnamed-chunk-6_7c63746294f736db99532750298a4aeb'}\n\n```{.r .cell-code}\ncolMeans(iris.scaled)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Sepal.Length   Sepal.Width  Petal.Length   Petal.Width \n-4.480675e-16  2.035409e-16 -2.844947e-17 -3.714621e-17 \n```\n:::\n:::\n\n::: {.cell hash='DataTrans_cache/html/unnamed-chunk-7_13f9bba63a68717494d72491698fd233'}\n\n```{.r .cell-code}\napply(iris.scaled, 2, sd)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n           1            1            1            1 \n```\n:::\n:::\n\n\nWe got effectively 0 mean and standard deviation of 1 for each column.  \n\n`scale()` outputs the scaled vector(s) along with the scaling and/or centering parameters saved as attributes in the output.\n\nNote that in both cases, whether a vector input or data.frame, the output is a **matrix**:\n\n\n::: {.cell hash='DataTrans_cache/html/unnamed-chunk-8_0b955332a232e8bccde8248836dae895'}\n\n```{.r .cell-code}\nclass(Petal.Length_scaled)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"matrix\" \"array\" \n```\n:::\n\n```{.r .cell-code}\nclass(iris.scaled)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"matrix\" \"array\" \n```\n:::\n:::\n\n\nGet the output attributes:\n\n\n::: {.cell hash='DataTrans_cache/html/unnamed-chunk-9_dcc641220930c6beb1b3c5b49a8997fb'}\n\n```{.r .cell-code}\nattributes(Petal.Length_scaled)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$dim\n[1] 150   1\n\n$`scaled:center`\n[1] 3.758\n\n$`scaled:scale`\n[1] 1.765298\n```\n:::\n:::\n\n\n`center` is the mean:\n\n\n::: {.cell hash='DataTrans_cache/html/unnamed-chunk-10_f81ae1481f71556a7153d66002cc10c9'}\n\n```{.r .cell-code}\nmean(iris$Petal.Length)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.758\n```\n:::\n:::\n\n\n`scale` is the standard deviation:\n\n\n::: {.cell hash='DataTrans_cache/html/unnamed-chunk-11_66a06b58e25cb2152be7eb8d6436fc6d'}\n\n```{.r .cell-code}\nsd(iris$Petal.Length)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.765298\n```\n:::\n:::\n\n\nFor a matrix/data.frame input, you get `center` and `scale` attributes per column:\n\n\n::: {.cell hash='DataTrans_cache/html/unnamed-chunk-12_507ba7c1c5944985dedf330e6e379845'}\n\n```{.r .cell-code}\nattributes(iris.scaled)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$dim\n[1] 150   4\n\n$dimnames\n$dimnames[[1]]\nNULL\n\n$dimnames[[2]]\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\" \n\n\n$`scaled:center`\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    5.843333     3.057333     3.758000     1.199333 \n\n$`scaled:scale`\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n   0.8280661    0.4358663    1.7652982    0.7622377 \n```\n:::\n:::\n\n\nLet's save the scale and center attributes and then double check the calculations:\n\n\n::: {.cell hash='DataTrans_cache/html/unnamed-chunk-13_55b7ceff606233a73cb8a50649aa03d1'}\n\n```{.r .cell-code}\n.center <- attr(iris.scaled, \"scaled:center\")\n.center\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    5.843333     3.057333     3.758000     1.199333 \n```\n:::\n\n```{.r .cell-code}\n.scale <- attr(iris.scaled, \"scaled:scale\")\n.scale\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n   0.8280661    0.4358663    1.7652982    0.7622377 \n```\n:::\n\n```{.r .cell-code}\nSepal.Length_scaled <- (iris$Sepal.Length - .center[1]) / .scale[1]\nall(Sepal.Length_scaled == iris.scaled[, \"Sepal.Length\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\nNote: Due to limitation in numerical precision, checking sets of floats for \nequality after multiple operations is \n\n\n::: {.cell hash='DataTrans_cache/html/unnamed-chunk-14_c7a31f4ff9f7180c082097d2a7d59c6a'}\n\n```{.r .cell-code}\nall.equal(Sepal.Length_scaled, iris.scaled[, \"Sepal.Length\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\n::: callout-note\nIf you are manually scaling and/or centering data for **supervised learning**, you must:  \n\n* Perform scaling and centering on your **training data**,\n* Save the **centering and scaling parameters** for each feature, and\n* Apply the training set-derived centering and scaling parameters to the **test set** *prior to prediction/inference*.\n:::\n\nA common mistake is to either scale training and testing data together in the beginning, or scale them independently.  \n\n### Normalization\n\n**Normalization** has different meanings in different contexts; in the context of a numeric variable it usually refers to converting to a 0-1 range.\n\nLet's write a simple function to achieve this:\n\n\n::: {.cell hash='DataTrans_cache/html/unnamed-chunk-15_09a31ec14bdb0b02fe5495f33014d6f8'}\n\n```{.r .cell-code}\nnormalize <- function(x) {\n  .min <- min(x, na.rm = TRUE)\n  (x - .min) / max(x - .min, na.rm = TRUE)\n}\n```\n:::\n\n::: {.cell hash='DataTrans_cache/html/unnamed-chunk-16_01295d43b7d96f8421ec445b5d6b9cf9'}\n\n```{.r .cell-code}\nx <- rnorm(20, mean = 13, sd = 1.4)\nx\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 13.062362 14.310444 14.547387 11.512227 12.151511 11.989984 13.145036\n [8] 13.477702 15.321401 10.732494  8.751632 16.506836 11.496250 11.937270\n[15] 11.801317 14.078019 11.571266 15.374553 14.845502 13.558719\n```\n:::\n:::\n\n::: {.cell hash='DataTrans_cache/html/unnamed-chunk-17_09813eac6228e92806953e4a7dcb6b36'}\n\n```{.r .cell-code}\nx_normalized <- normalize(x)\nx_normalized\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 0.72521513 0.68535625 0.54122137 0.53119869 0.05403545 0.52192876\n [7] 0.22970970 0.25551536 0.44474811 0.68204001 0.69525414 0.48455492\n[13] 0.50414494 0.51755532 0.02421227 0.68830059 0.33004795 0.00000000\n[19] 1.00000000 0.47967242\n```\n:::\n\n```{.r .cell-code}\nmin(x_normalized)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n\n```{.r .cell-code}\nmax(x_normalized)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\n\nNote that it is easy to make the `normalize()` function more general, by adding `lo` and `hi` arguments to convert to any range:\n\n\n::: {.cell hash='DataTrans_cache/html/unnamed-chunk-18_6ec589f0ff55d4a082d71ee81ef0a6b0'}\n\n```{.r .cell-code}\ndr <- function(x, lo = 0, hi = 1) {\n    .min <- min(x, na.rm = TRUE)\n   (x - .min) / max(x - .min, na.rm = TRUE) * (hi - lo) + lo\n  }\n```\n:::\n\n::: {.cell hash='DataTrans_cache/html/unnamed-chunk-19_36ef2d00f7148ddfe59b58240035367e'}\n\n```{.r .cell-code}\ndr(x, -1, 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]  0.450430250  0.370712498  0.082442732  0.062397385 -0.891929105\n [6]  0.043857516 -0.540580608 -0.488969277 -0.110503785  0.364080024\n[11]  0.390508273 -0.030890162  0.008289889  0.035110635 -0.951575459\n[16]  0.376601189 -0.339904101 -1.000000000  1.000000000 -0.040655158\n```\n:::\n:::\n\n\n### Log-transform with `log()`\n\n\n\n\n\nFor the following example, `x` is an unknown feature in a new dataset we were \njust given.\n\nWe start by plotting its distribution:\n\n\n::: {.cell hash='DataTrans_cache/html/unnamed-chunk-21_dcf67ce9e4a1d57f3163d13ae2659f76'}\n\n```{.r .cell-code}\nmplot3_x(x)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in polygon(c(densityl[[i]]$x, rev(densityl[[i]]$x)), c(densityl[[i]]$y,\n: semi-transparency is not supported on this device: reported only once per\npage\n```\n:::\n\n::: {.cell-output-display}\n![](DataTrans_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\nWe can see it is skewed right. A log transform can help here:\n\n\n::: {.cell hash='DataTrans_cache/html/unnamed-chunk-22_efb59c8f03ca3277e5d70e4fc3774f96'}\n\n```{.r .cell-code}\nmplot3_x(log(x))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in polygon(c(densityl[[i]]$x, rev(densityl[[i]]$x)), c(densityl[[i]]$y,\n: semi-transparency is not supported on this device: reported only once per\npage\n```\n:::\n\n::: {.cell-output-display}\n![](DataTrans_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n### Data binning with `cut()`\n\nA different approach for the above variable might be to bin it.  \nLet's look at a few different ways to bin continuous data.\n\n#### Evenly-spaced interval\n\n`cut()` allows us to bin a numeric variable into evenly-spaced intervals.  \nThe `breaks` argument defines the number of intervals:\n\n\n::: {.cell hash='DataTrans_cache/html/unnamed-chunk-23_51a8b6466b061377d8e3508c5a2cd0a8'}\n\n```{.r .cell-code}\nx_cut4 <- cut(x, breaks = 4)\nhead(x_cut4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] (0.291,178] (0.291,178] (0.291,178] (0.291,178] (0.291,178] (0.291,178]\nLevels: (0.291,178] (178,355] (355,533] (533,711]\n```\n:::\n\n```{.r .cell-code}\ntable(x_cut4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nx_cut4\n(0.291,178]   (178,355]   (355,533]   (533,711] \n        977          19           3           1 \n```\n:::\n:::\n\n\n::: callout-important\n**Interval Notation**\n\n`[3, 9)` represents the interval of [real numbers](https://en.wikipedia.org/wiki/Real_number) \nbetween 3 and 9, **including** 3 and **excluding** 9.\n:::\n\nBecause the data is so skewed, equal intervals are not helpful in this case. The majority of the data points get grouped into a single bin.\n\nLet's visualize the cuts:\n\n\n::: {.cell hash='DataTrans_cache/html/unnamed-chunk-24_ad5bf919e83db0881b7802900160b12b'}\n\n```{.r .cell-code}\nxcuts5 <- seq(min(x), max(x), length.out = 5)\nxcuts5\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]   1.0000 178.2453 355.4905 532.7358 709.9811\n```\n:::\n:::\n\n::: {.cell hash='DataTrans_cache/html/unnamed-chunk-25_540afa19dc7e420430b6fba65a60d757'}\n\n```{.r .cell-code}\nmplot3_x(x, par.reset = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in polygon(c(densityl[[i]]$x, rev(densityl[[i]]$x)), c(densityl[[i]]$y,\n: semi-transparency is not supported on this device: reported only once per\npage\n```\n:::\n\n```{.r .cell-code}\n# plot(density(x)) # in base R\nabline(v = xcuts5, col = \"red\", lwd = 1.5)\n```\n\n::: {.cell-output-display}\n![](DataTrans_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\n[Note: We used `par.reset = FALSE` to stop `mplot3_x()` from resetting its custom `par()` settings so that we can continue adding elements to the same plot, in this case with the `abline()` command.]\n\n#### Quantile cuts\n\nInstead of evenly-spaced intervals, we can get quantiles with `quantile()`. We ask for 5 quantiles using the `length.out` argument, which corresponds to 4 intervals:\n\n\n::: {.cell hash='DataTrans_cache/html/unnamed-chunk-26_41ba1b1a327bc323ec74fe491e3a8cc9'}\n\n```{.r .cell-code}\nxquants5 <- quantile(x, probs = seq(0, 1, length.out = 5))\nxquants5 <- quantile(x, probs = seq(0, 1, length.out = 5))\nmplot3_x(x, par.reset = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in polygon(c(densityl[[i]]$x, rev(densityl[[i]]$x)), c(densityl[[i]]$y,\n: semi-transparency is not supported on this device: reported only once per\npage\n```\n:::\n\n```{.r .cell-code}\n# plot(density(x)) # in base R\nabline(v = xquants5, col = \"green\", lwd = 1.5)\n```\n\n::: {.cell-output-display}\n![](DataTrans_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\nThe `breaks` argument of `cut()` allows us to pass either an integer to define the number of evenly-spaced breaks, or a numeric vector defining the position of breaks.\n\nWe can therefore pass the quantile values as break points.\n\nSince the quantile values begin at the lowest value in the data, we need to define \n`include.lowest = TRUE` so that the first interval is inclusive of the lowest value:\n\n\n::: {.cell hash='DataTrans_cache/html/unnamed-chunk-27_8476e88fffd5688b7a78031dbb38d689'}\n\n```{.r .cell-code}\nx_cutq4 <- cut(x, breaks = xquants5, include.lowest = TRUE)\ntable(x_cutq4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nx_cutq4\n   [1,11.5] (11.5,23.2] (23.2,47.2]  (47.2,710] \n        250         250         250         250 \n```\n:::\n:::\n\n\nWith quantile cuts, each bin contains roughly the same number of observations (+/- 1).  \n\n## Categorical variables\n\nMany algorithms (or their implementations) do not directly support categorical variables. To use \nthem, you must therefore convert all categorical variables to some type of numerical encoding.\n\n### Integer encoding\n\nIf the categorical data is ordinal, you can simply convert it to integers.  \nFor example, the following **ordered factor**:\n\n\n::: {.cell hash='DataTrans_cache/html/unnamed-chunk-28_c993e7d64680613f89d00701863f80b6'}\n\n```{.r .cell-code}\nbrightness <- factor(c(\"bright\", \"brightest\", \"darkest\",\n                        \"bright\", \"dark\", \"dim\", \"dark\"),\n                      levels = c(\"darkest\", \"dark\", \"dim\", \"bright\", \"brightest\"),\n                      ordered = TRUE)\nbrightness\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] bright    brightest darkest   bright    dark      dim       dark     \nLevels: darkest < dark < dim < bright < brightest\n```\n:::\n:::\n\n\ncan be directly coerced to an integer:\n\n\n::: {.cell hash='DataTrans_cache/html/unnamed-chunk-29_3782a2385d4390d3601510f7249fdcde'}\n\n```{.r .cell-code}\nas.integer(brightness)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4 5 1 4 2 3 2\n```\n:::\n:::\n\n\n### One-hot encoding\n\nWhen categorical features are **not** ordinal, and your algorithm cannot handle them directly, you \ncan one-hot encode them. In one-hot encoding, each categorical feature is converted to k binary \nfeatures, where k = number of unique values in the input, such that only one feature has the value 1 \nper case. This is similar to creating dummy variables in statistics, with the difference that dummy \nvariables create `k - 1` new variables.\n\n\n::: {.cell hash='DataTrans_cache/html/unnamed-chunk-30_05c4ac35169027ae0416b697693036da'}\n\n```{.r .cell-code}\nset.seed(21)\nadmission_reasons <- c(\"plannedSurgery\", \"emergencySurgery\", \"medical\")\nadmission <- sample(admission_reasons, size = 12, replace = TRUE)\nadmission\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"medical\"          \"plannedSurgery\"   \"medical\"          \"plannedSurgery\"  \n [5] \"emergencySurgery\" \"medical\"          \"plannedSurgery\"   \"medical\"         \n [9] \"medical\"          \"emergencySurgery\" \"emergencySurgery\" \"emergencySurgery\"\n```\n:::\n:::\n\n\nMultiple packages include functions that perform one-hot encoding. It's a simple operation and we don't necessarily need to install a large package with many dependencies.\n\nLet's write a simple function to perform one-hot encoding. Note, you may have heard that for loops can be slow in R, but that depends on the operations performed. Here, we loop over an integer matrix and it is plenty fast.\n\n\n::: {.cell hash='DataTrans_cache/html/unnamed-chunk-31_539f79ef2c33d740252af09ade275df1'}\n\n```{.r .cell-code}\nonehot <- function(x, xname = NULL) {\n  if (is.null(xname)) xname <- deparse(substitute(x))\n  x <- factor(x)\n  .levels <- levels(x)      # Get factor levels\n  ncases <- NROW(x)         # Get number of cases\n  index <- as.integer(x)    # Convert levels to integer\n  oh <- matrix(0, nrow = ncases, ncol = length(.levels))  # Initialize zeros matrix\n  colnames(oh) <- paste(xname, .levels, sep = \"_\")  # Name columns by levels\n  for (i in seq(ncases)) oh[i, index[i]] <- 1  # Assign \"1\" to appropriate level\n  oh\n}\n```\n:::\n\n\nLet's apply our new function to the admission vector:\n\n\n::: {.cell hash='DataTrans_cache/html/unnamed-chunk-32_71153ed163870878b97f5d468dc7cf78'}\n\n```{.r .cell-code}\nonehot(admission)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      admission_emergencySurgery admission_medical admission_plannedSurgery\n [1,]                          0                 1                        0\n [2,]                          0                 0                        1\n [3,]                          0                 1                        0\n [4,]                          0                 0                        1\n [5,]                          1                 0                        0\n [6,]                          0                 1                        0\n [7,]                          0                 0                        1\n [8,]                          0                 1                        0\n [9,]                          0                 1                        0\n[10,]                          1                 0                        0\n[11,]                          1                 0                        0\n[12,]                          1                 0                        0\n```\n:::\n:::\n\n\nNote: `deparse(substitute(x))` above is used to automatically get the name of the input object (in this case \"admission\"). This is similar to how many of R's internal functions (e.g. `plot()`) get the names of input objects.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}