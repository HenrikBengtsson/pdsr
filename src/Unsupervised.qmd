---
execute:
  warning: false
---
# Unsupervised Learning {#unsupervised}

```{r}
#| echo: false
knitr::opts_chunk$set(fig.width = 5, fig.height = 5,
                      comment = NA, cache = TRUE) 
options(rt.theme = "whitegrid")
options(rt.fit.theme = "whitegrid")
```

```{r, comment="", results="asis", echo=FALSE}
old.hooks <- fansi::set_knit_hooks(knitr::knit_hooks)
options(crayon.enabled = TRUE)
```

```{r}
library(rtemis)
```

Unsupervised learning aims to learn relationships within a dataset without focusing at a particular outcome. You will often hear of unsupervised learning being performed on unlabeled data. To be clear, it means it does not use the labels to guide learning - whether labels are available or not. You might, for example, perform unsupervised learning ahead of supervised learning as we shall see later. Unsupervised learning includes a number of approaches, most of which can be divided into two categories:

* __Clustering__: Cases are grouped together based on some derived measure of similarity / distance metric.
* __Dimensionality Reduction / Matrix decomposition__: Variables are combined / projected into a lower dimensional space.

In **rtemis**, clustering algorithms begin with `c_` and decomposition/dimensionality reduction algorithms begin with `d_`

## Decomposition / Dimensionality Reduction {#decomposition}

Use `decomSelect()` to get a listing of available decomposition algorithms:
```{r}
decomSelect()
```

We can further divide decomposition algorithms into linear (e.g. PCA, ICA, NMF) and nonlinear dimensionality reduction, (also called manifold learning, like LLE and tSNE).

### Principal Component Analysis (PCA)

```{r}
x <- iris[, 1:4]
iris_PCA <- d_PCA(x)
mplot3_xy(iris_PCA$projections.train[, 1], 
          iris_PCA$projections.train[, 2], 
          group = iris$Species,
          xlab = "1st PCA component", 
          ylab = "2nd PCA component", 
          main = "PCA on iris")
```

### Independent Component Analysis (ICA)

```{r}
iris_ICA <- d_ICA(x, k = 2)
mplot3_xy(iris_ICA$projections.train[, 1], 
          iris_ICA$projections.train[, 2], 
          group = iris$Species,
          xlab = "1st ICA component", 
          ylab = "2nd ICA component", 
          main = "ICA on iris")
```

### Non-negative Matrix Factorization (NMF)

```{r}
iris_NMF <- d_NMF(x, k = 2)
mplot3_xy(iris_NMF$projections.train[, 1], 
          iris_NMF$projections.train[, 2], 
          group = iris$Species,
          xlab = "1st NMF component", 
          ylab = "2nd NMF component", 
          main = "NMF on iris")
```

## Clustering

Use `clustSelect()` to get a listing of available clustering algorithms:
```{r}
clustSelect()
```

Let's cluster iris and we shall also use an NMF decomposition as we saw above to project to 2 dimensions.  
We'll use two of the most popular clustering algorithms, K-means and PAM, aka K-medoids.

```{r}
x <- iris[, 1:4]
iris_NMF <- d_NMF(x, k = 2)
```

### K-Means

```{r}
iris.KMEANS <- c_KMeans(x, k = 3)
mplot3_xy(iris_NMF$projections.train[, 1], iris_NMF$projections.train[, 2],
          group = iris.KMEANS$clusters.train,
          xlab = "1st NMF component", 
          ylab = "2nd NMF component", 
          main = "KMEANS on iris")
```

### Partitioning Around Medoids with k estimation (PAMK)

```{r}
iris_PAMK <- c_PAMK(x, krange = 3:10)
mplot3_xy(iris_NMF$projections.train[, 1], iris_NMF$projections.train[, 2],
          group = iris_PAMK$clusters.train,
          xlab = "1st NMF component", 
          ylab = "2nd NMF component", 
          main = "PAMK on iris")
```

## See also

- [rtemis Decomposition](https://rtemis.lambdamd.org/Decomposition.html)
- [rtemis Clustering](https://rtemis.lambdamd.org/Clustering.html)
