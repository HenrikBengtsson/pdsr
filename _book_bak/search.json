[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PDSR",
    "section": "",
    "text": "Welcome to PDSR!  This book is aimed as an introductory- to intermediate-level\nprogramming learning resource for R.\nIt is the online book for UCSF Biostat 213/4.  E.D. Gennatas MBBS AICSM PhD\nLaboratory of Computational Medicine,\nUniversity of California, San Francisco,\nSan Francisco, CA, August 2022"
  },
  {
    "objectID": "01_Preface.html",
    "href": "01_Preface.html",
    "title": "1  Preface",
    "section": "",
    "text": "Throughout this book you will see boxes with R code followed by its output, if any. The code (or input) is decorated with a teal border on the left to separate it from its output, like in the following example:\n\nx <- rnorm(200)\nx[1:20]\n\n [1]  1.43890051  0.30798483  0.70450976 -0.60107735 -0.36263414  1.22274118\n [7]  0.92326612  1.83287150 -0.83819416 -0.60649768  0.28315816 -2.15783263\n[13] -0.51911617  0.30228461  2.09433337  0.20119295  0.15636645 -0.65842281\n[19]  0.01900776  0.02839434\n\n\nNotice that R adds numbers in brackets in the beginning of each row. This happens when R prints the contents of a vector. The number is the integer index of the first element in that row. Therefore, the first one is always [1] and the number of the subsequent rows depends on how many elements fit in each line. If the output is a single element, it will still have [1] in front of it.\nAlso notice that if we enclose the assignment operation of a variable in parentheses, this prints the resulting value of the variable. Therefore, this:\n\n(y <- 4)\n\n[1] 4\n\n\nis equivalent to:\n\ny <- 4\ny\n\n[1] 4\n\n\nCurrently, this site uses Fira Code to display source code, which supports multiple character ligatures that make code prettier / easier to read.\n\n\n\n\n\nNote ligated versions of some common character combinations as they should appear in this site\n\n\n\n\nNote that if you mouse over the input code box, a clickable “Copy to clipboard” appears on the top right of the box allowing you to copy paste into an R session or file.\nLastly, you will see the following informational boxes at times:\n\n\n\nNote\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nImportant\n\n\n\n\n\n\nWarning\n\n\n\n\n\n\nCaution\n\n\n\nThis book was created using Quarto, ported from the previous version which used bookdown."
  },
  {
    "objectID": "02_Introduction.html",
    "href": "02_Introduction.html",
    "title": "2  Introduction",
    "section": "",
    "text": "R is a modern implementation of the S language and part of the GNU Project.\nR is an interpreted language, allowing interactive work with data. It is written in C, Fortran, and R itself.\nSome of R’s strengths:\n\nThe base language (i.e. what is included in R when you first install it) comes loaded with functionality for\n\ndata cleaning and manipulation\nstatistical testing and modeling\npowerful graphics\n\nThe vast ecosystem of third party packages brings unparalleled functionality for statistics, epidemiology, machine learning, visualization, image processing and much more. This includes specialized packages for many biomedical applications.\n\nSee also: What is R? on the R Project website.\nThis book was compiled using R version 4.2.1 (2022-06-23).\nMake sure you have the latest version by visiting the R project website\nIt’s a good idea to keep a log of the version of R and installed packages when beginning a new project. An easy way to do this is to save the output of sessionInfo():\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] fansi_1.0.3       digest_0.6.29     jsonlite_1.8.0    magrittr_2.0.3   \n [5] evaluate_0.15     rlang_1.0.4       stringi_1.7.6     cli_3.3.0        \n [9] rmarkdown_2.14    tools_4.2.1       stringr_1.4.0     htmlwidgets_1.5.4\n[13] xfun_0.31         yaml_2.3.5        fastmap_1.1.0     compiler_4.2.1   \n[17] htmltools_0.5.2   knitr_1.39       \n\n\n\n\n\n\nChambers, John M. 1998. Programming with Data: A Guide to the s Language. Springer Science & Business Media."
  },
  {
    "objectID": "04_Packages.html#r",
    "href": "04_Packages.html#r",
    "title": "3  R, IDEs, Packages, Docs",
    "section": "3.1 R",
    "text": "3.1 R\nThis book was compiled using R version 4.2.1 (2022-06-23).\nMake sure you have the latest version by visiting the R project website\nIt’s a good idea to keep a log of the version of R and installed packages when beginning a new project. An easy way to do this is to save the output of sessionInfo():\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.5.4 compiler_4.2.1    magrittr_2.0.3    fastmap_1.1.0    \n [5] cli_3.3.0         tools_4.2.1       htmltools_0.5.2   stringi_1.7.6    \n [9] rmarkdown_2.14    knitr_1.39        stringr_1.4.0     xfun_0.31        \n[13] digest_0.6.29     jsonlite_1.8.0    rlang_1.0.3       evaluate_0.15"
  },
  {
    "objectID": "04_Packages.html#ides",
    "href": "04_Packages.html#ides",
    "title": "3  R, IDEs, Packages, Docs",
    "section": "3.2 IDEs",
    "text": "3.2 IDEs\nAn Integrated Development Environment (IDE) is a software application that offers extensive functionality for programmers, including ability to read, write, and execute code, develop and test software packages, etc.\nIDEs that support R usually also allow viewing plots or launching web applications within the same environment. An IDE can make working in R easier, more productive, and, importantly, more fun.\n\n3.2.1 RStudio\nRStudio is a very popular Integrated Development Environment (IDE) for R. This is the recommended environment for beginners. Make sure to keep your installation up-to-date; new features are added often.\nIt is recommended to set up a new RStudio project for each data project:\nRStudio projects allows you to organize your work. Each project keeps track of your workspace, open source files, working directory, and history.\nTo create a new RStudio Project click on File > New Project… from the main menu or the “Create a project” icon (second from top-left usually) in the RStudio toolbar.\n\n\n3.2.2 VS Code\nVisual Studio Code, a.k.a. VS Code is a source code editor and one of the most popular IDEs across different languages. The VS Code marketplace includes a very large number of extensions.\nThe vscode-R extension allows using VS Code as an R IDE. To use it, you need to install the languageserver and rlang packages:\ninstall.packages(c(\"languageserver\", \"rlang\"))\nThe httpgd graphics device is recommended. Install it using:\nremotes::install_github(\"nx10/httpgd\")\nand enable it in the extension settings (“Plot: Use httpgd”)\nThe Remote - SSH extension allows using a local VS Code installation (e.g. on your laptop) and executing code (R, Python, etc.) on a remote server on which you have SSH access."
  },
  {
    "objectID": "04_Packages.html#r-packages",
    "href": "04_Packages.html#r-packages",
    "title": "3  R, IDEs, Packages, Docs",
    "section": "3.3 R packages",
    "text": "3.3 R packages\n\n3.3.1 CRAN\nThe Comprehensive R Archive Network (CRAN) is the official R package repository and currently hosts 16271 packages (as of 2020-09-13). To install a package from CRAN, use the builtin install.packages command:\n\ninstall.packages('glmnet')\n\n\n3.3.1.1 Check for outdated packages\n\nold.packages()\n\n\n\n3.3.1.2 Update installed packages\nIf you don’t set ask = FALSE, you will have to accept each package update separately.\n\nupdate.packages(ask = FALSE)\n\n\n\n\n3.3.2 GitHub\nGitHub contains a large number of R packages, some of which also exist in CRAN, but the GitHub version may be updated a lot more frequently. To install from GitHub, you need to have the remotes package from CRAN first:\n\ninstall.packages(\"remotes\")\n\n\nremotes::install_github(\"user/repo\")\n\nNote: Running remotes::install_github(\"user/repo\") will not reinstall a previously installed package, unless it has been updated.\n\n\n3.3.3 Bioconductor\nBioconductor is a repository which includes tools for the analysis and comprehension of high-throughput genomic data, among others. To install package from Bioconductor, first install the BiocManager package from CRAN:\n\ninstall.packages(\"BiocManager\")\n\nand then use that similar to the builtin install.packages:\n\nBiocManager::install(\"packageName\")\n\n\n\n3.3.4 Installed packages\nList all R packages installed on your system with installed.packages() (the following block has not been run to prevent a very long output)\n\ninstalled.packages()\n\nList attached packages with search():\n\nsearch()\n\n [1] \".GlobalEnv\"        \"tools:quarto\"      \"package:stats\"    \n [4] \"package:graphics\"  \"package:grDevices\" \"package:utils\"    \n [7] \"package:datasets\"  \"package:methods\"   \"Autoloads\"        \n[10] \"package:base\"     \n\n\nList attached packages with their system path:\n\nsearchpaths()\n\n [1] \".GlobalEnv\"                                                                    \n [2] \"tools:quarto\"                                                                  \n [3] \"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/stats\"    \n [4] \"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/graphics\" \n [5] \"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/grDevices\"\n [6] \"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/utils\"    \n [7] \"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/datasets\" \n [8] \"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/methods\"  \n [9] \"Autoloads\"                                                                     \n[10] \"/Library/Frameworks/R.framework/Resources/library/base\"                        \n\n\n\n\n3.3.5 Dependencies\nMost R packages, whether in CRAN, Bioconductor, or GitHub, themselves rely on other packages to run. These are called dependencies. Many of these dependencies get installed automatically when you call install.packages() or remotes::install_github(), etc. This depends largely on whether they are essential for the new package to work. Some packages, especially if they provide a large number of functions that may not all be used by all users, may make some dependencies optional. In that cases, if you try to execute a specific function that depends on uninstalled packages you may get a warning or error or some type of message indicating that you need to install further packages."
  },
  {
    "objectID": "04_Packages.html#builtin-documentation",
    "href": "04_Packages.html#builtin-documentation",
    "title": "4  R packages",
    "section": "4.6 Builtin Documentation",
    "text": "4.6 Builtin Documentation\nAfter you’ve successfully installed R and RStudio, one of the first things to know is how to access and search the builtin documentation.\n\n4.6.1 Get help on a specific item\nIf you know the name of what you’re looking for (an R function most commonly, but possibly also the name of a dataset, or a package itself), just type ? followed by the name of said function, dataset, etc. in the R prompt:\n\n?sample\n\nIn RStudio, the above example will bring up the documentation for the sample function in the dedicated “Help” window, commonly situated at the bottom right (but can be moved by the user freely). If you are running R directly at the system shell, the same information is printed directly at the console.\nTry running the above example on your system.\n\n\n4.6.2 Search the docs\nIf you do not know the name of what you are looking for, you can use double question marks, ??, followed by your query (this is short for the help.search command that provides a number of arguments you can look up using ?help.search):\n\n??bootstrap"
  },
  {
    "objectID": "05_BasicOps.html",
    "href": "05_BasicOps.html",
    "title": "4  Basic operations",
    "section": "",
    "text": "First, before even learning about data types and structures, it may be worth looking at some of the basic mathematical and statistical operations in R."
  },
  {
    "objectID": "05_BasicOps.html#arithmetic",
    "href": "05_BasicOps.html#arithmetic",
    "title": "4  Basic operations",
    "section": "4.1 Arithmetic",
    "text": "4.1 Arithmetic\n\nx <- 10\ny <- 3\n\nStandard arithmetic operations are as expected:\n\nx + y\n\n[1] 13\n\nx - y\n\n[1] 7\n\nx * y\n\n[1] 30\n\nx / y\n\n[1] 3.333333\n\n\nExponentiation uses ^: (The caret (^) is likely the most common but not the only symbol used for exponentiation across programming languages)\n\nx^3\n\n[1] 1000\n\n\nSquare root is sqrt():\n\nsqrt(81)\n\n[1] 9\n\n\nInteger division i.e. Divide and forget the remainder\n\nx %/% y\n\n[1] 3\n\n\ni.e. how many times the denominator fits in the numerator, without taking fractions of the denominator. It can be applied on decimals the same way:\n\n9.5 %/% 3.1\n\n[1] 3\n\n\nModulo operation i.e. Divide and return just the remainder\n\nx %% y\n\n[1] 1\n\n\n\nx <- (-10:10)[-11]\ny <- sample((-10:10)[-11], 20)\nx - (x %/% y) * y == x %% y\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[16] TRUE TRUE TRUE TRUE TRUE"
  },
  {
    "objectID": "05_BasicOps.html#logical-operations",
    "href": "05_BasicOps.html#logical-operations",
    "title": "4  Basic operations",
    "section": "4.2 Logical operations",
    "text": "4.2 Logical operations\nLogical AND: &\n\nT & T\n\n[1] TRUE\n\n\n\nT & F\n\n[1] FALSE\n\n\nLogical OR: |\n\nT | F\n\n[1] TRUE\n\n\nLogical negation: !\n\nx <- TRUE\n!x\n\n[1] FALSE\n\n\nExclusive OR: xor()\nXOR evaluates to TRUE when two logicals are different,\ni.e. one or the other is TRUE but not both.\n\na <- c(T, T, T, F, F, F)\nb <- c(F, F, T, F, T, T)\na & b\n\n[1] FALSE FALSE  TRUE FALSE FALSE FALSE\n\na | b\n\n[1]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n\nxor(a, b)\n\n[1]  TRUE  TRUE FALSE FALSE  TRUE  TRUE\n\n\nTest all elements of an object are TRUE with all():\n\nall(a)\n\n[1] FALSE\n\n\nTest if any element is TRUE with any():\n\nany(a)\n\n[1] TRUE"
  },
  {
    "objectID": "05_BasicOps.html#common-descriptive-stats",
    "href": "05_BasicOps.html#common-descriptive-stats",
    "title": "4  Basic operations",
    "section": "4.3 Common descriptive stats",
    "text": "4.3 Common descriptive stats\nFirst, let’s use the rnorm() function to draw 200 numbers at random from a normal distribution:\n\nx <- rnorm(200)\n\nBasic descriptive stat operations -\nmean, median, standard deviation, minimum, maximum, and range:\n\nmean(x)\n\n[1] -0.06926408\n\nmedian(x)\n\n[1] -0.1235862\n\nsd(x)\n\n[1] 0.9764968\n\nmin(x)\n\n[1] -2.354692\n\nmax(x)\n\n[1] 2.30238\n\nrange(x)\n\n[1] -2.354692  2.302380\n\n\n\nsummary(x)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-2.35469 -0.76434 -0.12359 -0.06926  0.62658  2.30238"
  },
  {
    "objectID": "07_Datatypes.html#base-types",
    "href": "07_Datatypes.html#base-types",
    "title": "6  Data Types",
    "section": "6.1 Base types",
    "text": "6.1 Base types\n\n\n\nAll data in R is stored in vectors, whether stand-alone, i.e. a 1-D collection of items of the same type (e.g. numeric, character, etc.), or within another data structure (e.g. a data.frame, list).\n\n\nData types in R are essentially different types of vectors.\n\n\n\nR includes a number of builtin data types.\nThese are defined by the R core team: users cannot define their own data types.\nUsers can define their own classes - see Classes and Object-Oriented Programming.\nSome of the most popular data types in R are:\n\nLogical (i.e. TRUE or FALSE, a.k.a. Boolean)\nNumeric, integer\nNumeric, double\nCharacter\nEnvironment\nClosure (essentially, function; technically, a function and an environment)\n\n\n\n\nMany errors in R occur because a variable is, or gets coerced to, the wrong type by accident.\n\n\n\n\n\n\nCheck variable types with typeof() and/or str()."
  },
  {
    "objectID": "07_Datatypes.html#assignment",
    "href": "07_Datatypes.html#assignment",
    "title": "6  Data Types",
    "section": "6.2 Assignment",
    "text": "6.2 Assignment\nUse <- for all assignments:\n\nx <- 3\n# You can add comments within code blocks using the usual \"#\" prefix\n\n\n\n\nIn RStudio the keyboard shortcut for the assignment operator <- is Option - (MacOS) or Alt - (Windows).\n\n\n\nTyping the name of an object…\n\nx\n\n[1] 3\n\n\n…is equivalent to printing it\n\nprint(x)\n\n[1] 3\n\n\nYou can also place any assignment in parentheses and this will perform the assignment and print the object:\n\n(x <- 3)\n\n[1] 3\n\n\n\n\n\nWhile you could use the equal sign ‘=’ for assignment, you should only use it to pass arguments to functions.\n\n\n\nYou can assign the same value to multiple objects - this can be useful when initializing variables.\n\nx <- z <- init <- 0\nx\n\n[1] 0\n\nz\n\n[1] 0\n\ninit\n\n[1] 0\n\n\nExcitingly, R allows assignment in the opposite direction as well:\n\n10 -> x\nx\n\n[1] 10\n\n\nWe shall see later that the -> assignment can be convenient at the end of a pipe.\nYou can even do this, which is fun (?) but unlikely to be useful:\n\nx <- 7 -> z\nx\n\n[1] 7\n\nz\n\n[1] 7"
  },
  {
    "objectID": "07_Datatypes.html#combine-values",
    "href": "07_Datatypes.html#combine-values",
    "title": "6  Data Types",
    "section": "6.3 Combine values",
    "text": "6.3 Combine values\nUse c() to combine multiple values into a vector:\n\nx <- c(-12, 3.5, 104)\nx\n\n[1] -12.0   3.5 104.0"
  },
  {
    "objectID": "07_Datatypes.html#initialize---coerce---test-types",
    "href": "07_Datatypes.html#initialize---coerce---test-types",
    "title": "6  Data Types",
    "section": "6.4 Initialize - coerce - test (types)",
    "text": "6.4 Initialize - coerce - test (types)\nThe following summary table lists the functions to initialize, coerce (=convert), and test the core data types, which are shown in more detail in the following paragraphs:\n\n\n\nInitialize\nCoerce\nTest\n\n\n\n\nlogical(n)\nas.logical(x)\nis.logical(x)\n\n\ninteger(n)\nas.integer(x)\nis.integer(x)\n\n\ndouble(n)\nas.double(x)\nis.double(x)\n\n\nnumeric(n)\nas.numeric(x)\nis.numeric(x)\n\n\ncharacter(n)\nas.character(x)\nis.character(x)\n\n\n\nNote: numeric and double functions on lines 3 and 4 above are equivalent. (Try printing numeric and double in the console)"
  },
  {
    "objectID": "07_Datatypes.html#logical",
    "href": "07_Datatypes.html#logical",
    "title": "6  Data Types",
    "section": "6.5 Logical",
    "text": "6.5 Logical\nIf you are writing code, (it’s good practice to) use TRUE and FALSE.\nOn the console, you can abbreviate to T and F, they are the same.\n\na <- c(TRUE, FALSE)\na <- c(T, F)\nx <- 4\nb <- x > 10\nb\n\n[1] FALSE\n\nstr(b)\n\n logi FALSE\n\ntypeof(b)\n\n[1] \"logical\""
  },
  {
    "objectID": "07_Datatypes.html#integer",
    "href": "07_Datatypes.html#integer",
    "title": "6  Data Types",
    "section": "6.6 Integer",
    "text": "6.6 Integer\nCreate a range of integers using colon notation start:end:\n\n(x <- 11:15)\n\n[1] 11 12 13 14 15\n\ntypeof(x)\n\n[1] \"integer\"\n\nstr(x)\n\n int [1:5] 11 12 13 14 15\n\n\nNote that assigning an integer defaults to type double:\n\nx <- 1\ntypeof(x)\n\n[1] \"double\"\n\nstr(x)\n\n num 1\n\n\nYou can force it to be stored as integer by adding an L suffix:\n\nx <- 1L\ntypeof(x)\n\n[1] \"integer\"\n\nstr(x)\n\n int 1\n\n\n\nx <- c(1L, 3L, 5L)\nstr(x)\n\n int [1:3] 1 3 5"
  },
  {
    "objectID": "07_Datatypes.html#double",
    "href": "07_Datatypes.html#double",
    "title": "6  Data Types",
    "section": "6.7 Double",
    "text": "6.7 Double\n\nx <- c(1.2, 3.4, 10.987632419834556)\nx\n\n[1]  1.20000  3.40000 10.98763\n\ntypeof(x)\n\n[1] \"double\"\n\nstr(x)\n\n num [1:3] 1.2 3.4 11"
  },
  {
    "objectID": "07_Datatypes.html#character",
    "href": "07_Datatypes.html#character",
    "title": "6  Data Types",
    "section": "6.8 Character",
    "text": "6.8 Character\nA character vector consists of one or more elements, each of which consists of one or more actual characters, i.e. it is not a vector of single characters. (The length of a character vector is the number of individual elements, and is not related to the number of characters in each element)\n\nx <- \"word\"\ntypeof(x)\n\n[1] \"character\"\n\nlength(x)\n\n[1] 1\n\n\n\n(x <- c(\"a\", \"b\", \"gamma\", \"delta\"))\n\n[1] \"a\"     \"b\"     \"gamma\" \"delta\"\n\ntypeof(x)\n\n[1] \"character\"\n\nlength(x)\n\n[1] 4"
  },
  {
    "objectID": "07_Datatypes.html#environment",
    "href": "07_Datatypes.html#environment",
    "title": "6  Data Types",
    "section": "6.9 Environment",
    "text": "6.9 Environment\nDefining your own environments is probably for advanced use only:\n\nx <- new.env()\nx$name <- \"Guava\"\nx$founded <- 2020\nx\n\n<environment: 0x11c992668>\n\ntypeof(x)\n\n[1] \"environment\""
  },
  {
    "objectID": "07_Datatypes.html#closure-function",
    "href": "07_Datatypes.html#closure-function",
    "title": "6  Data Types",
    "section": "6.10 Closure (function)",
    "text": "6.10 Closure (function)\nClosures are functions - they contain their own variable definitions.\nRead more on functions.\n\nsquare <- function(x) x^2\nsquare(3)\n\n[1] 9\n\ntypeof(square)\n\n[1] \"closure\""
  },
  {
    "objectID": "07_Datatypes.html#initialize-vectors",
    "href": "07_Datatypes.html#initialize-vectors",
    "title": "6  Data Types",
    "section": "6.11 Initialize vectors",
    "text": "6.11 Initialize vectors\nYou can create / initialize vectors of specific type with the vector command and specifying a mode or directly by calling the relevant function:\n\n(xl <- vector(mode = \"logical\", length = 10))\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n(xd <- vector(mode = \"double\", length = 10))\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\n(xn <- vector(mode = \"numeric\", length = 10)) # same as \"double\"\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\n(xi <- vector(mode = \"integer\", length = 10))\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\n(xc <- vector(mode = \"character\", length = 10))\n\n [1] \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\n\n\nThese are aliases of the vector command above (print their source code to see for yourself)\n\nxl <- logical(10)\nxd <- double(10)\nxn <- numeric(10) # same as double\nxi <- integer(10)\nxc <- character(10)"
  },
  {
    "objectID": "07_Datatypes.html#explicit-coercion",
    "href": "07_Datatypes.html#explicit-coercion",
    "title": "6  Data Types",
    "section": "6.12 Explicit coercion",
    "text": "6.12 Explicit coercion\nWe can explicitly convert objects of one type to a different type using as.* functions:\n\n(x <- c(1.2, 2.3, 3.4))\n\n[1] 1.2 2.3 3.4\n\n(x <- as.logical(x))\n\n[1] TRUE TRUE TRUE\n\n(x <- as.double(x))\n\n[1] 1 1 1\n\n(x <- as.numeric(x))\n\n[1] 1 1 1\n\n(x <- as.integer(x))\n\n[1] 1 1 1\n\n(x <- as.character(x))\n\n[1] \"1\" \"1\" \"1\"\n\n\nLogical vectors are converted to 1s and 0s as expected:\nTRUE becomes 1 and FALSE becomes 0\n\nx <- c(TRUE, TRUE, FALSE)\nas.numeric(x)\n\n[1] 1 1 0\n\n\nNote that converting from numeric to logical anything other than zero is TRUE:\n\nx <- seq(-2, 2, .5)\nas.logical(x)\n\n[1]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n\n\nNot all conversions are possible.\nThere is no meaningful/consistent way to convert a character vector to numeric.\nThe following outputs NA values and prints a (helpful) error message.\n\nx <- c(\"mango\", \"banana\", \"tangerine\")\nas.numeric(x)\n\nWarning: NAs introduced by coercion\n\n\n[1] NA NA NA"
  },
  {
    "objectID": "07_Datatypes.html#implicit-coercion",
    "href": "07_Datatypes.html#implicit-coercion",
    "title": "6  Data Types",
    "section": "6.13 Implicit coercion",
    "text": "6.13 Implicit coercion\nRemember, the language generally tries to make life easier. Sometimes this means it will automatically coerce one class to another to allow requested operations.\nFor example, you can get the sum of a logical vector.\nIt will automatically be converted to numeric as we saw earlier.\n\nx <- c(TRUE, TRUE, FALSE)\nsum(x)\n\n[1] 2\n\n\nOn the other hand, you cannot sum a factor, for example.\nYou get an error with an explanation:\n\nx <- factor(c(\"mango\", \"banana\", \"mango\"))\nsum(x)\n\nError in Summary.factor(structure(c(2L, 1L, 2L), levels = c(\"banana\", : 'sum' not meaningful for factors\n\n\n\n\n\nNote: We had to add error = TRUE in the Rmarkdown’s code block’s options (not visible in the HTML output), because otherwise compilation of the Rmarkdown document would stop at the error.\n\n\n\nIf for some reason it made sense, you could explicitly coerce to numeric and then sum:\n\nx <- factor(c(\"mango\", \"banana\", \"mango\"))\nsum(as.numeric(x))\n\n[1] 5"
  },
  {
    "objectID": "07_Datatypes.html#na-missing-values",
    "href": "07_Datatypes.html#na-missing-values",
    "title": "6  Data Types",
    "section": "6.14 NA: Missing Values",
    "text": "6.14 NA: Missing Values\nMissing values in any data type - logical, integer, double, or character - are coded using NA.\nTo check for the presence of NA values, use is.na():\n\n(x <- c(1.2, 5.3, 4.8, NA, 9.6))\n\n[1] 1.2 5.3 4.8  NA 9.6\n\nis.na(x)\n\n[1] FALSE FALSE FALSE  TRUE FALSE\n\n\n\n(x <- c(\"mango\", \"banana\", NA, \"sugar\", \"ackee\"))\n\n[1] \"mango\"  \"banana\" NA       \"sugar\"  \"ackee\" \n\nis.na(x)\n\n[1] FALSE FALSE  TRUE FALSE FALSE\n\n\n\n(x <- c(T, T, F, T, F, F, NA))\n\n[1]  TRUE  TRUE FALSE  TRUE FALSE FALSE    NA\n\nis.na(x)\n\n[1] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n\n\nis.na() works similarly on matrices:\n\nx <- matrix(1:20, 5)\nx[4, 3] <- NA\nis.na(x)\n\n      [,1]  [,2]  [,3]  [,4]\n[1,] FALSE FALSE FALSE FALSE\n[2,] FALSE FALSE FALSE FALSE\n[3,] FALSE FALSE FALSE FALSE\n[4,] FALSE FALSE  TRUE FALSE\n[5,] FALSE FALSE FALSE FALSE\n\n\n\n\n\nNote that is.na() returns a response for each element (i.e. is vectorized) in contrast to is.numeric(), is.logical(), etc. It makes sense, since the latter are chacking the type of a whole object, while the former is checking individual elements.\n\n\n\nanyNA() is a very useful function to check if there an any NA values in an object:\n\nanyNA(x)\n\n[1] TRUE\n\n\n\n\n\nAny operations on an NA results in NA\n\n\n\n\nx <- c(1.2, 5.3, 4.8, NA, 9.6)\nx*2\n\n[1]  2.4 10.6  9.6   NA 19.2\n\n\nMultiple functions that accept as input an object with multiple values (a vector, a matrix, a data.frame, etc.) will return NA if any element is NA:\n\nmean(x)\n\n[1] NA\n\nmedian(x)\n\n[1] NA\n\nsd(x)\n\n[1] NA\n\nmin(x)\n\n[1] NA\n\nmax(x)\n\n[1] NA\n\nrange(x)\n\n[1] NA NA\n\n\nFirst, make sure NA values represent legitimate missing data and not some error.\nThen, decide how you want to handle it.\nIn all of the above commands you can pass na.rm = TRUE to ignore NA values:\n\nmean(x, na.rm = TRUE)\n\n[1] 5.225\n\nmedian(x, na.rm = TRUE)\n\n[1] 5.05\n\nsd(x, na.rm = TRUE)\n\n[1] 3.441293\n\nmin(x, na.rm = TRUE)\n\n[1] 1.2\n\nmax(x, na.rm = TRUE)\n\n[1] 9.6\n\nrange(x, na.rm = TRUE)\n\n[1] 1.2 9.6\n\n\nMore generally, you can use na.exclude() to exclude NA values from R objects. This can be very useful for function that do not include a na.rm or similar argument to handle NA values.\n\nx <- c(1, 2, NA, 4)\nna.exclude(x)\n\n[1] 1 2 4\nattr(,\"na.action\")\n[1] 3\nattr(,\"class\")\n[1] \"exclude\"\n\n\nOn a data.frame, na.exclude() excludes rows with any NAs:\n\ndf <- data.frame(a = c(1, 2, NA, 4),\n                 b = c(11, NA, 13, 14))\nna.exclude(df)\n\n  a  b\n1 1 11\n4 4 14\n\n\nThe chapter on Handling Missing Data describes some approaches to handling missing data in the context of statistics or modeling, commonly supervised learning."
  },
  {
    "objectID": "07_Datatypes.html#nan-not-a-number",
    "href": "07_Datatypes.html#nan-not-a-number",
    "title": "6  Data Types",
    "section": "6.15 NaN: Not a number",
    "text": "6.15 NaN: Not a number\nNaN is a special case of NA and can be the result of undefined mathematical operations:\n\na <- log(-4)\n\nWarning in log(-4): NaNs produced\n\n\nNote that class() returns “numeric”:\n\nclass(a)\n\n[1] \"numeric\"\n\n\nTo test for NaNs, use:\n\nis.nan(a)\n\n[1] TRUE\n\n\nNaNs are also NA:\n\nis.na(a)\n\n[1] TRUE\n\n\nBut the opposite is not true:\n\nis.nan(NA)\n\n[1] FALSE\n\n\n\n\n\nNaN can be considered a subtype of NA, as such: is.na(NaN) is TRUE, but is.nan(NA) is FALSE."
  },
  {
    "objectID": "07_Datatypes.html#null-the-empty-object",
    "href": "07_Datatypes.html#null-the-empty-object",
    "title": "6  Data Types",
    "section": "6.16 NULL: the empty object",
    "text": "6.16 NULL: the empty object\nThe NULL object represents an empty object.\n\n\n\nNULL means empty, not missing, and is therefore entirely different from NA\n\n\n\nNULL shows up for example when initializing a list:\n\na <- vector(\"list\", 4)\na\n\n[[1]]\nNULL\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\n[[4]]\nNULL\n\n\nand it can be replaced normally:\n\na[[1]] <- 3\na\n\n[[1]]\n[1] 3\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\n[[4]]\nNULL\n\n\n\n6.16.1 Replacing with NULL\nYou cannot replace one or more elements of a vector/matrix/array with NULL because NULL has length 0 and replacement requires object of equal length:\n\na <- 11:15\na\n\n[1] 11 12 13 14 15\n\na[1] <- NULL\n\nError in a[1] <- NULL: replacement has length zero\n\n\nHowever, in lists and therefore also data frames, replacing an element with NULL removes that element:\n\nal <- list(alpha = 11:15,\n           beta = rnorm(10),\n           gamma = c(\"mango\", \"banana\", \"tangerine\"))\nal\n\n$alpha\n[1] 11 12 13 14 15\n\n$beta\n [1] -0.8016487 -0.7860426 -0.5259926  0.2113334  0.5138032 -0.5592976\n [7]  0.8519664 -0.5732299  0.4055569  0.6355769\n\n$gamma\n[1] \"mango\"     \"banana\"    \"tangerine\"\n\nal[[2]] <- NULL\nal\n\n$alpha\n[1] 11 12 13 14 15\n\n$gamma\n[1] \"mango\"     \"banana\"    \"tangerine\"\n\n\nFinally, NULL is often used as the default value in a function’s argument. The function definition must then determine what the default behavior/value should be."
  },
  {
    "objectID": "08_DataStructures.html",
    "href": "08_DataStructures.html",
    "title": "6  Data Structures",
    "section": "",
    "text": "There are 5 main data structures in R:\nHomogeneous vs. hetereogeneous refers to the kind of data types (integer, double, character, logical, factor, etc.) that a structure can hold. This means a matrix can hold only numbers or only characters, but a data frame can hold different types in different columns. That is why data frames are very popular data structure for statistical work."
  },
  {
    "objectID": "08_DataStructures.html#initialize---coerce---test-structures",
    "href": "08_DataStructures.html#initialize---coerce---test-structures",
    "title": "6  Data Structures",
    "section": "6.1 Initialize - coerce - test (structures)",
    "text": "6.1 Initialize - coerce - test (structures)\nThe following summary table lists the functions to initialize, coerce (=convert), and test the core data structures, which are shown in more detail in the following paragraphs:\n\n\n\nInitialize\nCoerce\nTest\n\n\n\n\nvector(n)\nas.vector(x)\nis.vector(x)\n\n\nmatrix(n)\nas.matrix(x)\nis.matrix(x)\n\n\narray(n)\nas.array(x)\nis.array(x)\n\n\nlist(n)\nas.list(x)\nis.list(x)\n\n\ndata.frame(n)\nas.data.frame(x)\nis.data.frame(x)"
  },
  {
    "objectID": "08_DataStructures.html#vectors",
    "href": "08_DataStructures.html#vectors",
    "title": "6  Data Structures",
    "section": "6.2 Vectors",
    "text": "6.2 Vectors\nA vector is the basic structure that contains data in R. Other structures that contain data are made up of one or more vectors.\n\n(x <- c(1, 3, 5, 7))\n\n[1] 1 3 5 7\n\nclass(x)\n\n[1] \"numeric\"\n\ntypeof(x)\n\n[1] \"double\"\n\n\nA vector has length() but no dim():\n\nlength(x)\n\n[1] 4\n\ndim(x)\n\nNULL\n\n\n\n(x2 <- 1:10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n(x3 <- rnorm(10))\n\n [1] -2.1572940  1.4080765 -0.4920152  0.1903149  0.3528404 -1.3746255\n [7]  1.1590791  0.5890638  0.2475779 -1.2081195\n\n(x4 <- seq(0, 1, .1))\n\n [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n\nseq(10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n(x5 <- sample(seq(100), 20))\n\n [1] 24 10 26 60 49 57 35 78 11 82 87 81 47 67 13 12 97 39 48 61\n\n\n\n6.2.1 Generating sequences with seq()\n\nfrom, to, by\n\n\nseq(1, 10, .5)\n\n [1]  1.0  1.5  2.0  2.5  3.0  3.5  4.0  4.5  5.0  5.5  6.0  6.5  7.0  7.5  8.0\n[16]  8.5  9.0  9.5 10.0\n\n\n\n1:n\n\n\n(seq(12))\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12\n\n# or \n(seq_len(12))\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12\n\n# is same as\n1:12\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12\n\n\n\nAlong the length of another object\n\n\nseq_along(iris)\n\n[1] 1 2 3 4 5\n\n1:ncol(iris)\n\n[1] 1 2 3 4 5\n\n\n\nfrom, to with length n\n\n\nseq(-5, 12, length.out = 11)\n\n [1] -5.0 -3.3 -1.6  0.1  1.8  3.5  5.2  6.9  8.6 10.3 12.0\n\n\n\n\n6.2.2 Initializing a vector\n\nx <- vector(length = 10)\nx <- vector(\"numeric\", 10)\nx <- vector(\"list\", 10)"
  },
  {
    "objectID": "08_DataStructures.html#matrices",
    "href": "08_DataStructures.html#matrices",
    "title": "6  Data Structures",
    "section": "6.3 Matrices",
    "text": "6.3 Matrices\nA matrix is a vector with 2 dimensions.\nTo create a matrix, you pass a vector to the matrix() command and specify number of rows using nrow and/or number of columns using ncol:\n\nx <- matrix(sample(seq(1000), 30),\n            nrow = 10, ncol = 3)\nx\n\n      [,1] [,2] [,3]\n [1,]  857  830  199\n [2,]  361  278  121\n [3,]   45   43  856\n [4,]   62  846  735\n [5,]  696  966  927\n [6,]  400  312  309\n [7,]  404  548  537\n [8,]  855  854  677\n [9,]  903  150   14\n[10,]  392  999  740\n\nclass(x)\n\n[1] \"matrix\" \"array\" \n\n\n\n\n\nA matrix has length (length(x)) equal to the number of all (i, j) elements or nrow * ncol (if i is the row index and j is the column index) and dimensions (dim(x)) as expected:\n\n\n\n\nlength(x)\n\n[1] 30\n\ndim(x)\n\n[1] 10  3\n\nnrow(x)\n\n[1] 10\n\nncol(x)\n\n[1] 3\n\n\n\n6.3.1 Construct by row or by column\nBy default, vectors are constructed by column (byrow = FALSE)\n\nx <- matrix(1:20, nrow = 10, ncol = 2, byrow = FALSE)\nx\n\n      [,1] [,2]\n [1,]    1   11\n [2,]    2   12\n [3,]    3   13\n [4,]    4   14\n [5,]    5   15\n [6,]    6   16\n [7,]    7   17\n [8,]    8   18\n [9,]    9   19\n[10,]   10   20\n\n\n\nx <- matrix(1:20, nrow = 10, ncol = 2, byrow = TRUE)\nx\n\n      [,1] [,2]\n [1,]    1    2\n [2,]    3    4\n [3,]    5    6\n [4,]    7    8\n [5,]    9   10\n [6,]   11   12\n [7,]   13   14\n [8,]   15   16\n [9,]   17   18\n[10,]   19   20\n\n\n\n\n6.3.2 Initialize a matrix\n\n(x <- matrix(NA, nrow = 6, ncol = 4))\n\n     [,1] [,2] [,3] [,4]\n[1,]   NA   NA   NA   NA\n[2,]   NA   NA   NA   NA\n[3,]   NA   NA   NA   NA\n[4,]   NA   NA   NA   NA\n[5,]   NA   NA   NA   NA\n[6,]   NA   NA   NA   NA\n\n(x <- matrix(0, nrow = 6, ncol = 4))\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    0    0    0\n[2,]    0    0    0    0\n[3,]    0    0    0    0\n[4,]    0    0    0    0\n[5,]    0    0    0    0\n[6,]    0    0    0    0\n\n\n\n\n6.3.3 Bind vectors by column or by row\nUse cbind (“column-bind”) to convert a set of input vectors to columns of a matrix. The vectors must be of the same length:\n\nx <- cbind(1:10, 11:20, 41:50)\nx\n\n      [,1] [,2] [,3]\n [1,]    1   11   41\n [2,]    2   12   42\n [3,]    3   13   43\n [4,]    4   14   44\n [5,]    5   15   45\n [6,]    6   16   46\n [7,]    7   17   47\n [8,]    8   18   48\n [9,]    9   19   49\n[10,]   10   20   50\n\nclass(x)\n\n[1] \"matrix\" \"array\" \n\n\nSimilarly, you can use rbind (“row-bind”) to convert a set of input vectors to rows of a matrix. The vectors again must be of the same length:\n\nx <- rbind(1:10, 11:20, 41:50)\nx\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    1    2    3    4    5    6    7    8    9    10\n[2,]   11   12   13   14   15   16   17   18   19    20\n[3,]   41   42   43   44   45   46   47   48   49    50\n\nclass(x)\n\n[1] \"matrix\" \"array\" \n\n\n\n\n6.3.4 Combine matrices\ncbind() and rbind() can be used to combine two or more matrices together - or vector and matrices:\n\ncbind(matrix(1, 5, 2), matrix(2, 5, 4))\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    1    1    2    2    2    2\n[2,]    1    1    2    2    2    2\n[3,]    1    1    2    2    2    2\n[4,]    1    1    2    2    2    2\n[5,]    1    1    2    2    2    2"
  },
  {
    "objectID": "08_DataStructures.html#arrays",
    "href": "08_DataStructures.html#arrays",
    "title": "6  Data Structures",
    "section": "6.4 Arrays",
    "text": "6.4 Arrays\nArrays are vectors with dimensions.\nYou can have 1D, 2D or any-D, i.e. ND arrays.\n\n6.4.1 1D array\nA 1D array is just like a vector but of class array and with dim(x) equal to length(x) (remember, vectors have only length(x) and undefined dim(x)):\n\nx <- 1:10\nxa <- array(1:10, dim = 10)\nclass(x)\n\n[1] \"integer\"\n\nis.vector(x)\n\n[1] TRUE\n\nlength(x)\n\n[1] 10\n\ndim(x)\n\nNULL\n\nclass(xa)\n\n[1] \"array\"\n\nis.vector(xa)\n\n[1] FALSE\n\nlength(xa)\n\n[1] 10\n\ndim(xa)\n\n[1] 10\n\n\nIt is quite unlikely you will need to use a 1D array instead of a vector.\n\n\n6.4.2 2D array\nA 2D array is a matrix:\n\nx <- array(1:40, dim = c(10, 4))\nclass(x)\n\n[1] \"matrix\" \"array\" \n\ndim(x)\n\n[1] 10  4\n\n\n\n\n6.4.3 ND array\nYou can build an N-dimensional array:\n\n(x <- array(1:60, dim = c(5, 4, 3)))\n\n, , 1\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    6   11   16\n[2,]    2    7   12   17\n[3,]    3    8   13   18\n[4,]    4    9   14   19\n[5,]    5   10   15   20\n\n, , 2\n\n     [,1] [,2] [,3] [,4]\n[1,]   21   26   31   36\n[2,]   22   27   32   37\n[3,]   23   28   33   38\n[4,]   24   29   34   39\n[5,]   25   30   35   40\n\n, , 3\n\n     [,1] [,2] [,3] [,4]\n[1,]   41   46   51   56\n[2,]   42   47   52   57\n[3,]   43   48   53   58\n[4,]   44   49   54   59\n[5,]   45   50   55   60\n\nclass(x)\n\n[1] \"array\"\n\n\nYou can provide names for each dimensions using the dimnames argument. It accepts a list where each elements is a character vector of legth equal to the dimension length. Using the same example as above, we pass three character vector of length 5, 4, and 3 to match the length of the dimensions:\n\nx <- array(1:60,\n            dim = c(5, 4, 3),\n            dimnames = list(letters[1:5],\n                            c(\"alpha\", \"beta\", \"gamma\", \"delta\"),\n                            c(\"x\", \"y\", \"z\")))\n\n3D arrays can be used to represent color images. Here, just for fun, we use rasterImage to show how you would visualize such an image:\n\nx <- array(sample(1:255, 432, TRUE), dim = c(12, 12, 3))\npar(\"pty\")\n\n[1] \"m\"\n\npar(pty = \"s\")\nplot(NULL, NULL,\n     xlim = c(0, 100), ylim = c(0, 100),\n     axes = F, ann = F, pty = \"s\")\nrasterImage(x/255, 0, 0, 100, 100)"
  },
  {
    "objectID": "08_DataStructures.html#lists",
    "href": "08_DataStructures.html#lists",
    "title": "6  Data Structures",
    "section": "6.5 Lists",
    "text": "6.5 Lists\nTo define a list, we use list() to pass any number of objects.\nIf these objects are passed as named arguments, the names will rename as element names:\n\nx <- list(one = 1:4,\n          two = sample(seq(0, 100, .1), 10),\n          three = c(\"mango\", \"banana\", \"tangerine\"),\n          four = median)\nclass(x)\n\n[1] \"list\"\n\nstr(x)\n\nList of 4\n $ one  : int [1:4] 1 2 3 4\n $ two  : num [1:10] 46 73.2 94.3 42.4 78.6 50.5 73.1 12 86.2 13.5\n $ three: chr [1:3] \"mango\" \"banana\" \"tangerine\"\n $ four :function (x, na.rm = FALSE, ...)  \n\n\n\n6.5.1 Nested lists\nSince each element can be any object at all, it is simple to build a nested list:\n\nx <- list(alpha = letters[sample(26, 4)],\n          beta = sample(12),\n          gamma = list(i = rnorm(10),\n                       j = runif(10),\n                       j = seq(0, 1000, length.out = 10)))\nx\n\n$alpha\n[1] \"p\" \"m\" \"o\" \"g\"\n\n$beta\n [1]  4 11  6  1 10  8  7  9 12  5  2  3\n\n$gamma\n$gamma$i\n [1] -0.92331776  0.71985256 -0.07201912  1.00296921 -0.29721180  0.87690497\n [7]  0.57970859 -1.03098624  1.46187517  1.17093238\n\n$gamma$j\n [1] 0.48195793 0.02273958 0.29249130 0.88741885 0.08562779 0.07454521\n [7] 0.42302021 0.99943692 0.02163897 0.61607215\n\n$gamma$j\n [1]    0.0000  111.1111  222.2222  333.3333  444.4444  555.5556  666.6667\n [8]  777.7778  888.8889 1000.0000\n\n\n\n\n6.5.2 Initialize a list\n\nx <- vector(\"list\", 4)\nx\n\n[[1]]\nNULL\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\n[[4]]\nNULL\n\n\n\n\n6.5.3 Combine lists\nYou can combine lists with c() (just like vectors):\n\nl1 <- list(q = 11:14, r = letters[11:14])\nl2 <- list(s = LETTERS[21:24], t = 100:97)\n(x <- c(l1, l2))\n\n$q\n[1] 11 12 13 14\n\n$r\n[1] \"k\" \"l\" \"m\" \"n\"\n\n$s\n[1] \"U\" \"V\" \"W\" \"X\"\n\n$t\n[1] 100  99  98  97\n\nlength(x)\n\n[1] 4\n\n\n\n\n6.5.4 Mixing types with c()\nIt’s best to use c() to either combine elements of the same type into a vector, or to combine lists. Otherwise you must inspect the outcome to be certain it was as intended.\nAs we’ve seen, if all arguments passed to c() are of a single type, you get a vector of that type:\n\n(x <- c(12.9, 94.67, 23.74, 46.901))\n\n[1] 12.900 94.670 23.740 46.901\n\nclass(x)\n\n[1] \"numeric\"\n\n\nIf arguments passed to c() are a mix of numeric and character, they all get coerced to character.\n\n(x <- c(23.54, \"mango\", \"banana\", 75))\n\n[1] \"23.54\"  \"mango\"  \"banana\" \"75\"    \n\nclass(x)\n\n[1] \"character\"\n\n\nIf you pass more types of objects (which cannot be coerced to character) you get a list, since it is the only structure that can support all of them together:\n\n(x <- c(42, mean, \"potatoes\"))\n\n[[1]]\n[1] 42\n\n[[2]]\nfunction (x, ...) \nUseMethod(\"mean\")\n<bytecode: 0x148ca9480>\n<environment: namespace:base>\n\n[[3]]\n[1] \"potatoes\"\n\nclass(x)\n\n[1] \"list\"\n\n\n\n\n\nOther than concatenating vectors of the same type or lists into a larger list, it probably best to avoid using c() and directly constructing the object you want using, e.g. list()."
  },
  {
    "objectID": "08_DataStructures.html#dataframestruc",
    "href": "08_DataStructures.html#dataframestruc",
    "title": "6  Data Structures",
    "section": "6.6 Data frames",
    "text": "6.6 Data frames\n\n\n\nA data frames is a special type of list where each element has the same length and forms a column, resulting in a 2D structure. Unlike matrices, each column can contain a different data type.\n\n\n\n\nx <- data.frame(Feat_1 = 1:5,\n                Feat_2 = rnorm(5),\n                Feat_3 = paste0(\"rnd_\", sample(seq(100), 5)))\nx\n\n  Feat_1    Feat_2 Feat_3\n1      1 0.5909129 rnd_11\n2      2 1.1436751 rnd_65\n3      3 0.4820415 rnd_92\n4      4 1.4752170 rnd_49\n5      5 0.7837454 rnd_47\n\nclass(x)\n\n[1] \"data.frame\"\n\nstr(x)\n\n'data.frame':   5 obs. of  3 variables:\n $ Feat_1: int  1 2 3 4 5\n $ Feat_2: num  0.591 1.144 0.482 1.475 0.784\n $ Feat_3: chr  \"rnd_11\" \"rnd_65\" \"rnd_92\" \"rnd_49\" ...\n\nclass(x$Feat_1)\n\n[1] \"integer\"\n\n\n\nmat <- matrix(1:100, 10)\nlength(mat)\n\n[1] 100\n\ndf <- as.data.frame(mat)\nlength(df)\n\n[1] 10"
  },
  {
    "objectID": "08_DataStructures.html#attributes",
    "href": "08_DataStructures.html#attributes",
    "title": "6  Data Structures",
    "section": "6.7 Attributes",
    "text": "6.7 Attributes\nR objects may have some builtin attributes but you can add arbitrary attributes to any R object. These are used to store additional information, sometimes called metadata.\n\n6.7.1 Print all attributes\nTo print an object’s attributes, use attributes:\n\nattributes(iris)\n\n$names\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"     \n\n$class\n[1] \"data.frame\"\n\n$row.names\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n[109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n[127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n[145] 145 146 147 148 149 150\n\n\nThis returns a named list. In this case we got names, class, and row.names of the iris data frame.\n\n\n6.7.2 Get or set specific attributes\nYou can assign new attributes using attr:\n\n(x <- c(1:10))\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nattr(x, \"name\") <- \"Very special vector\"\n\nPrinting the vector after adding a new attribute, prints the attribute name and value underneath the vector itself:\n\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\nattr(,\"name\")\n[1] \"Very special vector\"\n\n\nOur trusty str function will print attributes as well\n\nstr(x)\n\n int [1:10] 1 2 3 4 5 6 7 8 9 10\n - attr(*, \"name\")= chr \"Very special vector\"\n\n\n\n6.7.2.1 A matrix is a vector - a closer look\nLet’s see how a matrix is literally just a vector with assigned dimensions.\nStart with a vector of length 20:\n\nx <- 1:20\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\n\n\nThe vector has no attributes - yet:\n\nattributes(x)\n\nNULL\n\n\nTo convert to a matrix, we would normally pass our vector to the matrix() function and define number of rows and/or columns:\n\nxm <- matrix(x, 5)\nxm\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    6   11   16\n[2,]    2    7   12   17\n[3,]    3    8   13   18\n[4,]    4    9   14   19\n[5,]    5   10   15   20\n\nattributes(xm)\n\n$dim\n[1] 5 4\n\n\nJust for demonstration, let’s instead directly add a dimension attribute to our vector:\n\nattr(x, \"dim\") <- c(5, 4)\nx\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    6   11   16\n[2,]    2    7   12   17\n[3,]    3    8   13   18\n[4,]    4    9   14   19\n[5,]    5   10   15   20\n\nclass(x)\n\n[1] \"matrix\" \"array\" \n\n\nJust like that, we have a matrix.\n\n\n\n6.7.3 Common builtin attributes\nVectors can have named elements. A new vector has no names, but you can add them:\n\nx <- rnorm(10)\nnames(x)\n\nNULL\n\nnames(x) <- paste0(\"Value\", seq(x))\nx\n\n    Value1     Value2     Value3     Value4     Value5     Value6     Value7 \n 0.2207668 -1.5308554  0.9757982  0.9845394 -0.8417019 -0.1565539 -1.0194172 \n    Value8     Value9    Value10 \n 0.7681391 -1.6175819  0.9917961 \n\n\nMatrices and data frames can have column names (colnames) and row names (rownames):\n\nx <- matrix(1:15, 5)\ncolnames(x)\n\nNULL\n\nrownames(x)\n\nNULL\n\ncolnames(x) <- paste0(\"Feature\", seq(3))\nrownames(x) <- paste0(\"Case\", seq(5))\nx\n\n      Feature1 Feature2 Feature3\nCase1        1        6       11\nCase2        2        7       12\nCase3        3        8       13\nCase4        4        9       14\nCase5        5       10       15\n\n\nLists are vectors so they have names. These can be defined when a list is created using the name-value pairs or added/changed at any time.\n\nx <- list(HospitalName = \"CaliforniaGeneral\",\n          ParticipatingDepartments = c(\"Neurology\", \"Psychiatry\", \"Neurosurgery\"),\n          PatientIDs = 1001:1253)\nnames(x)\n\n[1] \"HospitalName\"             \"ParticipatingDepartments\"\n[3] \"PatientIDs\"              \n\n\nAdd/Change names:\n\nnames(x) <- c(\"Hospital\", \"Departments\", \"PIDs\")\nx\n\n$Hospital\n[1] \"CaliforniaGeneral\"\n\n$Departments\n[1] \"Neurology\"    \"Psychiatry\"   \"Neurosurgery\"\n\n$PIDs\n  [1] 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015\n [16] 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030\n [31] 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045\n [46] 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060\n [61] 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075\n [76] 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090\n [91] 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105\n[106] 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120\n[121] 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135\n[136] 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150\n[151] 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165\n[166] 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180\n[181] 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195\n[196] 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210\n[211] 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225\n[226] 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240\n[241] 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253\n\n\nRemember that data a frame is a special type of list. Therefore in data frames colnames and names are equivalent:\n\ncolnames(iris)\n\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"     \n\nnames(iris)\n\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"     \n\n\nNote: As we saw, matrices have colnames and rownames. Using names on a matrix will assign names to individual elements, as if it was a long vector - this is not usually useful."
  },
  {
    "objectID": "09_Factors.html",
    "href": "09_Factors.html",
    "title": "8  Factors",
    "section": "",
    "text": "Factors in R are used to store categorical variables and therefore have many important uses in statistics / data science / machine learning.\nFor example, factors can be used to store information on sex, race, diagnosis, treatment group, etc.\nIf you are new to R and to factors, read the “Factors (introductory)” section."
  },
  {
    "objectID": "09_Factors.html#factors-introductory",
    "href": "09_Factors.html#factors-introductory",
    "title": "8  Factors",
    "section": "8.1 Factors (introductory)",
    "text": "8.1 Factors (introductory)\n\n\n\nFactors in R are a special type of vector.\n\n\n\n\nEach element can take a value from a set of values known as the factor’s levels\n\n\n\n\nA factor’s levels are stored in a particular order, which affects how that factor is treated by some functions.\n\n\n\n\nYou can specify whether the order of the levels defines a quantitative relationship such that level1 < level2, etc.\n\n\n\n\n\nYou can create a factor by passing a numeric or character vector to factor() or to as.factor().\nThe difference is that as.factor() does not accept any arguments while factor() does.\nLet’s start with a character vector that includes three unique values - “a”, “b”, and “c”:\n\nx <- c(\"a\", \"c\", \"b\", \"b\", \"a\", \"a\", \"b\", \"c\")\nx\n\n[1] \"a\" \"c\" \"b\" \"b\" \"a\" \"a\" \"b\" \"c\"\n\n\nAssume that “a”, “b”, and “c” define three different groups and we want to convert this character vector to a factor.\nas.factor() and factor() without any arguments produce the same output:\n\nxf <- factor(x)\nxf\n\n[1] a c b b a a b c\nLevels: a b c\n\nclass(xf)\n\n[1] \"factor\"\n\nxftoo <- as.factor(x)\nxftoo\n\n[1] a c b b a a b c\nLevels: a b c\n\nclass(xftoo)\n\n[1] \"factor\"\n\n\nNotice that when a factor is printed in the R console:\n\nThe elements are printed without double quotes around them, differentiating them from character vectors.\nThe factor levels are printed below the vector values.\n\ntable() is a very useful function for factors - it gives the counts for each level:\n\ntable(xf)\n\nxf\na b c \n3 3 2 \n\n\nLet’s look at a different example. We define a factor to identify cases and controls:\n\ng <- factor(c(\"case\", \"control\", \"control\", \"case\", \"control\"))\ng\n\n[1] case    control control case    control\nLevels: case control\n\n\nBy default, the levels are ordered alphabeticaly.\n\n8.1.1 Set the order of factor levels\nYou can define the order of the factor levels with the level argument of the factor() function.\n(For example, the first factor level is used as the baseline in some statistical operations, e.g. glm(), in which case the “control” level should be first.)\n\ng <- factor(c(\"case\", \"control\", \"control\", \"case\", \"control\"),\n            levels = c(\"control\", \"case\"))\ng\n\n[1] case    control control case    control\nLevels: control case\n\n\nThe levels argument can include values not present in the input data vector. This may be used for example when some known categories are not present in your sample, but may be added in the future, or you want specifically show they are absent in a table or a plot, etc.\n\ng <- factor(c(\"Type I\", \"Type III\", \"Type III\", \"Type I\"),\n            levels = c(\"Type I\", \"Type II\", \"Type III\"))\ng\n\n[1] Type I   Type III Type III Type I  \nLevels: Type I Type II Type III\n\n\nOn the other hand, if the levels argument is specified and does not include one or more of the values present in the input data vector, the corresponding elements become NA:\n\ng <- factor(c(\"case\", \"control\", \"undefined\", \"control\", \"case\", \"control\", \"undefined\"),\n            levels = c(\"control\", \"case\"))\ng\n\n[1] case    control <NA>    control case    control <NA>   \nLevels: control case\n\n\n\n\n8.1.2 Define level labels\nYou can define level names or labels other than the values in the input vector using the labels argument:\nAssume you started with the following character vector\n\nx <- c(\"female\", \"female\", \"male\", \"female\", \"male\")\n\nYou can attach different labels to each level rather than default to “female” and “male” by passing a character vector to the labels arguments:\n\nxf <- factor(x, labels = c(\"F\", \"M\"))\nxf\n\n[1] F F M F M\nLevels: F M\n\n\nThe order of names in the labels argument must match the order of levels. In the above example, the levels default to c(\"female\", \"male\") because they are sorted alphabeticaly if not specified. Otherwise, we can specify both the levels and labels arguments to define both order of levels and provide new labels:\n\nxf <- factor(x, levels = c(\"male\", \"female\"), labels = c(\"M\", \"F\"))\nxf\n\n[1] F F M F M\nLevels: M F\n\n\n\n\n8.1.3 Change level labels of existing factor\nWe can change the level labels of a factor object\n\nusing the factor() command in the same way we create a factor from a character or other vector\nusing the levels() command\n\nStart with a factor of two groups and change labels using the factor() command:\n\nxf <- factor(c(\"GroupA\", \"GroupB\", \"GroupB\", \"GroupA\"))\nxf <- factor(xf, labels = c(\"A\", \"B\"))\nxf\n\n[1] A B B A\nLevels: A B\n\n\nStart with the same factor and change labels using the levels() command.\nThis is similar to using colnames() on a data.frame and is much faster than using factor() as above:\n\nxf <- factor(c(\"GroupA\", \"GroupB\", \"GroupB\", \"GroupA\"))\nlevels(xf) <- c(\"A\", \"B\")\nxf\n\n[1] A B B A\nLevels: A B\n\n\n\n\n\nThe levels() command changes the names of the levels. It cannot be used to change the order of levels, which must be done with factor()"
  },
  {
    "objectID": "09_Factors.html#factors-advanced",
    "href": "09_Factors.html#factors-advanced",
    "title": "8  Factors",
    "section": "8.2 Factors (advanced)",
    "text": "8.2 Factors (advanced)\n\n\n\nA factor is a vector A factor contains three crucial pieces of information:\n\n\n\n\nThe underlying integer vector\n\n\n\n\nThe mapping of integers to labels\n\n\n\n\nWhether the factor is ordered\n\n\n\n\n\nLet’s unpack these.\nBegin with a simple factor:\n\nx <- factor(c(\"female\", \"female\", \"female\", \"male\", \"male\"))\nx\n\n[1] female female female male   male  \nLevels: female male\n\n\nInternally, the command sees there are two distinct labels, female and male, and defaults to assigning integer numbers alphabetically, in this case female has been mapped to ‘1’ and male to ‘2’.\nPrinting a factor prints the vector of labels followed by the levels, i.e. the unique labels.\n\n8.2.1 The underlying integer vector\nEach level is assigned an integer. (Internally, this is the “data” that forms the elements of a factor vector). You don’t see these integers unless you convert the factor to numeric (as.numeric()) or look at the (truncated) output of str()\n\nas.numeric(x)\n\n[1] 1 1 1 2 2\n\n\n\n\n8.2.2 The mapping of integers to labels\nThis defines which integer is mapped to which label, i.e. whether 1 is mapped to male or female. You can store the same information regardless which one you choose to call 1 and 2.\nTo get the mapping you can use levels(). It prints the labels in order:\n\nlevels(x)\n\n[1] \"female\" \"male\"  \n\n\nAgain, this means that female is mapped to 1 and male is mapped to 2.\n\nstr(x)\n\n Factor w/ 2 levels \"female\",\"male\": 1 1 1 2 2\n\n\nThe above tells you that x is a factor,\nit has two levels labeled as “female” and “male”, in that order, i.e. female is level 1 and male is level 2.\nThe last part shows that the first five elements (in this case the whole vector) consists of three elements of level 1 (female) followed by 2 elements of level 2 (male)\n\n8.2.2.1 Setting new level labels\nYou can use the levels() command with an assignment to assign new labels to a factor (same syntax to how you use rownames() or colnames() to assign new row or column names to a matrix or data frame)\n\nxf <- factor(sample(c(\"patient_status_positive\", \"patient_status_negative\"), 10, T),\n             levels = c(\"patient_status_positive\", \"patient_status_negative\"))\nxf\n\n [1] patient_status_negative patient_status_positive patient_status_positive\n [4] patient_status_positive patient_status_negative patient_status_positive\n [7] patient_status_positive patient_status_negative patient_status_negative\n[10] patient_status_negative\nLevels: patient_status_positive patient_status_negative\n\n\n\nlevels(xf)\n\n[1] \"patient_status_positive\" \"patient_status_negative\"\n\nlevels(xf) <- c(\"positive\", \"negative\")\nxf\n\n [1] negative positive positive positive negative positive positive negative\n [9] negative negative\nLevels: positive negative\n\n\n\n\n8.2.2.2 Defining the mapping of labels to integers\nIf you want to define the mapping of labels to their integer representation (and not default to them sorted alphabeticaly), you use the levels arguments of the factor() function.\nThe vector passed to the levels arguments must include at least all unique values passed to factor(), otherwise you will get NA values\nWithout defining levels they are assigned alphabeticaly:\n\nx <- factor(c(\"alpha\", \"alpha\", \"gamma\", \"delta\", \"delta\"))\nx\n\n[1] alpha alpha gamma delta delta\nLevels: alpha delta gamma\n\n\nDefine levels:\n\nx <- factor(c(\"alpha\", \"alpha\", \"gamma\", \"delta\", \"delta\"),\n levels = c(\"alpha\", \"gamma\", \"delta\"))\nx\n\n[1] alpha alpha gamma delta delta\nLevels: alpha gamma delta\n\n\nThe table command has a number of useful applications, in it simplest form, it tabulates number of elements with each unique value found in a vector:\n\ntable(x)\n\nx\nalpha gamma delta \n    2     1     2 \n\n\nIf you forget (or choose to exclude) a level, all occurences are replaced by NA:\n\nx <- factor(c(\"alpha\", \"alpha\", \"gamma\", \"delta\", \"delta\"),\n levels = c(\"alpha\", \"gamma\"))\nx\n\n[1] alpha alpha gamma <NA>  <NA> \nLevels: alpha gamma\n\n\nIf you know that more levels exist, even if no examples are present in your sample, you can includes these extra levels:\n\nx <- factor(c(\"alpha\", \"alpha\", \"gamma\", \"delta\", \"delta\"),\n levels = c(\"alpha\", \"beta\", \"gamma\", \"delta\"))\nx\n\n[1] alpha alpha gamma delta delta\nLevels: alpha beta gamma delta\n\n\n\ntable(x)\n\nx\nalpha  beta gamma delta \n    2     0     1     2 \n\n\n\n\n\n8.2.3 Is the factor ordered\nWe looked at how you can define the order of levels using the levels argument in factor(), which affects the integer mapping to each label.\nThis can affect how some applications treat the different levels.\nOn top of the order of the mapping, you can further define if there is a quantitative relationship among levels of the form level 1 < level 2 < ... < level n. This, in turn, can affect how the factor is treated by some functions, like some functions that fit statistical models.\n\n\n\nAll factors’ levels appear in some order or other.\n\n\nAn ordered factor indicates that its levels have a quantitative relationship of the form level 1 < level 2 < … < level n.\n\n\n\nFirst an unordered factor:\n\ndat <- sample(c(\"small\", \"medium\", \"large\"), 10, TRUE)\nx <- factor(dat)\nx\n\n [1] small  medium large  small  large  medium small  medium small  small \nLevels: large medium small\n\n\nTo make the above into an ordered factor, we need to define the order of the levels with the levels arguments and also specify that it is ordered with the ordered argument:\n\nx <- factor(dat,\n            levels = c(\"small\", \"medium\", \"large\"),\n            ordered = TRUE)\nx\n\n [1] small  medium large  small  large  medium small  medium small  small \nLevels: small < medium < large\n\n\nNote how the levels now include the < sign between levels to indicate the ordering.\n\n\n8.2.4 Change order of levels or labels\nWe’ve seen how to create a factor with defined order of levels and how to change level labels already. Because these are prone to serious accidents, let’s look at them again, together.\nTo change the order of levels of an existing factor use factor():\n\nx <- factor(c(\"target\", \"target\", \"control\", \"control\", \"control\"))\nx\n\n[1] target  target  control control control\nLevels: control target\n\n\nChange the order so that target is first (i.e. corresponds to 1:\n\nx <- factor(x, levels = c(\"target\", \"control\"))\nx\n\n[1] target  target  control control control\nLevels: target control\n\n\nTo change the labels of the levels use levels():\n\nx\n\n[1] target  target  control control control\nLevels: target control\n\nlevels(x) <- c(\"hit\", \"decoy\")\nx\n\n[1] hit   hit   decoy decoy decoy\nLevels: hit decoy\n\n\n\n\n\nChanging the levels of a factor with levels() does not change the internal integer representation but changes every element’s label.\n\n\n\n\n\n8.2.5 Fatal error to avoid\nExample scenario: You receive a dataset for classification where the outcome is a factor of 1s and 0s:\n\noutcome <- factor(c(1, 1, 0, 0, 0, 1, 0))\noutcome\n\n[1] 1 1 0 0 0 1 0\nLevels: 0 1\n\n\nSome classification procedures expect the first level to be the ‘positive’ outcome, so you decide to reorder the levels.\nYou mistakenly use levels() instead of factor(x, levels=c(...)) hoping to achieve this.\nYou end up flipping all the outcome values.\n\nlevels(outcome) <- c(\"1\", \"0\")\noutcome\n\n[1] 0 0 1 1 1 0 1\nLevels: 1 0\n\n\nAll zeros became ones and ones became zeros.\nYour model does the exact opposite of what you intended.\n\n\n8.2.6 Factor to numeric\nWhile it often makes sense to have factors with words for labels, they can be any character and that includes numbers (i.e. numbers which are treated as labels)\n\nf <- factor(c(3, 7, 7, 9, 3, 3, 9))\nf\n\n[1] 3 7 7 9 3 3 9\nLevels: 3 7 9\n\n\nThis behaves just like any other factor with all the rules we learned above.\nThere is a very easy trap to fall into, if you ever decide to convert such a factor to numeric.\nThe first thing that usually comes to mind is to use as.numeric().\n\n# !don't do this!\nas.numeric(f)\n\n[1] 1 2 2 3 1 1 3\n\n\nBut! We already know this will return the integer index, it will not return the labels as numbers.\nBy understanding the internal representation of the factor, i.e. that a factor is an integer vector indexing a set of labels, you can convert labels to numeric exactly by indexing the set of labels:\n\nlevels(f)[f]\n\n[1] \"3\" \"7\" \"7\" \"9\" \"3\" \"3\" \"9\"\n\n\nThe above suggests that used as an index within the brackets, f is coerced to integer, therefore to understand the above:\n\nlevels(f)\n\n[1] \"3\" \"7\" \"9\"\n\nlevels(f)[as.integer(f)]\n\n[1] \"3\" \"7\" \"7\" \"9\" \"3\" \"3\" \"9\"\n\n# same as\nlevels(f)[f]\n\n[1] \"3\" \"7\" \"7\" \"9\" \"3\" \"3\" \"9\"\n\n\nA different way around this that may be less confusing is to simply convert the factor to character and then to numeric:\n\nas.numeric(as.character(f))\n\n[1] 3 7 7 9 3 3 9"
  },
  {
    "objectID": "09_Factors.html#summary",
    "href": "09_Factors.html#summary",
    "title": "8  Factors",
    "section": "8.3 Summary",
    "text": "8.3 Summary\n\n\n\n\n\nFactors in R are integer vectors with labels.\n\n\n\n\nA factor’s internal integer values range from 1 to the number of levels, i.e. categories.\n\n\n\n\nEach integer corresponds to a label.\n\n\n\n\nTo set order of levels: factor(x, levels = levels_in_desired_order) to order levels\n\n\n\n\nTo change labels: levels(x) <- newlabels or factor(x, labels = newlabels)\n\n\n\n\n\n\n\n\nTo avoid confusion, do not use numbers as level labels, if possible."
  },
  {
    "objectID": "10_Indexing.html",
    "href": "10_Indexing.html",
    "title": "9  Indexing",
    "section": "",
    "text": "An index is used to select elements of a vector, matrix, array, list or data frame.\nYou can select (or exclude) one or multiple elements at a time.\nThere are two general ways to identify which elements you want to select:\nAn index can be one of two types:\nLogical indexes are usually created as the output of a logical operation, e.g. an elemntwise comparison.\nThe main indexing operator in R is the square bracket [.\nInteger indexing in R is 1-based, meaning the first item of a vector is in position 1.\n(If you are wondering why we have to mention this, know that many programming languages use 0-based indexing where the first element is in the 0th position, the second in the 1st, and the nth in the n-1 position)\nTo understand indexing make sure you are clear on the basic R data structures (vectors, matrices, lists, data.frames)\nIndexing can be used to extract values from an object or to replace values in an object."
  },
  {
    "objectID": "10_Indexing.html#indexvectors",
    "href": "10_Indexing.html#indexvectors",
    "title": "9  Indexing",
    "section": "9.1 Vectors",
    "text": "9.1 Vectors\nStart with a simple vector:\n\nx <- 15:24\nx\n\n [1] 15 16 17 18 19 20 21 22 23 24\n\n\n\n9.1.1 Integer Index\nGet the 5th element of a vector:\n\nx[5]\n\n[1] 19\n\n\nGet elements 6 through 9 of the same vector:\n\nx[6:9]\n\n[1] 20 21 22 23\n\n\n\n\n9.1.2 Logical Index\nSelect elements with value greater than 19 (logical index):\n\nx[x > 19]\n\n[1] 20 21 22 23 24\n\n\nNote that an integer index can be used to repeat elements:\n\nx[c(1, 1, 1, 4)]\n\n[1] 15 15 15 18\n\n\n\n\n9.1.3 Extract vs. Replace\n\nx <- c(24, 32, 41, 37, 999, 999, 999)\n\nIndexing allows you to access specific elements, for example to perform calculation on them.\nGet the mean of elements 2:5:\n\nmean(x[1:4])\n\n[1] 33.5\n\n\nYou can combine indexing with assignment to replace elements of an object.\nReplace elements 1:4 with their log:\n\nx[1:4] <- log(x[1:4])\nx\n\n[1]   3.178054   3.465736   3.713572   3.610918 999.000000 999.000000 999.000000\n\n\nReplace elements that are equal to 999 with NA:\n\nx[x == 999] <- NA\nx\n\n[1] 3.178054 3.465736 3.713572 3.610918       NA       NA       NA"
  },
  {
    "objectID": "10_Indexing.html#indexmatrices",
    "href": "10_Indexing.html#indexmatrices",
    "title": "9  Indexing",
    "section": "9.2 Matrices",
    "text": "9.2 Matrices\nReminder:\n\nA matrix is a 2D vector and contains elements of the same type (numeric, integer, character).\nA data frame is a 2D list and each column can contain a different data type.\n\nTo index a 2D structure, whether a matrix or data frame, we use the form [row, column]\nThe following indexing operations are therefore the same whether applied on a matrix or a data frame.\n\nmat <- matrix(21:60, 10)\ncolnames(mat) <- paste0(\"Feature_\", seq(ncol(mat)))\nrownames(mat) <- paste0(\"Row_\", seq(nrow(mat)))\nmat\n\n       Feature_1 Feature_2 Feature_3 Feature_4\nRow_1         21        31        41        51\nRow_2         22        32        42        52\nRow_3         23        33        43        53\nRow_4         24        34        44        54\nRow_5         25        35        45        55\nRow_6         26        36        46        56\nRow_7         27        37        47        57\nRow_8         28        38        48        58\nRow_9         29        39        49        59\nRow_10        30        40        50        60\n\ndf <- as.data.frame(mat)\ndf\n\n       Feature_1 Feature_2 Feature_3 Feature_4\nRow_1         21        31        41        51\nRow_2         22        32        42        52\nRow_3         23        33        43        53\nRow_4         24        34        44        54\nRow_5         25        35        45        55\nRow_6         26        36        46        56\nRow_7         27        37        47        57\nRow_8         28        38        48        58\nRow_9         29        39        49        59\nRow_10        30        40        50        60\n\n\nTo get the contents of the fifth row, second column:\n\nmat[5, 2]\n\n[1] 35\n\ndf[5, 2]\n\n[1] 35\n\n\nWe show the following on matrices, but they work just the same on data.frames.\nIf you want to select an entire row or an entire column, you leave the row or column index blank, but must use a comma:\nGet the first row:\n\nmat[1, ]\n\nFeature_1 Feature_2 Feature_3 Feature_4 \n       21        31        41        51 \n\n\nGet the second column:\n\nmat[, 2]\n\n Row_1  Row_2  Row_3  Row_4  Row_5  Row_6  Row_7  Row_8  Row_9 Row_10 \n    31     32     33     34     35     36     37     38     39     40 \n\n\nNote that colnames and rownames where added to the matrix above for convenience - if they are absent, there are no labels above each element.\nYou can define ranges for both rows and columns:\n\nmat[6:7, 2:4]\n\n      Feature_2 Feature_3 Feature_4\nRow_6        36        46        56\nRow_7        37        47        57\n\n\nYou can return rows and/or columns reversed if desired:\n\nmat[7:6, 4:2]\n\n      Feature_4 Feature_3 Feature_2\nRow_7        57        47        37\nRow_6        56        46        36\n\n\nYou can use vectors to specify any combination of rows and columns.\nGet rows 2, 4, and 7 of columns 1, 4, and 3:\n\nmat[c(2, 4, 7), c(1, 4, 3)]\n\n      Feature_1 Feature_4 Feature_3\nRow_2        22        52        42\nRow_4        24        54        44\nRow_7        27        57        47\n\n\nSince a matrix is a vector with 2 dimensions, you can also index the underlying vector directly. Regardless of whether a matrix was created by row or by column (default), the data is stored and acceesed by column. You can see that by converting the matrix to a 1D vector:\n\nas.vector(mat)\n\n [1] 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45\n[26] 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60\n\n\nsame as:\n\nc(mat)\n\n [1] 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45\n[26] 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60\n\n\nFor example, ‘mat’ has 10 rows and 4 columns, therefore the 11th element is in row 1 column 2\n\nmat[11]\n\n[1] 31\n\n\nis the same as:\n\nmat[1, 2]\n\n[1] 31\n\n\nThis only works with matrices, not data.frames.\n\n9.2.1 Matrix of indexes\nThis is quite less common, but potentially useful. It allows you to specify a series of individual [i, j] indexes, i.e. is a way to select multiple individual non-contiguous elements\n\nidm <- matrix(c(2, 4, 7, 4, 3, 1), 3)\nidm\n\n     [,1] [,2]\n[1,]    2    4\n[2,]    4    3\n[3,]    7    1\n\n\nAn n-by-2 matrix can be used to index as a length n vector of [row, colum] indexes. Therefore, the above matrix, will return elements [2, 4], [4, 3], [7, 1]:\n\nmat[idm]\n\n[1] 52 44 27\n\n\n\n\n9.2.2 Logical index\nSelect all rows with values greater than 15 on the second column:\nThe logical index for this operation is:\n\nmat[, 2] > 15\n\n Row_1  Row_2  Row_3  Row_4  Row_5  Row_6  Row_7  Row_8  Row_9 Row_10 \n  TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE \n\n\nIt can be used directly to index the matrix:\n\nmat[mat[, 2] > 15, ]\n\n       Feature_1 Feature_2 Feature_3 Feature_4\nRow_1         21        31        41        51\nRow_2         22        32        42        52\nRow_3         23        33        43        53\nRow_4         24        34        44        54\nRow_5         25        35        45        55\nRow_6         26        36        46        56\nRow_7         27        37        47        57\nRow_8         28        38        48        58\nRow_9         29        39        49        59\nRow_10        30        40        50        60\n\n\nIndexing a matrix or a data.frame can return either a smaller matrix/data.frame or a vector.\nIn general, objects in R are returned in their most simple form unless otherwise specified. This means that if you extract a column or a row, you get a vector:\nGet the third column:\n\nmat[, 3]\n\n Row_1  Row_2  Row_3  Row_4  Row_5  Row_6  Row_7  Row_8  Row_9 Row_10 \n    41     42     43     44     45     46     47     48     49     50 \n\nclass(mat[, 3])\n\n[1] \"integer\"\n\n\nYou can specify drop = FALSE to stop R from dropping the unused dimension and return a matrix or data.frame of a single column:\n\nmat[, 3, drop = FALSE]\n\n       Feature_3\nRow_1         41\nRow_2         42\nRow_3         43\nRow_4         44\nRow_5         45\nRow_6         46\nRow_7         47\nRow_8         48\nRow_9         49\nRow_10        50\n\ndf[, 3, drop = FALSE]\n\n       Feature_3\nRow_1         41\nRow_2         42\nRow_3         43\nRow_4         44\nRow_5         45\nRow_6         46\nRow_7         47\nRow_8         48\nRow_9         49\nRow_10        50\n\n\nCheck it is still a matrix or data.frame:\n\nclass(mat[, 3, drop = FALSE])\n\n[1] \"matrix\" \"array\" \n\nclass(df[, 3, drop = FALSE])\n\n[1] \"data.frame\""
  },
  {
    "objectID": "10_Indexing.html#indexlists",
    "href": "10_Indexing.html#indexlists",
    "title": "9  Indexing",
    "section": "9.3 Lists",
    "text": "9.3 Lists\nReminder: A list can contain elements of different class and of different length:\n\nx <- list(one = 1:4,\n           two = sample(seq(0, 100, .1), 10),\n           three = c(\"mango\", \"banana\", \"tangerine\"),\n           four = median)\nx\n\n$one\n[1] 1 2 3 4\n\n$two\n [1] 35.3 98.9  0.3 41.6 35.9 96.7 94.2  3.7 38.0 77.8\n\n$three\n[1] \"mango\"     \"banana\"    \"tangerine\"\n\n$four\nfunction (x, na.rm = FALSE, ...) \nUseMethod(\"median\")\n<bytecode: 0x13caf3f98>\n<environment: namespace:stats>\n\n\nYou can access a list element with:\n\n$ followed by name of the element (therefore only works if elements are named)\nusing double brackets [[ with either name or integer index\n\nTo access the third element:\n\nx$three\n\n[1] \"mango\"     \"banana\"    \"tangerine\"\n\n\nsame as:\n\nx[[3]]\n\n[1] \"mango\"     \"banana\"    \"tangerine\"\n\n\nsame as:\n\nx[[\"three\"]]\n\n[1] \"mango\"     \"banana\"    \"tangerine\"\n\n\nTo access an element with a name or integer index stored in a variable, only the bracket notation works. Therefore, programmatically, you would always use double brackets to access different elements:\n\nidi <- 3\nidc <- \"three\"\nx[[idi]]\n\n[1] \"mango\"     \"banana\"    \"tangerine\"\n\nx[[idc]]\n\n[1] \"mango\"     \"banana\"    \"tangerine\"\n\n\n$ or [[ return an element.\nIn contrast, single bracket [ indexing of a list returns a pruned list:\n\nx[[idi]]\n\n[1] \"mango\"     \"banana\"    \"tangerine\"\n\nclass(x[[idi]])\n\n[1] \"character\"\n\n\nvs.\n\nx[idi]\n\n$three\n[1] \"mango\"     \"banana\"    \"tangerine\"\n\nclass(x[idi])\n\n[1] \"list\"\n\n\nExtract multiple list elements with single brackets, as expected:\n\nx[2:3]\n\n$two\n [1] 35.3 98.9  0.3 41.6 35.9 96.7 94.2  3.7 38.0 77.8\n\n$three\n[1] \"mango\"     \"banana\"    \"tangerine\"\n\nclass(x[2:3])\n\n[1] \"list\"\n\n\nBeware (confusing) recursive indexing.\n(This is probably rarely used).\nUnlike in the single brackets example above, where you can use a colon to specify a range of elements to index, colon notation within double brackets accesses elements recursively at the given position.\nFor example, the following extracts the 3rd element of the 2nd element of the list:\n\nx[[2:3]]\n\n[1] 0.3\n\n\nYou can convert a list to one lone vector containing all the individual components of the original list using unlist(). Notice how names are automatically created based on the original structure:\n\nx <- list(alpha = sample(seq(100), 10),\n          beta = sample(seq(100), 10),\n          gamma = sample(seq(100), 10))\nx\n\n$alpha\n [1]  3 10 46 23 80 95 36 27 38 11\n\n$beta\n [1] 81 46 51 61 25 13  5 30 79  4\n\n$gamma\n [1]  7 37 99 93 75 76 23 65 94  5\n\nunlist(x)\n\n alpha1  alpha2  alpha3  alpha4  alpha5  alpha6  alpha7  alpha8  alpha9 alpha10 \n      3      10      46      23      80      95      36      27      38      11 \n  beta1   beta2   beta3   beta4   beta5   beta6   beta7   beta8   beta9  beta10 \n     81      46      51      61      25      13       5      30      79       4 \n gamma1  gamma2  gamma3  gamma4  gamma5  gamma6  gamma7  gamma8  gamma9 gamma10 \n      7      37      99      93      75      76      23      65      94       5 \n\n\nIf you want to drop the names, you can wrap the above in unname():\n\nunname(unlist(x))\n\n [1]  3 10 46 23 80 95 36 27 38 11 81 46 51 61 25 13  5 30 79  4  7 37 99 93 75\n[26] 76 23 65 94  5\n\n\n\n9.3.1 Logical index\nWe can use a logical index on a list with single bracket:\n\nx[c(T, F, T, F)]\n\n$alpha\n [1]  3 10 46 23 80 95 36 27 38 11\n\n$gamma\n [1]  7 37 99 93 75 76 23 65 94  5"
  },
  {
    "objectID": "10_Indexing.html#indexdfs",
    "href": "10_Indexing.html#indexdfs",
    "title": "9  Indexing",
    "section": "9.4 Data frames",
    "text": "9.4 Data frames\nWe’ve already seen above that a data frame can be indexed in many ways similar to a matrix, i.e. by defining rows and columns. At the same time, we know that a data frame is a rectangular list. Like a list, its elements are vectors of any type (integer, double, character, factor, and more) but, unlike a list, they have to be of the same length. A data frame can also be indexed the same way as a list and similar to list indexing, notice that some methods return a smaller data frame, while others return vectors.\n\n\n\nYou can index a data frame using all the ways you can index a list and all the ways you can index a matrix.\n\n\n\nLet’s create a simple data frame:\n\nx <- data.frame(Feat_1 = 21:25,\n                Feat_2 = rnorm(5),\n                Feat_3 = paste0(\"rnd_\", sample(seq(100), 5)))\nx\n\n  Feat_1     Feat_2 Feat_3\n1     21  0.5096966 rnd_24\n2     22  0.2328109 rnd_89\n3     23  0.8612651  rnd_4\n4     24 -0.2623318 rnd_52\n5     25 -1.3491615 rnd_92\n\n\n\n9.4.1 Extract column(s)\nJust like in a list, using the $ operator or double bracket [[ returns an element, i.e. a vector:\n\nx$Feat_2\n\n[1]  0.5096966  0.2328109  0.8612651 -0.2623318 -1.3491615\n\nclass(x$Feat_2)\n\n[1] \"numeric\"\n\n\n\nx[[2]]\n\n[1]  0.5096966  0.2328109  0.8612651 -0.2623318 -1.3491615\n\nclass(x[[2]])\n\n[1] \"numeric\"\n\n\nAccessing a column by name with square brackets, returns a single-column data.frame:\n\nx[\"Feat_2\"]\n\n      Feat_2\n1  0.5096966\n2  0.2328109\n3  0.8612651\n4 -0.2623318\n5 -1.3491615\n\nclass(x[\"Feat_2\"])\n\n[1] \"data.frame\"\n\n\nAccessing a column by [row, column] either by position or name, return a vector by default:\n\nx[, 2]\n\n[1]  0.5096966  0.2328109  0.8612651 -0.2623318 -1.3491615\n\nclass(x[, 2])\n\n[1] \"numeric\"\n\nx[, \"Feat_2\"]\n\n[1]  0.5096966  0.2328109  0.8612651 -0.2623318 -1.3491615\n\nclass(x[, \"Feat_2\"])\n\n[1] \"numeric\"\n\n\nAs we saw earlier, we can specify drop = FALSE to return a data.frame:\n\nclass(x[, 2, drop = FALSE])\n\n[1] \"data.frame\"\n\nclass(x[, \"Feat_2\", drop = FALSE])\n\n[1] \"data.frame\"\n\n\nAs in lists, all indexing and slicing operations, with the exception of the $ notation, work with a variable holding either a column name of or an integer location:\n\nidi <- 2\nidc <- \"Feat_2\"\nx[idi]\n\n      Feat_2\n1  0.5096966\n2  0.2328109\n3  0.8612651\n4 -0.2623318\n5 -1.3491615\n\nx[idc]\n\n      Feat_2\n1  0.5096966\n2  0.2328109\n3  0.8612651\n4 -0.2623318\n5 -1.3491615\n\nx[[idi]]\n\n[1]  0.5096966  0.2328109  0.8612651 -0.2623318 -1.3491615\n\nx[[idc]]\n\n[1]  0.5096966  0.2328109  0.8612651 -0.2623318 -1.3491615\n\nx[, idi]\n\n[1]  0.5096966  0.2328109  0.8612651 -0.2623318 -1.3491615\n\nx[, idc]\n\n[1]  0.5096966  0.2328109  0.8612651 -0.2623318 -1.3491615\n\nx[, idi, drop = F]\n\n      Feat_2\n1  0.5096966\n2  0.2328109\n3  0.8612651\n4 -0.2623318\n5 -1.3491615\n\nx[, idc, drop = F]\n\n      Feat_2\n1  0.5096966\n2  0.2328109\n3  0.8612651\n4 -0.2623318\n5 -1.3491615\n\n\nExtracting multiple columns returns a data frame:\n\nx[, 2:3]\n\n      Feat_2 Feat_3\n1  0.5096966 rnd_24\n2  0.2328109 rnd_89\n3  0.8612651  rnd_4\n4 -0.2623318 rnd_52\n5 -1.3491615 rnd_92\n\nclass(x[, 2:3])\n\n[1] \"data.frame\"\n\n\n\n\n9.4.2 Extract rows\nUnlike indexing a row of a matrix, indexing a row of a data.frame returns a single-row data.frame, since it contains multiple columns of potentially different types:\n\nx[1, ]\n\n  Feat_1    Feat_2 Feat_3\n1     21 0.5096966 rnd_24\n\nclass(x[1, ])\n\n[1] \"data.frame\"\n\n\nConvert into a list using c():\n\nc(x[1, ])\n\n$Feat_1\n[1] 21\n\n$Feat_2\n[1] 0.5096966\n\n$Feat_3\n[1] \"rnd_24\"\n\nclass(c(x[1, ]))\n\n[1] \"list\"\n\n\nConvert into a (named) vector using unlist():\n\nunlist(x[1, ])\n\n             Feat_1              Feat_2              Feat_3 \n               \"21\" \"0.509696614600682\"            \"rnd_24\" \n\nclass(unlist(x[1, ]))\n\n[1] \"character\"\n\n\n\n\n9.4.3 Logical index\n\nx[x$Feat_1 > 22, ]\n\n  Feat_1     Feat_2 Feat_3\n3     23  0.8612651  rnd_4\n4     24 -0.2623318 rnd_52\n5     25 -1.3491615 rnd_92"
  },
  {
    "objectID": "10_Indexing.html#logical---integer-indexing",
    "href": "10_Indexing.html#logical---integer-indexing",
    "title": "9  Indexing",
    "section": "9.5 Logical <-> Integer indexing",
    "text": "9.5 Logical <-> Integer indexing\nWe have seen that there are two types of indexes/indices: integer and logical.\n\n\n\n\n\nA logical index needs to be of the same dimensions as the object it is indexing (unless you really want to recycle values - see chapter on vectorization): you are specifying whether to include or exclude each element\n\n\n\n\nAn integer index will be shorter than the object it is indexing: you are specifying which subset of elements to include (or with a - in front, which elements to exclude)\n\n\n\n\n\nIt’s easy to convert between the two types.\nFor example, start with a sequence of integers:\n\nx <- 21:30\nx\n\n [1] 21 22 23 24 25 26 27 28 29 30\n\n\nLet’s create a logical index based on two inequalities:\n\nlogical_index <- x > 23 & x < 28\nlogical_index\n\n [1] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n\n\n\n9.5.1 Logical to integer index with which():\n\n\n\nThe common mistake is to attempt to convert a logical index to an integer index using as.integer(). This results in a vector of 1’s and 0’s, NOT an integer index. which() converts a logical index to an integer index.\n\n\n\nwhich() literally gives the position of all TRUE elements in a vector, thus converting a logical to an integer index:\n\ninteger_index <- which(logical_index)\ninteger_index\n\n[1] 4 5 6 7\n\n\ni.e. positions 4, 5, 6, 7 of the logical_index are TRUE\n\n\n\nA logical and an integer index are equivalent if they select the exact same elements\n\n\n\nLet’s check than when used to index x, they both return the same result:\n\nx[logical_index]\n\n[1] 24 25 26 27\n\nx[integer_index]\n\n[1] 24 25 26 27\n\nall(x[logical_index] == x[integer_index])\n\n[1] TRUE\n\n\n\n\n9.5.2 Integer to logical index\nOn the other hand, if we want to convert an integer index to a logical index, we can begin with a logical vector of the same length or dimension as the object we want to index with all FALSE values:\n\nlogical_index_too <- vector(length = length(x))\nlogical_index_too\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nAnd use the integer index to replace the corresponding elements to TRUE:\n\nlogical_index_too[integer_index] <- TRUE\nlogical_index_too\n\n [1] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n\n\nThis, of course, is the same as the logical index we started with.\n\nall(logical_index == logical_index_too)\n\n[1] TRUE"
  },
  {
    "objectID": "10_Indexing.html#exclude-cases-using-an-index",
    "href": "10_Indexing.html#exclude-cases-using-an-index",
    "title": "9  Indexing",
    "section": "9.6 Exclude cases using an index",
    "text": "9.6 Exclude cases using an index\nVery often, we want to use an index, whether logical or integer, to exclude cases instead of to select cases.\nTo do that with a logical integer, we simply use an exclamation point in front of the index to negate each element (convert each TRUE to FALSE and each FALSE to TRUE):\n\nlogical_index\n\n [1] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n\n!logical_index\n\n [1]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n\n\n\nx[!logical_index]\n\n[1] 21 22 23 28 29 30\n\n\nTo exclude elements using an integer index, R allows you to use negative indexing:\n\nx[-integer_index]\n\n[1] 21 22 23 28 29 30\n\n\n\n\n\nTo get the complement of an index, you negate a logical index (!logical_index) or you subtract an integer index (-integer_index):"
  },
  {
    "objectID": "12_Vectorization.html",
    "href": "12_Vectorization.html",
    "title": "9  Vectorized Operations",
    "section": "",
    "text": "Most built-in R functions are vectorized and many functions from external packages are as well.\nVectorization is very efficient: it can save both human (your) time and machine time.\nIn many cases, applying a function on all elements simultaneously may seem obvious or expected behavior, but since not all functions are vectorized, make sure to check the documentation if unsure."
  },
  {
    "objectID": "12_Vectorization.html#operations-between-vectors-of-equal-length",
    "href": "12_Vectorization.html#operations-between-vectors-of-equal-length",
    "title": "9  Vectorized Operations",
    "section": "9.1 Operations between vectors of equal length",
    "text": "9.1 Operations between vectors of equal length\nSuch operations are applied between corresponding elements of each vector:\n\nx <- 1:10\nz <- 11:20\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nz\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\nx + z\n\n [1] 12 14 16 18 20 22 24 26 28 30\n\n\ni.e. the above is equal to c(x[1] + z[1], x[2] + z[2], ..., x[n] + z[n])"
  },
  {
    "objectID": "12_Vectorization.html#operations-between-a-vector-and-a-scalar",
    "href": "12_Vectorization.html#operations-between-a-vector-and-a-scalar",
    "title": "9  Vectorized Operations",
    "section": "9.2 Operations between a vector and a scalar",
    "text": "9.2 Operations between a vector and a scalar\nIn this cases, the scalar is repeated to match the length of the vector, i.e. recycled:\n\n(x + 10)\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\n(x * 2)\n\n [1]  2  4  6  8 10 12 14 16 18 20\n\n(x / 10)\n\n [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n\n(x ^ 2)\n\n [1]   1   4   9  16  25  36  49  64  81 100"
  },
  {
    "objectID": "12_Vectorization.html#operations-between-vectors-of-unequal-length-value-recycling",
    "href": "12_Vectorization.html#operations-between-vectors-of-unequal-length-value-recycling",
    "title": "9  Vectorized Operations",
    "section": "9.3 Operations between vectors of unequal length: value recycling",
    "text": "9.3 Operations between vectors of unequal length: value recycling\nOperations between a vector and a scalar are a special case of operations between vectors of unequal length. Whenever you perform an operation between two objects of different length, the shorter object’s elements are recycled:\n\nx + c(2:1)\n\n [1]  3  3  5  5  7  7  9  9 11 11\n\n\n\n\n\nOperations between objects of unequal length can occur by mistake. If the shorter object’s length is a multiple of the longer object’s length, there will be no error or warning, as above. Otherwise, there is a warning (which may be confusing at first) BUT recycling still happens and is highly unlikely to be intentional:\n\n\n\n\nx + c(1, 3, 9)\n\nWarning in x + c(1, 3, 9): longer object length is not a multiple of shorter\nobject length\n\n\n [1]  2  5 12  5  8 15  8 11 18 11"
  },
  {
    "objectID": "12_Vectorization.html#vectorized-matrix-operations",
    "href": "12_Vectorization.html#vectorized-matrix-operations",
    "title": "9  Vectorized Operations",
    "section": "9.4 Vectorized matrix operations",
    "text": "9.4 Vectorized matrix operations\nOperations between matrices are similarly vectorized, i.e. performed between corresponding elements:\n\na <- matrix(1:4, 2)\nb <- matrix(11:14, 2)\na\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nb\n\n     [,1] [,2]\n[1,]   11   13\n[2,]   12   14\n\na + b\n\n     [,1] [,2]\n[1,]   12   16\n[2,]   14   18\n\na * b\n\n     [,1] [,2]\n[1,]   11   39\n[2,]   24   56\n\na / b\n\n           [,1]      [,2]\n[1,] 0.09090909 0.2307692\n[2,] 0.16666667 0.2857143"
  },
  {
    "objectID": "12_Vectorization.html#vectorized-functions",
    "href": "12_Vectorization.html#vectorized-functions",
    "title": "9  Vectorized Operations",
    "section": "9.5 Vectorized functions",
    "text": "9.5 Vectorized functions\nSome examples of common mathematical operations that are vectorized:\n\nlog(x)\n\n [1] 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379 1.7917595 1.9459101\n [8] 2.0794415 2.1972246 2.3025851\n\nsqrt(x)\n\n [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427\n [9] 3.000000 3.162278\n\nsin(x)\n\n [1]  0.8414710  0.9092974  0.1411200 -0.7568025 -0.9589243 -0.2794155\n [7]  0.6569866  0.9893582  0.4121185 -0.5440211\n\ncos(x)\n\n [1]  0.5403023 -0.4161468 -0.9899925 -0.6536436  0.2836622  0.9601703\n [7]  0.7539023 -0.1455000 -0.9111303 -0.8390715"
  },
  {
    "objectID": "12_Vectorization.html#ifelse",
    "href": "12_Vectorization.html#ifelse",
    "title": "9  Vectorized Operations",
    "section": "9.6 ifelse()",
    "text": "9.6 ifelse()\nifelse() is vectorized and can be a great and compact alternative to a more complicated expression:\n\na <- 1:10\n(y <- ifelse(a > 5, 11:20, 21:30))\n\n [1] 21 22 23 24 25 16 17 18 19 20\n\n\nso what did this do?\nIt is equivalent to:\n\nidl <- a > 5\nyes <- 11:20\nno <- 21:30\nout <- vector(\"numeric\", 10)\nfor (i in seq(a)) {\n  if (idl[i]) {\n    out[i] <- yes[i]\n  } else {\n    out[i] <- no[i]\n  }\n}\nout\n\n [1] 21 22 23 24 25 16 17 18 19 20\n\n\ni.e.\n\nCreate a logical index using test\nfor each element i in test:\n\nif the element i is TRUE, return yes[i], else no[i]\n\n\nFor another example, lets take integers 1:11 and square the odd ones and cube the even ones. We use the modulo operation %% to test if each element is odd or even:\n\nx <- 1:11\nxsc <- ifelse(x %% 2 == 0, c(1:11)^3, c(1:11)^2)\nxsc\n\n [1]    1    8    9   64   25  216   49  512   81 1000  121"
  },
  {
    "objectID": "15_InputOutput.html#r-datasets",
    "href": "15_InputOutput.html#r-datasets",
    "title": "10  Data Input/Output",
    "section": "10.1 R datasets",
    "text": "10.1 R datasets\n\n10.1.1 Datasets included with R (in package ‘datasets’)\nList built-in datasets with data() and no arguments:\n\ndata()\n\nThese built-in datasets are normally readily available in the R console (because the datasets package is automatically loaded)\nYou can check if this is the case using search()\n\nsearch()\n\n [1] \".GlobalEnv\"        \"tools:quarto\"      \"package:stats\"    \n [4] \"package:graphics\"  \"package:grDevices\" \"package:utils\"    \n [7] \"package:datasets\"  \"package:methods\"   \"Autoloads\"        \n[10] \"package:base\"     \n\n\n\n\n10.1.2 Datasets included with other packages\nList a dataset included with some R package:\n\ndata(package = \"glmnet\")\ndata(package = \"MASS\")\ndata(package = \"mlbench\")\n\nLoad a dataset from some R package:\n\ndata(Sonar, package = \"mlbench\")\n\nNote: quotes around “Sonar” in the data() command above are optional."
  },
  {
    "objectID": "15_InputOutput.html#system-commands",
    "href": "15_InputOutput.html#system-commands",
    "title": "10  Data Input/Output",
    "section": "10.2 System commands",
    "text": "10.2 System commands\nGet working directory with getwd()\n\ngetwd()\n\nYou can set a different working directory with setwd()\nList files in current directory:\n\ndir()\n\nYou can execute a command of you operating system (OS) -i.e. MacOS, Linux, Windows- from within R using the system() function:\n\nsystem(\"uname -a\")\n\nNote: See issue here"
  },
  {
    "objectID": "15_InputOutput.html#data-io",
    "href": "15_InputOutput.html#data-io",
    "title": "10  Data Input/Output",
    "section": "10.3 Data I/O",
    "text": "10.3 Data I/O\n\n\n\n\n\nCommon Data Input/Output commands in R\n\n\n\n\n\n10.3.1 Read local CSV\nread.table() is the core function that reads data from formatted text files in R, where cases correspond to lines and variables to columns. Its many arguments allow to read different formats.\nread.csv() is an alias for read.table() that defaults to commas as separators and dots for decimal points. (Run read.csv in the console to print its source read the documentation with ?read.table).\nSome important arguments for read.table() listed here with their default values for read.csv():\n\nsep = \",\": Character that separate entries. Default is a comma; use “ for tab-separated files (default setting in read.delim())\ndec = \".\": Character for the decimal point. Default is a dot; in some cases where a comma is used as the decimal point, the entry separator sep may be a semicolon (default setting in read.csv2())\nna.strings = \"NA\": Character vector of strings to be coded as “NA”\ncolClasses = NA: Either a character vector defining each column’s type (e.g. c(“character”, “numeric”, “numeric”) recycled as necessary or a named vector defining specific columns’ types (e.g. c(ICD9 = “character”, Sex = “factor”, SBP = “numeric”, DOB = “Date”)). Unspecified columns automatically determined. Note: Set a column to “NULL” (with quotes) to exclude column.\n\n\nmen <-  read.csv(\"../Data/pone.0204161.s001.csv\")\n\n\n\n10.3.2 Read data from the web\nread.csv() can directly read an online file. In the second example below, we also define that missing data is coded with ? using the na.strings argument:\n\nparkinsons <- read.csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data\")\n\nsleep <- read.csv(\"https://www.openml.org/data/get_csv/53273/sleep.arff\",\n                  na.strings = \"?\")\n\nThe above files are read from two very popular online data repositories. Confusingly, neither file ends in .csv, but they both work with read.csv(). Always look at the plain text file first to determine if it can work with read.table() /read.csv() and what settings to use.\n\n\n10.3.3 Read zipped data from the web\n\n10.3.3.1 using gzcon() and csv.read()\nread.table() /read.csv() also accepts a “connection” as input.\nHere we define a connection to a zipped file by nesting gzcon() and url():\n\ncon <- gzcon(url(\"https://github.com/EpistasisLab/pmlb/raw/master/datasets/breast_cancer_wisconsin/breast_cancer_wisconsin.tsv.gz\"),\n             text = TRUE)\n\nWe read the connection and specify the file is tab-separated, or call read.delim():\n\nbcw <- read.csv(con, header = TRUE, sep = \"\\t\")\n\n#same as\nbcw <- read.delim(con, header = TRUE)\n\n\n\n10.3.3.2 using data.table’s fread()\nYou can also use data.table’s fread(), which will directly handle zipped files:\n\nlibrary(data.table)\nbcw2 <- fread(\"https://github.com/EpistasisLab/penn-ml-benchmarks/raw/master/datasets/classification/breast-cancer-wisconsin/breast-cancer-wisconsin.tsv.gz\")\n\nIf you want to stick to using data frames, set the argument data.table to FALSE:\n\nbcw2 <- fread(\"https://github.com/EpistasisLab/penn-ml-benchmarks/raw/master/datasets/classification/breast-cancer-wisconsin/breast-cancer-wisconsin.tsv.gz\",\n              data.table = FALSE)\n\n\n\n\n10.3.4 Write to CSV\nUse the write.csv() function to write an R object (usually data frame or matrix) to a CSV file. Setting row.names = FALSE is usually a good idea. (Instead of storing data in rownames, it’s usually best to create a new column.)\n\nwrite.csv(iris, \"../Data/iris.csv\", row.names = FALSE)\n\nNote that in this case we did not need to save row names (which are just integers 1 to 150 and would add a useless extra column in the output)\n\n\n10.3.5 Read .xslx using openxlsx::read.xlsx()\nAs an example, we can read the csv we saved earlier into Excel and then save it as a .xlsx file.\n\niris.path <- normalizePath(\"../Data/iris.xlsx\")\niris2 <- openxlsx::read.xlsx(iris.path)\n\nNote: openxlsx::read.xlsx() does not work with a relative path like \"./Data/iris.xlsc\". Therefore we used the normalizePath() function to give us the full path of the file without having to type it out.\nCheck that the data is still identical:\n\nall(iris == iris2)\n\n\n\n10.3.6 Write an R object to RDS\nYou can write any R object directly to file so that you can recover it at any time, share it, etc. Remember that since a list can contain any number of objects of any type, you can save any collection of objects as an RDS file. For multiple objects, see also the save.image() command below.\n\nsaveRDS(iris, \"iris.rds\")\n\nTo load an object saved in an rds file, assign it to an object using readRDS():\n\niris_fromFile <- readRDS(\"iris.rds\")\nall(iris == iris_fromFile)\n\n\n\n10.3.7 Write multiple R objects to RData file using save()\n\nmat1 <- sapply(seq_len(10), function(i) rnorm(500))\nmat2 <- sapply(seq_len(10), function(i) rnorm(500))\nsave(mat1, mat2, file = \"./mat.RData\")\n\nNote: we will learn how to use sapply() later under “Loop functions”\nTo load the variables in the .RData file you saved, use the load() command:\n\nload(\"./Rmd/mat.RData\")\n\nNote that load() adds the objects to your workspace using with their original names. You do not assign them to a new object, unlike with the readRDS() call above.\n\n\n10.3.8 Write your entire workspace to a RData image using save.image()\nYou can save your entire workspace to a RData file using the save.image() function.\n\nsave.image(\"workspace_10_05_2020.RData\")\n\nSame as above, to re-load the workspace saved in the .RData file, use the load() command:\n\nload(\"workspace_10_05_2020.RData\")"
  },
  {
    "objectID": "20_ControlFlow.html",
    "href": "20_ControlFlow.html",
    "title": "11  Control flow",
    "section": "",
    "text": "Code is often not executed linearly (i.e. line-by-line). Control flow (or flow of control) operations define the order in which code segments are executed.\nExecution is often conditional (if - else or switch).\nSegments of code may be repeated multiple times (for) or as long as certain conditions are met (while).\nControl flow operations form some of the fundamental building blocks of programs. Each operation is very simple - combine enough of them and you can build up to any amount of complexity."
  },
  {
    "objectID": "20_ControlFlow.html#if---else",
    "href": "20_ControlFlow.html#if---else",
    "title": "11  Control flow",
    "section": "11.1 if - else:",
    "text": "11.1 if - else:\n\na <- 4\nif (a < 10) {\n  cat(\"a is not that big\")\n} else {\n  cat(\"a is not too small\")\n}\n\na is not that big"
  },
  {
    "objectID": "20_ControlFlow.html#if---else-if---else",
    "href": "20_ControlFlow.html#if---else-if---else",
    "title": "11  Control flow",
    "section": "11.2 if - else if - else:",
    "text": "11.2 if - else if - else:\n\na <- sample(seq(-2, 2, .5), 1)\na\n\n[1] 1.5\n\nif (a > 0) {\n  result <- \"positive\"\n} else if (a == 0) {\n  result <- \"zero\"\n} else {\n  result <- \"negative\"\n}\nresult\n\n[1] \"positive\""
  },
  {
    "objectID": "20_ControlFlow.html#conditional-assignment-with-if---else",
    "href": "20_ControlFlow.html#conditional-assignment-with-if---else",
    "title": "11  Control flow",
    "section": "11.3 Conditional assignment with if - else:",
    "text": "11.3 Conditional assignment with if - else:\nYou can use an if statement as part of an assignment:\n\na <- 8\ny <- if (a > 5) {\n  10\n} else {\n  0\n}"
  },
  {
    "objectID": "20_ControlFlow.html#conditional-assignment-with-ifelse",
    "href": "20_ControlFlow.html#conditional-assignment-with-ifelse",
    "title": "11  Control flow",
    "section": "11.4 Conditional assignment with ifelse:",
    "text": "11.4 Conditional assignment with ifelse:\n\na <- 3\n(y <- ifelse(a > 5, 10, 0))\n\n[1] 0\n\n\nifelse is vectorized:\n\na <- 1:10\n(y <- ifelse(a > 7, a^2, a))\n\n [1]   1   2   3   4   5   6   7  64  81 100"
  },
  {
    "objectID": "20_ControlFlow.html#for-loops",
    "href": "20_ControlFlow.html#for-loops",
    "title": "11  Control flow",
    "section": "11.5 for loops",
    "text": "11.5 for loops\n\n\n\nUse for loops to repeat execution of a block of code a certain number of times.\n\n\n\nThe for loop syntax is for (var in vector) expression.\nThe expression is usually surrounded by curly brackets and can include any number of lines, any amount of code:\n\nfor (i in 1:3) {\n  print(\"I love coffee\")\n}\n\n[1] \"I love coffee\"\n[1] \"I love coffee\"\n[1] \"I love coffee\"\n\n\nThe loop executes for length(vector) times.\nAt iteration i, var = vector[i].\nYou will often use the value of var inside the loop (but you don’t have to):\n\nfor (i in seq(10)) {\n  cat(i^2, \"\\n\")\n}\n\n1 \n4 \n9 \n16 \n25 \n36 \n49 \n64 \n81 \n100 \n\n\nletters is a built-in constant that includes all 26 lowercase letters of the Roman alphabet; LETTERS similarly includes all 26 uppercase letters.\n\nfor (letter in letters[1:5]) {\n  cat(letter, \"is a letter!\\n\")\n}\n\na is a letter!\nb is a letter!\nc is a letter!\nd is a letter!\ne is a letter!\n\n\n\n11.5.1 Working on data within a for loop\nA common scenario involves working on a data object, whether a vector, matrix, list, data.frame, and performing an operation on each elements, one at a time. While a lot of these operations are often performed using loop functions instead, for loops can certainly be used.\nYou can start by initializing an object of the appropriate class and dimensions to hold the output. Then, each iteration of the for loop will assign its output to the corresponding element/s of this object.\nIn the following example we transform the mtcars built-in dataset’s features to z-scores. The built-in command scale() will do this for quickly and conveniently, this is for demonstration purposes:\nFirst, initialize the output to be the desired class and dimensions:\n\nclass(mtcars)\n\n[1] \"data.frame\"\n\ndim(mtcars)\n\n[1] 32 11\n\nmtcars_z <- data.frame(matrix(0, 32, 11))\ncolnames(mtcars_z) <- colnames(mtcars)\n\nor, it is much simpler to just make a copy of mtcars to be overwritten by the for loop later:\n\nmtcars_z <- mtcars\n\nStandardization involves subtracting the mean and dividing by the standard deviation.\nHere is the for loop - we iterate through each column and assign the transformed data:\n\nfor (i in 1:ncol(mtcars)) {\n  mtcars_z[, i] <- (mtcars[, i] - mean(mtcars[, i])) / sd(mtcars[, i])\n}\n\nLet’s compare to the output of the scale() command by print the first 3 rows and columns of each:\n\nmtcars_z2 <- as.data.frame(scale(mtcars))\nmtcars_z[1:3, 1:3]\n\n                    mpg        cyl       disp\nMazda RX4     0.1508848 -0.1049878 -0.5706198\nMazda RX4 Wag 0.1508848 -0.1049878 -0.5706198\nDatsun 710    0.4495434 -1.2248578 -0.9901821\n\nmtcars_z2[1:3, 1:3]\n\n                    mpg        cyl       disp\nMazda RX4     0.1508848 -0.1049878 -0.5706198\nMazda RX4 Wag 0.1508848 -0.1049878 -0.5706198\nDatsun 710    0.4495434 -1.2248578 -0.9901821\n\n\nNote that we wrapped scale() around as.data.frame() because it outputs a matrix.\nWe can check that all elements are the same with all():\n\nall(mtcars_z == mtcars_z2)\n\n[1] FALSE\n\n\n\n\n11.5.2 Nested for loops\n\na <- matrix(1:9, 3)\nfor (i in seq(3)) {\n  for (j in seq(3)) {\n    cat(\"  a[\", i, \",\", j, \"] is \", a[i, j], \"\\n\", sep = \"\")\n  }\n}\n\n  a[1,1] is 1\n  a[1,2] is 4\n  a[1,3] is 7\n  a[2,1] is 2\n  a[2,2] is 5\n  a[2,3] is 8\n  a[3,1] is 3\n  a[3,2] is 6\n  a[3,3] is 9\n\n\n\n\n11.5.3 Printing within a for loop\nIn the R console objects get printed just by typing their name:\n\na <- 4\na\n\n[1] 4\n\n# same as\nprint(a)\n\n[1] 4\n\n\nThis “automatic printing” does not happen within a for loop, so you simply use print() (or cat() as preferred):\nThe following loop does not print out anything:\n\na <- 0\nfor (i in 1:4) {\n  a <- a + i^2\n  a\n}\n\nbut this does:\n\na <- 0\nfor (i in 1:4) {\n  a <- a + i^2\n  print(a)\n}\n\n[1] 1\n[1] 5\n[1] 14\n[1] 30"
  },
  {
    "objectID": "20_ControlFlow.html#select-one-of-multiple-alternatives-with-switch",
    "href": "20_ControlFlow.html#select-one-of-multiple-alternatives-with-switch",
    "title": "11  Control flow",
    "section": "11.6 Select one of multiple alternatives with switch",
    "text": "11.6 Select one of multiple alternatives with switch\nInstead of using multiple if - else if statements, we can build a more compact call using switch. (It is best suited for options that are of type character, rather than numeric)\n\ny <- sample(letters[seq(8)], 1)\ny\n\n[1] \"e\"\n\noutput <- switch(y,                      # 1. Some expression\n                 a = \"Well done\",        # 2. The possible values of the expression, unquoted\n                 b = \"Not bad\",          #    followed by the `=` and the conditional output\n                 c = \"Nice try\",\n                 d = \"Not a nice try\",\n                 e = \"This is bad\",\n                 f = \"Fail\",\n                 \"This is not even a possible grade\") # 3. An optional last argument is the default\n                                                      #    value, if there is no match above\noutput\n\n[1] \"This is bad\"\n\n\n\na <- rnorm(1)\na\n\n[1] -0.03464858\n\nout <- switch(as.integer(a > 0),\n              `1` = \"Input is positive\",\n              `0` = \"Input is not positive\")\nout\n\nNULL\n\n\n\na <- rnorm(1)\na\n\n[1] -0.2548535\n\nout <- switch(as.character(a > 0),\n              `TRUE` = \"Input is positive\",\n              `FALSE` = \"Input is not positive\")\nout\n\n[1] \"Input is not positive\"\n\n\n\n11.6.1 switch example: HTTP Status Codes\n\nstatus <- sample(400:410, 1)\nstatus\n\n[1] 403\n\nresponse <- switch(as.character(status),\n                   `400` = \"Bad Request\",\n                   `401` = \"Unauthorized\",\n                   `402` = \"Payment Required\",\n                   `403` = \"Forbidden\",\n                   `404` = \"Not Found\",\n                   `405` = \"Method Not Allowed\",\n                   `406` = \"Not Acceptable\",\n                   `407` = \"Proxy Authentication Required\",\n                   `408` = \"Request Timeout\",\n                   `409` = \"Conflict\",\n                   `410` = \"Gone\")\nresponse\n\n[1] \"Forbidden\""
  },
  {
    "objectID": "20_ControlFlow.html#while-loops",
    "href": "20_ControlFlow.html#while-loops",
    "title": "11  Control flow",
    "section": "11.7 while loops",
    "text": "11.7 while loops\n\na <- 10\nwhile (a > 0) {\n  a <- a - 1\n  cat(\"a is equal to\", a, \"\\n\")\n}\n\na is equal to 9 \na is equal to 8 \na is equal to 7 \na is equal to 6 \na is equal to 5 \na is equal to 4 \na is equal to 3 \na is equal to 2 \na is equal to 1 \na is equal to 0 \n\ncat(\"when all is said and done, a is\", a)\n\nwhen all is said and done, a is 0"
  },
  {
    "objectID": "20_ControlFlow.html#break-stops-execution-of-a-loop",
    "href": "20_ControlFlow.html#break-stops-execution-of-a-loop",
    "title": "11  Control flow",
    "section": "11.8 break stops execution of a loop:",
    "text": "11.8 break stops execution of a loop:\n\nfor (i in seq(10)) {\n  if (i == 5) break\n  cat(i, \"squared is\", i^2, \"\\n\")\n}\n\n1 squared is 1 \n2 squared is 4 \n3 squared is 9 \n4 squared is 16"
  },
  {
    "objectID": "20_ControlFlow.html#next-skips-the-current-iteration",
    "href": "20_ControlFlow.html#next-skips-the-current-iteration",
    "title": "11  Control flow",
    "section": "11.9 next skips the current iteration:",
    "text": "11.9 next skips the current iteration:\n\nfor (i in seq(7)) {\n  if (i == 5) next\n  cat(i, \"squared is\", i^2, \"\\n\")\n}\n\n1 squared is 1 \n2 squared is 4 \n3 squared is 9 \n4 squared is 16 \n6 squared is 36 \n7 squared is 49"
  },
  {
    "objectID": "20_ControlFlow.html#repeat-loops",
    "href": "20_ControlFlow.html#repeat-loops",
    "title": "11  Control flow",
    "section": "11.10 repeat loops",
    "text": "11.10 repeat loops\nA repeat block initiates an infinite loop and you must use break to exit.\n\ni <- 10\nrepeat {\n i <- i - 1\n if (i == 0) break\n cat(\"i is\", i, \"\\n\")\n}\n\ni is 9 \ni is 8 \ni is 7 \ni is 6 \ni is 5 \ni is 4 \ni is 3 \ni is 2 \ni is 1"
  },
  {
    "objectID": "24_Summarize.html",
    "href": "24_Summarize.html",
    "title": "12  Summarizing Data",
    "section": "",
    "text": "Let’s read in a dataset from OpenML:"
  },
  {
    "objectID": "24_Summarize.html#get-summary-of-an-r-object-with-summary",
    "href": "24_Summarize.html#get-summary-of-an-r-object-with-summary",
    "title": "12  Summarizing Data",
    "section": "12.1 Get summary of an R object with summary()",
    "text": "12.1 Get summary of an R object with summary()\nR includes summary() methods for a number of different objects.\n\nsummary(heart)\n\n      age            sex             chest_pain           trestbps    \n Min.   :28.00   Length:294         Length:294         Min.   : 92.0  \n 1st Qu.:42.00   Class :character   Class :character   1st Qu.:120.0  \n Median :49.00   Mode  :character   Mode  :character   Median :130.0  \n Mean   :47.83                                         Mean   :132.6  \n 3rd Qu.:54.00                                         3rd Qu.:140.0  \n Max.   :66.00                                         Max.   :200.0  \n                                                       NA's   :1      \n      chol           fbs              restecg             thalach     \n Min.   : 85.0   Length:294         Length:294         Min.   : 82.0  \n 1st Qu.:209.0   Class :character   Class :character   1st Qu.:122.0  \n Median :243.0   Mode  :character   Mode  :character   Median :140.0  \n Mean   :250.8                                         Mean   :139.1  \n 3rd Qu.:282.5                                         3rd Qu.:155.0  \n Max.   :603.0                                         Max.   :190.0  \n NA's   :23                                            NA's   :1      \n    exang              oldpeak          slope                 ca     \n Length:294         Min.   :0.0000   Length:294         Min.   :0    \n Class :character   1st Qu.:0.0000   Class :character   1st Qu.:0    \n Mode  :character   Median :0.0000   Mode  :character   Median :0    \n                    Mean   :0.5861                      Mean   :0    \n                    3rd Qu.:1.0000                      3rd Qu.:0    \n                    Max.   :5.0000                      Max.   :0    \n                                                        NA's   :291  \n     thal               num           \n Length:294         Length:294        \n Class :character   Class :character  \n Mode  :character   Mode  :character"
  },
  {
    "objectID": "24_Summarize.html#fast-builtin-column-and-row-operations",
    "href": "24_Summarize.html#fast-builtin-column-and-row-operations",
    "title": "12  Summarizing Data",
    "section": "12.2 Fast builtin column and row operations",
    "text": "12.2 Fast builtin column and row operations\nWe saw in Loop Functions how we can apply functions on rows, columns, or other subsets of our data. R has optimized builtin functions for some very common operations, with self-explanatory names:\n\ncolSums(): column sums\nrowSums(): row sums\ncolMeans(): column means\nrowMeans(): row means\n\n\na <- matrix(1:20, 5)\na\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    6   11   16\n[2,]    2    7   12   17\n[3,]    3    8   13   18\n[4,]    4    9   14   19\n[5,]    5   10   15   20\n\n\n\ncolSums(a)\n\n[1] 15 40 65 90\n\n# same as\napply(a, 2, sum)\n\n[1] 15 40 65 90\n\n\n\nrowSums(a)\n\n[1] 34 38 42 46 50\n\n# same as\napply(a, 1, sum)\n\n[1] 34 38 42 46 50\n\n\n\ncolMeans(a)\n\n[1]  3  8 13 18\n\n# same as\napply(a, 2, mean)\n\n[1]  3  8 13 18\n\n\n\nrowMeans(a)\n\n[1]  8.5  9.5 10.5 11.5 12.5\n\n# same as\napply(a, 1, mean)\n\n[1]  8.5  9.5 10.5 11.5 12.5"
  },
  {
    "objectID": "24_Summarize.html#optimized-matrix-operations-with-matrixstats",
    "href": "24_Summarize.html#optimized-matrix-operations-with-matrixstats",
    "title": "12  Summarizing Data",
    "section": "12.3 Optimized matrix operations with matrixStats",
    "text": "12.3 Optimized matrix operations with matrixStats\nWhile the builtin operations above are already optimized and faster than the equivalent calls, the matrixStats package (Bengtsson 2019) offers a number of futher optimized matrix operations, including drop-in replacements of the above. These should be prefered when dealing with bigger data:\n\nlibrary(matrixStats)\ncolSums2(a)\n\n[1] 15 40 65 90\n\nrowSums2(a)\n\n[1] 34 38 42 46 50\n\ncolMeans2(a)\n\n[1]  3  8 13 18\n\nrowMeans2(a)\n\n[1]  8.5  9.5 10.5 11.5 12.5\n\n\nNote: matrixStats provides replacement functions named almost identically to their base counterpart - so they are easy to find - but are different - so they don’t mask the base functions (this is important and good software design)."
  },
  {
    "objectID": "24_Summarize.html#see-alos",
    "href": "24_Summarize.html#see-alos",
    "title": "12  Summarizing Data",
    "section": "12.4 See alos",
    "text": "12.4 See alos\naggregate() for grouped summary statistics\n\n\n\n\nBengtsson, Henrik. 2019. matrixStats: Functions That Apply to Rows and Columns of Matrices (and to Vectors). https://CRAN.R-project.org/package=matrixStats."
  },
  {
    "objectID": "26_Aggregate.html",
    "href": "26_Aggregate.html",
    "title": "13  Aggregate",
    "section": "",
    "text": "aggregate() is a powerful way to apply functions on splits of your data. It can replicate functionality of the *apply() family, but can be more flexible. This may come with a performance penalty, only noticeable with big data, in which case it is recommended to use data.table for fast group-by data summarization.\naggregate() can work either with a formula notation or directly on data.frames and vectors. We show how to perform each operation below with either approach. The formula interface might be easier to work with interactivly on the console. While you can code with the formula interface, the regular approach is a lot more straightforward to do so.\nFor this example, we get the penguin data:\nSee example below for 1 or multiple variables by 1 or more groups using either the formula interface, or working directly on objects with $ indexing or using with():"
  },
  {
    "objectID": "26_Aggregate.html#single-variable-by-single-group",
    "href": "26_Aggregate.html#single-variable-by-single-group",
    "title": "13  Aggregate",
    "section": "13.1 Single variable by single group",
    "text": "13.1 Single variable by single group\nNote that the formula method defaults to na.action = na.omit\nUsing the formula interface:\n\naggregate(bill_length_mm ~ species,\n          penguins, mean)\n\n    species bill_length_mm\n1    Adelie       38.79139\n2 Chinstrap       48.83382\n3    Gentoo       47.50488\n\n\nDirectly working with vectors:\n\naggregate(penguins$bill_length_mm,\n          by = list(penguins$species),\n          mean, na.rm = T)\n\n    Group.1        x\n1    Adelie 38.79139\n2 Chinstrap 48.83382\n3    Gentoo 47.50488\n\n\nUsing with():\n\nwith(penguins,\n     aggregate(bill_length_mm,\n               by = list(species),\n               mean, na.rm = TRUE))\n\n    Group.1        x\n1    Adelie 38.79139\n2 Chinstrap 48.83382\n3    Gentoo 47.50488"
  },
  {
    "objectID": "26_Aggregate.html#multiple-variables-by-single-group",
    "href": "26_Aggregate.html#multiple-variables-by-single-group",
    "title": "13  Aggregate",
    "section": "13.2 Multiple variables by single group",
    "text": "13.2 Multiple variables by single group\n\naggregate(cbind(bill_length_mm, flipper_length_mm) ~ species,\n          penguins, mean)\n\n    species bill_length_mm flipper_length_mm\n1    Adelie       38.79139          189.9536\n2 Chinstrap       48.83382          195.8235\n3    Gentoo       47.50488          217.1870\n\n\n\naggregate(penguins[, c(\"bill_length_mm\", \"flipper_length_mm\")],\n          by = list(penguins$species),\n          mean, na.rm = TRUE)\n\n    Group.1 bill_length_mm flipper_length_mm\n1    Adelie       38.79139          189.9536\n2 Chinstrap       48.83382          195.8235\n3    Gentoo       47.50488          217.1870\n\n\n\nwith(penguins,\n     aggregate(cbind(bill_length_mm, flipper_length_mm),\n               by = list(species),\n               mean, na.rm = TRUE))\n\n    Group.1 bill_length_mm flipper_length_mm\n1    Adelie       38.79139          189.9536\n2 Chinstrap       48.83382          195.8235\n3    Gentoo       47.50488          217.1870"
  },
  {
    "objectID": "26_Aggregate.html#single-variable-by-multiple-groups",
    "href": "26_Aggregate.html#single-variable-by-multiple-groups",
    "title": "13  Aggregate",
    "section": "13.3 Single variable by multiple groups",
    "text": "13.3 Single variable by multiple groups\n\naggregate(bill_length_mm ~ species + island, penguins, mean)\n\n    species    island bill_length_mm\n1    Adelie    Biscoe       38.97500\n2    Gentoo    Biscoe       47.50488\n3    Adelie     Dream       38.50179\n4 Chinstrap     Dream       48.83382\n5    Adelie Torgersen       38.95098\n\n\n\naggregate(penguins$bill_length_mm,\n          by = list(penguins$species, penguins$island),\n          mean, na.rm = TRUE)\n\n    Group.1   Group.2        x\n1    Adelie    Biscoe 38.97500\n2    Gentoo    Biscoe 47.50488\n3    Adelie     Dream 38.50179\n4 Chinstrap     Dream 48.83382\n5    Adelie Torgersen 38.95098\n\n\n\nwith(penguins,\n     aggregate(bill_length_mm,\n               by = list(species, island),\n               mean, na.rm = TRUE))\n\n    Group.1   Group.2        x\n1    Adelie    Biscoe 38.97500\n2    Gentoo    Biscoe 47.50488\n3    Adelie     Dream 38.50179\n4 Chinstrap     Dream 48.83382\n5    Adelie Torgersen 38.95098"
  },
  {
    "objectID": "26_Aggregate.html#multiple-variables-by-multiple-groups",
    "href": "26_Aggregate.html#multiple-variables-by-multiple-groups",
    "title": "13  Aggregate",
    "section": "13.4 Multiple variables by multiple groups",
    "text": "13.4 Multiple variables by multiple groups\n\naggregate(cbind(bill_length_mm, flipper_length_mm) ~ species + island,\n          penguins, mean)\n\n    species    island bill_length_mm flipper_length_mm\n1    Adelie    Biscoe       38.97500          188.7955\n2    Gentoo    Biscoe       47.50488          217.1870\n3    Adelie     Dream       38.50179          189.7321\n4 Chinstrap     Dream       48.83382          195.8235\n5    Adelie Torgersen       38.95098          191.1961\n\n\n\naggregate(penguins[, c(\"bill_length_mm\", \"flipper_length_mm\")],\n          by = list(penguins$species, penguins$island),\n          mean, na.rm = TRUE)\n\n    Group.1   Group.2 bill_length_mm flipper_length_mm\n1    Adelie    Biscoe       38.97500          188.7955\n2    Gentoo    Biscoe       47.50488          217.1870\n3    Adelie     Dream       38.50179          189.7321\n4 Chinstrap     Dream       48.83382          195.8235\n5    Adelie Torgersen       38.95098          191.1961\n\n\n\nwith(penguins,\n     aggregate(cbind(bill_length_mm, flipper_length_mm),\n               by = list(species, island),\n               mean, na.rm = TRUE))\n\n    Group.1   Group.2 bill_length_mm flipper_length_mm\n1    Adelie    Biscoe       38.97500          188.7955\n2    Gentoo    Biscoe       47.50488          217.1870\n3    Adelie     Dream       38.50179          189.7321\n4 Chinstrap     Dream       48.83382          195.8235\n5    Adelie Torgersen       38.95098          191.1961"
  },
  {
    "objectID": "26_Aggregate.html#see-also",
    "href": "26_Aggregate.html#see-also",
    "title": "13  Aggregate",
    "section": "13.5 See also",
    "text": "13.5 See also\ntapply() for an alternative methods of applying function on subsets of a single variable (probably faster)"
  },
  {
    "objectID": "30_Functions.html",
    "href": "30_Functions.html",
    "title": "15  Functions",
    "section": "",
    "text": "Writing functions is a core part of programming.\nWhen should you write a function?\nWhenever you find yourself repeating pieces of code.\nWhy is it important?\nWriting functions helps reduce the total amount of code, which increases efficiency, reduces the chance of error, and can make code more readable.\nFunctions in R are “first-class objects”.\nThis means they can be passed in and out of other functions or objects like any other R structure.\nFor example, you can use a command like apply(mat, 2, mean)\nFunctions in R are for the most part like mathematical functions: they have one or more inputs and one output. The inputs are known as the function arguments. If you want to return multiple outputs, you can return a list containing any number of R objects."
  },
  {
    "objectID": "30_Functions.html#simple-functions",
    "href": "30_Functions.html#simple-functions",
    "title": "15  Functions",
    "section": "15.1 Simple functions",
    "text": "15.1 Simple functions\nLet’s start with a very simple function: single argument with no default value:\n\nsquare <- function(x) {\n  x^2\n}\n\nsquare(3)\n\n[1] 9\n\n\nNotice above that x^2 is automatically returned by the function. It is the same as explicitly returning it with return():\n\nsquare <- function(x) {\n  out <- x^2\n  return(out)\n}\n\nsquare(4)\n\n[1] 16\n\n\nwhich is the same as:\n\nsquare <- function(x) {\n  out <- x^2\n  out\n}\n\nsquare(5)\n\n[1] 25\n\n\nA function returns either:\n\nan object passed to return()\nthe value of the last expression within the function definition such as out or x^2 above.\n\nreturn() is a way to end evaluation early:\n\nsquare.pos <- function(x) {\n  if (x > 0) {\n    return(x^2)\n  } else {\n    x\n  }\n  cat(\"The input was left unchanged\\n\")\n}\n\nx <- sample(-10:10, 1)\nx\n\n[1] 6\n\nsquare.pos(x)\n\n[1] 36\n\n\nMultiple arguments, with and without defaults:\n\nraise <- function(x, power = 2) {\n  x^power\n}\n\nx <- sample(10, 1)\nx\n\n[1] 9\n\nraise(x)\n\n[1] 81\n\nraise(x, power = 3)\n\n[1] 729\n\nraise(x, 3)\n\n[1] 729"
  },
  {
    "objectID": "30_Functions.html#argument-matching",
    "href": "30_Functions.html#argument-matching",
    "title": "15  Functions",
    "section": "15.2 Argument matching",
    "text": "15.2 Argument matching\nR will match unambiguous abbreviations of arguments:\n\nfn <- function(alpha = 2, beta = 3, gamma = 4) {\n  alpha * beta + gamma\n}\nfn(g = 2)\n\n[1] 8"
  },
  {
    "objectID": "30_Functions.html#arguments-with-prescribed-set-of-allowed-values",
    "href": "30_Functions.html#arguments-with-prescribed-set-of-allowed-values",
    "title": "15  Functions",
    "section": "15.3 Arguments with prescribed set of allowed values",
    "text": "15.3 Arguments with prescribed set of allowed values\nYou can match specific values for an argument using match.arg():\n\nmyfn <- function(type = c(\"alpha\", \"beta\", \"gamma\")) {\n  type <- match.arg(type)\n  cat(\"You have selected type '\", type, \"'\\n\", sep = \"\")\n}\n\nmyfn(\"a\")\n\nYou have selected type 'alpha'\n\nmyfn(\"b\")\n\nYou have selected type 'beta'\n\nmyfn(\"g\")\n\nYou have selected type 'gamma'\n\nmyfn(\"d\")\n\nError in match.arg(type): 'arg' should be one of \"alpha\", \"beta\", \"gamma\"\n\n\nAbove you see that partial matching using match.arg() was able to identify a valid option, and when there was no match, an informative error was printed.\nPartial matching is also automatically done on the argument names themselves, but it’s important to avoid depending on that.\n\nadsr <- function(attack = 100,\n                 decay = 250,\n                 sustain = 40,\n                 release = 1000) {\n  cat(\"Attack time:\", attack, \"ms\\n\",\n      \"Decay time:\", decay, \"ms\\n\",\n      \"Sustain level:\", sustain, \"\\n\",\n      \"Release time:\", release, \"ms\\n\")\n}\n\nadsr(50, s = 100, r = 500)\n\nAttack time: 50 ms\n Decay time: 250 ms\n Sustain level: 100 \n Release time: 500 ms"
  },
  {
    "objectID": "30_Functions.html#passing-extra-arguments-to-another-function-with-the-...-argument",
    "href": "30_Functions.html#passing-extra-arguments-to-another-function-with-the-...-argument",
    "title": "15  Functions",
    "section": "15.4 Passing extra arguments to another function with the ... argument",
    "text": "15.4 Passing extra arguments to another function with the ... argument\nMany functions include a ... argument at the end. Any arguments not otherwise matched are collected there. A common use for this is to pass them to another function:\n\ncplot <- function(x, y,\n                  cex = 1.5,\n                  pch = 16,\n                  col = \"#18A3AC\",\n                  bty = \"n\", ...) {\n  plot(x, y, cex = cex, pch = pch, col = col, bty = bty, ...)\n                  }\n\n... is also used for variable number of iputs, often as the first argument of a function. For example, look at the documentation of c, cat, cbind, rbind, paste\nNote: Any arguments after the ..., must be named fully, i.e. will not be partially matched."
  },
  {
    "objectID": "30_Functions.html#return-multiple-objects",
    "href": "30_Functions.html#return-multiple-objects",
    "title": "15  Functions",
    "section": "15.5 Return multiple objects",
    "text": "15.5 Return multiple objects\nR function can only return a single object. This is not much of a problem because you can simply put any collection of objects into a list and return it:\n\nlfn <- function(x, fn = square) {\n  xfn <- fn(x)\n  \n  list(x = x,\n       xfn = xfn,\n       fn = fn)\n}\n\nlfn(3)\n\n$x\n[1] 3\n\n$xfn\n[1] 9\n\n$fn\nfunction(x) {\n  out <- x^2\n  out\n}\n<bytecode: 0x10a2c3f10>"
  },
  {
    "objectID": "30_Functions.html#warnings-and-errors",
    "href": "30_Functions.html#warnings-and-errors",
    "title": "15  Functions",
    "section": "15.6 Warnings and errors",
    "text": "15.6 Warnings and errors\nYou can use warning(\"some warning message\") at any point inside a function to produce a warning message during execution. The message gets printed to the R console, but function execution is not stopped.\nOn the other hand, you can use stop(\"some error message\") to print an error message to console and stop function execution.\nThe following function (el10) calculates:\n\\[ e^{log_{10}(x)} \\]\n\nel10 <- function(x) {\n  exp(log10(x))\n}\n\nwhich is not defined for negative x. In this case, we could let R give a warning when it tries to compute log10(x):\n\nval1 <- el10(-3)\n\nWarning in el10(-3): NaNs produced\n\n\nWe could instead produce our own warning message:\n\nel10 <- function(x) {\n  if (x < 0) warning(\"x must be positive\")\n  exp(log10(x))\n}\nval2 <- el10(-3)\n\nWarning in el10(-3): x must be positive\n\n\nWarning in el10(-3): NaNs produced\n\nval2\n\n[1] NaN\n\n\nAs you see, the output (NaN) still gets returned.\nAlternatively, we can use stop() to end function execution:\n\nel10 <- function(x) {\n  if (x < 0) stop(\"x must be positive\")\n  exp(log10(x))\n}\nval3 <- el10(-3)\n\nError in el10(-3): x must be positive\n\n\nNote how, in this case, function evalutation is stopped and no value is returned."
  },
  {
    "objectID": "30_Functions.html#scoping",
    "href": "30_Functions.html#scoping",
    "title": "15  Functions",
    "section": "15.7 Scoping",
    "text": "15.7 Scoping\nFunctions exist in their own environment, i.e. contain their own variable definitions.\n\nx <- 3\ny <- 4\nfn <- function(x, y) {\n  x <- 10*x\n  y <- 20*y\n  cat(\"Inside the function, x = \", x, \" and y = \", y, \"\\n\")\n}\n\nfn(x, y)\n\nInside the function, x =  30  and y =  80 \n\ncat(\"Outside the function, x = \", x, \" and y = \", y, \"\\n\")\n\nOutside the function, x =  3  and y =  4 \n\n\nHowever, if a variable is referenced within a function but no local definition exists, the interpreter will look for the variable at the parent directory. It is best ensure all objects needed within a function are specified as arguments and passed appropriately when the function is called.\nIn the following example, x is only defined outside the function definition, but referenced within it.\n\nx <- 21\n\nitfn <- function(y, lr = 1) {\n  x + lr * y\n}\n\nitfn(3)\n\n[1] 24\n\n\n\n15.7.1 function vs. for loop\nLet’s z-score the built-in mtcars dataset once with a for loop and once with a custom function. This links back to the example seen earlier in the for loop section. In practice, this would be performed with the scale() command:\nWithin the for loop, we are assigning columns directly to the object initialized before the loop. In the following example, we use print(environment()) to print the environment outside and inside the loop function to show that it is the same. This is purely for demonstration:\n\n# initialize new object 'mtcars_z'\nmtcars_z <- mtcars\ncat(\"environment outside for loop is: \")\n\nenvironment outside for loop is: \n\nprint(environment())\n\n<environment: R_GlobalEnv>\n\n# z-score one column at a time in a for loop\nfor (i in 1:ncol(mtcars)) {\n  mtcars_z[, i] <- (mtcars[, i] - mean(mtcars[, i])) / sd(mtcars[, i])\n  cat(\"environment inside for loop is: \")\n  print(environment())\n}\n\nenvironment inside for loop is: <environment: R_GlobalEnv>\nenvironment inside for loop is: <environment: R_GlobalEnv>\nenvironment inside for loop is: <environment: R_GlobalEnv>\nenvironment inside for loop is: <environment: R_GlobalEnv>\nenvironment inside for loop is: <environment: R_GlobalEnv>\nenvironment inside for loop is: <environment: R_GlobalEnv>\nenvironment inside for loop is: <environment: R_GlobalEnv>\nenvironment inside for loop is: <environment: R_GlobalEnv>\nenvironment inside for loop is: <environment: R_GlobalEnv>\nenvironment inside for loop is: <environment: R_GlobalEnv>\nenvironment inside for loop is: <environment: R_GlobalEnv>\n\n\nIn contrast, all operations remain local within a function and the output must be returned:\n\nztransform <- function(x) {\n  cat(\"environment inside function body is: \")\n  print(environment())\n  z <- as.data.frame(sapply(mtcars, function(i) (i - mean(i))/sd(i)))\n  rownames(z) <- rownames(x)\n  z\n}\nmtcars_z2 <- ztransform(mtcars)\n\nenvironment inside function body is: <environment: 0x109b7a820>\n\ncat(\"environment outside function body is: \")\n\nenvironment outside function body is: \n\nprint(environment())\n\n<environment: R_GlobalEnv>\n\n\nNotice how the environment outside and inside the loop function is the same, it is the Global environemnt, but the environment within the function is different. That is why any objects created or changed within a function must be returned if we want to make them available."
  },
  {
    "objectID": "30_Functions.html#pipe",
    "href": "30_Functions.html#pipe",
    "title": "15  Functions",
    "section": "15.8 The pipe operator",
    "text": "15.8 The pipe operator\n\n\n\n\n\nIllustration of pipes in R\n\n\n\n\nA pipe operator was first introduced to R by the magrittr package with the %>% symbol. Note that a number of other packages that endorse the use of pipes export the pipe operator as well.\nStarting with R version 4.1, a native pipe operator is included with the |> symbol.\nA pipe allows writing f(x) as x |> f() (native pipe) or x %>% f (magrittr).\nNote that the native pipe requires parentheses, but magrittr works with or without them.\nA pipe is often used to:\n\navoid multiple temporary assignments in a multistep procedure, or\nas an alternative to nesting functions.\n\nSome packages and developers promote its use, others discourage it. You should try and see if/when it suits your needs.\nThe following:\n\nx <- f1(x)\nx <- f2(x)\nx <- f3(x)\n\nis equivalent to:\n\nx <- f3(f2(f1(x)))\n\nis equivalent to:\n\nx <- x |> f1() |> f2() |> f3()\n\n\niris[, -5] |>\n  split(iris$Species) |>\n  lapply(function(i) sapply(i, mean))\n\n$setosa\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n       5.006        3.428        1.462        0.246 \n\n$versicolor\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n       5.936        2.770        4.260        1.326 \n\n$virginica\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n       6.588        2.974        5.552        2.026 \n\n\nPipes are used extensively in the tidyverse packages and many other third-party packages.\nYou can learn more about the magrittr pipe operator in the vignette\n\n\n\nIn RStudio the keyboard shortcut for the pipe operator is Shift-Command-M (MacOS) or Ctrl-Shift-M (Windows)\n\n\n\n\n15.8.1 Differences between native pipe and magrittr\n\nnative pipe requires () after function name, magrittr works with or without them\n\n\nx <- rnorm(300)\n\n\nx |> mean()\n\n[1] -0.05065302\n\n\nbut this would fail:\n\nx |> mean\n\nwhile either works in magrittr\n\nlibrary(magrittr)\nx %>% mean()\n\n[1] -0.05065302\n\n\n\nx %>% mean\n\n[1] -0.05065302\n\n\n\nnative pipe by design only pipes its LHS to the first unnamed argument on the RHS. magrittr allows using a period . to pipe to any position on the RHS. The native pipe workaround is using an anonymous function (can use the new shorter syntax \\(x) instead of function(x))\n\ne.g.: Find the position of “r” in the latin alphabet\nIn this example, we want to pass the LHS to the second argument of grep().\nUsing native pipe, we name the first argument pattern and the LHS is passed to the first unnamed argument, i.e. the second (which is x, the character vector where matches are looked for)\n\nletters |> grep(pattern = \"r\")\n\n[1] 18\n\n\nwith magrittr you can use the dot notation to specify where to pipe into:\n\nletters %>% grep(\"r\", .)\n\n[1] 18\n\n\nFor demonstration, here’s the slightly involved way you would achieve this with an anonymous function and the native pipe. This may make sense for more complex calls.\n\nletters |> {\\(x) grep(\"r\", x)}()\n\n[1] 18"
  },
  {
    "objectID": "32_LoopFunctions.html",
    "href": "32_LoopFunctions.html",
    "title": "16  Loop Functions",
    "section": "",
    "text": "Loop functions are some of the most widely used R functions. They replace longer expressions created with a for loop, for example.\nThey can result in more compact and readable code and are often faster to execute than a for loop.\nBefore starting to use the above functions, we need to learn about anonymous functions, which are often used within the apply functions."
  },
  {
    "objectID": "32_LoopFunctions.html#apply",
    "href": "32_LoopFunctions.html#apply",
    "title": "16  Loop Functions",
    "section": "16.1 apply()",
    "text": "16.1 apply()\n\n\n\napply() applies a function over one or more dimensions of an array of 2 dimensions or more (this includes matrices) or a data frame:\n\n\napply(array, MARGIN, FUN)\n\n\n\nMARGIN can be an integer vector or character indicating the dimensions over which ‘FUN’ will be applied.\nBy convention, rows come first (just like in indexing), therefore:\nMARGIN = 1: apply function on each row MARGIN = 2: apply function on each column\nLet’s calculate the mean value of each of the first four columns of the iris dataset:\n\nx <- iris[, -5]\niris_column_mean <- apply(x, MARGIN = 2, FUN = mean) \niris_column_mean\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    5.843333     3.057333     3.758000     1.199333 \n\n\n\n\n\nHint: It is possibly easiest to think of the “MARGIN” as the dimension you want to keep. In the above case, we want the mean for each variable, i.e. we want to keep columns and collapse rows.\n\n\n\nThe above is equivalent to:\n\niris_column_mean <- numeric(ncol(x))\nnames(iris_column_mean) <- names(x)\n\nfor (i in seq(x)) {\n  iris_column_mean[i] <- mean(x[, i])\n}\niris_column_mean\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    5.843333     3.057333     3.758000     1.199333 \n\n\nIf you wanted to get the mean of the rows (makes little sense in this case):\n\nhead(apply(x, 1, mean))\n\n[1] 2.550 2.375 2.350 2.350 2.550 2.850\n\n\n\n\n\napply() only works on objects with defined (i.e. non-NULL) dim(), i.e. arrays.\n\n\n\n\n\n\nTry to think why you can’t use apply() to apply a function fn() on a vector v.\n\n\n…\n\n\n…\n\n\nBecause that would be fn(v)"
  },
  {
    "objectID": "32_LoopFunctions.html#lapply",
    "href": "32_LoopFunctions.html#lapply",
    "title": "16  Loop Functions",
    "section": "16.2 lapply()",
    "text": "16.2 lapply()\n\n\n\nlapply() applies a function on each element of its input and returns a list of the outputs.\n\n\n\nNote: The ‘elements’ of a data frame are its columns (remember, a data frame is a list with equal-length elements). The ‘elements’ of a matrix are each cell one by one, by column. Therefore lapply() has a very different effect on a data frame and a matrix. lapply() is commonly used to iterate over the columns of a data frame.\nlapply() is the only function of the *apply() family that always returns a list.\n\niris.median <- lapply(iris[, -5], median)\niris.median\n\n$Sepal.Length\n[1] 5.8\n\n$Sepal.Width\n[1] 3\n\n$Petal.Length\n[1] 4.35\n\n$Petal.Width\n[1] 1.3\n\n\nThe above is equivalent to:\n\niris.median <- vector(\"list\", 4)\nnames(iris.median) <- colnames(iris[, -5])\nfor (i in 1:4) {\n  iris.median[[i]] <- median(iris[, 1])\n}"
  },
  {
    "objectID": "32_LoopFunctions.html#sapply",
    "href": "32_LoopFunctions.html#sapply",
    "title": "16  Loop Functions",
    "section": "16.3 sapply()",
    "text": "16.3 sapply()\nsapply() is an alias for lapply(), followed by a call to simplify2array().\n(Check the source code for sapply() by typing sapply at the console and hitting Enter).\n\n\n\nUnlike lapply(), the output of sapply() is variable: it is the simplest R object that can hold the data type(s) resulting from the operations, i.e. a vector, matrix, data frame, or list.\n\n\n\n\niris.median <- sapply(iris[, -5], median)\niris.median\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n        5.80         3.00         4.35         1.30 \n\n\n\niris.summary <- data.frame(Mean = sapply(iris[, -5], mean),\n                           SD = sapply(iris[, -5], sd))\niris.summary\n\n                 Mean        SD\nSepal.Length 5.843333 0.8280661\nSepal.Width  3.057333 0.4358663\nPetal.Length 3.758000 1.7652982\nPetal.Width  1.199333 0.7622377"
  },
  {
    "objectID": "32_LoopFunctions.html#vapply",
    "href": "32_LoopFunctions.html#vapply",
    "title": "16  Loop Functions",
    "section": "16.4 vapply()",
    "text": "16.4 vapply()\nMuch less commonly used (possibly underused) than lapply() or sapply(), vapply() allows you to specify what the expected output looks like - for example a numeric vector of length 2, a character vector of length 1.\nThis can have two advantages:\n\nIt is safer against errors\nIt will sometimes be a little faster\n\nYou add the argument FUN.VALUE which must be of the correct type and length of the expected result of each iteration.\n\nvapply(iris[, -5], median, FUN.VALUE = .1)\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n        5.80         3.00         4.35         1.30 \n\n\nHere, each iteration returns the median of each column, i.e. a numeric vector of length 1. Therefore FUN.VALUE can be any numeric scalar.\nFor example, if we instead returned the range of each column, FUN.VALUE should be a numeric vector of length 2:\n\nvapply(iris[, -5], range, FUN.VALUE = rep(.1, 2))\n\n     Sepal.Length Sepal.Width Petal.Length Petal.Width\n[1,]          4.3         2.0          1.0         0.1\n[2,]          7.9         4.4          6.9         2.5\n\n\nIf FUN.VALUE does not match the returned value, we get an informative error:\n\nvapply(iris[, -5], range, FUN.VALUE = .1)\n\nError in vapply(iris[, -5], range, FUN.VALUE = 0.1): values must be length 1,\n but FUN(X[[1]]) result is length 2"
  },
  {
    "objectID": "32_LoopFunctions.html#tapply",
    "href": "32_LoopFunctions.html#tapply",
    "title": "16  Loop Functions",
    "section": "16.5 tapply()",
    "text": "16.5 tapply()\ntapply() is one way (of many) to apply a function on subgroups of data as defined by one or more factors.\nIn the following example, we calculate the mean Sepal.Length by species on the iris dataset:\n\nmean_Sepal.Length_by_Species <- tapply(iris$Sepal.Length, iris$Species, mean)\nmean_Sepal.Length_by_Species\n\n    setosa versicolor  virginica \n     5.006      5.936      6.588 \n\n\nThe above is equivalent to:\n\nspecies <- levels(iris$Species)\nmean_Sepal.Length_by_Species <- vector(\"numeric\", length(species))\nnames(mean_Sepal.Length_by_Species) <- species\n\nfor (i in seq(species)) {\n  mean_Sepal.Length_by_Species[i] <- \n    mean(iris$Sepal.Length[iris$Species == species[i]])\n}\nmean_Sepal.Length_by_Species\n\n    setosa versicolor  virginica \n     5.006      5.936      6.588"
  },
  {
    "objectID": "32_LoopFunctions.html#mapply",
    "href": "32_LoopFunctions.html#mapply",
    "title": "16  Loop Functions",
    "section": "16.6 mapply()",
    "text": "16.6 mapply()\nThe above functions all work well when you iterating over elements of a single object.\nmapply() allows you to execute a function that accepts two or more inputs, say fn(x, z) using the i-th element of each input, and will return:\nfn(x[1], z[1]), fn(x[2], z[2]), …, fn(x[n], z[n])\nLet’s create a simple function that accepts two numeric arguments, and two vectors length 5 each:\n\nraise <- function(x, power) x^power\nx <- 2:6\np <- 6:2\n\nUse mapply to raise each x to the corresponding p:\n\nout <- mapply(raise, x, p)\nout\n\n[1]  64 243 256 125  36\n\n\nThe above is equivalent to:\n\nout <- vector(\"numeric\", 5)\nfor (i in seq(5)) {\n  out[i] <- raise(x[i], p[i])\n}\nout\n\n[1]  64 243 256 125  36"
  },
  {
    "objectID": "32_LoopFunctions.html#iterating-over-a-sequence-instead-of-an-object",
    "href": "32_LoopFunctions.html#iterating-over-a-sequence-instead-of-an-object",
    "title": "16  Loop Functions",
    "section": "16.7 Iterating over a sequence instead of an object",
    "text": "16.7 Iterating over a sequence instead of an object\nWith lapply(), sapply() and vapply() there is a very simple trick that may often come in handy:\nInstead of iterating over elements of an object, you can iterate over an integer index of whichever elements you want to access and use it accordingly within the anonymous function.\nThis alternative approach is much closer to how we would use an integer sequence in a for loop.\nIt will be clearer through an example:\nGet the mean of the first four columns of iris:\n\n# original way: iterate through elements i.e. columns:\nsapply(iris[, -5], function(i) mean(i))\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    5.843333     3.057333     3.758000     1.199333 \n\n# alternative way: iterate over integer index of elements:\nsapply(1:4, function(i) mean(iris[, i]))\n\n[1] 5.843333 3.057333 3.758000 1.199333\n\n# equivalent to:\nfor (i in 1:4) {\n  mean(iris[, i])\n}\n\nNotice that in this approach, since you are not passing the object (iris, in the above example) as the input to lapply(), it needs to be accessed within the anonymous function."
  },
  {
    "objectID": "32_LoopFunctions.html#applying-on-matrices-vs.-data-frames",
    "href": "32_LoopFunctions.html#applying-on-matrices-vs.-data-frames",
    "title": "16  Loop Functions",
    "section": "16.8 *apply()ing on matrices vs. data frames",
    "text": "16.8 *apply()ing on matrices vs. data frames\nTo consolidate some of what was learned above, let’s focus on the difference between working on a matrix vs. a data frame.\nFirst, let’s create a matrix and a data frame with the same data:\n\namat <- matrix(21:70, 10)\ncolnames(amat) <- paste0(\"Feature_\", 1:ncol(amat))\namat\n\n      Feature_1 Feature_2 Feature_3 Feature_4 Feature_5\n [1,]        21        31        41        51        61\n [2,]        22        32        42        52        62\n [3,]        23        33        43        53        63\n [4,]        24        34        44        54        64\n [5,]        25        35        45        55        65\n [6,]        26        36        46        56        66\n [7,]        27        37        47        57        67\n [8,]        28        38        48        58        68\n [9,]        29        39        49        59        69\n[10,]        30        40        50        60        70\n\nadf <- as.data.frame(amat)\nadf\n\n   Feature_1 Feature_2 Feature_3 Feature_4 Feature_5\n1         21        31        41        51        61\n2         22        32        42        52        62\n3         23        33        43        53        63\n4         24        34        44        54        64\n5         25        35        45        55        65\n6         26        36        46        56        66\n7         27        37        47        57        67\n8         28        38        48        58        68\n9         29        39        49        59        69\n10        30        40        50        60        70\n\n\nWe’ve seen that with apply() we specify the dimension to operate on and it works the same way on both matrices and data frames:\n\napply(amat, 2, mean)\n\nFeature_1 Feature_2 Feature_3 Feature_4 Feature_5 \n     25.5      35.5      45.5      55.5      65.5 \n\napply(adf, 2, mean)\n\nFeature_1 Feature_2 Feature_3 Feature_4 Feature_5 \n     25.5      35.5      45.5      55.5      65.5 \n\n\nHowever, sapply() (and lapply(), vapply()) acts on each element of the object, therefore it is not meaningful to pass a matrix to it:\n\nsapply(amat, mean)\n\n [1] 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45\n[26] 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70\n\n\nThe above returns the mean of each element, i.e. the element itself, which is pointless.\nSince a data frame is a list, and its columns are its elements, it works great for column operations on data frames:\n\nsapply(adf, mean)\n\nFeature_1 Feature_2 Feature_3 Feature_4 Feature_5 \n     25.5      35.5      45.5      55.5      65.5 \n\n\nIf you want to use sapply() on a matrix, you could iterate over an integer sequence as shown in the previous section:\n\nsapply(1:ncol(amat), function(i) mean(amat[, i]))\n\n[1] 25.5 35.5 45.5 55.5 65.5\n\n\nThis is shown to help emphasize the differences between the function and the data structures. In practice, you would use apply() on a matrix."
  },
  {
    "objectID": "32_LoopFunctions.html#anonfns",
    "href": "32_LoopFunctions.html#anonfns",
    "title": "16  Loop Functions",
    "section": "16.9 Anonymous functions",
    "text": "16.9 Anonymous functions\nAnonymous functions are just like regular functions but they are not assigned to an object - i.e. they are not “named”.\nThey are usually passed as arguments to other functions to be used once, hence no need to name them.\nIn R, anonymous functions are often used with the apply family of functions.\nExample of a simple regular function:\n\nsquared <- function(x) {\n  x^2\n}\n\nBecause this is a short function definition, it can also be written in a single line without curly brackets:\n\nsquared <- function(x) x^2\n\nThe equivalent anonymous function is the same, but omitting the assignment:\n\nfunction(x) x^2\n\nfunction(x) x^2\n\n\nLet’s use the squared() function within sapply() to square the first four columns of the iris dataset. In these examples, we often wrap functions around head() which prints the first few lines of an object to avoid:\n\nhead(iris[, 1:4])\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1          5.1         3.5          1.4         0.2\n2          4.9         3.0          1.4         0.2\n3          4.7         3.2          1.3         0.2\n4          4.6         3.1          1.5         0.2\n5          5.0         3.6          1.4         0.2\n6          5.4         3.9          1.7         0.4\n\niris_sq <- sapply(iris[, 1:4], squared)\nhead(iris_sq)\n\n     Sepal.Length Sepal.Width Petal.Length Petal.Width\n[1,]        26.01       12.25         1.96        0.04\n[2,]        24.01        9.00         1.96        0.04\n[3,]        22.09       10.24         1.69        0.04\n[4,]        21.16        9.61         2.25        0.04\n[5,]        25.00       12.96         1.96        0.04\n[6,]        29.16       15.21         2.89        0.16\n\n\nLet’s do the same as above, but this time using an anonymous function:\n\niris_sqtoo <- sapply(iris[, 1:4], function(x) x^2)\nhead(iris_sqtoo)\n\n     Sepal.Length Sepal.Width Petal.Length Petal.Width\n[1,]        26.01       12.25         1.96        0.04\n[2,]        24.01        9.00         1.96        0.04\n[3,]        22.09       10.24         1.69        0.04\n[4,]        21.16        9.61         2.25        0.04\n[5,]        25.00       12.96         1.96        0.04\n[6,]        29.16       15.21         2.89        0.16\n\n\nThe entire anonymous function definition is passed in the function argument (FUN in the R documentation)."
  },
  {
    "objectID": "40_DataFrames.html",
    "href": "40_DataFrames.html",
    "title": "16  Data frames",
    "section": "",
    "text": "R’s data.frame is the main object used to hold data for statistical analysis, visualization, and predictive modeling.\nSee the Data frames section in the Data Structures chapter for basic info on data.frames."
  },
  {
    "objectID": "40_DataFrames.html#column-and-row-names",
    "href": "40_DataFrames.html#column-and-row-names",
    "title": "16  Data frames",
    "section": "16.1 Column and row names",
    "text": "16.1 Column and row names\nLet’s start with a simple example data.frame:\n\ndf <- data.frame(PID = c(111:119),\n                 Hospital = c(\"UCSF\", \"HUP\", \"Stanford\",\n                             \"Stanford\", \"UCSF\", \"HUP\", \n                             \"HUP\", \"Stanford\", \"UCSF\"),\n                Age = c(22, 34, 41, 19, 53, 21, 63, 22, 19),\n                Sex = c(1, 1, 0, 1, 0, 0, 1, 0, 0))\ndf\n\n  PID Hospital Age Sex\n1 111     UCSF  22   1\n2 112      HUP  34   1\n3 113 Stanford  41   0\n4 114 Stanford  19   1\n5 115     UCSF  53   0\n6 116      HUP  21   0\n7 117      HUP  63   1\n8 118 Stanford  22   0\n9 119     UCSF  19   0\n\n\nThe optional row.names argument (see data.frame usage in the R documentation) can be used to define row names at the time of the data frame creation. It accepts either - a single integer or a character specifying a column of the data.frame being created whose values should be used as row names, or - a vector of values (character or integer) of the row names to be used\nFor example, we can use the “PID” column:\n\ndf <- data.frame(PID = c(111:119),\n                 Hospital = c(\"UCSF\", \"HUP\", \"Stanford\",\n                             \"Stanford\", \"UCSF\", \"HUP\", \n                             \"HUP\", \"Stanford\", \"UCSF\"),\n                Age = c(22, 34, 41, 19, 53, 21, 63, 22, 19),\n                Sex = c(1, 1, 0, 1, 0, 0, 1, 0, 0),\n                row.names = \"PID\")\n\n\n\n\nIt is recommended to not use/depend on row names to identify or index data.frames, and instead include a column of case IDs.\n\n\n\nWe can get column names and row names with colnames() and rownames(), respectively:\n\ncolnames(df)\n\n[1] \"Hospital\" \"Age\"      \"Sex\"     \n\nrownames(df)\n\n[1] \"111\" \"112\" \"113\" \"114\" \"115\" \"116\" \"117\" \"118\" \"119\"\n\n\nTo set new column or row names use the form:\ncolnames(df) <- new.colnames\nrownames(df) <- new.rownames\nwhere new.colnames and new.rownames is a character vector.\nYou can rename all columns/rows or use indexing to replace specific names:\nRename all rows:\n\nrownames(df) <- paste0(\"Patient_\", 1:9)\ndf\n\n          Hospital Age Sex\nPatient_1     UCSF  22   1\nPatient_2      HUP  34   1\nPatient_3 Stanford  41   0\nPatient_4 Stanford  19   1\nPatient_5     UCSF  53   0\nPatient_6      HUP  21   0\nPatient_7      HUP  63   1\nPatient_8 Stanford  22   0\nPatient_9     UCSF  19   0\n\n\nRename first two columns:\n\ncolnames(df)[1:2] <- c(\"Center\", \"Age_at_Dx\")\ndf\n\n            Center Age_at_Dx Sex\nPatient_1     UCSF        22   1\nPatient_2      HUP        34   1\nPatient_3 Stanford        41   0\nPatient_4 Stanford        19   1\nPatient_5     UCSF        53   0\nPatient_6      HUP        21   0\nPatient_7      HUP        63   1\nPatient_8 Stanford        22   0\nPatient_9     UCSF        19   0"
  },
  {
    "objectID": "40_DataFrames.html#delete-columns-or-rows",
    "href": "40_DataFrames.html#delete-columns-or-rows",
    "title": "16  Data frames",
    "section": "16.2 Delete columns or rows",
    "text": "16.2 Delete columns or rows\nTo delete a data.frame column, set it to NULL:\n\ndf$Sex <- NULL\ndf\n\n            Center Age_at_Dx\nPatient_1     UCSF        22\nPatient_2      HUP        34\nPatient_3 Stanford        41\nPatient_4 Stanford        19\nPatient_5     UCSF        53\nPatient_6      HUP        21\nPatient_7      HUP        63\nPatient_8 Stanford        22\nPatient_9     UCSF        19\n\n\nTo delete a data.frame row, you can “index it out”.\nFor example, to remove the third and fifths rows of the above data.frame using an integer index:\n\ndf <- df[-c(3, 5), ]\ndf\n\n            Center Age_at_Dx\nPatient_1     UCSF        22\nPatient_2      HUP        34\nPatient_4 Stanford        19\nPatient_6      HUP        21\nPatient_7      HUP        63\nPatient_8 Stanford        22\nPatient_9     UCSF        19\n\n\nYou can similarly exclude a row using a logical index. Logical indexing occurs usually following some filtering condition.\nFor example, exclude patients under 20 years old:\n\ndf <- df[!df$Age < 20, ]\ndf\n\n            Center Age_at_Dx\nPatient_1     UCSF        22\nPatient_2      HUP        34\nPatient_6      HUP        21\nPatient_7      HUP        63\nPatient_8 Stanford        22"
  },
  {
    "objectID": "40_DataFrames.html#subset",
    "href": "40_DataFrames.html#subset",
    "title": "16  Data frames",
    "section": "16.3 subset()",
    "text": "16.3 subset()\nsubset() allows you to\n\nfilter cases that meet certain conditions using the subset argument\nselect columns using the select argument\n\n(head() returns the first few lines of a data frame. We use it to avoid printing too many lines)\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\niris_sl.gt.med <- subset(iris, Sepal.Length > median(Sepal.Length))\n\nNote: You can use the column name Sepal.Length directly, i.e. unquoted and you don’t need to use iris$Sepal.Length. This is called Non-Standard Evaluation (NSE).\n\nx <- data.frame(one = 1:10,\n                two = rnorm(10),\n                group = c(rep(\"alpha\", 4),  rep(\"beta\", 6)))\nsubset(x, subset = two > 0, select = two)\n\n         two\n2  0.5856080\n4  0.1058498\n6  0.2447270\n9  0.4479935\n10 0.1625784\n\nsubset(x, two > 0, -one)\n\n         two group\n2  0.5856080 alpha\n4  0.1058498 alpha\n6  0.2447270  beta\n9  0.4479935  beta\n10 0.1625784  beta\n\nsubset(x, two > 0, two:one)\n\n         two one\n2  0.5856080   2\n4  0.1058498   4\n6  0.2447270   6\n9  0.4479935   9\n10 0.1625784  10\n\nsubset(x, two > 0, two:group)\n\n         two group\n2  0.5856080 alpha\n4  0.1058498 alpha\n6  0.2447270  beta\n9  0.4479935  beta\n10 0.1625784  beta"
  },
  {
    "objectID": "40_DataFrames.html#split",
    "href": "40_DataFrames.html#split",
    "title": "16  Data frames",
    "section": "16.4 split()",
    "text": "16.4 split()\nSplit a data frame into multiple data frames by groups defined by a factor:\n\nx_by_group <- split(x, x$group)\nx_by_group\n\n$alpha\n  one        two group\n1   1 -0.6487744 alpha\n2   2  0.5856080 alpha\n3   3 -0.1661124 alpha\n4   4  0.1058498 alpha\n\n$beta\n   one         two group\n5    5 -1.71623392  beta\n6    6  0.24472698  beta\n7    7 -0.02745815  beta\n8    8 -0.49893161  beta\n9    9  0.44799346  beta\n10  10  0.16257844  beta"
  },
  {
    "objectID": "40_DataFrames.html#with",
    "href": "40_DataFrames.html#with",
    "title": "16  Data frames",
    "section": "16.5 with()",
    "text": "16.5 with()\nWithin a with() expression, you can access list elements or data.frame columns without quoting or using the $ operator:\n\nwith(x, one + two)\n\n [1]  0.3512256  2.5856080  2.8338876  4.1058498  3.2837661  6.2447270\n [7]  6.9725419  7.5010684  9.4479935 10.1625784\n\nwith(x, x[group == \"alpha\", ])\n\n  one        two group\n1   1 -0.6487744 alpha\n2   2  0.5856080 alpha\n3   3 -0.1661124 alpha\n4   4  0.1058498 alpha\n\nwith(x, x[two > 0, ])\n\n   one       two group\n2    2 0.5856080 alpha\n4    4 0.1058498 alpha\n6    6 0.2447270  beta\n9    9 0.4479935  beta\n10  10 0.1625784  beta"
  },
  {
    "objectID": "40_DataFrames.html#feature-transformation-with-transform",
    "href": "40_DataFrames.html#feature-transformation-with-transform",
    "title": "16  Data frames",
    "section": "16.6 Feature transformation with transform()",
    "text": "16.6 Feature transformation with transform()\nMake up some data:\n\ndat <- data.frame(Sex = c(0, 0, 1, 1, 0),\n                  Height = c(1.5, 1.6, 1.55, 1.73, 1.8),\n                  Weight = c(55, 70, 69, 76, 91))\n\n\ndat <- transform(dat, BMI = Weight/Height^2)\ndat\n\n  Sex Height Weight      BMI\n1   0   1.50     55 24.44444\n2   0   1.60     70 27.34375\n3   1   1.55     69 28.72008\n4   1   1.73     76 25.39343\n5   0   1.80     91 28.08642\n\n\ntransform() is probably not used too often, because it is trivial to do the same with direct assignment:\n\ndat$BMI <- dat$Weight/dat$Height^2\n\nbut can be useful when adding multiple variables and/or used in a pipe:\n\ndat |> \n  subset(Sex == 0) |> \n  transform(DeltaWeightFromMean = Weight - mean(Weight),\n            BMI = Weight/Height^2,\n            CI = Weight/Height^3)\n\n  Sex Height Weight      BMI DeltaWeightFromMean       CI\n1   0    1.5     55 24.44444                 -17 16.29630\n2   0    1.6     70 27.34375                  -2 17.08984\n5   0    1.8     91 28.08642                  19 15.60357"
  },
  {
    "objectID": "40_DataFrames.html#identify-and-remove-duplicated-row-with-duplicated-and-unique",
    "href": "40_DataFrames.html#identify-and-remove-duplicated-row-with-duplicated-and-unique",
    "title": "16  Data frames",
    "section": "16.7 Identify and remove duplicated row with duplicated() and unique()",
    "text": "16.7 Identify and remove duplicated row with duplicated() and unique()\nThe duplicated() function when applied on a data.frame returns a logical index specifying the location of duplicated rows - specifically, of row which are the duplicate of another row further up the data.frame. This means that if rows 20 and 23 are identical, duplicated() will return TRUE for row 23.\nOn the other hand, unique() will remove duplicate rows from a data.frame.\n\nx <- data.frame(ID = c(203, 808, 909, 707, 808),\n                Age = c(23, 44, 33, 42, 44))\n\n\nduplicated(x)\n\n[1] FALSE FALSE FALSE FALSE  TRUE\n\n\n\nunique(x)\n\n   ID Age\n1 203  23\n2 808  44\n3 909  33\n4 707  42"
  },
  {
    "objectID": "42_TableJoins.html",
    "href": "42_TableJoins.html",
    "title": "17  Table joins",
    "section": "",
    "text": "We often have data from separate sources that we want to combine into a single data.frame. Table joins allow you to specify how to perform such a merge.\nScenario: You have received two tables with clinical data. Each table contains a column with a unique identifier (ID) plus a number of variables which are unique to each table. You want to merge them into one big table so that for each ID you have all available variables. You want to make sure that the same ID number (e.g. 108) corresponds to the same case in both datasets, but not all IDs needs to be present in both datasets.\nLet’s make up some synthetic data:\nThere are four main types of join operations:"
  },
  {
    "objectID": "42_TableJoins.html#merge",
    "href": "42_TableJoins.html#merge",
    "title": "17  Table joins",
    "section": "17.1 merge()",
    "text": "17.1 merge()\nThe merge() command in R is used to perform table joins.\nSyntax:\nmerge(x, y, by)\nwhere x and y and the two data.frames to join, and by is the column name of the ID variable used to identify rows. If the two datasets’ ID column has a different name, e.g. “PatientID” in one and “PID” in the other, you can either rename one of them, or probably best, use the following syntax:\nmerge(x, y, by.x, by.y)\nwhere by.x should be the name of the ID column for the x dataset and by.y should be the name of the ID column for the y dataset.\nIf you do not specify by or by.x and by.y arguments, merge() defaults to using the intersection of column names of the two input datasets. Look at the merge()’s documentation:\nby = intersect(names(x), names(y))\nIn our example datasets above, this works as expected and identifies “PID” as the common column:\n\nintersect(names(a), names(b))\n\n[1] \"PID\""
  },
  {
    "objectID": "42_TableJoins.html#inner-join",
    "href": "42_TableJoins.html#inner-join",
    "title": "17  Table joins",
    "section": "17.2 Inner join",
    "text": "17.2 Inner join\nThe default arguments of merge() perform an inner join:\n\n(ab.inner <- merge(a, b))\n\n  PID Hospital Age Sex  V1 Department\n1 106      HUP  21   0 153  Neurology\n2 107      HUP  63   1  89  Radiology\n3 108 Stanford  22   0 112  Emergency\n4 109     UCSF  19   0 228 Cardiology\n\n# same as\n(ab.inner <- merge(a, b, by = \"PID\"))\n\n  PID Hospital Age Sex  V1 Department\n1 106      HUP  21   0 153  Neurology\n2 107      HUP  63   1  89  Radiology\n3 108 Stanford  22   0 112  Emergency\n4 109     UCSF  19   0 228 Cardiology\n\n# same as\n(ab.inner <- merge(a, b, all = FALSE))\n\n  PID Hospital Age Sex  V1 Department\n1 106      HUP  21   0 153  Neurology\n2 107      HUP  63   1  89  Radiology\n3 108 Stanford  22   0 112  Emergency\n4 109     UCSF  19   0 228 Cardiology\n\n\nNote that the resulting table only contains cases found in both datasets, i.e. IDs 106 through 109"
  },
  {
    "objectID": "42_TableJoins.html#outer-join",
    "href": "42_TableJoins.html#outer-join",
    "title": "17  Table joins",
    "section": "17.3 Outer join",
    "text": "17.3 Outer join\nYou can perform an outer join by specifying all = TRUE:\n\n(ab.outer <- merge(a, b, all = TRUE))\n\n   PID Hospital Age Sex  V1 Department\n1  101     UCSF  22   1  NA       <NA>\n2  102      HUP  34   1  NA       <NA>\n3  103 Stanford  41   0  NA       <NA>\n4  104 Stanford  19   1  NA       <NA>\n5  105     UCSF  53   0  NA       <NA>\n6  106      HUP  21   0 153  Neurology\n7  107      HUP  63   1  89  Radiology\n8  108 Stanford  22   0 112  Emergency\n9  109     UCSF  19   0 228 Cardiology\n10 110     <NA>  NA  NA  91    Surgery\n11 111     <NA>  NA  NA 190  Neurology\n12 112     <NA>  NA  NA 101 Psychiatry\n\n(ab.outer <- merge(a, b, by = \"PID\", all = TRUE))\n\n   PID Hospital Age Sex  V1 Department\n1  101     UCSF  22   1  NA       <NA>\n2  102      HUP  34   1  NA       <NA>\n3  103 Stanford  41   0  NA       <NA>\n4  104 Stanford  19   1  NA       <NA>\n5  105     UCSF  53   0  NA       <NA>\n6  106      HUP  21   0 153  Neurology\n7  107      HUP  63   1  89  Radiology\n8  108 Stanford  22   0 112  Emergency\n9  109     UCSF  19   0 228 Cardiology\n10 110     <NA>  NA  NA  91    Surgery\n11 111     <NA>  NA  NA 190  Neurology\n12 112     <NA>  NA  NA 101 Psychiatry\n\n\nNote that the resulting data frame contains all cases found in either dataset and missing values are represented with NA."
  },
  {
    "objectID": "42_TableJoins.html#left-outer-join",
    "href": "42_TableJoins.html#left-outer-join",
    "title": "17  Table joins",
    "section": "17.4 Left outer join",
    "text": "17.4 Left outer join\nYou can perform a left outer join by specifying all.x = TRUE:\n\n(ab.leftOuter <- merge(a, b, all.x = TRUE))\n\n  PID Hospital Age Sex  V1 Department\n1 101     UCSF  22   1  NA       <NA>\n2 102      HUP  34   1  NA       <NA>\n3 103 Stanford  41   0  NA       <NA>\n4 104 Stanford  19   1  NA       <NA>\n5 105     UCSF  53   0  NA       <NA>\n6 106      HUP  21   0 153  Neurology\n7 107      HUP  63   1  89  Radiology\n8 108 Stanford  22   0 112  Emergency\n9 109     UCSF  19   0 228 Cardiology\n\n\nNote that the resulting data frame contains all cases present in the left input dataset (i.e. the one defined first in the arguments) only."
  },
  {
    "objectID": "42_TableJoins.html#right-outer-join",
    "href": "42_TableJoins.html#right-outer-join",
    "title": "17  Table joins",
    "section": "17.5 Right outer join",
    "text": "17.5 Right outer join\nYou can perform a right outer join by specifying all.y = TRUE:\n\n(ab.rightOuter <- merge(a, b, all.y = TRUE))\n\n  PID Hospital Age Sex  V1 Department\n1 106      HUP  21   0 153  Neurology\n2 107      HUP  63   1  89  Radiology\n3 108 Stanford  22   0 112  Emergency\n4 109     UCSF  19   0 228 Cardiology\n5 110     <NA>  NA  NA  91    Surgery\n6 111     <NA>  NA  NA 190  Neurology\n7 112     <NA>  NA  NA 101 Psychiatry\n\n\nNote how the resulting data frame contains all cases present in the right input dataset (i.e. the one defined seecond in the arguments) only."
  },
  {
    "objectID": "42_TableJoins.html#specifying-columns",
    "href": "42_TableJoins.html#specifying-columns",
    "title": "17  Table joins",
    "section": "17.6 Specifying columns",
    "text": "17.6 Specifying columns\nAs mentioned above, if the ID columns in the two data.frames to be merged do not have the same name, you can specify them directly:\n\na <- data.frame(PID = c(101:109),\n                Hospital = c(\"UCSF\", \"HUP\", \"Stanford\",\n                             \"Stanford\", \"UCSF\", \"HUP\", \n                             \"HUP\", \"Stanford\", \"UCSF\"),\n                Age = c(22, 34, 41, 19, 53, 21, 63, 22, 19),\n                Sex = c(1, 1, 0, 1, 0, 0, 1, 0, 0))\n\nb <- data.frame(PatientID = c(106:112),\n                 V1 = c(153, 89, 112, 228,  91, 190, 101),\n                 Department = c(\"Neurology\", \"Radiology\",\n                                \"Emergency\", \"Cardiology\",\n                                \"Surgery\", \"Neurology\", \"Psychiatry\"))\n\n\nmerge(a, b, by.x = \"PID\", by.y = \"PatientID\")\n\n  PID Hospital Age Sex  V1 Department\n1 106      HUP  21   0 153  Neurology\n2 107      HUP  63   1  89  Radiology\n3 108 Stanford  22   0 112  Emergency\n4 109     UCSF  19   0 228 Cardiology"
  },
  {
    "objectID": "44_Reshaping.html",
    "href": "44_Reshaping.html",
    "title": "18  Reshaping",
    "section": "",
    "text": "Wide and Long data format example. Take a moment to notice how the wide table on the left with 3 cases (3 IDs) and 3 variables gets converted from a 3 x 4 table to a 9 x 3 long table on the right. The values (outlined in magenta) are present once in each table: on the wide table they form an ID x Variable matrix, while on the long they are stacked on a single column. The IDs have to be repeated on the long table, once for each variable and there is a new ‘Variable’ column to provide the information present in the wide table’s column names.\nA wide dataset contains only a single row per case (e.g. patient), while a long dataset can contain multiple rows per case (e.g. for multiple timepoints). We want to be able to reshape from one form to the other because different programs (e.g. statistical models, visualization) may expect data in one of the other format for different applications (e.g. longitudinal modeling or grouped visualizations)."
  },
  {
    "objectID": "44_Reshaping.html#wide-to-long",
    "href": "44_Reshaping.html#wide-to-long",
    "title": "18  Reshaping",
    "section": "18.1 Wide to Long",
    "text": "18.1 Wide to Long\nLet’s create an example data frame:\n\ndat_wide <- data.frame(ID = c(1, 2, 3),\n                       mango = c(1.1, 2.1, 3.1),\n                       banana = c(1.2, 2.2, 3.2),\n                       tangerine = c(1.3, 2.3, 3.3),\n                       Group = c(\"a\", \"b\", \"b\"))\ndat_wide\n\n  ID mango banana tangerine Group\n1  1   1.1    1.2       1.3     a\n2  2   2.1    2.2       2.3     b\n3  3   3.1    3.2       3.3     b\n\n\n\n18.1.1 base\nThe reshape() function is probably one of the more complicated builtin functions because its documentation is not entirely clear, especially if you’re not used to the jargon and specifically with regards to which arguments refer to the input vs. output data frame. Use the following figure as a guide to understand reshape()’s syntax. You can use it as a reference when building your own reshape() command by following steps 1 through 5:\n\n\n\n\n\nreshape() syntax for Wide to Long transformation.\n\n\n\n\n\ndat_wide2long <- reshape(# Data in wide format\n                         data = dat_wide,\n                         # The column name that defines case ID\n                         idvar = \"ID\",\n                         # The columns whose values we want to keep\n                         varying = list(2:4),\n                         # The name of the new column which will contain all \n                         # the values from the columns above\n                         v.names = \"Score\",\n                         # The values/names, of length = (N columns in \"varying\"), \n                         #that will be recycled to indicate which column from the \n                         #wide dataset each row corresponds to\n                         times = c(colnames(dat_wide)[2:4]),\n                         # The name of the new column created to hold the values \n                         # defined by \"times\"\n                         timevar = \"Fruit\",                  \n                         direction = \"long\")\ndat_wide2long\n\n            ID Group     Fruit Score\n1.mango      1     a     mango   1.1\n2.mango      2     b     mango   2.1\n3.mango      3     b     mango   3.1\n1.banana     1     a    banana   1.2\n2.banana     2     b    banana   2.2\n3.banana     3     b    banana   3.2\n1.tangerine  1     a tangerine   1.3\n2.tangerine  2     b tangerine   2.3\n3.tangerine  3     b tangerine   3.3\n\n\nYou can also define varying with a character vector:\nvarying = list(c(\"mango\", \"banana\",\"tangerine\")\nExplore the resulting data frame’s attributes:\n\nattributes(dat_wide2long)\n\n$row.names\n[1] \"1.mango\"     \"2.mango\"     \"3.mango\"     \"1.banana\"    \"2.banana\"   \n[6] \"3.banana\"    \"1.tangerine\" \"2.tangerine\" \"3.tangerine\"\n\n$names\n[1] \"ID\"    \"Group\" \"Fruit\" \"Score\"\n\n$class\n[1] \"data.frame\"\n\n$reshapeLong\n$reshapeLong$varying\n$reshapeLong$varying[[1]]\n[1] \"mango\"     \"banana\"    \"tangerine\"\n\n\n$reshapeLong$v.names\n[1] \"Score\"\n\n$reshapeLong$idvar\n[1] \"ID\"\n\n$reshapeLong$timevar\n[1] \"Fruit\"\n\n\nThese attributes are present if and only if a long data.frame was created from a wide data.frame as above. In this case, reshaping back to wide format is as easy as calling reshape() on the previously converted data.frame with no arguments:\n\ndat_wideagain <- reshape(dat_wide2long)\ndat_wideagain\n\n        ID Group mango banana tangerine\n1.mango  1     a   1.1    1.2       1.3\n2.mango  2     b   2.1    2.2       2.3\n3.mango  3     b   3.1    3.2       3.3\n\n\nNote that the reverse does not work, you need to specify the wide to long reshaping normally.\n\n\n18.1.2 tidyr\n\ndat_wide2long_tv <- pivot_longer(dat_wide,\n                           cols = 2:4,\n                           names_to = \"Fruit\",\n                           values_to = \"Score\")\ndat_wide2long_tv\n\n# A tibble: 9 × 4\n     ID Group Fruit     Score\n  <dbl> <chr> <chr>     <dbl>\n1     1 a     mango       1.1\n2     1 a     banana      1.2\n3     1 a     tangerine   1.3\n4     2 b     mango       2.1\n5     2 b     banana      2.2\n6     2 b     tangerine   2.3\n7     3 b     mango       3.1\n8     3 b     banana      3.2\n9     3 b     tangerine   3.3\n\n\n\n\n18.1.3 data.table\n\ndat_wide_dt <- as.data.table(dat_wide)\ndat_wide2long_dt <- melt(dat_wide_dt,\n                         id.vars = c(1, 5),\n                         measure.vars = 2:4,\n                         variable.name = \"Fruit\",\n                         value.name = \"Score\")\nsetorder(dat_wide2long_dt, \"ID\")\ndat_wide2long_dt\n\n   ID Group     Fruit Score\n1:  1     a     mango   1.1\n2:  1     a    banana   1.2\n3:  1     a tangerine   1.3\n4:  2     b     mango   2.1\n5:  2     b    banana   2.2\n6:  2     b tangerine   2.3\n7:  3     b     mango   3.1\n8:  3     b    banana   3.2\n9:  3     b tangerine   3.3"
  },
  {
    "objectID": "44_Reshaping.html#long-to-wide",
    "href": "44_Reshaping.html#long-to-wide",
    "title": "18  Reshaping",
    "section": "18.2 Long to Wide",
    "text": "18.2 Long to Wide\nLet’s recreate the same long dataset:\n\ndat_long <- data.frame(ID = c(1, 2, 3, 1, 2, 3, 1, 2, 3),\n                       Fruit = c(\"mango\", \"mango\", \"mango\", \n                                 \"banana\", \"banana\", \"banana\", \n                                 \"tangerine\", \"tangerine\", \"tangerine\"),\n                       Score = c(1.1, 2.1, 3.1, 1.2, 2.2, 3.2, 1.3, 2.3, 3.3),\n                       Group = c(\"a\", \"b\", \"b\", \"a\", \"b\", \"b\", \"a\", \"b\", \"b\"))\ndat_long\n\n  ID     Fruit Score Group\n1  1     mango   1.1     a\n2  2     mango   2.1     b\n3  3     mango   3.1     b\n4  1    banana   1.2     a\n5  2    banana   2.2     b\n6  3    banana   3.2     b\n7  1 tangerine   1.3     a\n8  2 tangerine   2.3     b\n9  3 tangerine   3.3     b\n\n\n\n18.2.1 base\nUsing base reshape() for long-to-wide transformation is simpler than wide-to-long:\n\n\n\n\n\nreshape() syntax for Long to Wide transformation.\n\n\n\n\n\ndat_long2wide <- reshape(dat_long,\n                         idvar = \"ID\",\n                         timevar = \"Fruit\",\n                         v.names = \"Score\",\n                         direction = \"wide\")\n# Optionally rename columns\ncolnames(dat_long2wide) <- gsub(\"Score.\", \"\", colnames(dat_long2wide))\ndat_long2wide\n\n  ID Group mango banana tangerine\n1  1     a   1.1    1.2       1.3\n2  2     b   2.1    2.2       2.3\n3  3     b   3.1    3.2       3.3\n\n\n\n\n18.2.2 tidyr\n\ndat_long2wide_tv <- pivot_wider(dat_long,\n                                id_cols = c(\"ID\", \"Group\"),\n                                names_from = \"Fruit\",\n                                values_from = \"Score\")\ndat_long2wide_tv\n\n# A tibble: 3 × 5\n     ID Group mango banana tangerine\n  <dbl> <chr> <dbl>  <dbl>     <dbl>\n1     1 a       1.1    1.2       1.3\n2     2 b       2.1    2.2       2.3\n3     3 b       3.1    3.2       3.3\n\n\n\n\n18.2.3 data.table\ndata.table’s long to wide procedure is defined with a convenient formula notation:\n\ndat_long_dt <- as.data.table(dat_long)\ndat_long2wide_dt <- dcast(dat_long_dt,\n                          ID + Group ~ Fruit,\n                          value.var = \"Score\")\ndat_long2wide_dt\n\n   ID Group banana mango tangerine\n1:  1     a    1.2   1.1       1.3\n2:  2     b    2.2   2.1       2.3\n3:  3     b    3.2   3.1       3.3"
  },
  {
    "objectID": "46_DataTrans.html#continuous-variables",
    "href": "46_DataTrans.html#continuous-variables",
    "title": "19  Data Transformations",
    "section": "19.1 Continuous variables",
    "text": "19.1 Continuous variables\n\n19.1.1 Standardization / Scaling & Centering with scale()\nScaling of a numeric vector is achieved by elementwise division with the standard deviation. A scaled vector therefore has standard deviation equal to 1.\nCentering of a numeric vector is achieved by elementwise subtraction of its mean. A centered vector therefore has mean equal to 0.\nStandardizing, a.k.a. converting to Z-scores, involves scaling and centering. Scaling and centering is performed in R with the scale() function.\nDepending on your modeling needs / the algorithms you plan to use, it is often important to scale and/or center your data. Note that many functions, but not all, may automatically scale and center data internally if it is required by the algorithm. Check the function documentation to see if you should manually scale or not.\nscale() can be applied to a single vector or a matrix/data.frame. In the case of a matrix or data.frame, scaling is applied on each column individually. By default, both arguments scale and center are set to TRUE.\nScale a vector:\n\nhead(iris$Sepal.Length)\n\n[1] 5.1 4.9 4.7 4.6 5.0 5.4\n\nPetal.Length_scaled <- scale(iris$Petal.Length)\nhead(Petal.Length_scaled)\n\n          [,1]\n[1,] -1.335752\n[2,] -1.335752\n[3,] -1.392399\n[4,] -1.279104\n[5,] -1.335752\n[6,] -1.165809\n\n\nScale multiple columns of a matrix/data.frame:\n\niris.scaled <- scale(iris[, -5])\nhead(iris.scaled)\n\n     Sepal.Length Sepal.Width Petal.Length Petal.Width\n[1,]   -0.8976739  1.01560199    -1.335752   -1.311052\n[2,]   -1.1392005 -0.13153881    -1.335752   -1.311052\n[3,]   -1.3807271  0.32731751    -1.392399   -1.311052\n[4,]   -1.5014904  0.09788935    -1.279104   -1.311052\n[5,]   -1.0184372  1.24503015    -1.335752   -1.311052\n[6,]   -0.5353840  1.93331463    -1.165809   -1.048667\n\n\nFirst, let’s verify that scale() did what we wanted:\n\ncolMeans(iris.scaled)\n\n Sepal.Length   Sepal.Width  Petal.Length   Petal.Width \n-1.457168e-15 -1.638319e-15 -1.292300e-15 -5.543714e-16 \n\n\n\napply(iris.scaled, 2, sd)\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n           1            1            1            1 \n\n\nWe got effectively 0 mean and standard deviation of 1 for each column.\nscale() outputs the scaled vector(s) along with the scaling and/or centering parameters saved as attributes in the output.\nNote that in both cases, whether a vector input or data.frame, the output is a matrix:\n\nclass(Petal.Length_scaled)\n\n[1] \"matrix\" \"array\" \n\nclass(iris.scaled)\n\n[1] \"matrix\" \"array\" \n\n\nGet the output attributes:\n\nattributes(Petal.Length_scaled)\n\n$dim\n[1] 150   1\n\n$`scaled:center`\n[1] 3.758\n\n$`scaled:scale`\n[1] 1.765298\n\n\ncenter is the mean:\n\nmean(iris$Petal.Length)\n\n[1] 3.758\n\n\nscale is the standard deviation:\n\nsd(iris$Petal.Length)\n\n[1] 1.765298\n\n\nFor a matrix/data.frame input, you get center and scale attributes per column:\n\nattributes(iris.scaled)\n\n$dim\n[1] 150   4\n\n$dimnames\n$dimnames[[1]]\nNULL\n\n$dimnames[[2]]\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\" \n\n\n$`scaled:center`\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    5.843333     3.057333     3.758000     1.199333 \n\n$`scaled:scale`\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n   0.8280661    0.4358663    1.7652982    0.7622377 \n\n\nLet’s save the scale and center attributes and then double check the calculations:\n\n.center <- attr(iris.scaled, \"scaled:center\")\n.center\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    5.843333     3.057333     3.758000     1.199333 \n\n.scale <- attr(iris.scaled, \"scaled:scale\")\n.scale\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n   0.8280661    0.4358663    1.7652982    0.7622377 \n\nSepal.Length_scaled <- (iris$Sepal.Length - .center[1]) / .scale[1]\nall(Sepal.Length_scaled == iris.scaled[, \"Sepal.Length\"])\n\n[1] TRUE\n\n\nNote: Due to limitation in numerical precision, checking sets of floats for equality after multiple operations is not recommended. One simple option is to plot:\n\nmplot3_fit(Sepal.Length_scaled, iris.scaled[, \"Sepal.Length\"])\n\n\n\n\n\n\n\nIf you are manually scaling and/or centering data for supervised learning, you must:\n\n\n\nPerform scaling and centering on your training data\n\n\nSave the centering and scaling parameters for each feature\n\n\nApply the training set-derived centering and scaling parameters to the test set prior to prediction/inference.\n\n\n\n\nA common mistake is to either scale training and testing data together in the beginning, or scale them independently.\n\n\n19.1.2 Normalization\nNormalization has different meanings in different contexts; in the context of a numeric variable it usually refers to converting to a 0-1 range.\nLet’s write a simple function to achieve this:\n\nnormalize <- function(x) {\n  .min <- min(x, na.rm = TRUE)\n  (x - .min) / max(x - .min, na.rm = TRUE)\n}\n\n\nx <- rnorm(20, 13, 1.4)\nx\n\n [1] 12.22644 12.39197 14.13380 13.72045 11.43102 11.63011 12.82893 13.28091\n [9] 11.90526 14.78913 12.53989 15.22782 14.86845 10.50048 12.62079 14.32180\n[17] 13.42731 12.14975 15.88160 13.56729\n\n\n\nx_normalized <- normalize(x)\nx_normalized\n\n [1] 0.3207430 0.3515049 0.6751981 0.5983823 0.1729270 0.2099246 0.4327062\n [8] 0.5167004 0.2610566 0.7969805 0.3789938 0.8785051 0.8117213 0.0000000\n[15] 0.3940277 0.7101342 0.5439073 0.3064909 1.0000000 0.5699207\n\nmin(x_normalized)\n\n[1] 0\n\nmax(x_normalized)\n\n[1] 1\n\n\nNote that it is easy to make the normalize() function more general, by adding lo and hi arguments to convert to any range:\n\ndr <- function(x, lo = 0, hi = 1) {\n    .min <- min(x, na.rm = TRUE)\n   (x - .min) / max(x - .min, na.rm = TRUE) * (hi - lo) + lo\n  }\n\n\ndr(x, -1, 1)\n\n [1] -0.35851402 -0.29699024  0.35039611  0.19676470 -0.65414598 -0.58015075\n [7] -0.13458764  0.03340082 -0.47788671  0.59396106 -0.24201241  0.75701014\n[13]  0.62344269 -1.00000000 -0.21194463  0.42026832  0.08781456 -0.38701815\n[19]  1.00000000  0.13984147\n\n\n\n\n19.1.3 Log-transform with log()\nFor the following example, x is an unknown feature in a new dataset we were just given.\nWe start by plotting its distribution:\n\nmplot3_x(x)\n\n\n\n\nWe can see it is skewed right. A log transform can help here:\n\nmplot3_x(log(x))\n\n\n\n\n\n\n19.1.4 Data binning with cut()\nA different approach for the above variable might be to bin it.\nLet’s look at a few different ways to bin continuous data.\n\n19.1.4.1 Evenly-spaced interval\ncut() allows us to bin a numeric variable into evenly-spaced intervals.\nThe breaks argument defines the number of intervals:\n\nx_cut4 <- cut(x, breaks = 4)\nhead(x_cut4)\n\n[1] (0.291,178] (0.291,178] (0.291,178] (0.291,178] (0.291,178] (0.291,178]\nLevels: (0.291,178] (178,355] (355,533] (533,711]\n\ntable(x_cut4)\n\nx_cut4\n(0.291,178]   (178,355]   (355,533]   (533,711] \n        977          19           3           1 \n\n\n\n\n\nInterval Notation\n\n\n[3, 9) represents the interval of real numbers between 3 and 9, including 3 and excluding 9.\n\n\n\nBecause the data is so skewed, equal intervals are not helpful in this case. The majority of the data points get grouped into a single bin.\nLet’s visualize the cuts:\n\nxcuts5 <- seq(min(x), max(x), length.out = 5)\nxcuts5\n\n[1]   1.0000 178.2453 355.4905 532.7358 709.9811\n\n\n\nmplot3_x(x, par.reset = FALSE)\n# plot(density(x)) # in base R\nabline(v = xcuts5, col = \"red\", lwd = 1.5)\n\n\n\n\n[Note: We used par.reset = FALSE to stop mplot3_x() from resetting its custom par() settings so that we can continue adding elements to the same plot, in this case with the abline() command.]\n\n\n19.1.4.2 Quantile cuts\nInstead of evenly-spaced intervals, we can get quantiles with quantile(). We ask for 5 quantiles using the length.out argument, which corresponds to 4 intervals:\n\nxquants5 <- quantile(x, seq(0, 1, length.out = 5))\nxquants5 <- quantile(x, seq(0, 1, length.out = 5))\nmplot3_x(x, par.reset = F)\n# plot(density(x)) # in base R\nabline(v = xquants5, col = \"green\", lwd = 1.5)\n\n\n\n\nThe breaks argument of cut() allows us to pass either an integer to define the number of evenly-spaced breaks, or a numeric vector defining the position of breaks.\nWe can therefore pass the quantile values as break points.\nSince the quantile values begin at the lowest value in the data, we need to define include.lowest = TRUE so that the first interval is inclusive of the lowest value:\n\nx_cutq4 <- cut(x, breaks = xquants5, include.lowest = TRUE)\ntable(x_cutq4)\n\nx_cutq4\n   [1,11.5] (11.5,23.2] (23.2,47.2]  (47.2,710] \n        250         250         250         250 \n\n\nWith quantile cuts, each bin contains roughly the same number of observations (+/- 1)."
  },
  {
    "objectID": "46_DataTrans.html#categorical-variables",
    "href": "46_DataTrans.html#categorical-variables",
    "title": "19  Data Transformations",
    "section": "19.2 Categorical variables",
    "text": "19.2 Categorical variables\nMany algorithms (or their implementations) do not directly support categorical variables. To use them, you must therefore convert all categorical variables to some type of numerical encoding.\n\n19.2.1 Integer encoding\nIf the categorical data is ordinal, you can simply convert it to integers.\nFor example, the following ordered factor:\n\nbrightness <- factor(c(\"bright\", \"brightest\", \"darkest\",\n                        \"bright\", \"dark\", \"dim\", \"dark\"),\n                      levels = c(\"darkest\", \"dark\", \"dim\", \"bright\", \"brightest\"),\n                      ordered = TRUE)\nbrightness\n\n[1] bright    brightest darkest   bright    dark      dim       dark     \nLevels: darkest < dark < dim < bright < brightest\n\n\n…can be directly coerced to an integer:\n\nas.integer(brightness)\n\n[1] 4 5 1 4 2 3 2\n\n\n\n\n19.2.2 One-hot encoding\nWhen categorical features are not ordinal, and your algorithm cannot handle them directly, you can one-hot encode them. In one-hot encoding, each categorical feature is converted to k binary features, where k = number of unique values in the input, such that only one feature has the value 1 per case. This is similar to creating dummy variables in statistics, with the difference that dummy variables create k - 1 new variables.\n\nset.seed(21)\nadmission_reasons <- c(\"plannedSurgery\", \"emergencySurgery\", \"medical\")\nadmission <- sample(admission_reasons, 12, T)\nadmission\n\n [1] \"medical\"          \"plannedSurgery\"   \"medical\"          \"plannedSurgery\"  \n [5] \"emergencySurgery\" \"medical\"          \"plannedSurgery\"   \"medical\"         \n [9] \"medical\"          \"emergencySurgery\" \"emergencySurgery\" \"emergencySurgery\"\n\n\nMultiple packages include functions that perform one-hot encoding. It’s a simple operation and we don’t necessarily need to install a large package with many dependencies.\nLet’s write a simple function to perform one-hot encoding. (Note: you may have heard that for loops can be slow in R, but that depends on the operations performed. Here, we loop over an integer matrix and it is plenty fast)\n\nonehot <- function(x, xname = NULL) {\n  if (is.null(xname)) xname <- deparse(substitute(x))\n  x <- factor(x)\n  .levels <- levels(x)      # Get factor levels\n  ncases <- NROW(x)         # Get number of cases\n  index <- as.integer(x)    # Convert levels to integer\n  oh <- matrix(0, ncases, length(.levels))   # Initialize zeros matrix\n  colnames(oh) <- paste(xname, .levels, sep = \"_\")  # Name columns by levels\n  for (i in seq(ncases)) oh[i, index[i]] <- 1  # Assign \"1\" to appropriate level\n  oh\n}\n\nLet’s apply our new function to the admission vector:\n\nonehot(admission)\n\n      admission_emergencySurgery admission_medical admission_plannedSurgery\n [1,]                          0                 1                        0\n [2,]                          0                 0                        1\n [3,]                          0                 1                        0\n [4,]                          0                 0                        1\n [5,]                          1                 0                        0\n [6,]                          0                 1                        0\n [7,]                          0                 0                        1\n [8,]                          0                 1                        0\n [9,]                          0                 1                        0\n[10,]                          1                 0                        0\n[11,]                          1                 0                        0\n[12,]                          1                 0                        0\n\n\nNote: deparse(substitute(x)) above is used to automatically get the name of the input object (in this case “admission”). This is similar to how many of R’s internal functions (e.g. plot()) get the names of input objects."
  },
  {
    "objectID": "48_StringOps.html#reminder-create-coerce-check",
    "href": "48_StringOps.html#reminder-create-coerce-check",
    "title": "20  String Operations",
    "section": "20.1 Reminder: create, coerce, check",
    "text": "20.1 Reminder: create, coerce, check\n\ncharacter(): Initialize empty character vector\nas.character(): Coerce any vector to a character vector\nis.character(): Check object is character\n\n\nx <- character(10)\nx\n\n [1] \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\n\n\n\nv <- c(10, 20, 22, 43)\nx <- as.character(v)\nx\n\n[1] \"10\" \"20\" \"22\" \"43\"\n\n\n\nx <- c(\"PID\", \"Age\", \"Sex\", \"Handedness\")\nis.character(x)\n\n[1] TRUE"
  },
  {
    "objectID": "48_StringOps.html#cat-concatenate-and-print",
    "href": "48_StringOps.html#cat-concatenate-and-print",
    "title": "20  String Operations",
    "section": "20.2 cat(): Concatenate and print",
    "text": "20.2 cat(): Concatenate and print\ncat() concatenates strings in order to print to screen (console) or to file.\nIt does not return any object. It is therefore useful to produce informative messages in your programs.\n\nsbp <- 130\ntemp <- 98.4\ncat(\"The blood pressure was\", sbp, \"and the temperature was\", temp, \"\\n\")\n\nThe blood pressure was 130 and the temperature was 98.4 \n\n\nUse the file argument to write to a text file. The append argument allows using multiple cat() calls to append to the same file."
  },
  {
    "objectID": "48_StringOps.html#paste-concatenate-character-vectors",
    "href": "48_StringOps.html#paste-concatenate-character-vectors",
    "title": "20  String Operations",
    "section": "20.3 paste(): Concatenate character vectors",
    "text": "20.3 paste(): Concatenate character vectors\npaste() is a commonly used command.\nIn its simplest form, it acts like as.character():\n\nv <- c(10, 20, 22, 43)\npaste(v)\n\n[1] \"10\" \"20\" \"22\" \"43\"\n\n\nBut its main job is to combine strings from multiple vectors elementwise:\n\nid = c(\"001\", \"010\", \"018\", \"020\", \"021\", \"051\")\ndept = c(\"Emergency\", \"Cardiology\", \"Neurology\",\n         \"Anesthesia\", \"Surgery\", \"Psychiatry\")\nid\n\n[1] \"001\" \"010\" \"018\" \"020\" \"021\" \"051\"\n\ndept\n\n[1] \"Emergency\"  \"Cardiology\" \"Neurology\"  \"Anesthesia\" \"Surgery\"   \n[6] \"Psychiatry\"\n\n\n\npaste(id, dept)\n\n[1] \"001 Emergency\"  \"010 Cardiology\" \"018 Neurology\"  \"020 Anesthesia\"\n[5] \"021 Surgery\"    \"051 Psychiatry\"\n\n\nThe sep argument defines the separator:\n\npaste(id, dept, sep = \"+++\")\n\n[1] \"001+++Emergency\"  \"010+++Cardiology\" \"018+++Neurology\"  \"020+++Anesthesia\"\n[5] \"021+++Surgery\"    \"051+++Psychiatry\"\n\n\npaste0() is an alias for the commonly used paste(..., sep = \"\"):\n\npaste0(id, dept)\n\n[1] \"001Emergency\"  \"010Cardiology\" \"018Neurology\"  \"020Anesthesia\"\n[5] \"021Surgery\"    \"051Psychiatry\"\n\n\nAs with other vectorized operations, value recycling can be very convenient:\n\npaste0(\"Feature_\", 1:10)\n\n [1] \"Feature_1\"  \"Feature_2\"  \"Feature_3\"  \"Feature_4\"  \"Feature_5\" \n [6] \"Feature_6\"  \"Feature_7\"  \"Feature_8\"  \"Feature_9\"  \"Feature_10\"\n\n\nThe argument collapse helps output a single character element after collapsing with some string:\n\npaste0(\"Feature_\", 1:10, collapse = \", \")\n\n[1] \"Feature_1, Feature_2, Feature_3, Feature_4, Feature_5, Feature_6, Feature_7, Feature_8, Feature_9, Feature_10\""
  },
  {
    "objectID": "48_StringOps.html#nchar-get-number-of-characters-in-element",
    "href": "48_StringOps.html#nchar-get-number-of-characters-in-element",
    "title": "20  String Operations",
    "section": "20.4 nchar(): Get number of characters in element",
    "text": "20.4 nchar(): Get number of characters in element\nnchar() counts the number of characters in each element of type character in a vector:\n\nx <- c(\"a\", \"bb\", \"ccc\")\nnchar(x)\n\n[1] 1 2 3"
  },
  {
    "objectID": "48_StringOps.html#substr-get-substring",
    "href": "48_StringOps.html#substr-get-substring",
    "title": "20  String Operations",
    "section": "20.5 substr(): Get substring",
    "text": "20.5 substr(): Get substring\nsubstr() allows you to get and set individual (literal) characters from a character (R base type) vector, by position:\n\n20.5.1 Extract\ne.g. Get characters 1-3:\n\nx <- c(\"001Emergency\", \"010Cardiology\", \"018Neurology\", \n       \"020Anesthesia\", \"021Surgery\", \"051Psychiatry\")\nsubstr(x, start = 1, stop = 3)\n\n[1] \"001\" \"010\" \"018\" \"020\" \"021\" \"051\"\n\n\nNeither start nor stop need to be valid character positions.\nFor example, if you want to get all characters from the fourth one to the last one, you can specify a very large stop\n\nsubstr(x, 4, 99)\n\n[1] \"Emergency\"  \"Cardiology\" \"Neurology\"  \"Anesthesia\" \"Surgery\"   \n[6] \"Psychiatry\"\n\n\nIf you start with too high an index, you end up with empty strings:\n\nsubstr(x, 20, 24)\n\n[1] \"\" \"\" \"\" \"\" \"\" \"\"\n\n\nNote: substring() is also available, with similar syntax to substr(): (first, last) instead of (start, stop). It is available for compatibility with S (check its source code to see how it’s an alias for substr())\n\n\n20.5.2 Replace\n\nx <- c(\"Jan_1987\")\nx\n\n[1] \"Jan_1987\"\n\n\nReplace the first three letters:\n\nsubstr(x, 1, 3) <- \"Feb\"\nx\n\n[1] \"Feb_1987\"\n\n\nNote that if the replacement is longer, it is “cropped” to the length of the substring being replaced:\n\nsubstr(x, 1, 3) <- \"April\"\nx\n\n[1] \"Apr_1987\""
  },
  {
    "objectID": "48_StringOps.html#strsplit-split-strings",
    "href": "48_StringOps.html#strsplit-split-strings",
    "title": "20  String Operations",
    "section": "20.6 strsplit(): Split strings",
    "text": "20.6 strsplit(): Split strings\nstrsplit() allows you to split a character vector elements based on any character or regular expression\n\nx <- \"This is one sentence\"\nstrsplit(x, \" \")\n\n[[1]]\n[1] \"This\"     \"is\"       \"one\"      \"sentence\"\n\n\n\nx <- \"14,910\"\nstrsplit(x, \",\")\n\n[[1]]\n[1] \"14\"  \"910\"\n\n\nAs with any functions, you can compose string operations in complex ways (though it may often be considerably easier to perform multiple separate operations instead):\n\nx <- c(\"1,950\", \"2,347\")\nx\n\n[1] \"1,950\" \"2,347\"\n\n\n\nlapply(strsplit(x, \",\"), function(i) \n  paste(i, c(\"thousand\", \"dollars\"), collapse = \" and \"))\n\n[[1]]\n[1] \"1 thousand and 950 dollars\"\n\n[[2]]\n[1] \"2 thousand and 347 dollars\""
  },
  {
    "objectID": "48_StringOps.html#string-formatting",
    "href": "48_StringOps.html#string-formatting",
    "title": "20  String Operations",
    "section": "20.7 String formatting",
    "text": "20.7 String formatting\n\n20.7.1 Change case with toupper() and tolower()\n\nfeatures <- c(\"id\", \"age\", \"sex\", \"sbp\", \"dbp\", \"hct\", \"urea\", \"creatinine\")\nfeatures\n\n[1] \"id\"         \"age\"        \"sex\"        \"sbp\"        \"dbp\"       \n[6] \"hct\"        \"urea\"       \"creatinine\"\n\n\n\nfeatures_upper <- toupper(features)\nfeatures_upper\n\n[1] \"ID\"         \"AGE\"        \"SEX\"        \"SBP\"        \"DBP\"       \n[6] \"HCT\"        \"UREA\"       \"CREATININE\"\n\n\n\nfeatures_lower <- tolower(features_upper)\nfeatures_lower\n\n[1] \"id\"         \"age\"        \"sex\"        \"sbp\"        \"dbp\"       \n[6] \"hct\"        \"urea\"       \"creatinine\"\n\n\n\n\n20.7.2 abbreviate()\nabbreviate() allows to reduce elements of a character vector to short, unique abbreviations of a minimumn length (defaults to 4)\n\nx <- c(\"Emergency\", \"Cardiology\", \"Surgery\", \"Anesthesia\", \"Neurology\", \"Psychiatry\", \"Clinical Psychology\")\nabbreviate(x)\n\n          Emergency          Cardiology             Surgery          Anesthesia \n             \"Emrg\"              \"Crdl\"              \"Srgr\"              \"Anst\" \n          Neurology          Psychiatry Clinical Psychology \n             \"Nrlg\"              \"Psyc\"              \"ClnP\" \n\nabbreviate(x, minlength = 4)\n\n          Emergency          Cardiology             Surgery          Anesthesia \n             \"Emrg\"              \"Crdl\"              \"Srgr\"              \"Anst\" \n          Neurology          Psychiatry Clinical Psychology \n             \"Nrlg\"              \"Psyc\"              \"ClnP\" \n\nabbreviate(x, minlength = 5)\n\n          Emergency          Cardiology             Surgery          Anesthesia \n            \"Emrgn\"             \"Crdlg\"             \"Srgry\"             \"Ansth\" \n          Neurology          Psychiatry Clinical Psychology \n            \"Nrlgy\"             \"Psych\"             \"ClncP\""
  },
  {
    "objectID": "48_StringOps.html#pattern-matching",
    "href": "48_StringOps.html#pattern-matching",
    "title": "20  String Operations",
    "section": "20.8 Pattern matching",
    "text": "20.8 Pattern matching\nA very common task in programming is to find +/- replace string patterns in a vector of strings.\ngrep() and grepl() help find strings that contain a given pattern.\nsub() and gsub() help find and replace strings.\n\n20.8.1 grep(): Get an integer index of elements that include a pattern\n\nx <- c(\"001Age\", \"002Sex\", \"010Temp\", \"014SBP\", \"018Hct\", \"022PFratio\", \"030GCS\", \"112SBP-DBP\")\ngrep(pattern = \"SBP\", x = x)\n\n[1] 4 8\n\n\ngrep()’s value arguments which defaults to FALSE, allows returning the matched string itself (the value of the element) instead of its integer index:\n\ngrep(\"SBP\", x, value = TRUE)\n\n[1] \"014SBP\"     \"112SBP-DBP\"\n\n\n\n\n20.8.2 grepl(): Get a logical index of elements that include a pattern\ngrepl() is similar to grep(), but returns a logical index instead:\n\ngrepl(\"SBP\", x)\n\n[1] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE\n\n\n\n\n20.8.3 sub(): Find replace first match of a pattern\n\nx <- c(\"The most important variable was PF ratio. Other significant variables are listed in the supplementary information.\")\nsub(pattern = \"variable\", replacement = \"feature\", x = x)\n\n[1] \"The most important feature was PF ratio. Other significant variables are listed in the supplementary information.\"\n\n\n“First match” refers to each element of a character vector:\n\nx <- c(\"var 1, var 2\", \"var 3, var 4\")\nsub(\"var\", \"feat\", x)\n\n[1] \"feat 1, var 2\" \"feat 3, var 4\"\n\n\n\n\n20.8.4 gsub(): Find and replace all matches of a pattern\n\nx <- c(\"The most important variable was PF ratio. Other significant variables are listed in the supplementary information.\")\ngsub(pattern = \"variable\", replacement = \"feature\", x = x)\n\n[1] \"The most important feature was PF ratio. Other significant features are listed in the supplementary information.\"\n\n\n“All matches” means all matches across all elements:\n\nx <- c(\"var 1, var 2\", \"var 3, var 4\")\ngsub(\"var\", \"feat\", x)\n\n[1] \"feat 1, feat 2\" \"feat 3, feat 4\""
  },
  {
    "objectID": "48_StringOps.html#regex",
    "href": "48_StringOps.html#regex",
    "title": "20  String Operations",
    "section": "20.9 Regular expressions",
    "text": "20.9 Regular expressions\nRegular expressions allow you to perform flexible pattern matching. For example, you can look for a pattern specifically at the beginning or the end of a word, or for a variable pattern with certain characteristics.\nRegular expressions are very powerful and heavily used. They exist in multiple programming languages - with many similarities and some differences in their syntax.\nThere are many rules in defining regular expressions and take a little getting used to. You can read the R manual by typing ?base::regex.\nSome of the most important rules are liste below:\n\n20.9.1 Match a pattern at the beginning of a line/string with ^/\\\\<:\nUse the caret sign ^ in the beginning of a pattern to only match strings that begin with this pattern.\npattern 012 matches both 2nd and 3rd elements:\n\nx <- c(\"001xyz993\", \"012qwe764\", \"029aqw012\")\nx\n\n[1] \"001xyz993\" \"012qwe764\" \"029aqw012\"\n\ngrep(\"012\", x)\n\n[1] 2 3\n\n\nBy adding ^ or \\\\<, only the 2nd element matches:\n\ngrep(\"^012\", x)\n\n[1] 2\n\ngrep(\"\\\\<012\", x)\n\n[1] 2\n\n\n\n\n20.9.2 Match a pattern at the end of a line/string with $/\\\\>\nThe dollar sign $ is used at the end of a pattern to only match strings which end with this pattern:\n\nx\n\n[1] \"001xyz993\" \"012qwe764\" \"029aqw012\"\n\ngrep(\"012$\", x)\n\n[1] 3\n\ngrep(\"012\\\\>\", x)\n\n[1] 3\n\n\n\nx <- c(\"1one\", \"2one\", \"3two\", \"3three\")\ngrep(\"one$\", x)\n\n[1] 1 2\n\ngrep(\"one\\\\>\", x)\n\n[1] 1 2\n\n\n\n\n20.9.3 .: Match any character\n\ngrep(\"e.X\", c(\"eX\", \"enX\", \"ennX\", \"ennnX\", \"ennnnX\"))\n\n[1] 2\n\n\n\n\n20.9.4 +: Match preceding character one or more times:\n\ngrep(\"en+X\", c(\"eX\", \"enX\", \"ennX\", \"ennnX\", \"ennnnX\"))\n\n[1] 2 3 4 5\n\n\n\n\n20.9.5 {n}: Match preceding character n times:\n\ngrep(\"en{2}X\", c(\"eX\", \"enX\", \"ennX\", \"ennnX\", \"ennnnX\"))\n\n[1] 3\n\n\n\n\n20.9.6 {n,}: Match preceding character n or more times:\n\ngrep(\"en{2,}X\", c(\"eX\", \"enX\", \"ennX\", \"ennnX\", \"ennnnX\"))\n\n[1] 3 4 5\n\n\n\n\n20.9.7 {n,m}: Match preceding character at least n times and no more than m times:\n\ngrep(\"en{2,3}X\", c(\"eX\", \"enX\", \"ennX\", \"ennnX\", \"ennnnX\"))\n\n[1] 3 4\n\n\n\n\n20.9.8 Character classes\nYou can define a set of characters to be matched using square brackets. Any number of the characters in the set will be matched.\nFor example match and replace $ and/or @ with an underscore:\n\nx <- c(\"Feat1$alpha\", \"Feat2$gamma@5\", \"Feat9@zeta2\")\ngsub(\"[$@]\", \"_\", x)\n\n[1] \"Feat1_alpha\"   \"Feat2_gamma_5\" \"Feat9_zeta2\"  \n\n\n\n20.9.8.1 Predefined character classes\nA number of character classes are predefined. Slightly confusingly, they are themselves surrounded by brackets and to use them as a character class, you need a seconds set of brackets around them. Some of the most common ones include:\n\n[:alnum:]: alphanumeric, i.e. all letters and numbers\n[:alpha:]: all letters\n[:digit:]: all numbers\n[:lower:]: all lowercase letters\n[:upper:]: all uppercase letters\n[:punct:]: all punctuation characters (! ” # $ % & ’ ( ) * + , - . / : ; < = > ? @ [  ] ^ _ ` { | } ~.)\n[:blank:]: all spaces and tabs\n[:space:]: all spaces, tabs, newline characters, and some more\n\nLet’s look at some examples using them.\nHere we use [:digit:] to remove all numbers:\n\nx <- c(\"001Emergency\", \"010Cardiology\", \"018Neurology\", \"020Anesthesia\", \n       \"021Surgery\", \"051Psychiatry\")\nx\n\n[1] \"001Emergency\"  \"010Cardiology\" \"018Neurology\"  \"020Anesthesia\"\n[5] \"021Surgery\"    \"051Psychiatry\"\n\ngsub(\"[[:digit:]]\", \"\", x)\n\n[1] \"Emergency\"  \"Cardiology\" \"Neurology\"  \"Anesthesia\" \"Surgery\"   \n[6] \"Psychiatry\"\n\n\nWe can use [:alpha:] to remove all letters instead:\n\ngsub(\"[[:alpha:]]\", \"\", x)\n\n[1] \"001\" \"010\" \"018\" \"020\" \"021\" \"051\"\n\n\nWe can use a caret ^ in the beginning of a character class to match any character not in the character set:\n\nx <- c(\"001$Emergency\", \"010@Cardiology\", \"018*Neurology\", \"020!Anesthesia\", \n       \"021!Surgery\", \"051*Psychiatry\")\ngsub(\"[^[:alnum:]]\", \"_\", x)\n\n[1] \"001_Emergency\"  \"010_Cardiology\" \"018_Neurology\"  \"020_Anesthesia\"\n[5] \"021_Surgery\"    \"051_Psychiatry\"\n\n\n\n\n\n20.9.9 Combining character classes\nUse | to match from multiple character classes:\n\nx <- c(\"123#$%alphaBeta\")\ngsub(\"[[:digit:]|[:punct:]]\", \"\", x)\n\n[1] \"alphaBeta\"\n\n\n\n\n\nFor more information on regular expressions, start by reading the built-in documentation: ?regex\n\n\n\n\n\n20.9.10 Escaping metacharacters\nMetacharacters are characters that have a special meaning within a regular expression. They include:\n. \\ | ( ) [ { ^ $ * + ?.\nFor example, we have seen above that the period matches any character and the square brackets are used to define character classes If you want to match one of these characters itself, you must “escape” it using a double backslash. Escaping a character simply means “this is not part of a regular expression, match it as is”.\nFor example, to match a period (.) and replace it with underscores:\n\nx <- c(\"systolic.blood.pressure\", \"diastolic.blood.pressure\")\nx\n\n[1] \"systolic.blood.pressure\"  \"diastolic.blood.pressure\"\n\ngsub(\"\\\\.\", \"_\", x)\n\n[1] \"systolic_blood_pressure\"  \"diastolic_blood_pressure\"\n\n\nIf we didn’t escape the period above, it would have matched every character:\n\ngsub(\".\", \"_\", x)\n\n[1] \"_______________________\"  \"________________________\"\n\n\nAnother example, include an escaped metacharacter within a regular expression. In the example below we want to remove everything up to and including the dollar sign:\n\nx <- c(\"df$ID\", \"df$Age\")\ngsub(\".*\\\\$\", \"\", x)\n\n[1] \"ID\"  \"Age\"\n\n\nOur regular expression .*\\\\$, decomposed:\n\n.: match any character\n.*: match any character any number of times\n.*\\\\$: match any character any number of times till you find a dollar sign\n\nIf we had not escaped the $, it wouldn’t have worked:\n\ngsub(\".*$\", \"\", x)\n\n[1] \"\" \"\""
  },
  {
    "objectID": "50_DateTime.html",
    "href": "50_DateTime.html",
    "title": "21  Date and Time",
    "section": "",
    "text": "R includes builtin support for working with date +/- time data. A number of external packages further extend this support.\nThere are three builtin classes:\nBackground info: Portable Operating System Interface (POSIX) is a set of standards for maintaining compatibility among operating systems."
  },
  {
    "objectID": "50_DateTime.html#date-objects",
    "href": "50_DateTime.html#date-objects",
    "title": "21  Date and Time",
    "section": "21.1 Date objects",
    "text": "21.1 Date objects\n\n21.1.1 Character to Date: as.Date()\nYou can create a Date object from a string:\n\nx <- as.Date(\"1981-02-12\")\nx\n\n[1] \"1981-02-12\"\n\nclass(x)\n\n[1] \"Date\"\n\n\nThe tryFormats argument defines which formats are recognized.\nThe default is tryFormats = c(\"%Y-%m-%d\", \"%Y/%m/%d\"), i.e. a date of the form “2020-11-16” or “2020/11/16”\n\n\n21.1.2 Get current date & time\nGet current data:\n\ntoday <- Sys.Date()\ntoday\n\n[1] \"2022-05-10\"\n\nclass(today)\n\n[1] \"Date\"\n\n\nGet current date and time:\n\nnow <- Sys.time()\nnow\n\n[1] \"2022-05-10 16:55:48 PDT\"\n\nclass(now)\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\nGet local timezone:\n\nSys.timezone()\n\n[1] \"America/Los_Angeles\"\n\n\n\n\n21.1.3 Math on Dates\nThe reason we care about Date objects in R is because we can apply useful mathematical operations on them.\nFor example, we can substract date objects to get time intervals:\n\nstart_date <- as.Date(\"2020-09-15\")\ntime_diff <- Sys.Date() - start_date\ntime_diff\n\nTime difference of 602 days\n\nclass(time_diff)\n\n[1] \"difftime\"\n\n\nNote: While you can use the subtraction operator -, it is advised you use the difftime() function to perform subtraction on dates instead, because it allows you to specify units:\n\ntimepoint1 <- as.Date(\"2020-01-07\")\ntimepoint2 <- as.Date(\"2020-02-03\")\ndifftime(timepoint2, timepoint1, units = \"weeks\")\n\nTime difference of 3.857143 weeks\n\ndifftime(timepoint2, timepoint1, units = \"days\")\n\nTime difference of 27 days\n\ndifftime(timepoint2, timepoint1, units = \"hours\")\n\nTime difference of 648 hours\n\ndifftime(timepoint2, timepoint1, units = \"mins\")\n\nTime difference of 38880 mins\n\ndifftime(timepoint2, timepoint1, units = \"secs\")\n\nTime difference of 2332800 secs\n\n\n\n\n\nWhy is there no option for “months” or “years” in units?\n\n\nThink about it.\n\n\nBecause, unlike seconds, minutes, hours, days, and weeks, months and years do not have fixed length, i.e. literally a month or a year are not “units” of time.\n\n\nYou can always get a difference in days and divide by 365 (or 365.242.\n\n\n\n\nDOB <- as.Date(\"1969-08-04\")\nAge <- Sys.Date() - DOB\nAge\n\nTime difference of 19272 days\n\ncat(\"Age today is\", round(Age/365), \"years\")\n\nAge today is 53 years\n\n\n\n\n21.1.4 mean/median Date\n\nx <- as.Date(c(5480, 5723, 5987, 6992), origin = \"1970-01-01\")\nx\n\n[1] \"1985-01-02\" \"1985-09-02\" \"1986-05-24\" \"1989-02-22\"\n\nmean(x)\n\n[1] \"1986-07-21\"\n\nmedian(x)\n\n[1] \"1986-01-12\"\n\n\nTo check the median, we can do a mathematical operation using mmultiplication subtraction and addition, and the result is still a Date(!):\n\nx[2] + .5 * (x[3] - x[2])\n\n[1] \"1986-01-12\"\n\n\n\n\n21.1.5 Sequence of dates\nYou can create a sequence of dates using seq().\nIf an integer is passed to by, the unit is assumed to be days:\n\nstart_date <- as.Date(\"2020-09-14\")\nend_date <- as.Date(\"2020-12-07\")\nseq(from = start_date, to = end_date, by = 7)\n\n [1] \"2020-09-14\" \"2020-09-21\" \"2020-09-28\" \"2020-10-05\" \"2020-10-12\"\n [6] \"2020-10-19\" \"2020-10-26\" \"2020-11-02\" \"2020-11-09\" \"2020-11-16\"\n[11] \"2020-11-23\" \"2020-11-30\" \"2020-12-07\"\n\n\nUnlike mathematical operations like difftime() which require strict units of time, seq() can work with months and years.\nby can be one of:\n“day”, “week”, “month”, “quarter”, “year”.\nThe above is therefore equivalent to:\n\nseq(from = start_date, to = end_date, by = \"week\")\n\n [1] \"2020-09-14\" \"2020-09-21\" \"2020-09-28\" \"2020-10-05\" \"2020-10-12\"\n [6] \"2020-10-19\" \"2020-10-26\" \"2020-11-02\" \"2020-11-09\" \"2020-11-16\"\n[11] \"2020-11-23\" \"2020-11-30\" \"2020-12-07\"\n\n\nAs with numeric sequences, you can also define the length.out argument:\n\nstart_date <- as.Date(\"2020-01-20\")\nseq(from = start_date, by = \"year\", length.out = 4)\n\n[1] \"2020-01-20\" \"2021-01-20\" \"2022-01-20\" \"2023-01-20\"\n\n\nAn integer can be provided as part of character input to by:\n\nstart_date <- as.Date(\"2020-01-20\")\nend_date <- as.Date(\"2021-01-20\")\nseq(start_date, end_date, by = \"2 months\")\n\n[1] \"2020-01-20\" \"2020-03-20\" \"2020-05-20\" \"2020-07-20\" \"2020-09-20\"\n[6] \"2020-11-20\" \"2021-01-20\""
  },
  {
    "objectID": "50_DateTime.html#date-time-objects",
    "href": "50_DateTime.html#date-time-objects",
    "title": "21  Date and Time",
    "section": "21.2 Date-Time objects",
    "text": "21.2 Date-Time objects\n\n21.2.1 Character to Date-Time: as.POSIXct(), as.POSIXlt(), strptime():\n(As always, it can be very informative to look at the source code. Many of these functions call eachother internally)\nRead strptime()’s documentation for conversion specifications. These define the order and format of characters to be read as year, month, day, hour, minute, and second information.\nFor example, the ISO 8601 international standard is defined as:\n\"%Y-%m-%d %H:%M:%S\"\n\n%Y: Year with century, (0-9999 accepted) e.g. 2020\n%m: Month, 01-12, e.g. 03\n%d: Day, 01-31, e.g. 04\n%H: Hours, 00-23, e.g. 13\n%M: Minutes, 00-59, e.g. 38\n%S: Seconds, 00-61 (!) allowing for up to two leap seconds, e.g. 54\n\n\ndt <- \"2020-03-04 13:38:54\"\ndt\n\n[1] \"2020-03-04 13:38:54\"\n\nclass(dt)\n\n[1] \"character\"\n\n\nUse attributres() to see the difference between the POSIXct and POSIXlt classes:\n\ndt_posixct <- as.POSIXct(dt)\ndt_posixct\n\n[1] \"2020-03-04 13:38:54 PST\"\n\nclass(dt_posixct)\n\n[1] \"POSIXct\" \"POSIXt\" \n\nstr(dt_posixct)\n\n POSIXct[1:1], format: \"2020-03-04 13:38:54\"\n\nattributes(dt_posixct)\n\n$class\n[1] \"POSIXct\" \"POSIXt\" \n\n$tzone\n[1] \"\"\n\n\n\ndt_posixlt <- as.POSIXlt(dt)\ndt_posixlt\n\n[1] \"2020-03-04 13:38:54 PST\"\n\nclass(dt_posixlt)\n\n[1] \"POSIXlt\" \"POSIXt\" \n\nstr(dt_posixlt)\n\n POSIXlt[1:1], format: \"2020-03-04 13:38:54\"\n\ndt_posixlt$year\n\n[1] 120\n\nattributes(dt_posixlt)\n\n$names\n [1] \"sec\"    \"min\"    \"hour\"   \"mday\"   \"mon\"    \"year\"   \"wday\"   \"yday\"  \n [9] \"isdst\"  \"zone\"   \"gmtoff\"\n\n$class\n[1] \"POSIXlt\" \"POSIXt\" \n\n\nYou can compose a really large number of combination formats to match your data.\n\ndt2 <- c(\"03.04.20 01:38.54 pm\")\ndt2_posix <- as.POSIXct(dt2, format = \"%m.%d.%y %I:%M.%S %p\")\ndt2_posix\n\n[1] \"2020-03-04 13:38:54 PST\""
  },
  {
    "objectID": "50_DateTime.html#format-dates",
    "href": "50_DateTime.html#format-dates",
    "title": "21  Date and Time",
    "section": "21.3 format() Dates",
    "text": "21.3 format() Dates\nformat() operates on Date and POSIX objects to convert between representations\nDefine Date in US format:\n\ndt_us <- as.Date(\"07-04-2020\", format = \"%m-%d-%Y\")\ndt_us\n\n[1] \"2020-07-04\"\n\n\nConvert to European format:\n\ndt_eu <- format(dt_us, \"%d.%m.%y\")\ndt_eu\n\n[1] \"04.07.20\""
  },
  {
    "objectID": "50_DateTime.html#extract-partial-date-information",
    "href": "50_DateTime.html#extract-partial-date-information",
    "title": "21  Date and Time",
    "section": "21.4 Extract partial date information",
    "text": "21.4 Extract partial date information\n\nweekdays(): Get name of day of the week\nmonths(): Get name of month\nquarters(): Get quarter\njulia(): Get number of days since a specific origin\n\n\nx <- as.Date(c(18266, 18299, 18359, 18465), origin = \"1970-01-01\")\nx\n\n[1] \"2020-01-05\" \"2020-02-07\" \"2020-04-07\" \"2020-07-22\"\n\n\n\nweekdays(x)\n\n[1] \"Sunday\"    \"Friday\"    \"Tuesday\"   \"Wednesday\"\n\nmonths(x)\n\n[1] \"January\"  \"February\" \"April\"    \"July\"    \n\nquarters(x)\n\n[1] \"Q1\" \"Q1\" \"Q2\" \"Q3\"\n\njulian(x)\n\n[1] 18266 18299 18359 18465\nattr(,\"origin\")\n[1] \"1970-01-01\"\n\njulian(x, origin = as.Date(\"2020-01-01\"))\n\n[1]   4  37  97 203\nattr(,\"origin\")\n[1] \"2020-01-01\""
  },
  {
    "objectID": "50_DateTime.html#handling-dates-with-lubridate",
    "href": "50_DateTime.html#handling-dates-with-lubridate",
    "title": "21  Date and Time",
    "section": "21.5 Handling dates with lubridate",
    "text": "21.5 Handling dates with lubridate\nInstead of defining Date and/or time formats using POSIX standard abbreviations, we can let the lubridate package do some guesswork for us, which works well most of the time.\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\ndt <- c(\"2020-03-04 13:38:54\")\ndt_posix <- as_datetime(dt)\ndt_posix\n\n[1] \"2020-03-04 13:38:54 UTC\"\n\n\nNote that timezone defaults to UTC (Coordinated Universal Time) and must be set manually. PST is defined with “America/Los_Angeles” or the (officially deprecated) “US/Pacific” (tz database)\n\ndt_posix <- as_datetime(dt, tz = \"America/Los_Angeles\")\ndt_posix\n\n[1] \"2020-03-04 13:38:54 PST\"\n\n\n\ndt2_posix <- as_datetime(dt2)\ndt2_posix\n\n[1] \"2003-04-20 13:38:54 UTC\"\n\n\ndt2 got misinterpreted as year-month-day.\nFor these cases, lubridate includes a number of convenient functions to narrow down the guessing. The functions are named using all permutations of y, m, and d. The letter order signifies the order the information appears in the character you are trying to import, i.e. ymd, dmy, mdy, ydm, myd\n\ndt2 <- c(\"03.04.20 01:38.54 pm\")\ndt2_posix <- mdy_hms(dt2, tz = \"America/Los_Angeles\")\ndt2_posix\n\n[1] \"2020-03-04 13:38:54 PST\""
  },
  {
    "objectID": "52_MissingData.html",
    "href": "52_MissingData.html",
    "title": "22  Handling Missing data",
    "section": "",
    "text": "Missing data is a very common issue in statistics and data science.\nData may be missing for a variety of reasons. We often characterize the type of missingness using the following three types (Mack, Su, and Westreich 2018):"
  },
  {
    "objectID": "52_MissingData.html#check-for-missing-data",
    "href": "52_MissingData.html#check-for-missing-data",
    "title": "22  Handling Missing data",
    "section": "22.1 Check for missing data",
    "text": "22.1 Check for missing data\nYou can use your favorite base R commands to check for missing data, count NA elements by row, by column, total, etc.\nLet’s load the PimaIndiansDiabetes2 dataset from the mlbench package and make a copy of it to variable dat. Remember to check the class of a new object you didn’t create yourself with class(), check its dimensions, if applicable, with dim(), and a get a summary of its structure including data types with str():\n\ndata(\"PimaIndiansDiabetes2\", package = \"mlbench\")\ndat <- PimaIndiansDiabetes2\nclass(dat)\n\n[1] \"data.frame\"\n\ndim(dat)\n\n[1] 768   9\n\nstr(dat)\n\n'data.frame':   768 obs. of  9 variables:\n $ pregnant: num  6 1 8 1 0 5 3 10 2 8 ...\n $ glucose : num  148 85 183 89 137 116 78 115 197 125 ...\n $ pressure: num  72 66 64 66 40 74 50 NA 70 96 ...\n $ triceps : num  35 29 NA 23 35 NA 32 NA 45 NA ...\n $ insulin : num  NA NA NA 94 168 NA 88 NA 543 NA ...\n $ mass    : num  33.6 26.6 23.3 28.1 43.1 25.6 31 35.3 30.5 NA ...\n $ pedigree: num  0.627 0.351 0.672 0.167 2.288 ...\n $ age     : num  50 31 32 21 33 30 26 29 53 54 ...\n $ diabetes: Factor w/ 2 levels \"neg\",\"pos\": 2 1 2 1 2 1 2 1 2 2 ...\n\n\nCheck if there are any missing values in the data.frame with anyNA():\n\nanyNA(dat)\n\n[1] TRUE\n\n\nThe above suggests there is one or more NA values in the dataset.\nWe can create a logical index of NA values using is.na(). Remember that the output of is.na() is a logical matrix with the same dimensions as the dataset:\n\nna_index <- is.na(dat)\ndim(na_index)\n\n[1] 768   9\n\nhead(na_index)\n\n  pregnant glucose pressure triceps insulin  mass pedigree   age diabetes\n1    FALSE   FALSE    FALSE   FALSE    TRUE FALSE    FALSE FALSE    FALSE\n2    FALSE   FALSE    FALSE   FALSE    TRUE FALSE    FALSE FALSE    FALSE\n3    FALSE   FALSE    FALSE    TRUE    TRUE FALSE    FALSE FALSE    FALSE\n4    FALSE   FALSE    FALSE   FALSE   FALSE FALSE    FALSE FALSE    FALSE\n5    FALSE   FALSE    FALSE   FALSE   FALSE FALSE    FALSE FALSE    FALSE\n6    FALSE   FALSE    FALSE    TRUE    TRUE FALSE    FALSE FALSE    FALSE\n\n\nOne way to count missing values is with sum(is.na()). Remember that a logical array is coerced to an integer array for mathematical operations, where TRUE becomes 1 and FALSE becomes 0. Therefore, calling sum() on a logical index counts the number of TRUE elements (and since we are applying it on the index of NA values, it counts the number of elements with missing values):\n\nsum(is.na(dat))\n\n[1] 652\n\n\nThere are 652 NA values in total in the data.frame.\nLet’s count the number of missing values per feature, i.e. per column, using sapply()(#sapply):\n\nsapply(dat, function(i) sum(is.na(i)))\n\npregnant  glucose pressure  triceps  insulin     mass pedigree      age \n       0        5       35      227      374       11        0        0 \ndiabetes \n       0 \n\n\nThe features insulin and triceps have the most NA values.\nLet’s count the number of missing values per case (i.e. row):\n\nsapply(1:nrow(dat), function(i) sum(is.na(dat[i, ])))\n\n  [1] 1 1 2 0 0 2 0 3 0 3 2 2 2 0 0 3 0 2 0 0 0 2 2 1 0 0 2 0 0 2 1 0 0 2 1 0 2\n [38] 1 1 0 0 2 1 0 2 1 2 1 1 4 0 0 0 0 0 1 0 0 2 0 4 2 2 0 2 1 1 2 0 0 0 0 2 0\n [75] 1 2 2 1 3 1 1 4 0 1 2 0 1 0 0 1 2 0 0 2 0 0 1 0 0 0 2 2 2 0 2 0 2 0 0 0 0\n[112] 0 0 2 0 2 2 2 1 0 0 1 0 2 2 0 0 0 0 2 0 2 0 1 0 0 0 0 2 0 2 1 0 2 0 2 1 0\n[149] 2 1 0 2 0 0 2 1 0 0 0 0 1 0 0 1 2 0 1 2 2 0 2 0 2 0 0 0 2 0 2 2 2 0 1 2 2\n[186] 1 0 0 0 0 2 0 2 3 1 0 2 0 0 0 1 2 1 0 0 1 0 2 0 1 1 1 1 0 0 0 0 0 1 2 0 2\n[223] 3 0 0 0 2 1 0 0 2 0 0 2 0 2 0 1 1 2 1 0 2 0 0 1 2 0 0 1 2 2 0 1 0 1 1 1 0\n[260] 0 0 3 1 1 2 0 3 1 2 3 1 0 2 0 2 0 1 0 2 0 2 0 0 2 2 0 0 0 0 0 0 0 0 0 2 0\n[297] 0 0 0 2 3 0 0 2 2 0 0 0 0 0 1 0 0 0 1 0 0 2 0 2 0 1 1 0 1 0 0 2 0 0 1 0 3\n[334] 2 0 0 3 2 0 2 0 0 2 2 2 0 0 3 0 2 2 2 1 0 2 2 0 2 0 0 0 2 1 2 0 0 2 1 0 0\n[371] 0 1 0 0 0 0 0 0 2 0 0 1 0 0 0 0 1 1 0 0 0 2 0 0 2 0 0 1 2 1 2 2 0 1 2 0 2\n[408] 2 2 0 1 0 0 0 0 0 1 1 2 0 0 0 0 1 0 0 4 0 0 0 3 0 0 2 1 3 1 2 1 2 1 0 0 2\n[445] 1 0 0 0 0 0 0 2 0 3 0 1 2 0 0 0 0 2 0 1 2 0 0 0 3 0 1 1 1 2 2 1 0 0 0 1 0\n[482] 1 0 0 3 0 0 0 1 2 0 1 1 0 4 2 2 0 0 0 0 1 2 0 1 2 0 0 0 2 1 0 2 2 0 0 0 2\n[519] 2 0 0 0 4 2 2 1 0 0 0 2 0 2 0 3 0 3 2 2 0 0 0 0 1 0 0 0 0 0 0 1 1 0 2 0 0\n[556] 0 1 2 1 2 2 0 0 0 2 0 0 0 0 0 2 2 0 0 0 0 0 2 2 1 1 1 1 2 0 1 2 2 0 3 1 0\n[593] 2 0 0 0 2 0 2 0 1 3 1 0 3 1 0 0 0 0 0 0 0 1 0 2 2 0 1 3 0 1 2 0 2 0 2 2 2\n[630] 1 2 0 2 0 2 2 2 0 0 0 0 2 2 3 0 0 0 0 0 1 0 0 0 2 0 0 0 0 2 0 2 1 0 0 1 0\n[667] 1 1 0 0 0 1 0 0 2 2 2 2 2 0 0 1 0 2 3 0 2 1 0 0 2 2 0 0 2 0 0 3 0 2 0 1 1\n[704] 3 0 1 4 0 2 0 0 0 1 0 2 0 0 1 0 1 1 0 0 0 2 1 0 1 2 2 0 2 0 0 2 1 0 1 0 2\n[741] 0 0 0 2 0 0 1 0 0 2 2 0 1 0 1 0 1 2 2 2 0 1 2 0 1 0 2 1\n\n\nIf we wanted to get the row with the most missing values, we can use which.max():\n\nwhich.max(sapply(1:nrow(dat), function(i) sum(is.na(dat[i, ]))))\n\n[1] 50\n\n\n\nsum(is.na(dat[50, ]))\n\n[1] 4\n\n\nRow 50 has 4 missing values.\n\n22.1.1 Visualize\nIt may be helpful to visualize missing data to get a quick impression of missingness. The rtemis package includes the function mplot3_missing():\n\nlibrary(rtemis)\n\n  .:rtemis 0.91 🌊 aarch64-apple-darwin20 (64-bit)\n  Defaults\n  |   Theme: whitegrid\n  |    Font: Fira Sans\n  | Palette: rtCol1\n  |    Plan: multicore\n  |   Cores: 8/10 available\n  Resources\n  | Documentation: https://rtemis.lambdamd.org\n  |       Learn R: https://class.lambdamd.org/pdsr\n  | rtemis themes: https://egenn.lambdamd.org/software/#rtemis_themes\n  |          Cite: `citation(\"rtemis\")`\n  Setup\n  | Enable progress reporting: `progressr::handlers(global = TRUE)`\n\nmplot3_missing(dat)\n\n\n\n\nMissing data is shown in magenta by default. The row below the image shows total NA values per column\n\n\n22.1.2 Summarize\nGet N of missing per column:\n\nsapply(dat, function(i) sum(is.na(i)))\n\npregnant  glucose pressure  triceps  insulin     mass pedigree      age \n       0        5       35      227      374       11        0        0 \ndiabetes \n       0 \n\n\nrtemis::checkData() includes information on missing data:\n\ncheckData(dat)\n dat: A data.frame with 768 rows and 9 features\n\n  Data types________________\n  * 8 continuous features\n  * 0 integer features\n  * 1 categorical feature, which is not ordered\n  * 0 character features\n  * 0 date features\n\n  Issues____________________\n  * 0 constant features\n  * 0 duplicated cases\n  * 5 features include 'NA' values; 652 'NA' values total\n     - Max percent missing in a feature is 48.70% (insulin)\n     - Max percent missing in a case is 44.44% (case #50)\n\n  Recommendations___________\n  * Consider imputing missing values or use complete cases only"
  },
  {
    "objectID": "52_MissingData.html#handle-missing-data",
    "href": "52_MissingData.html#handle-missing-data",
    "title": "22  Handling Missing data",
    "section": "22.2 Handle missing data",
    "text": "22.2 Handle missing data\nDifferent approaches can be used to handle missing data:\n\nDo nothing - if your algorithm(s) can handle missing data (decision trees!)\nExclude data: Use complete cases only\nFill in (make up) data: Replace or Impute\n\nReplace with median/mean\nPredict missing from present\n\nSingle imputation\nMultiple imputation\n\n\n\n\n22.2.1 Do nothing\nAlgorithms like decision trees and ensemble methods that use decision trees like random forest and gradient boosting can handle missing data, depending on the particular implementation. For example, rpart::rpart() which is used by rtemis::s_CART() has no trouble with missing data in the predictors:\n\ndat.cart <- s_CART(dat)\n[2022-06-27 15:22:13 s_CART] Hello, egenn \n\n[2022-06-27 15:22:13 dataPrepare] Imbalanced classes: using Inverse Probability Weighting \n\n.:Classification Input Summary\nTraining features: 768 x 8 \n Training outcome: 768 x 1 \n Testing features: Not available\n  Testing outcome: Not available\n\n[2022-06-27 15:22:14 s_CART] Training CART... \n\n.:CART Classification Training Summary\n                   Reference \n        Estimated  neg  pos  \n              neg  426   89\n              pos   74  179\n\n                   Overall  \n      Sensitivity  0.8520 \n      Specificity  0.6679 \nBalanced Accuracy  0.7600 \n              PPV  0.8272 \n              NPV  0.7075 \n               F1  0.8394 \n         Accuracy  0.7878 \n              AUC  0.7854 \n\n  Positive Class:  neg \n\n\n\n\n[2022-06-27 15:22:14 s_CART] Run completed in 0.01 minutes (Real: 0.84; User: 0.45; System: 0.04) \n\n\n\n\n22.2.2 Use complete cases only\nR’s builtin complete.cases() function returns, as the name suggests, a logical index of cases (i.e. rows) that have no missing values, i.e. are complete.\n\ndim(dat)\n\n[1] 768   9\n\nindex_cc <- complete.cases(dat)\nclass(index_cc)\n\n[1] \"logical\"\n\nlength(index_cc)\n\n[1] 768\n\nhead(index_cc)\n\n[1] FALSE FALSE FALSE  TRUE  TRUE FALSE\n\ndat_cc <- dat[index_cc, ]\ndim(dat_cc)\n\n[1] 392   9\n\n\nWe lost 376 cases in the above example. That’s quite a few, so, for this dataset, we probably want to look at options that do not exclude cases.\n\n\n22.2.3 Replace with a fixed value\nWe can manually replace missing values with the mean or median in the case of a continuous variable, or with the mode in the case of a categorical feature.\nFor example, to replace the first feature’s missing values with the mean:\n\npressure_mean <- mean(dat$pressure, na.rm = TRUE)\ndat_im <- dat\ndat_im$pressure[is.na(dat_im$pressure)] <- pressure_mean\n\nrtemis::preprocess() can replace missing values with mean (for numeric features) and the mode (for factors) for all columns:\n\ndat_pre <- preprocess(dat, impute = TRUE, impute.type = \"meanMode\")\n[2022-06-27 15:22:14 preprocess] Imputing missing values using mean and getMode... \n[2022-06-27 15:22:14 preprocess] Done \n\n\nVerify there are no missing data by rerunning checkData():\n\ncheckData(dat_pre)\n dat_pre: A data.frame with 768 rows and 9 features\n\n  Data types________________\n  * 8 continuous features\n  * 0 integer features\n  * 1 categorical feature, which is not ordered\n  * 0 character features\n  * 0 date features\n\n  Issues____________________\n  * 0 constant features\n  * 0 duplicated cases\n  * 0 features include 'NA' values\n\n  Recommendations___________\n  * Everything looks good\n\n\nYou may want to include a “missingness” column that indicates which cases were imputed to include in your model. You can create this simply by running:\n\npressure_missing = factor(as.integer(is.na(dat$pressure)))\n\npreprocess() includes the option missingness to add indicator columns after imputation:\n\ndat_pre <- preprocess(dat, impute = TRUE, impute.type = \"meanMode\",\n                      missingness = TRUE)\n[2022-06-27 15:22:14 preprocess] Created missingness indicator for glucose... \n[2022-06-27 15:22:14 preprocess] Created missingness indicator for pressure... \n[2022-06-27 15:22:14 preprocess] Created missingness indicator for triceps... \n[2022-06-27 15:22:14 preprocess] Created missingness indicator for insulin... \n[2022-06-27 15:22:14 preprocess] Created missingness indicator for mass... \n[2022-06-27 15:22:14 preprocess] Imputing missing values using mean and getMode... \n[2022-06-27 15:22:14 preprocess] Done \n\nhead(dat_pre)\n\n  pregnant glucose pressure  triceps  insulin mass pedigree age diabetes\n1        6     148       72 35.00000 155.5482 33.6    0.627  50      pos\n2        1      85       66 29.00000 155.5482 26.6    0.351  31      neg\n3        8     183       64 29.15342 155.5482 23.3    0.672  32      pos\n4        1      89       66 23.00000  94.0000 28.1    0.167  21      neg\n5        0     137       40 35.00000 168.0000 43.1    2.288  33      pos\n6        5     116       74 29.15342 155.5482 25.6    0.201  30      neg\n  glucose_missing pressure_missing triceps_missing insulin_missing mass_missing\n1               0                0               0               1            0\n2               0                0               0               1            0\n3               0                0               1               1            0\n4               0                0               0               0            0\n5               0                0               0               0            0\n6               0                0               1               1            0\n\n\n\n22.2.3.1 Add new level “missing”\nOne option to handle missing data in categorical variables, is to introduce a new level of “missing” to the factor, instead of replacing with the mode, for example. If we bin a continuous variable to convert to categorical, the same can then also be applied.\nSince no factors have missing values in the current dataset we create a copy and replace some data with NA:\n\ndat2 <- dat\ndat2$diabetes[sample(1:NROW(dat2), 35)] <- NA\nsum(is.na(dat2$diabetes))\n\n[1] 35\n\nlevels(dat2$diabetes)\n\n[1] \"neg\" \"pos\"\n\n\nReplace NA values with new level missing:\n\ndat_pre2 <- preprocess(dat2, factorNA2missing = TRUE)\n[2022-06-27 15:22:14 preprocess] Converting NA in factors to level \"missing\"... \n[2022-06-27 15:22:14 preprocess] Done \n\nanyNA(dat_pre2$diabetes)\n\n[1] FALSE\n\nlevels(dat_pre2$diabetes)\n\n[1] \"neg\"     \"pos\"     \"missing\"\n\n\n\n\n\n22.2.4 Last observation carried forward (LOCF)\nIn longitudinal / timeseries data, we may want to replace missing values with the last observed value. This is called last observation carried forward (LOCF). As always, whether this procedure is appropriate depend the reasons for missingness. The zoo and DescTool packages provide commands to perform LOCF.\nSome simulated data. We are missing blood pressure measurements on Saturdays and Sundays:\n\ndat <- data.frame(Day = rep(c(\"Mon\", \"Tues\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"), 3),\n                  SBP = sample(105:125, 21, TRUE))\ndat$SBP[dat$Day == \"Sat\" | dat$Day == \"Sun\"] <- NA\ndat\n\n    Day SBP\n1   Mon 120\n2  Tues 120\n3   Wed 114\n4   Thu 115\n5   Fri 119\n6   Sat  NA\n7   Sun  NA\n8   Mon 107\n9  Tues 116\n10  Wed 107\n11  Thu 106\n12  Fri 117\n13  Sat  NA\n14  Sun  NA\n15  Mon 123\n16 Tues 123\n17  Wed 106\n18  Thu 123\n19  Fri 119\n20  Sat  NA\n21  Sun  NA\n\n\nThe zoo package includes the na.locf().\n\ndat$SBPlocf <- zoo::na.locf(dat$SBP)\ndat\n\n    Day SBP SBPlocf\n1   Mon 120     120\n2  Tues 120     120\n3   Wed 114     114\n4   Thu 115     115\n5   Fri 119     119\n6   Sat  NA     119\n7   Sun  NA     119\n8   Mon 107     107\n9  Tues 116     116\n10  Wed 107     107\n11  Thu 106     106\n12  Fri 117     117\n13  Sat  NA     117\n14  Sun  NA     117\n15  Mon 123     123\n16 Tues 123     123\n17  Wed 106     106\n18  Thu 123     123\n19  Fri 119     119\n20  Sat  NA     119\n21  Sun  NA     119\n\n\nSimilar functionality is included in DescTools’ LOCF() function:\n\nDescTools::LOCF(dat$SBP)\n\n [1] 120 120 114 115 119 119 119 107 116 107 106 117 117 117 123 123 106 123 119\n[20] 119 119\n\n\n\n\n22.2.5 Replace missing values with estimated values\n\n22.2.5.1 Single imputation\nYou can use non-missing data to predict missing data in an iterative procedure (Buuren and Groothuis-Oudshoorn 2010; Stekhoven and Bühlmann 2012). The missRanger package uses the optimized (and parallel-capable) package ranger (Wright and Ziegler 2015) to iteratively train random forest models for imputation.\n\nlibrary(missRanger)\ndat <- iris\nset.seed(2020)\ndat[sample(1:150, 5), 1] <- dat[sample(1:150, 22), 4] <- dat[sample(1:150, 18), 4] <- NA\ndat_rfimp <- missRanger(dat, num.trees = 100)\n\n\nMissing value imputation by random forests\n\n  Variables to impute:      Sepal.Length, Petal.Width\n  Variables used to impute: Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, Species\niter 1: ..\niter 2: ..\niter 3: ..\niter 4: ..\n\nhead(dat_rfimp)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1     5.100000         3.5          1.4         0.2  setosa\n2     4.900000         3.0          1.4         0.2  setosa\n3     4.732533         3.2          1.3         0.2  setosa\n4     4.600000         3.1          1.5         0.2  setosa\n5     5.000000         3.6          1.4         0.2  setosa\n6     5.400000         3.9          1.7         0.4  setosa\n\ncheckData(dat_rfimp)\n dat_rfimp: A data.frame with 150 rows and 5 features\n\n  Data types________________\n  * 4 continuous features\n  * 0 integer features\n  * 1 categorical feature, which is not ordered\n     - 1 unordered categorical feature has more than 2 levels\n  * 0 character features\n  * 0 date features\n\n  Issues____________________\n  * 0 constant features\n  * 1 duplicated case\n  * 0 features include 'NA' values\n\n  Recommendations___________\n  * Remove the duplicated case\n  * Check the unordered categorical feature with more than 2 levels and consider\n    if ordering would make sense\n\n\nNote: The default method for preprocess(impute = TRUE) is to use missRanger.\n\n\n22.2.5.2 Multiple imputation\nMultiple imputation creates multiple estimates of the missing data. It is more statistically valid for small datasets, especially when the goal is to get accurate estimates of a summary statistics, but may not be practical for larger datasets. It is not usually considered an option for machine learning (where duplicating cases may add bias and complexity in resampling). The package mice is a popular choice for multiple imputation in R.\n\nlibrary(mice)\ndat_mice <- mice(dat)\n\n\n\n\n\nBuuren, S van, and Karin Groothuis-Oudshoorn. 2010. “Mice: Multivariate Imputation by Chained Equations in r.” Journal of Statistical Software, 1–68.\n\n\nMack, Christina, Zhaohui Su, and Daniel Westreich. 2018. “Managing Missing Data in Patient Registries: Addendum to Registries for Evaluating Patient Outcomes: A User’s Guide, [Internet].”\n\n\nStekhoven, Daniel J, and Peter Bühlmann. 2012. “MissForest—Non-Parametric Missing Value Imputation for Mixed-Type Data.” Bioinformatics 28 (1): 112–18.\n\n\nWright, Marvin N, and Andreas Ziegler. 2015. “Ranger: A Fast Implementation of Random Forests for High Dimensional Data in c++ and r.” arXiv Preprint arXiv:1508.04409."
  },
  {
    "objectID": "54_ClassesAndOOP.html",
    "href": "54_ClassesAndOOP.html",
    "title": "23  Classes and Object-Oriented Programming",
    "section": "",
    "text": "Object-Oriented Programming (OOP) is a programming paradigm built around objects with associated data, known as attributes, and functions, known as methods.\nThere are 4 main class systems in R:\nS3 and S4 methods are part of generic functions. RC and R6 methods are part of the object, but you can (and should) write generic functions for them as well.\nThis chapter will focus on the ubiquitous S3 system. For more advanced (and real OOP) applications, we recommend looking into the R6 system."
  },
  {
    "objectID": "54_ClassesAndOOP.html#s3",
    "href": "54_ClassesAndOOP.html#s3",
    "title": "23  Classes and Object-Oriented Programming",
    "section": "23.1 S3",
    "text": "23.1 S3\nMost R objects we have been using so far are S3 objects. Data frames are some of the most common S3 objects.\nGeneric functions are functions that act differently based on the class of the input object. We have already used many of them. For example, summary() works differently on a data.frame, on a factor, or a glm object, etc.\nGeneric functions in R are saved as functionname.classname() and called automatically, based on the class of the first argument. This allows the same function, e.g. print(), summary(), c(), to have a different effect on objects of different classes. For example, the print() function applied on a data frame, will actually call print.data.frame(), while applied on a factor, it will call print.factor().\nThis means that when you type print(iris) this calls print.data.frame(iris)\nNote how the R documentation lists usage information separately for each S3 method, e.g. ## S3 method for class 'factor'\n\n23.1.1 methods()\nTo get a list of all available methods defined for a specific class,\ni.e. “What different functions can I use on this object?”\n\nmethods(class = \"data.frame\")\n\n [1] [             [[            [[<-          [<-           $<-          \n [6] aggregate     anyDuplicated anyNA         as.data.frame as.list      \n[11] as.matrix     as.vector     by            cbind         coerce       \n[16] dim           dimnames      dimnames<-    droplevels    duplicated   \n[21] edit          format        formula       head          initialize   \n[26] is.na         Math          merge         na.exclude    na.omit      \n[31] Ops           plot          print         prompt        rbind        \n[36] row.names     row.names<-   rowsum        show          slotsFromS3  \n[41] split         split<-       stack         str           subset       \n[46] summary       Summary       t             tail          transform    \n[51] type.convert  unique        unstack       within        xtfrm        \nsee '?methods' for accessing help and source code\n\n\nConversely, to get a list of all available methods for a generic function (i.e. which classes have)\n(i.e. “What objects can I use this function on?”)\n\nmethods(generic.function = \"plot\")\n\n [1] plot.acf*           plot.data.frame*    plot.decomposed.ts*\n [4] plot.default        plot.dendrogram*    plot.density*      \n [7] plot.ecdf           plot.factor*        plot.formula*      \n[10] plot.function       plot.hclust*        plot.histogram*    \n[13] plot.HoltWinters*   plot.isoreg*        plot.lm*           \n[16] plot.medpolish*     plot.mlm*           plot.ppr*          \n[19] plot.prcomp*        plot.princomp*      plot.profile.nls*  \n[22] plot.raster*        plot.spec*          plot.stepfun       \n[25] plot.stl*           plot.table*         plot.ts            \n[28] plot.tskernel*      plot.TukeyHSD*     \nsee '?methods' for accessing help and source code\n\n\n\n\n23.1.2 Defining custom S3 classes\nIt very simple to assign an object to a new class.\nThere is no formal class definition, an object is directly assigned to a class by name. An object can belong to multiple classes:\n\nx <- 1:10\nclass(x) <- c(\"specialvector\", \"numeric\")\nclass(x)\n\n[1] \"specialvector\" \"numeric\"      \n\n\nThe hierarchy of classes goes left to right, meaning that generic methods are searched for classes in the order they appear in the output of class().\nIf we print x, since there is no print method for class specialvector or for numeric, the default print.default() command is automatically called:\n\nprint(x)\n\n [1]  1  2  3  4  5  6  7  8  9 10\nattr(,\"class\")\n[1] \"specialvector\" \"numeric\"      \n\nprint.default(x)\n\n [1]  1  2  3  4  5  6  7  8  9 10\nattr(,\"class\")\n[1] \"specialvector\" \"numeric\"      \n\n\nTo create a custom print() function for out new class specialvector, we define a function named print.[classname]:\n\nprint.specialvector <- function(x, ...) {\n  cat(\"This is a special vector of length\", length(x), \"\\n\")\n  cat(\"Its mean value is\", mean(x, na.rm = TRUE), \"and its median is\", median(x, na.rm = TRUE))\n  cat(\"\\nHere are the first few elements:\\n\", head(x), \"\\n\")\n}\n\nNow, when you print an object of class specialvector, the custom print() command is invoked:\n\nx\n\nThis is a special vector of length 10 \nIts mean value is 5.5 and its median is 5.5\nHere are the first few elements:\n 1 2 3 4 5 6 \n\n\nIf needed, you can call the default or another appropriate method directly:\n\nprint.default(x)\n\n [1]  1  2  3  4  5  6  7  8  9 10\nattr(,\"class\")\n[1] \"specialvector\" \"numeric\"      \n\n\nYou can change the vector back to a regular numeric vector, or a different class, just as easily:\n\nclass(x) <- \"numeric\"\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10"
  },
  {
    "objectID": "60_DataTable.html",
    "href": "60_DataTable.html",
    "title": "24  Efficient data analysis with data.table",
    "section": "",
    "text": "The data.table package provides a modern and highly optimized version of R’s data.frame structure. It is highly memory efficient and automatically parallelizes internal operations to achieve substantial speed improvements over data.frames. The data.table package weighs in at just a few kilobytes, has zero dependencies, and maintains compatibility with R versions going as far back as possible."
  },
  {
    "objectID": "60_DataTable.html#data.table-singificantly-extends-the-power-of-data.frame",
    "href": "60_DataTable.html#data.table-singificantly-extends-the-power-of-data.frame",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.1 data.table singificantly extends the power of data.frame",
    "text": "24.1 data.table singificantly extends the power of data.frame\nSome of the ways in which a data.table differs from a data.frame:\n\nA lot more than indexing can be done within a data.table’s “frame” (dt[i, j, by]): filter, select & operate on columns, group-by operations\nAccess column names directly without quoting\nMany operations can be performed “in-place” (i.e. with no assignment)\nWorking on big data within a data.table can be orders of magnitude faster.\n\ndata.table operations remain as close as possible to data.frame operations, trying to extend rather than replace the latter’s functionality.\ndata.table includes thorough and helpful error messages that often point to a solution. This includes common mistakes new users may make when trying commands that would work on a data.frame but are different on a data.table."
  },
  {
    "objectID": "60_DataTable.html#dtinstallation",
    "href": "60_DataTable.html#dtinstallation",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.2 Installation",
    "text": "24.2 Installation\nYou can install data.table from CRAN or GitHub. Check out the data.table wiki for more info.\nTo get the latest version on CRAN:\n\ninstall.packages(\"data.table\")\n\nTo get the latest development version:\n\n# install the \"remotes\" package if you don't already have it and then\nremotes::install_github(\"Rdatatable/data.table\")\n\ndata.table also includes a built-in command to update to the latest development version:\n\ndata.table::update.dev.pkg()\n\n\n24.2.1 Load the data.table package\n\nlibrary(data.table)"
  },
  {
    "objectID": "60_DataTable.html#create-a-data.table",
    "href": "60_DataTable.html#create-a-data.table",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.3 Create a data.table",
    "text": "24.3 Create a data.table\n\n24.3.1 By assignment: data.table()\nLet’s create a data.frame and a data.table to explore side by side.\n\ndf <- data.frame(A = 1:5,\n                 B = c(1.2, 4.3, 9.7, 5.6, 8.1),\n                 C = c(\"a\", \"b\", \"b\", \"a\", \"a\"))\nclass(df)\n\n[1] \"data.frame\"\n\ndf\n\n  A   B C\n1 1 1.2 a\n2 2 4.3 b\n3 3 9.7 b\n4 4 5.6 a\n5 5 8.1 a\n\n\ndata.table() syntax is similar to data.frame() (differs in some arguments)\n\ndt <- data.table(A = 1:5,\n                 B = c(1.2, 4.3, 9.7, 5.6, 8.1),\n                 C = c(\"a\", \"b\", \"b\", \"a\", \"a\"))\ndt\n\n   A   B C\n1: 1 1.2 a\n2: 2 4.3 b\n3: 3 9.7 b\n4: 4 5.6 a\n5: 5 8.1 a\n\nclass(dt)\n\n[1] \"data.table\" \"data.frame\"\n\n\nNotice how a data.table object also inherits from data.frame. This means that if a method does not exist for data.table, the method for data.frame will be used - review classes and generic functions.\nAs part of improving efficieny, data.tables do away with row names. Instead of using rownames, you can and should add an extra column (e.g. “ID”) with the same information - this is advisable when working with data.frames as well.\nA rather convenient option is to have data.tables print each column’s class below the column name. You can pass the argument class = TRUE to print() or set the global option datatable.print.class using options()\n\noptions(datatable.print.class = TRUE)\ndt\n\n       A     B      C\n   <int> <num> <char>\n1:     1   1.2      a\n2:     2   4.3      b\n3:     3   9.7      b\n4:     4   5.6      a\n5:     5   8.1      a\n\n\nSame as with a data.frame, to automatically convert string to factors, you can use the stringsAsFactors argument (or factor() directly):\n\ndt <- data.table(A = 1:5,\n                 B = c(1.2, 4.3, 9.7, 5.6, 8.1),\n                 C = c(\"a\", \"b\", \"b\", \"a\", \"a\"),\n                 stringsAsFactors = TRUE)\ndt\n\n       A     B      C\n   <int> <num> <fctr>\n1:     1   1.2      a\n2:     2   4.3      b\n3:     3   9.7      b\n4:     4   5.6      a\n5:     5   8.1      a\n\n\n\n\n24.3.2 By coercion: as.data.table()\n\ndat <- data.frame(A = 1:5,\n                  B = c(1.2, 4.3, 9.7, 5.6, 8.1),\n                  C = c(\"a\", \"b\", \"b\", \"a\", \"a\"),\n                  stringsAsFactors = TRUE)\ndat\n\n  A   B C\n1 1 1.2 a\n2 2 4.3 b\n3 3 9.7 b\n4 4 5.6 a\n5 5 8.1 a\n\ndat2 <- as.data.table(dat)\ndat2\n\n       A     B      C\n   <int> <num> <fctr>\n1:     1   1.2      a\n2:     2   4.3      b\n3:     3   9.7      b\n4:     4   5.6      a\n5:     5   8.1      a\n\n\n\n\n24.3.3 By coercion in-place: setDT()\nsetDT converts a list or data.frame into a data.table in-place. Note: the original object itself is changed, you do not need to assign the output of setDT to a new name.\n\ndat <- data.frame(A = 1:5,\n                  B = c(1.2, 4.3, 9.7, 5.6, 8.1),\n                  C = c(\"a\", \"b\", \"b\", \"a\", \"a\"))\nclass(dat)\n\n[1] \"data.frame\"\n\nsetDT(dat)\nclass(dat)\n\n[1] \"data.table\" \"data.frame\"\n\n\nYou can similarly convert a data.table to a data.frame, in-place:\n\nsetDF(dat)\nclass(dat)\n\n[1] \"data.frame\"\n\n\n\n\n24.3.4 Read into a data.table from file with fread()\ndata.table includes the fread() function to read data from files, in a similar way as the base functions read.csv() and read.table(). It is orders of magnitude faster for very large data (e.g. thousands to millions of rows) and it can read directly from URLs and zipped files. The sep argument defines the separator (same as in read.csv() and read.table()), but when set to \"auto\" (the default) it does a great job of figuring it out by itself.\n\ndat <- fread(\"path/to/input.csv\")\ndat <- fread(\"https::/url/to/input.csv.gz\")\n\nFor its speed and convenience, fread() is recommended over read.csv()/read.table() even if you intend to work with a data.frame exclusively, in which case you can pass the argument data.table = FALSE.\n\n\n24.3.5 Write a data.table to a CSV: fwrite()\nSimilar to fread(), fwrite() can be a lot faster than write.csv()\n\nfwrite(dt, \"/path/to/text.csv\")\n\n\n\n24.3.6 Save data.table to an RDS:\nSame as any R object, you can save a data.table to disk using saveRDS(). Suppose you have read data in with fread() or coerced a dataset using as.data.table(), done some cleaning up, type conversions, data transformations, etc, this is the preferred way to save your work, so you can reload at any time.\n\nsaveRDS(dt, \"/path/to/data.rds\")"
  },
  {
    "objectID": "60_DataTable.html#display-data.table-structure-with-str",
    "href": "60_DataTable.html#display-data.table-structure-with-str",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.4 Display data.table structure with str()",
    "text": "24.4 Display data.table structure with str()\nstr() works the same (and you should keep using it!)\n\nstr(df)\n\n'data.frame':   5 obs. of  3 variables:\n $ A: int  1 2 3 4 5\n $ B: num  1.2 4.3 9.7 5.6 8.1\n $ C: chr  \"a\" \"b\" \"b\" \"a\" ...\n\n\n\nstr(dt)\n\nClasses 'data.table' and 'data.frame':  5 obs. of  3 variables:\n $ A: int  1 2 3 4 5\n $ B: num  1.2 4.3 9.7 5.6 8.1\n $ C: Factor w/ 2 levels \"a\",\"b\": 1 2 2 1 1\n - attr(*, \".internal.selfref\")=<externalptr>"
  },
  {
    "objectID": "60_DataTable.html#combine-data.tables",
    "href": "60_DataTable.html#combine-data.tables",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.5 Combine data.tables",
    "text": "24.5 Combine data.tables\ncbind() and rbind() work on data.tables the same as on data.frames:\n\ndt1 <- data.table(a = 1:5)\ndt2 <- data.table(b = 11:15)\ncbind(dt1, dt2)\n\n       a     b\n   <int> <int>\n1:     1    11\n2:     2    12\n3:     3    13\n4:     4    14\n5:     5    15\n\nrbind(dt1, dt1)\n\n        a\n    <int>\n 1:     1\n 2:     2\n 3:     3\n 4:     4\n 5:     5\n 6:     1\n 7:     2\n 8:     3\n 9:     4\n10:     5"
  },
  {
    "objectID": "60_DataTable.html#filter-rows",
    "href": "60_DataTable.html#filter-rows",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.6 Filter rows",
    "text": "24.6 Filter rows\nThere are many similarities and some notable differences in how indexing works in a data.table vs. a data.frame.\nFiltering rows with an integer or logical index is largely the same in a data.frame and a data.table, but in a data.table you can omit the comma to select all columns:\n\ndf[c(1, 3, 5), ]\n\n  A   B C\n1 1 1.2 a\n3 3 9.7 b\n5 5 8.1 a\n\ndt[c(1, 3, 5), ]\n\n       A     B      C\n   <int> <num> <fctr>\n1:     1   1.2      a\n2:     3   9.7      b\n3:     5   8.1      a\n\ndt[c(1, 3, 5)]\n\n       A     B      C\n   <int> <num> <fctr>\n1:     1   1.2      a\n2:     3   9.7      b\n3:     5   8.1      a\n\n\nUsing a variable that holds a row index, whether integer or logical:\n\nrowid <- c(1, 3, 5)\ndf[rowid, ]\n\n  A   B C\n1 1 1.2 a\n3 3 9.7 b\n5 5 8.1 a\n\ndt[rowid, ]\n\n       A     B      C\n   <int> <num> <fctr>\n1:     1   1.2      a\n2:     3   9.7      b\n3:     5   8.1      a\n\ndt[rowid]\n\n       A     B      C\n   <int> <num> <fctr>\n1:     1   1.2      a\n2:     3   9.7      b\n3:     5   8.1      a\n\n\n\nrowbn <- c(T, F, T, F, T)\ndf[rowbn, ]\n\n  A   B C\n1 1 1.2 a\n3 3 9.7 b\n5 5 8.1 a\n\ndt[rowbn, ]\n\n       A     B      C\n   <int> <num> <fctr>\n1:     1   1.2      a\n2:     3   9.7      b\n3:     5   8.1      a\n\ndt[rowbn]\n\n       A     B      C\n   <int> <num> <fctr>\n1:     1   1.2      a\n2:     3   9.7      b\n3:     5   8.1      a\n\n\n\n24.6.1 Conditional filtering\nAs a reminder, there are a few ways to conditionally filter cases in a data.frame:\n\ndf[df$A > mean(df$A) & df$B > mean(df$B), ]\n\n  A   B C\n5 5 8.1 a\n\nsubset(df, A > mean(A) & B > mean(B))\n\n  A   B C\n5 5 8.1 a\n\nwith(df, df[A > mean(A) & B > mean(B), ])\n\n  A   B C\n5 5 8.1 a\n\n\ndata.table allows you to refer to column names directly and unquoted, which makes writing filter conditions easier/more compact:\n\ndt[A > mean(A) & B > mean(B)]\n\n       A     B      C\n   <int> <num> <fctr>\n1:     5   8.1      a\n\n\nThe data.table package also includes an S3 method for subset() that works the same way as with a data.frame:\n\nsubset(dt, A > mean(A) & B > mean(B))\n\n       A     B      C\n   <int> <num> <fctr>\n1:     5   8.1      a\n\n\nAs another example, exclude cases base on missingness in a specific column:\n\nadf <- as.data.frame(sapply(1:5, function(i) rnorm(10)))\nadf |> head()\n\n          V1          V2         V3         V4          V5\n1  0.1311821 -0.41242261  0.5897420 -0.4643061  1.58410885\n2 -1.5757546 -1.46085796  0.1111828  1.5812159  0.14120671\n3 -0.2646751 -0.63027344  0.5977524 -0.3590879  0.01795485\n4 -0.3430240  0.80000771 -1.8080567  0.7472244  0.82739404\n5  1.4605731  0.09614699  1.1030527  0.5022653 -1.23512662\n6  1.0582400 -0.74200541 -1.3492387 -1.1937673 -0.77901035\n\nadf[1, 3] <- adf[3, 4] <- adf[5, 3] <- adf[7, 3] <- NA\nadt <- as.data.table(adf)\n\n\nadf[!is.na(adf$V3), ]\n\n           V1         V2         V3         V4          V5\n2  -1.5757546 -1.4608580  0.1111828  1.5812159  0.14120671\n3  -0.2646751 -0.6302734  0.5977524         NA  0.01795485\n4  -0.3430240  0.8000077 -1.8080567  0.7472244  0.82739404\n6   1.0582400 -0.7420054 -1.3492387 -1.1937673 -0.77901035\n8  -0.5440897 -0.9472814 -0.1800953 -0.6742933 -0.82677733\n9  -0.5628776  0.7821239  0.7993246 -0.2182100 -1.18673450\n10 -0.6635323 -0.4209989  1.6235166 -0.2309067  0.11117008\n\nadt[!is.na(V3)]\n\n           V1         V2         V3         V4          V5\n        <num>      <num>      <num>      <num>       <num>\n1: -1.5757546 -1.4608580  0.1111828  1.5812159  0.14120671\n2: -0.2646751 -0.6302734  0.5977524         NA  0.01795485\n3: -0.3430240  0.8000077 -1.8080567  0.7472244  0.82739404\n4:  1.0582400 -0.7420054 -1.3492387 -1.1937673 -0.77901035\n5: -0.5440897 -0.9472814 -0.1800953 -0.6742933 -0.82677733\n6: -0.5628776  0.7821239  0.7993246 -0.2182100 -1.18673450\n7: -0.6635323 -0.4209989  1.6235166 -0.2309067  0.11117008"
  },
  {
    "objectID": "60_DataTable.html#select-columns",
    "href": "60_DataTable.html#select-columns",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.7 Select columns",
    "text": "24.7 Select columns\n\n24.7.1 By position(s)\nSelecting a single column in data.table does not drop to a vector, similar to using drop = FALSE in a data.frame:\n\ndf[, 1]\n\n[1] 1 2 3 4 5\n\ndf[, 1, drop = FALSE]\n\n  A\n1 1\n2 2\n3 3\n4 4\n5 5\n\ndt[, 1]\n\n       A\n   <int>\n1:     1\n2:     2\n3:     3\n4:     4\n5:     5\n\n\nDouble bracket indexing of a single column works the same on a data.frame and a data.table, returning a vector:\n\ndf[[2]]\n\n[1] 1.2 4.3 9.7 5.6 8.1\n\ndt[[2]]\n\n[1] 1.2 4.3 9.7 5.6 8.1\n\n\nA vector of column positions returns a smaller data.table, similar to how it returns a smaller data.frame :\n\ndf[, c(1, 2)]\n\n  A   B\n1 1 1.2\n2 2 4.3\n3 3 9.7\n4 4 5.6\n5 5 8.1\n\ndt[, c(1, 2)]\n\n       A     B\n   <int> <num>\n1:     1   1.2\n2:     2   4.3\n3:     3   9.7\n4:     4   5.6\n5:     5   8.1\n\n\n\n\n24.7.2 By name(s)\nIn data.table, you access column names directly without quoting or using the $ notation:\n\ndf[, \"B\"]\n\n[1] 1.2 4.3 9.7 5.6 8.1\n\ndf$B\n\n[1] 1.2 4.3 9.7 5.6 8.1\n\ndt[, B]\n\n[1] 1.2 4.3 9.7 5.6 8.1\n\n\nBecause of the above, data.table requires a slightly different syntax to use a variable as a column index which can contain integer positions, logical index, or column names:\n\ncolid <- c(1, 2)\ncolbn <- c(F, T, T)\ncolnm <- c(\"A\", \"C\")\ndf[, colid]\n\n  A   B\n1 1 1.2\n2 2 4.3\n3 3 9.7\n4 4 5.6\n5 5 8.1\n\ndf[, colbn]\n\n    B C\n1 1.2 a\n2 4.3 b\n3 9.7 b\n4 5.6 a\n5 8.1 a\n\ndf[, colnm]\n\n  A C\n1 1 a\n2 2 b\n3 3 b\n4 4 a\n5 5 a\n\n\nTo use a variable holding a column index in a data.table, prefix it with two periods:\n\ndt[, ..colid]\n\n       A     B\n   <int> <num>\n1:     1   1.2\n2:     2   4.3\n3:     3   9.7\n4:     4   5.6\n5:     5   8.1\n\ndt[, ..colbn]\n\n       B      C\n   <num> <fctr>\n1:   1.2      a\n2:   4.3      b\n3:   9.7      b\n4:   5.6      a\n5:   8.1      a\n\ndt[, ..colnm]\n\n       A      C\n   <int> <fctr>\n1:     1      a\n2:     2      b\n3:     3      b\n4:     4      a\n5:     5      a\n\n\nIf you are familiar with the system shell:\nThink of working inside the data.table frame (i.e. within the “[…]”) like an environment. You have direct access to the variables within it. If you want to refer to variables outside the data.table, you prefix the variable name with .. similar to how you access the directory above your current working directory in the system shell:\nAlternatively, you can use the .SD special symbol together with the .SDcols argument:\n\ndt[, .SD, .SDcols = colid]\n\n       A     B\n   <int> <num>\n1:     1   1.2\n2:     2   4.3\n3:     3   9.7\n4:     4   5.6\n5:     5   8.1\n\n\nThink of .SD as a sub-data.table with columns defined by .SDcols (if SDcols is not defined, .SD refers to the entire data.table).\nThe two dots tell the data.table to not look for the variable within the data.table columns, but in the enclosing environment.\nSelecting a single column by name returns a vector:\n\ndt[, A]\n\n[1] 1 2 3 4 5\n\n\nSelecting one or more columns by name enclosed in list() or .() (which, in this case, is short for list()), always returns a data.table:\n\ndt[, .(A)]\n\n       A\n   <int>\n1:     1\n2:     2\n3:     3\n4:     4\n5:     5\n\ndt[, .(A, B)]\n\n       A     B\n   <int> <num>\n1:     1   1.2\n2:     2   4.3\n3:     3   9.7\n4:     4   5.6\n5:     5   8.1"
  },
  {
    "objectID": "60_DataTable.html#add-new-column-in-place",
    "href": "60_DataTable.html#add-new-column-in-place",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.8 Add new column in-place",
    "text": "24.8 Add new column in-place\nUse := assignment to add a new column in the existing data.table. In-place assignment means you do not have to assign the result to a variable, the existing data.table will be modified.\n\ndt[, AplusB := A + B]\ndt\n\n       A     B      C AplusB\n   <int> <num> <fctr>  <num>\n1:     1   1.2      a    2.2\n2:     2   4.3      b    6.3\n3:     3   9.7      b   12.7\n4:     4   5.6      a    9.6\n5:     5   8.1      a   13.1\n\n\nNote how dt was modified even though we did not run dt <- dt[, AplusB := A + B]"
  },
  {
    "objectID": "60_DataTable.html#add-multiple-columns-in-place",
    "href": "60_DataTable.html#add-multiple-columns-in-place",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.9 Add multiple columns in-place",
    "text": "24.9 Add multiple columns in-place\nYou can define multiple new column names using a character vector of new column names on the left of := and a list on the right.\n\ndt[, c(\"AtimesB\", \"AoverB\") := list(A*B, A/B)]\n\nWe can use lapply() since it always returns a list:\n\nvnames <- c(\"A\", \"B\")\ndt[, paste0(\"log\", vnames) := lapply(.SD, log), .SDcols = vnames]\ndt\n\n       A     B      C AplusB AtimesB    AoverB      logA      logB\n   <int> <num> <fctr>  <num>   <num>     <num>     <num>     <num>\n1:     1   1.2      a    2.2     1.2 0.8333333 0.0000000 0.1823216\n2:     2   4.3      b    6.3     8.6 0.4651163 0.6931472 1.4586150\n3:     3   9.7      b   12.7    29.1 0.3092784 1.0986123 2.2721259\n4:     4   5.6      a    9.6    22.4 0.7142857 1.3862944 1.7227666\n5:     5   8.1      a   13.1    40.5 0.6172840 1.6094379 2.0918641\n\n\nYou can also use := in a little more awkward syntax:\n\ndt[, `:=`(AminusB = A - B, AoverC = A / B)]\ndt\n\n       A     B      C AplusB AtimesB    AoverB      logA      logB AminusB\n   <int> <num> <fctr>  <num>   <num>     <num>     <num>     <num>   <num>\n1:     1   1.2      a    2.2     1.2 0.8333333 0.0000000 0.1823216    -0.2\n2:     2   4.3      b    6.3     8.6 0.4651163 0.6931472 1.4586150    -2.3\n3:     3   9.7      b   12.7    29.1 0.3092784 1.0986123 2.2721259    -6.7\n4:     4   5.6      a    9.6    22.4 0.7142857 1.3862944 1.7227666    -1.6\n5:     5   8.1      a   13.1    40.5 0.6172840 1.6094379 2.0918641    -3.1\n      AoverC\n       <num>\n1: 0.8333333\n2: 0.4651163\n3: 0.3092784\n4: 0.7142857\n5: 0.6172840"
  },
  {
    "objectID": "60_DataTable.html#convert-column-type",
    "href": "60_DataTable.html#convert-column-type",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.10 Convert column type",
    "text": "24.10 Convert column type\n\n24.10.1 Assignment by reference with :=\nUse any base R coercion function (as.*) to convert a column in-place using the := notation\n\ndt[, A := as.numeric(A)]\ndt\n\n       A     B      C AplusB AtimesB    AoverB      logA      logB AminusB\n   <num> <num> <fctr>  <num>   <num>     <num>     <num>     <num>   <num>\n1:     1   1.2      a    2.2     1.2 0.8333333 0.0000000 0.1823216    -0.2\n2:     2   4.3      b    6.3     8.6 0.4651163 0.6931472 1.4586150    -2.3\n3:     3   9.7      b   12.7    29.1 0.3092784 1.0986123 2.2721259    -6.7\n4:     4   5.6      a    9.6    22.4 0.7142857 1.3862944 1.7227666    -1.6\n5:     5   8.1      a   13.1    40.5 0.6172840 1.6094379 2.0918641    -3.1\n      AoverC\n       <num>\n1: 0.8333333\n2: 0.4651163\n3: 0.3092784\n4: 0.7142857\n5: 0.6172840\n\n\n\n\n24.10.2 Delete columns in-place with :=\nTo delete a column, use := to set it to NULL:\n\ndt[, AoverB := NULL]\ndt\n\n       A     B      C AplusB AtimesB      logA      logB AminusB    AoverC\n   <num> <num> <fctr>  <num>   <num>     <num>     <num>   <num>     <num>\n1:     1   1.2      a    2.2     1.2 0.0000000 0.1823216    -0.2 0.8333333\n2:     2   4.3      b    6.3     8.6 0.6931472 1.4586150    -2.3 0.4651163\n3:     3   9.7      b   12.7    29.1 1.0986123 2.2721259    -6.7 0.3092784\n4:     4   5.6      a    9.6    22.4 1.3862944 1.7227666    -1.6 0.7142857\n5:     5   8.1      a   13.1    40.5 1.6094379 2.0918641    -3.1 0.6172840\n\n\nDelete multiple columns\n\ndt[, c(\"logA\", \"logB\") := NULL]\n\nOr:\n\ndt[, `:=`(AplusB = NULL, AminusB = NULL)]\ndt\n\n       A     B      C AtimesB    AoverC\n   <num> <num> <fctr>   <num>     <num>\n1:     1   1.2      a     1.2 0.8333333\n2:     2   4.3      b     8.6 0.4651163\n3:     3   9.7      b    29.1 0.3092784\n4:     4   5.6      a    22.4 0.7142857\n5:     5   8.1      a    40.5 0.6172840\n\n\n\n\n24.10.3 Fast loop-able assignment with set()\ndata.table’s set() is a looop-able version of the \"= operator. Use it in a for loop to operate on multiple columns.\nSyntax: set(dt, i, j, value)\n\ndt the data.table to operate on\ni optionally define which rows to operate on. i = NULL to operate on all\nj column names or index to be assigned value\nvalue values to be assigned to j by reference\n\nAs a simple example, transform the first two columns in-place by squaring:\n\nfor (i in 1:2) {\n  set(dt, i = NULL, j = i, value = dt[[i]]^2)\n}"
  },
  {
    "objectID": "60_DataTable.html#summarize",
    "href": "60_DataTable.html#summarize",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.11 Summarize",
    "text": "24.11 Summarize\nYou can apply one or multiple summary functions on one of multiple columns. Surround the operations in list() or .() to output a new data.table holding the outputs of the operations (the input data.table remains unchanged).\n\nAsummary <- dt[, .(Amax = max(A), Amin = min(A), Asd = sd(A))]\nAsummary\n\n    Amax  Amin     Asd\n   <num> <num>   <num>\n1:    25     1 9.66954\n\n\nExample: Get sd of all numeric columns:\n\nnumid <- sapply(dt, is.numeric)\ndt_mean <- dt[, lapply(.SD, sd), .SDcols = numid]\ndt_mean\n\n         A        B  AtimesB    AoverC\n     <num>    <num>    <num>     <num>\n1: 9.66954 37.35521 15.74462 0.2060219\n\n\nIf your function returns more than one value, the output will have multiple rows:\n\ndt_range <- dt[, lapply(.SD, range), .SDcols = numid]\ndt_range\n\n       A     B AtimesB    AoverC\n   <num> <num>   <num>     <num>\n1:     1  1.44     1.2 0.3092784\n2:    25 94.09    40.5 0.8333333"
  },
  {
    "objectID": "60_DataTable.html#group-by-operations",
    "href": "60_DataTable.html#group-by-operations",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.12 Group-by operations",
    "text": "24.12 Group-by operations\nUp to now, we have learned how to use the data.table frame dat[i, j] to filter cases in i or add/remove/transform columns in-place in j. dat[i, j, by] allows to perform operations separately on groups of cases.\n\ndt <- data.table(A = 1:5,\n                 B = c(1.2, 4.3, 9.7, 5.6, 8.1),\n                 C = rnorm(5),\n                 Group = c(\"a\", \"b\", \"b\", \"a\", \"a\"))\ndt\n\n       A     B           C  Group\n   <int> <num>       <num> <char>\n1:     1   1.2 -0.01919352      a\n2:     2   4.3  1.76291989      b\n3:     3   9.7 -1.07362454      b\n4:     4   5.6 -0.40038723      a\n5:     5   8.1  0.59648264      a\n\n\n\n24.12.1 Group-by summary\nAs we’ve seen, using .() or list() in j, returns a new data.table:\n\ndt[, .(meanAbyGroup = mean(A)), by = Group]\n\n    Group meanAbyGroup\n   <char>        <num>\n1:      a     3.333333\n2:      b     2.500000\n\ndt[, list(medianBbyGroup = median(B)), by = Group]\n\n    Group medianBbyGroup\n   <char>          <num>\n1:      a            5.6\n2:      b            7.0\n\n\n\n\n24.12.2 Group-by operation and assignment\nMaking an assignment with := in j, adds a column in-place. Since here we are grouping, the same value will be assigned to all cases of the group:\n\ndt[, meanAbyGroup := mean(A), by = Group]\ndt\n\n       A     B           C  Group meanAbyGroup\n   <int> <num>       <num> <char>        <num>\n1:     1   1.2 -0.01919352      a     3.333333\n2:     2   4.3  1.76291989      b     2.500000\n3:     3   9.7 -1.07362454      b     2.500000\n4:     4   5.6 -0.40038723      a     3.333333\n5:     5   8.1  0.59648264      a     3.333333\n\n\nFor more complex operations, you may need to refer to the slice of the data.table defined by by within j. There is a special notation for this: .SD (think sub-data.table):\n\ndt[, B_DiffFromGroupMin := B - min(B), by = Group]\ndt\n\n       A     B           C  Group meanAbyGroup B_DiffFromGroupMin\n   <int> <num>       <num> <char>        <num>              <num>\n1:     1   1.2 -0.01919352      a     3.333333                0.0\n2:     2   4.3  1.76291989      b     2.500000                0.0\n3:     3   9.7 -1.07362454      b     2.500000                5.4\n4:     4   5.6 -0.40038723      a     3.333333                4.4\n5:     5   8.1  0.59648264      a     3.333333                6.9\n\n\n\n\n\nBy now, it should be clearer that the data.table frame provides a very flexible way to perform a very wide range of operations with minimal new notation."
  },
  {
    "objectID": "60_DataTable.html#apply-functions-to-columns",
    "href": "60_DataTable.html#apply-functions-to-columns",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.13 Apply functions to columns",
    "text": "24.13 Apply functions to columns\nAny function that returns a list can be used in j to return a new data.table - therefore lapply is perfect for getting summary on multiple columns:\n\ndt1 <- as.data.table(sapply(1:3, \\(i) rnorm(10)))\ndt1\n\n            V1          V2          V3\n         <num>       <num>       <num>\n 1: -0.6453819 -0.54140706  0.89180270\n 2: -1.1746275 -1.45071346  0.56509219\n 3:  0.7874612 -0.52234949 -1.43719131\n 4: -0.1689379  0.95496904 -1.07333478\n 5: -0.3129802  1.09500706 -0.73230191\n 6: -0.6912456  0.76002077  0.22109337\n 7: -0.5406528 -0.47616616  1.41270075\n 8:  0.6509229 -0.65033604 -0.59408632\n 9: -0.3552445  0.90686899  0.09407682\n10: -0.9574488  0.03100915  0.19432775\n\nsetnames(dt1, names(dt1), c(\"Alpha\", \"Beta\", \"Gamma\"))\ndt1[, lapply(.SD, mean)]\n\n        Alpha       Beta       Gamma\n        <num>      <num>       <num>\n1: -0.3408135 0.01069028 -0.04578207\n\n\nYou can specify which columns to operate on using the .SDcols argument:\n\ndt2 <- data.table(A = 1:5,\n                  B = c(1.2, 4.3, 9.7, 5.6, 8.1),\n                  C = rnorm(5),\n                  Group = c(\"a\", \"b\", \"b\", \"a\", \"a\"))\ndt2\n\n       A     B           C  Group\n   <int> <num>       <num> <char>\n1:     1   1.2  0.04655708      a\n2:     2   4.3  0.84235568      b\n3:     3   9.7 -0.57372497      b\n4:     4   5.6 -1.34500563      a\n5:     5   8.1 -0.52578439      a\n\ndt2[, lapply(.SD, mean), .SDcols = 1:2]\n\n       A     B\n   <num> <num>\n1:     3  5.78\n\n# same as\ndt2[, lapply(.SD, mean), .SDcols = c(\"A\", \"B\")]\n\n       A     B\n   <num> <num>\n1:     3  5.78\n\ncols <- c(\"A\", \"B\")\ndt2[, lapply(.SD, mean), .SDcols = cols]\n\n       A     B\n   <num> <num>\n1:     3  5.78\n\n\nYou can combine .SDcols and by:\n\ndt2[, lapply(.SD, median), .SDcols = c(\"B\", \"C\"), by = Group]\n\n    Group     B          C\n   <char> <num>      <num>\n1:      a   5.6 -0.5257844\n2:      b   7.0  0.1343154\n\n\nCreate multiple new columns from transformation of existing and store with custom prefix:\n\ndt1\n\n         Alpha        Beta       Gamma\n         <num>       <num>       <num>\n 1: -0.6453819 -0.54140706  0.89180270\n 2: -1.1746275 -1.45071346  0.56509219\n 3:  0.7874612 -0.52234949 -1.43719131\n 4: -0.1689379  0.95496904 -1.07333478\n 5: -0.3129802  1.09500706 -0.73230191\n 6: -0.6912456  0.76002077  0.22109337\n 7: -0.5406528 -0.47616616  1.41270075\n 8:  0.6509229 -0.65033604 -0.59408632\n 9: -0.3552445  0.90686899  0.09407682\n10: -0.9574488  0.03100915  0.19432775\n\ndt1[, paste0(names(dt1), \"_abs\") := lapply(.SD, abs)]\ndt1\n\n         Alpha        Beta       Gamma Alpha_abs   Beta_abs  Gamma_abs\n         <num>       <num>       <num>     <num>      <num>      <num>\n 1: -0.6453819 -0.54140706  0.89180270 0.6453819 0.54140706 0.89180270\n 2: -1.1746275 -1.45071346  0.56509219 1.1746275 1.45071346 0.56509219\n 3:  0.7874612 -0.52234949 -1.43719131 0.7874612 0.52234949 1.43719131\n 4: -0.1689379  0.95496904 -1.07333478 0.1689379 0.95496904 1.07333478\n 5: -0.3129802  1.09500706 -0.73230191 0.3129802 1.09500706 0.73230191\n 6: -0.6912456  0.76002077  0.22109337 0.6912456 0.76002077 0.22109337\n 7: -0.5406528 -0.47616616  1.41270075 0.5406528 0.47616616 1.41270075\n 8:  0.6509229 -0.65033604 -0.59408632 0.6509229 0.65033604 0.59408632\n 9: -0.3552445  0.90686899  0.09407682 0.3552445 0.90686899 0.09407682\n10: -0.9574488  0.03100915  0.19432775 0.9574488 0.03100915 0.19432775\n\n\n\ndt2\n\n       A     B           C  Group\n   <int> <num>       <num> <char>\n1:     1   1.2  0.04655708      a\n2:     2   4.3  0.84235568      b\n3:     3   9.7 -0.57372497      b\n4:     4   5.6 -1.34500563      a\n5:     5   8.1 -0.52578439      a\n\ncols <- c(\"A\", \"C\")\ndt2[, paste0(cols, \"_groupMean\") := lapply(.SD, mean), .SDcols = cols, by = Group]\ndt2\n\n       A     B           C  Group A_groupMean C_groupMean\n   <int> <num>       <num> <char>       <num>       <num>\n1:     1   1.2  0.04655708      a    3.333333  -0.6080776\n2:     2   4.3  0.84235568      b    2.500000   0.1343154\n3:     3   9.7 -0.57372497      b    2.500000   0.1343154\n4:     4   5.6 -1.34500563      a    3.333333  -0.6080776\n5:     5   8.1 -0.52578439      a    3.333333  -0.6080776"
  },
  {
    "objectID": "60_DataTable.html#row-wise-operations",
    "href": "60_DataTable.html#row-wise-operations",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.14 Row-wise operations",
    "text": "24.14 Row-wise operations\n\ndt <- data.table(a = 1:5, b = 11:15, c = 21:25, \n                 d = 31:35, e = 41:45)\ndt\n\n       a     b     c     d     e\n   <int> <int> <int> <int> <int>\n1:     1    11    21    31    41\n2:     2    12    22    32    42\n3:     3    13    23    33    43\n4:     4    14    24    34    44\n5:     5    15    25    35    45\n\n\nTo operate row-wise, we can use by = 1:nrow(dt). For example, to add a column, in-place, with row-wise sums of variables b through d:\n\ndt[, bcd.sum := sum(.SD[, b:d]), by = 1:nrow(dt)]\ndt\n\n       a     b     c     d     e bcd.sum\n   <int> <int> <int> <int> <int>   <int>\n1:     1    11    21    31    41      63\n2:     2    12    22    32    42      66\n3:     3    13    23    33    43      69\n4:     4    14    24    34    44      72\n5:     5    15    25    35    45      75"
  },
  {
    "objectID": "60_DataTable.html#melt",
    "href": "60_DataTable.html#melt",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.15 Wide <=> Long",
    "text": "24.15 Wide <=> Long\n\n24.15.1 Wide to long: melt()\n\ndt_wide <- data.table(ID = 1:4, Timepoint_A = 11:14,\n                      Timepoint_B = 21:24, Timepoint_C = 51:54)\ndt_wide\n\n      ID Timepoint_A Timepoint_B Timepoint_C\n   <int>       <int>       <int>       <int>\n1:     1          11          21          51\n2:     2          12          22          52\n3:     3          13          23          53\n4:     4          14          24          54\n\ndt_long <- melt(dt_wide, id.vars = \"ID\",\n                measure.vars = 2:4, # defaults to all non-id columns\n                variable.name = \"Timepoint\",\n                value.name = c(\"Score\"))\ndt_long\n\n       ID   Timepoint Score\n    <int>      <fctr> <int>\n 1:     1 Timepoint_A    11\n 2:     2 Timepoint_A    12\n 3:     3 Timepoint_A    13\n 4:     4 Timepoint_A    14\n 5:     1 Timepoint_B    21\n 6:     2 Timepoint_B    22\n 7:     3 Timepoint_B    23\n 8:     4 Timepoint_B    24\n 9:     1 Timepoint_C    51\n10:     2 Timepoint_C    52\n11:     3 Timepoint_C    53\n12:     4 Timepoint_C    54\n\n\n\n\n24.15.2 Long to wide: dcast()\n\ndt_long\n\n       ID   Timepoint Score\n    <int>      <fctr> <int>\n 1:     1 Timepoint_A    11\n 2:     2 Timepoint_A    12\n 3:     3 Timepoint_A    13\n 4:     4 Timepoint_A    14\n 5:     1 Timepoint_B    21\n 6:     2 Timepoint_B    22\n 7:     3 Timepoint_B    23\n 8:     4 Timepoint_B    24\n 9:     1 Timepoint_C    51\n10:     2 Timepoint_C    52\n11:     3 Timepoint_C    53\n12:     4 Timepoint_C    54\n\ndcast(dt_long, ID ~ Timepoint,\n      value.var = \"Score\")\n\n      ID Timepoint_A Timepoint_B Timepoint_C\n   <int>       <int>       <int>       <int>\n1:     1          11          21          51\n2:     2          12          22          52\n3:     3          13          23          53\n4:     4          14          24          54\n\n\n\n24.15.2.1 dcast() + aggregate\nIf your ID ~ Timepoint combination does not define a unique row in your input dataset, you need to specify an aggregate function.\nFor example, suppose you have four subjects with IDs “A”, “B”, “C”, “D” who had a couple variables measured 3 times in the AM and 3 times in the PM.\n\ndt_long2 <- data.table(ID = rep(LETTERS[1:4], each = 6),\n                      Timepoint = rep(c(\"AM\", \"PM\"), length.out = 24, each = 3),\n                      Var1 = rnorm(24, 10),\n                      Var2 = rnorm(24, 20))\n\ndt_long2[sample(24, 4), Var1 := NA]\ndt_long2[sample(24, 4), Var2 := NA]\ndt_long2\n\n        ID Timepoint      Var1     Var2\n    <char>    <char>     <num>    <num>\n 1:      A        AM 11.180429 18.83875\n 2:      A        AM        NA 19.75811\n 3:      A        AM 10.858896       NA\n 4:      A        PM  8.696999 20.67314\n 5:      A        PM        NA       NA\n 6:      A        PM 12.426356 20.00126\n 7:      B        AM  8.763709 19.83046\n 8:      B        AM  9.919536 19.57749\n 9:      B        AM  9.147396 19.40920\n10:      B        PM  9.056259 20.90345\n11:      B        PM 11.392569       NA\n12:      B        PM 10.241746 21.03389\n13:      C        AM 10.029698 20.75804\n14:      C        AM 12.481805 20.13352\n15:      C        AM  9.169376 20.60449\n16:      C        PM  8.320664 21.81184\n17:      C        PM 11.048416 20.99195\n18:      C        PM 11.660641 19.49680\n19:      D        AM  9.227025 19.06158\n20:      D        AM        NA 20.54046\n21:      D        AM 11.884037 20.68816\n22:      D        PM 10.878042       NA\n23:      D        PM 10.334032 19.21043\n24:      D        PM        NA 19.15026\n        ID Timepoint      Var1     Var2\n\n\nIf you wanted to convert the above data.table to wide format and get mean AM and PM values using the fun.aggregate argument:\n\ndcast(dt_long2,\n      ID ~ Timepoint,\n      value.var = c(\"Var1\", \"Var2\"),\n      fun.aggregate = mean, na.rm = T)\n\n       ID  Var1_AM  Var1_PM  Var2_AM  Var2_PM\n   <char>    <num>    <num>    <num>    <num>\n1:      A 11.01966 10.56168 19.29843 20.33720\n2:      B  9.27688 10.23019 19.60572 20.96867\n3:      C 10.56029 10.34324 20.49868 20.76686\n4:      D 10.55553 10.60604 20.09674 19.18035\n\n\nYou can apply multiple aggregating functions by passing a list to fun.aggregate:\n\ndcast(dt_long2,\n      ID ~ Timepoint,\n      value.var = c(\"Var1\", \"Var2\"),\n      fun.aggregate = list(mean, max, min), na.rm = T)\n\n       ID Var1_mean_AM Var1_mean_PM Var2_mean_AM Var2_mean_PM Var1_max_AM\n   <char>        <num>        <num>        <num>        <num>       <num>\n1:      A     11.01966     10.56168     19.29843     20.33720   11.180429\n2:      B      9.27688     10.23019     19.60572     20.96867    9.919536\n3:      C     10.56029     10.34324     20.49868     20.76686   12.481805\n4:      D     10.55553     10.60604     20.09674     19.18035   11.884037\n   Var1_max_PM Var2_max_AM Var2_max_PM Var1_min_AM Var1_min_PM Var2_min_AM\n         <num>       <num>       <num>       <num>       <num>       <num>\n1:    12.42636    19.75811    20.67314   10.858896    8.696999    18.83875\n2:    11.39257    19.83046    21.03389    8.763709    9.056259    19.40920\n3:    11.66064    20.75804    21.81184    9.169376    8.320664    20.13352\n4:    10.87804    20.68816    19.21043    9.227025   10.334032    19.06158\n   Var2_min_PM\n         <num>\n1:    20.00126\n2:    20.90345\n3:    19.49680\n4:    19.15026\n\n\nNote how na.rm = T was successfully applied to all aggregating functions"
  },
  {
    "objectID": "60_DataTable.html#table-joins",
    "href": "60_DataTable.html#table-joins",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.16 Table Joins",
    "text": "24.16 Table Joins\ndata.table allow you to perform table joins with the base merge() function using the same syntax as for data.frame objects or the “data.table way” using bracket notation:\n\na <- data.table(PID = c(1:9),\n                Hospital = c(\"UCSF\", \"HUP\", \"Stanford\", \n                             \"Stanford\", \"UCSF\", \"HUP\", \n                             \"HUP\", \"Stanford\", \"UCSF\"),\n                Age = c(22, 34, 41, 19, 53, 21, 63, 22, 19),\n                Sex = c(1, 1, 0, 1, 0, 0, 1, 0, 0),\n                key = \"PID\")\na\n\n     PID Hospital   Age   Sex\n   <int>   <char> <num> <num>\n1:     1     UCSF    22     1\n2:     2      HUP    34     1\n3:     3 Stanford    41     0\n4:     4 Stanford    19     1\n5:     5     UCSF    53     0\n6:     6      HUP    21     0\n7:     7      HUP    63     1\n8:     8 Stanford    22     0\n9:     9     UCSF    19     0\n\nb <- data.table(PID = c(6:12),\n                V1 = c(153, 89, 112, 228,  91, 190, 101),\n                Department = c(\"Neurology\", \"Radiology\", \"Emergency\",\n                               \"Cardiology\", \"Surgery\", \"Neurology\",\n                               \"Psychiatry\"),\n                key = \"PID\")\nb\n\n     PID    V1 Department\n   <int> <num>     <char>\n1:     6   153  Neurology\n2:     7    89  Radiology\n3:     8   112  Emergency\n4:     9   228 Cardiology\n5:    10    91    Surgery\n6:    11   190  Neurology\n7:    12   101 Psychiatry\n\n\nIn the above command we use the key argument to set PID as key. This can be performed after the data.table has been created using the setkey() command:\n\nsetkey(a, PID)\n\nMultiple keys can be set, in order, with the same setkey() command, separated by commas, e.g.:\n\nsetkey(a, PID, Hospital)\n\nKeys sort the data.table by the corresponding columns and can be used to perform left and right joins with bracket notation seen later.\n\n24.16.1 Inner\n\nmerge(a, b)\n\n     PID Hospital   Age   Sex    V1 Department\n   <int>   <char> <num> <num> <num>     <char>\n1:     6      HUP    21     0   153  Neurology\n2:     7      HUP    63     1    89  Radiology\n3:     8 Stanford    22     0   112  Emergency\n4:     9     UCSF    19     0   228 Cardiology\n\n\n\n\n24.16.2 Outer\n\nmerge(a, b, all = TRUE)\n\n      PID Hospital   Age   Sex    V1 Department\n    <int>   <char> <num> <num> <num>     <char>\n 1:     1     UCSF    22     1    NA       <NA>\n 2:     2      HUP    34     1    NA       <NA>\n 3:     3 Stanford    41     0    NA       <NA>\n 4:     4 Stanford    19     1    NA       <NA>\n 5:     5     UCSF    53     0    NA       <NA>\n 6:     6      HUP    21     0   153  Neurology\n 7:     7      HUP    63     1    89  Radiology\n 8:     8 Stanford    22     0   112  Emergency\n 9:     9     UCSF    19     0   228 Cardiology\n10:    10     <NA>    NA    NA    91    Surgery\n11:    11     <NA>    NA    NA   190  Neurology\n12:    12     <NA>    NA    NA   101 Psychiatry\n\n\n\n\n24.16.3 Left outer\nUsing merge():\n\nmerge(a, b, all.x = TRUE)\n\n     PID Hospital   Age   Sex    V1 Department\n   <int>   <char> <num> <num> <num>     <char>\n1:     1     UCSF    22     1    NA       <NA>\n2:     2      HUP    34     1    NA       <NA>\n3:     3 Stanford    41     0    NA       <NA>\n4:     4 Stanford    19     1    NA       <NA>\n5:     5     UCSF    53     0    NA       <NA>\n6:     6      HUP    21     0   153  Neurology\n7:     7      HUP    63     1    89  Radiology\n8:     8 Stanford    22     0   112  Emergency\n9:     9     UCSF    19     0   228 Cardiology\n\n\nUsing bracket notation:\n\nb[a, ]\n\n     PID    V1 Department Hospital   Age   Sex\n   <int> <num>     <char>   <char> <num> <num>\n1:     1    NA       <NA>     UCSF    22     1\n2:     2    NA       <NA>      HUP    34     1\n3:     3    NA       <NA> Stanford    41     0\n4:     4    NA       <NA> Stanford    19     1\n5:     5    NA       <NA>     UCSF    53     0\n6:     6   153  Neurology      HUP    21     0\n7:     7    89  Radiology      HUP    63     1\n8:     8   112  Emergency Stanford    22     0\n9:     9   228 Cardiology     UCSF    19     0\n\n\nIf keys were not set for a and b, you could specify the column to match on using the on argument:\n\nb[a, on = \"PID\"]\n\n     PID    V1 Department Hospital   Age   Sex\n   <int> <num>     <char>   <char> <num> <num>\n1:     1    NA       <NA>     UCSF    22     1\n2:     2    NA       <NA>      HUP    34     1\n3:     3    NA       <NA> Stanford    41     0\n4:     4    NA       <NA> Stanford    19     1\n5:     5    NA       <NA>     UCSF    53     0\n6:     6   153  Neurology      HUP    21     0\n7:     7    89  Radiology      HUP    63     1\n8:     8   112  Emergency Stanford    22     0\n9:     9   228 Cardiology     UCSF    19     0\n\n\n\n\n\nThe easy way to understand the bracket notation merges is to think that the data.table inside the bracket is used to index the data.table on the outside, therefore the resulting table will have rows dictated by the inside table’s key.\n\n\n\n\n\n24.16.4 Right outer\n\nmerge(a, b, all.y = TRUE)\n\n     PID Hospital   Age   Sex    V1 Department\n   <int>   <char> <num> <num> <num>     <char>\n1:     6      HUP    21     0   153  Neurology\n2:     7      HUP    63     1    89  Radiology\n3:     8 Stanford    22     0   112  Emergency\n4:     9     UCSF    19     0   228 Cardiology\n5:    10     <NA>    NA    NA    91    Surgery\n6:    11     <NA>    NA    NA   190  Neurology\n7:    12     <NA>    NA    NA   101 Psychiatry\n\n\nUsing bracket notation:\n\na[b, ]\n\n     PID Hospital   Age   Sex    V1 Department\n   <int>   <char> <num> <num> <num>     <char>\n1:     6      HUP    21     0   153  Neurology\n2:     7      HUP    63     1    89  Radiology\n3:     8 Stanford    22     0   112  Emergency\n4:     9     UCSF    19     0   228 Cardiology\n5:    10     <NA>    NA    NA    91    Surgery\n6:    11     <NA>    NA    NA   190  Neurology\n7:    12     <NA>    NA    NA   101 Psychiatry"
  },
  {
    "objectID": "60_DataTable.html#understanding-reference-semantics-in-data.table",
    "href": "60_DataTable.html#understanding-reference-semantics-in-data.table",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.17 Understanding reference semantics in data.table",
    "text": "24.17 Understanding reference semantics in data.table\n\n24.17.1 Get object’s location in memory with address()\nWhen you add a new column to an existing data.frame, the data.frame is copied behind the scenes - you can tell becasue its memory address (where it’s physically stored in your computer) changes:\n\ndf1 <- data.frame(alpha = 1:5, beta = 11:15)\naddress(df1)\n\n[1] \"0x105107e88\"\n\ndf1$gamma <- df1$alpha + df1$beta\naddress(df1)\n\n[1] \"0x105f21b08\"\n\n\nWhen you add a new column in a data.table in-place its address remains unchanged:\n\ndt1 <- data.table(alpha = 1:5, beta = 11:15)\naddress(dt1)\n\n[1] \"0x109ba7e00\"\n\ndt1[, gamma := alpha + beta]\naddress(dt1)\n\n[1] \"0x109ba7e00\"\n\n\n\n\n24.17.2 Reference semantics at work\nUp to now, you are likely used to working with regular R objects that behave like this:\n\ndf1 <- data.frame(a = rep(1, 5))\ndf1\n\n  a\n1 1\n2 1\n3 1\n4 1\n5 1\n\ndf2 <- df1\ndf2\n\n  a\n1 1\n2 1\n3 1\n4 1\n5 1\n\ndf2$a <- df2$a*2\ndf2\n\n  a\n1 2\n2 2\n3 2\n4 2\n5 2\n\ndf1\n\n  a\n1 1\n2 1\n3 1\n4 1\n5 1\n\naddress(df1)\n\n[1] \"0x105b7b2a8\"\n\naddress(df2)\n\n[1] \"0x105d9d660\"\n\n\ndata.table uses “reference semantics” or “pass-by-reference”. Be very careful or you might be mightily confused:\n\ndt1 <- data.table(a = rep(1, 5))\ndt1\n\n       a\n   <num>\n1:     1\n2:     1\n3:     1\n4:     1\n5:     1\n\ndt2 <- dt1\ndt2\n\n       a\n   <num>\n1:     1\n2:     1\n3:     1\n4:     1\n5:     1\n\ndt2[, a := a * 2]\ndt2\n\n       a\n   <num>\n1:     2\n2:     2\n3:     2\n4:     2\n5:     2\n\ndt1\n\n       a\n   <num>\n1:     2\n2:     2\n3:     2\n4:     2\n5:     2\n\naddress(dt1)\n\n[1] \"0x119caa600\"\n\naddress(dt2)\n\n[1] \"0x119caa600\"\n\n\n\n\n\nIf you want to create a copy of a data.table, use copy():\n\n\n\n\ndt3 <- copy(dt1)\ndt3\n\n       a\n   <num>\n1:     2\n2:     2\n3:     2\n4:     2\n5:     2\n\naddress(dt3)\n\n[1] \"0x10491ba00\"\n\ndt3[, a := a * 2]\ndt3\n\n       a\n   <num>\n1:     4\n2:     4\n3:     4\n4:     4\n5:     4\n\ndt1\n\n       a\n   <num>\n1:     2\n2:     2\n3:     2\n4:     2\n5:     2"
  },
  {
    "objectID": "60_DataTable.html#dtresources",
    "href": "60_DataTable.html#dtresources",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.18 Resources",
    "text": "24.18 Resources\ndata.table GitHub data.table docs Introduction to data.table vignette"
  },
  {
    "objectID": "62_dplyr.html",
    "href": "62_dplyr.html",
    "title": "25  Introduction to dplyr",
    "section": "",
    "text": "The dplyr package offers functionality for data manipulation and is part of what is known as the tidyverse.\ndplyr’s functions are named after verbs and depend heavily on the usage of the pipe operator to build pipelines. The package offers a large number of functions in total, often with multiple versions of the same “verb”. It has undergone many major changes since its introduction, so always make sure to consult the latest documentation. Some of the welcome recent changes aim to reduce the total number of functions exported by the package.\nCore operations include:\ndplyr operates on data.frames as well as the tidyverse’s data.frame replacement, known by the bizarre name of tibble. Here, we convert iris to a tibble as an easy way to limit the number of rows printed by default in the output (without changing other Rmarkdown options, custom hooks, etc.) and as an introduction to tiblles.\nNote that dplyr masks a number of builtin functions when loaded."
  },
  {
    "objectID": "62_dplyr.html#filter",
    "href": "62_dplyr.html#filter",
    "title": "25  Introduction to dplyr",
    "section": "25.1 Filter",
    "text": "25.1 Filter\n\niris |> filter(Species == \"setosa\")\n\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# … with 40 more rows"
  },
  {
    "objectID": "62_dplyr.html#select",
    "href": "62_dplyr.html#select",
    "title": "25  Introduction to dplyr",
    "section": "25.2 Select",
    "text": "25.2 Select\n\niris |> select(\"Sepal.Length\")\n\n# A tibble: 150 × 1\n   Sepal.Length\n          <dbl>\n 1          5.1\n 2          4.9\n 3          4.7\n 4          4.6\n 5          5  \n 6          5.4\n 7          4.6\n 8          5  \n 9          4.4\n10          4.9\n# … with 140 more rows"
  },
  {
    "objectID": "62_dplyr.html#mutate",
    "href": "62_dplyr.html#mutate",
    "title": "25  Introduction to dplyr",
    "section": "25.3 Mutate",
    "text": "25.3 Mutate\n\niris |> mutate(Sepal_minus_Petal_length = Sepal.Length - Petal.Length)\n\n# A tibble: 150 × 6\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species Sepal_minus_Petal_…\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>                 <dbl>\n 1          5.1         3.5          1.4         0.2 setosa                  3.7\n 2          4.9         3            1.4         0.2 setosa                  3.5\n 3          4.7         3.2          1.3         0.2 setosa                  3.4\n 4          4.6         3.1          1.5         0.2 setosa                  3.1\n 5          5           3.6          1.4         0.2 setosa                  3.6\n 6          5.4         3.9          1.7         0.4 setosa                  3.7\n 7          4.6         3.4          1.4         0.3 setosa                  3.2\n 8          5           3.4          1.5         0.2 setosa                  3.5\n 9          4.4         2.9          1.4         0.2 setosa                  3  \n10          4.9         3.1          1.5         0.1 setosa                  3.4\n# … with 140 more rows\n\n\n\n25.3.1 Grouped\n\niris |> \n  group_by(Species) |> \n  mutate(Sepal.Length_minus_mean = Sepal.Length - mean(Sepal.Length))\n\n# A tibble: 150 × 6\n# Groups:   Species [3]\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species Sepal.Length_minus…\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>                 <dbl>\n 1          5.1         3.5          1.4         0.2 setosa              0.0940 \n 2          4.9         3            1.4         0.2 setosa             -0.106  \n 3          4.7         3.2          1.3         0.2 setosa             -0.306  \n 4          4.6         3.1          1.5         0.2 setosa             -0.406  \n 5          5           3.6          1.4         0.2 setosa             -0.00600\n 6          5.4         3.9          1.7         0.4 setosa              0.394  \n 7          4.6         3.4          1.4         0.3 setosa             -0.406  \n 8          5           3.4          1.5         0.2 setosa             -0.00600\n 9          4.4         2.9          1.4         0.2 setosa             -0.606  \n10          4.9         3.1          1.5         0.1 setosa             -0.106  \n# … with 140 more rows"
  },
  {
    "objectID": "62_dplyr.html#summarize",
    "href": "62_dplyr.html#summarize",
    "title": "25  Introduction to dplyr",
    "section": "25.4 Summarize",
    "text": "25.4 Summarize\n\n25.4.1 Single variable\n\niris |> summarize(mean(Sepal.Length))\n\n# A tibble: 1 × 1\n  `mean(Sepal.Length)`\n                 <dbl>\n1                 5.84\n\n\n\n\n25.4.2 Multiple variables\n\niris |> summarize(across(c(Sepal.Length, Petal.Length), mean))\n\n# A tibble: 1 × 2\n  Sepal.Length Petal.Length\n         <dbl>        <dbl>\n1         5.84         3.76\n\n\n\n\n25.4.3 Grouped single var\n\niris |> \n  group_by(Species) |> \n  summarize(mean(Sepal.Length))\n\n# A tibble: 3 × 2\n  Species    `mean(Sepal.Length)`\n  <fct>                     <dbl>\n1 setosa                     5.01\n2 versicolor                 5.94\n3 virginica                  6.59\n\n\n\n\n25.4.4 Grouped multivar\n\niris |> \n  group_by(Species) |> \n  summarize(across(c(Sepal.Length, Petal.Length), mean))\n\n# A tibble: 3 × 3\n  Species    Sepal.Length Petal.Length\n  <fct>             <dbl>        <dbl>\n1 setosa             5.01         1.46\n2 versicolor         5.94         4.26\n3 virginica          6.59         5.55"
  },
  {
    "objectID": "62_dplyr.html#arrange",
    "href": "62_dplyr.html#arrange",
    "title": "25  Introduction to dplyr",
    "section": "25.5 Arrange",
    "text": "25.5 Arrange\n\niris |> arrange(Sepal.Length)\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          4.3         3            1.1         0.1 setosa \n 2          4.4         2.9          1.4         0.2 setosa \n 3          4.4         3            1.3         0.2 setosa \n 4          4.4         3.2          1.3         0.2 setosa \n 5          4.5         2.3          1.3         0.3 setosa \n 6          4.6         3.1          1.5         0.2 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          4.6         3.6          1           0.2 setosa \n 9          4.6         3.2          1.4         0.2 setosa \n10          4.7         3.2          1.3         0.2 setosa \n# … with 140 more rows\n\n\n\n25.5.1 Grouped\n\niris |> \n  group_by(Species) |> \n  arrange(Sepal.Length)\n\n# A tibble: 150 × 5\n# Groups:   Species [3]\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          4.3         3            1.1         0.1 setosa \n 2          4.4         2.9          1.4         0.2 setosa \n 3          4.4         3            1.3         0.2 setosa \n 4          4.4         3.2          1.3         0.2 setosa \n 5          4.5         2.3          1.3         0.3 setosa \n 6          4.6         3.1          1.5         0.2 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          4.6         3.6          1           0.2 setosa \n 9          4.6         3.2          1.4         0.2 setosa \n10          4.7         3.2          1.3         0.2 setosa \n# … with 140 more rows"
  },
  {
    "objectID": "62_dplyr.html#specifying-multiple-variables",
    "href": "62_dplyr.html#specifying-multiple-variables",
    "title": "25  Introduction to dplyr",
    "section": "25.6 Specifying multiple variables",
    "text": "25.6 Specifying multiple variables\ndplyr includes a number of ways to identify multiple variables. The latest version of dplyr suggests using across() within dplyr functions that allow specifying columns.\nThis replaces separate functions previously used for each of filter/select/mutate/summarize/arrange that had independent functions ending with *_all(), *_at(), *_each(), *_if().\nWe’ll use summarize() to demonstrate.\n\n25.6.1 By integer column index\n\niris |> summarize(across(1:4, mean))\n\n# A tibble: 1 × 4\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n         <dbl>       <dbl>        <dbl>       <dbl>\n1         5.84        3.06         3.76        1.20\n\n\n\n\n25.6.2 By character name range\n\niris |> summarize(across(Sepal.Length:Petal.Width, mean))\n\n# A tibble: 1 × 4\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n         <dbl>       <dbl>        <dbl>       <dbl>\n1         5.84        3.06         3.76        1.20\n\n\n\n\n25.6.3 Pattern-matching\n\niris |> summarize(across(starts_with(\"Sepal\"), mean))\n\n# A tibble: 1 × 2\n  Sepal.Length Sepal.Width\n         <dbl>       <dbl>\n1         5.84        3.06\n\n\n\niris |> summarize(across(ends_with(\"Length\"), mean))\n\n# A tibble: 1 × 2\n  Sepal.Length Petal.Length\n         <dbl>        <dbl>\n1         5.84         3.76\n\n\n\n\n25.6.4 Using predicate function wrapped in where()\n\niris |> summarize(across(where(is.numeric), mean))\n\n# A tibble: 1 × 4\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n         <dbl>       <dbl>        <dbl>       <dbl>\n1         5.84        3.06         3.76        1.20\n\n\nUsing table() in this way does not output level names:\n\niris |> summarize(across(where(is.factor), table))\n\n# A tibble: 3 × 1\n  Species\n  <table>\n1 50     \n2 50     \n3 50"
  },
  {
    "objectID": "62_dplyr.html#dplyrresources",
    "href": "62_dplyr.html#dplyrresources",
    "title": "25  Introduction to dplyr",
    "section": "25.7 Resources",
    "text": "25.7 Resources\n\ndplyr website\ndplyr cheatsheet"
  },
  {
    "objectID": "70_BaseGraphics.html",
    "href": "70_BaseGraphics.html",
    "title": "26  Base Graphics",
    "section": "",
    "text": "R has powerful graphical capabilities built in to the core language. This chapter is an introduction to what is known as base graphics which is provided by the graphics built-in package. Their defaults produce minimalist plots, but they can be customized extensively. In this chapter we shall begin with default plots and demonstrate some of the more common/useful ways to customize them.\nR documentation for each of the above commands provides extensive coverage of graphical parameters. ?par gives the main documentation file for a long list of graphical parameters. These can be set either with the par() command before using any plotting command.\nLet’s create some synthetic data:"
  },
  {
    "objectID": "70_BaseGraphics.html#scatter-plot",
    "href": "70_BaseGraphics.html#scatter-plot",
    "title": "26  Base Graphics",
    "section": "26.1 Scatter plot",
    "text": "26.1 Scatter plot\nInput: 2 numeric vectors\nA 2D scatterplot displays points using two numeric vectors as X and Y coordinates.\n\nplot(x, y)\n\n\n\n\n\n26.1.1 col: point color\nSee Colors in R to learn about the different ways to define colors in R.\nSome common ways include:\n\nBy name using one of 657 names given by colors(), e.g. “red”, “magenta”, “blue”, “navy”, “cyan”\nBy RGB code\n\n\nplot(x, y, col = \"red\")\n\n\n\n\n\n\n26.1.2 bty: box type\nThere are 7 bty options: “o” “l”, “7”, “c”, “u”, or “]” and “none”. They produce a box that resembles the corresponding symbol. “none” draws no box but allows the axes to show:\n\nplot(x, y, bty = \"l\")\n\n\n\n\n\nplot(x, y, bty = \"none\")\n\n\n\n\n\n\n26.1.3 pch: point character\nThe default point character is a circle as seen above. This helps visualize overlapping points (especially for devices that do not support transparency).\nThere are 25 point characters, designated by integers 1 through 25.\nHere’s a preview of all 25 pch options. pch types 21 through 25 can be filled by a color specified by bg.\n\nplot(1:25, rep(1, 25), pch = 1:25, bg = \"blue\")\n\n\n\n\nLet’s use a solid disc:\n\nplot(x, y, bty = \"n\", pch = 16)\n\n\n\n\nWe cannot tell how many points are overlapping in the middle and therefore it’s a good idea to make the points a little transparent.\nThere are different ways to add transparency (see Colors). The easiest way is probably to use adjustcolor(). In the context of colors, alpha refers to transparency: a = 1 is opaque and a = 0 is completely transparent (therefore use a value greater than 0).\n\nplot(x, y,\n     bty = \"n\", pch = 16,\n     col = adjustcolor(\"skyblue\", alpha.f = .5))\n\n\n\n\n\n\n26.1.4 grid\nWe can add a grid behind the plot area using the panel.first argument, which accepts a graphical expression (a function that draws something), which will be evaluated before plotting the points on the graph (therefore appears behind the points).\n\nplot(x, y,\n     bty = \"n\", pch = 16,\n     col = adjustcolor(\"skyblue\", alpha.f = .5),\n     panel.first = grid(lty = 1, col = 'gray90'))\n\n\n\n\n\n\n26.1.5 main, xlab, ylab: Title and axes labels\n\nplot(x, y,\n     bty = \"n\", pch = 16,\n     col = adjustcolor(\"skyblue\", alpha.f = .5),\n     panel.first = grid(lty = 1, col = 'gray90'),\n     main = \"y vs. x\",\n     xlab = \"Variable x (xunits)\",\n     ylab = \"Variable y (yunits)\")\n\n\n\n\nNote that depending on where you intend to display the plot, you may leave the title blank and instead place it in the figure caption along with an explanation of the data (e.g. in a journal article)"
  },
  {
    "objectID": "70_BaseGraphics.html#histogram",
    "href": "70_BaseGraphics.html#histogram",
    "title": "26  Base Graphics",
    "section": "26.2 Histogram",
    "text": "26.2 Histogram\nInput: numeric vector\nA histogram displays an approximation of the distribution of a numeric vector. First the data is binned and then the number of elements that falls in each bin is counted. The histogram plot draws bars for each bin whose heights corresponds to the count of elements in the corresponding interval.\n\nhist(x)\n\n\n\n\n\n26.2.1 col: bar color\n\nhist(x, col = \"slategrey\")\n\n\n\n\n\n\n26.2.2 border: border color\nSetting border color to the same as the background gives a clean look:\n\nhist(x, col = \"slategrey\", border = \"white\")\n\n\n\n\n\n\n26.2.3 breaks: number or value of breakpoints\nThe breaks argument can be used to define the breakpoints to use for the binning of the values of the input to hist(). See the documentation in ?hist for the full range of options. An easy way to control the number of bins is to pass an integer to the breaks argument. Depending on the length of x and its distribution, it may or may not be possible to use the exact number requested, but the closest possible number will be automatically chosen.\n\nhist(x, col = \"slategrey\", border = \"white\",\n     breaks = 8)"
  },
  {
    "objectID": "70_BaseGraphics.html#density-plot",
    "href": "70_BaseGraphics.html#density-plot",
    "title": "26  Base Graphics",
    "section": "26.3 Density plot",
    "text": "26.3 Density plot\nInput: numeric vector\nA density plot is a different way to display an approximation of the distribution of a numeric vector. The density() function estimates the density of x and can be passed to plot() directly:\n\nplot(density(x))\n\n\n\n\nYou can use main = NA or main = \"\" to suppress printing a title.\n\nplot(density(x), col = \"blue\",\n     bty = \"n\",\n     main = NA)"
  },
  {
    "objectID": "70_BaseGraphics.html#barplot",
    "href": "70_BaseGraphics.html#barplot",
    "title": "26  Base Graphics",
    "section": "26.4 Barplot",
    "text": "26.4 Barplot\nInput: vector or matrix\nLet’s look at the VADeaths built-in dataset which describes death rater per 1000 population per year broken down by age range and population group.\n\n26.4.1 Single vector\nWe can plot a single column or row. Note how R automatically gets the corresponding dimension names. For this example we use the builtin VADeaths dataset, which is a matrix.\n\nbarplot(VADeaths[, 1])\n\n\n\n\n\nbarplot(VADeaths[1, ])\n\n\n\n\n\n26.4.1.1 col and border: bar fill and border color\nAs in most plotting functions, color is controlled by the col argument. border can be set to any color separately, or to NA to omit, which gives a clean look:\n\nbarplot(VADeaths[, 1],\n        col = \"aquamarine3\", border = NA)\n\n\n\n\n\n\n\n26.4.2 Matrix\nWe can draw barplots of multiple columns at the same time by passing a matrix input. The grouping on the x-axis is based on the columns. By default, data from different rows is stacked. The argument legend.text can be used to add a legend with the row labels:\n\nbarplot(VADeaths, legend.text = TRUE)\n\n\n\n\nAlternatively, we can draw groups of bars beside each other with the argument beside = TRUE:\n\nbarplot(VADeaths, beside = TRUE,\n        legend.text = TRUE, args.legend = list(x = \"topright\"))\n\n\n\n\nTo use custom colors, we pass a vector of length equal to the number of bars within each group. These will get recycled across groups, giving a consistent color coding.\nHere, we use the adjustcolor() function again to produce 5 shades of navy.\n\ncol <- sapply(seq(.2, .8, length.out = 5), function(i) adjustcolor(\"navy\", i))\nbarplot(VADeaths,\n        col = col,\n        border = NA,\n        beside = TRUE,\n        legend.text = TRUE, args.legend = list(x = \"topright\"))\n\n\n\n\n\n\n26.4.3 Formula interface"
  },
  {
    "objectID": "70_BaseGraphics.html#boxplot",
    "href": "70_BaseGraphics.html#boxplot",
    "title": "26  Base Graphics",
    "section": "26.5 Boxplot",
    "text": "26.5 Boxplot\nInput: One or more vectors of any length\nA boxplot is another way to visualize the distribution of one or more vectors. Each vector does not need to be of the same length. For example if you are plotting lab results of a patient and control group, they do not have to contain the same number of individuals.\nThere are two ways to use the boxplot() function. Either pass two separate vectors of data (whet)\nboxplot() makes it easy to plot your data from different objects. It can accept:\n\nindividual vectors\ncolumns of a matrix, columns/elements of a data.frame, elements of a list\nformula interface of the form variable ~ factor\n\n\n26.5.1 Single vector\n\na <- rnorm(500, mean = 12, sd = 2)\nboxplot(a)\n\n\n\n\n\n\n26.5.2 Anatomy of a boxplot\nA boxplot shows:\n\nthe median\nfirst and third quartiles\noutliers (defined as x < Q1 - 1.5 * IQR | x > Q3 + 1.5 * IQR)\nrange after excluding outliers\n\n\n\n\n\n\nBoxplot anatomy\n\n\n\n\nSome synthetic data:\n\nalpha <- rnorm(10)\nbeta <- rnorm(100)\ngamma <- rnorm(200, 1, 2)\ndl <- list(alpha = alpha, beta = beta, gamma = gamma)\n\n\n\n26.5.3 Multiple vectors\n\nboxplot(alpha, beta, gamma)\n\n\n\n\n\n\n26.5.4 List\n\nboxplot(dl)\n\n\n\n\n\n\n26.5.5 Matrix\nPassing a matrix to boxplot() draws one boxplot per column:\n\nmat <- sapply(seq(5), function(i) rnorm(20))\nboxplot(mat)\n\n\n\n\n\n\n26.5.6 Formula interface\nThe formula interface can be used to group any vector by a factor of the same length.\nLet’s use the built-in sleep dataset which shows the effect of two different drugs in increasing hours of sleep compared to a control group.\n\nboxplot(extra ~ group, sleep)\n\n\n\n\nThe col and border arguments work as expected. Here we define two custom colors using their hexadecimal RGB code and use the solid version for the border and a 50% transparent version for the fill. Note that we do not need two separate colors to produce an unambiguous plot since they are clearly labeled in the y-axis. It is often considered desirable/preferred to use the minimum number of different colors that is necessary. (Color coding like the following could be useful if for example data from the two groups were used on a different plot, like a scatterplot, in a multi-panel figure).\n\nborder <- c(\"#18A3AC\", \"#F48024\")\ncol <- c(adjustcolor(\"#18A3AC\", .5), adjustcolor(\"#F48024\", .5))\nboxplot(extra ~ group, sleep,\n        col = col, border = border)\n\n\n\n\n\n\n26.5.7 names: group labels\nThe x-axis group names can be defined with the names argument:\n\nboxplot(extra ~ group, sleep,\n        col = col, border = border,\n        names = c(\"Drug A\", \"Drug B\"))"
  },
  {
    "objectID": "70_BaseGraphics.html#heatmap",
    "href": "70_BaseGraphics.html#heatmap",
    "title": "26  Base Graphics",
    "section": "26.6 Heatmap",
    "text": "26.6 Heatmap\nInput: matrix\nA heatmap is a 2D matrix-like plot with x- and y-axis labels and a value in each cell. It can be used to display many different types of data. A common usage in data science is to plot the correlation matrix of a set of numerical features. In many cases, the rows and/or columns of a heatmap can be reordered based on hierarchical clustering.\n\nx <- sapply(1:20, function(i) rnorm(20))\nx_cor <- cor(x)\n\nBy default, the heatmap() function draws marginal dendrograms and rearranges rows and columns. We can prevent that by setting Rowv and Colv to NA:\n\nheatmap(x_cor, Rowv = NA, Colv = NA)\n\n\n\n\nTo allow clustering and row and column reordering, use the defaults:\n\nheatmap(x_cor)"
  },
  {
    "objectID": "70_BaseGraphics.html#mosaicplot",
    "href": "70_BaseGraphics.html#mosaicplot",
    "title": "26  Base Graphics",
    "section": "26.7 Mosaic plot",
    "text": "26.7 Mosaic plot\nMosaic plots are used to visualize contingency tables. They can be informative to look at during data exploration. They are less likely to be included in a research article where the table itself is more likely to be included.\nSynthetic data:\n\nset.seed(2021)\nCohort <- factor(sample(c(\"Control\", \"Case\"), 500, TRUE),\n                 levels = c(\"Control\", \"Case\"))\nSex <- factor(\n  sapply(seq(Cohort), \\(i) sample(c(\"Male\", \"Female\"), 1,\n                                  prob = if (Cohort[i] == \"Control\") c(1, 1) else c(2, 1))))\n\nUse mosaicplot() on the output of table():\n\nmosaicplot(table(Cohort), main = \"Cases vs. Controls\")\n\n\n\n\nWe can plot the breakdown of sexes, this time also adding color:\n\nmosaicplot(table(Sex), main = \"Males vs. Females\",\n           col = c(\"orchid\", \"skyblue\"))\n\n\n\n\nCross-tabulating is usually most informatively. We us the same color for the sexes, which will be recycled. We also remove the border for a cleaner look:\n\nmosaicplot(table(Cohort, Sex),\n           color = c(\"orchid\", \"skyblue\"),\n           border = NA,\n           main = \"Cohort x Sex\")"
  },
  {
    "objectID": "70_BaseGraphics.html#graphical-parameters",
    "href": "70_BaseGraphics.html#graphical-parameters",
    "title": "26  Base Graphics",
    "section": "26.8 Graphical parameters",
    "text": "26.8 Graphical parameters\nThe par() function allows setting or querying graphical parameters of the base graphics system. Have a look at its documentation (?par).\nSome graphical parameters can only be set with a call to par() prior to using a base plotting function. However, many parameters can also be passed using the ... construct of each base plotting function.\nSome common base graphical parameters:\n\npch: Point character\ncol: Color\ncex: Character expansion, i.e. relative size\nbty: Box type\nxlab: x-axis label\nylab: y-axis label\nmain: Main title\nmar: Plot margins\n\nYou can see what the current value of these parameters is by calling par() or directly accessing a specific parameter:\n\npar()$mar\n\n[1] 5.1 4.1 4.1 2.1\n\n\nmar sets the plot margins. It is a vector of length 4 and each number corresponds to the bottom-left-top-right margin, in that order. Use it to reduce empty white space between plots or add space if labels are getting cropped, for example.\nAlways make sure that your plotting characters, axis labels and titles are legible. You must avoid, at all costs, ever using a huge graph with tiny letters spread over an entire slide in a presentation.\n\ncex: Character expansion for the plotting characters\ncex.axis: cex for axis annotation\ncex.lab: cex for x and y labels\ncex.main: cex for main title\n\nNote: All of these can be set either with a call to par() prior to plotting or passed as arguments in a plotting command, like plot().\nThere is one important distinction: cex set with par() (which defaults to 1), sets the baseline and all other cex parameters multiply it. However, cex set within plot() still multiplies cex set with par(), but only affects the plotting character size.\n\n26.8.1 Save and reload graphical parameters\nYou can make a copy of all current graphical parameters:\n\npar_default <- par()\n\nThere are a few parameters that you cannot control, those are read-only. You can optionally exclude those since you cannot edit them anyway:\n\npar_default <- par(no.readonly = T)\n\nIf you make changes to par() to produce plots and you want to recover the parameters you saved above, you can use reload them by passing them to par():\n\npar(par_default)\n\nAlternatively, you can always restart the graphics device using dev.off() and then making a new plot.\nNote: here “device” does not refere to a physical device but software graphics interfaces that show a plot to screen or save to file.\n\ndev.off() # shuts down graphics device\n\nnull device \n          1 \n\nplot(rnorm(10))"
  },
  {
    "objectID": "70_BaseGraphics.html#multipanel-plots",
    "href": "70_BaseGraphics.html#multipanel-plots",
    "title": "26  Base Graphics",
    "section": "26.9 Multipanel plots",
    "text": "26.9 Multipanel plots\nThere are different ways to create multipanel plots, but probably the most straightforward is to use either the mfrow or the mfcol argument of par().\n\nset.seed(2020)\nx <- rnorm(500)\ny <- x^3 + rnorm(500) * 2\nz <- x^2 + rnorm(500)\n\nBoth mfrow and mfcol accept an integer vector of length 2 indicating number of rows and number of columns, respectively. With mfrow, the plots are drawn row-wise and with mfcol they are drawn column-wise. Remember to reset mfrow or mfcol back to c(1, 1)\nFor example, let’s plot a 2-by-3 panel of plots, drawn row-wise:\n\npar(mfrow = c(2, 3), mar = c(4, 4, 1, 1))\nhist(x, col = \"#052049bb\", border = \"white\", main = \"\")\nhist(y, col = \"#052049bb\", border = \"white\", main = \"\")\nhist(z, col = \"#052049bb\", border = \"white\", main = \"\")\nplot(x, y, col = \"#05204955\", pch = 16, bty = \"n\")\nplot(x, z, col = \"#05204955\", pch = 16, bty = \"n\")\nplot(y, z, col = \"#05204955\", pch = 16, bty = \"n\")\n\n\n\npar(mfrow = c(1, 1))"
  },
  {
    "objectID": "70_BaseGraphics.html#saveplots",
    "href": "70_BaseGraphics.html#saveplots",
    "title": "26  Base Graphics",
    "section": "26.10 Saving plots to file",
    "text": "26.10 Saving plots to file\nYou can save base graphics to disk using a number of different file formats. To do this, you have to:\n\nOpen a graphics device - e.g. pdf(\"path/to/xy_scatter.pdf\")\nWrite to it - e.g. plot(x, y)\nClose graphics device - dev.off()\n\nThe following commands are used to open graphical devices that will save to a file of the corresponding type:\n\nbmp(filename = \"path/to/file\", width = [in pixels], height = [in pixels])\njpeg(filename = \"path/to/file\", width = [in pixels], height = [in pixels])\npng(filename = \"path/to/file\", width = [in pixels], height = [in pixels])\ntiff(filename = \"path/to/file\", width = [in pixels], height = [in pixels])\nsvg(filename = \"path/to/file\", width = [in inches], height = [in inches]\npdf(file = \"path/to/file\", width = [in inches], height = [in inches])\n\nNotice that when writing to a vector graphics format (svg and pdf), you defined width and height in inches, not pixels. Also, you specify file instead of filename in Notice the difference when writing to PDF: you define a file instead of a filename, and width and height are in inches, not pixels.\nIt is recommended to save plots in PDF format because it handles vector graphics therefore plots will scale, and it is easy to export to other graphics formats later on if needed.\n\npdf(\"~/Desktop/plot.pdf\", width = 5, height = 5)\nplot(iris$Sepal.Length, iris$Petal.Length,\n     pch = 16,\n     col = \"#18A3AC66\",\n     cex = 1.8,\n     bty = \"n\", # also try \"l\"\n     xlab = \"Sepal Length\", ylab = \"Petal Length\")\ndev.off()"
  },
  {
    "objectID": "72_3xGraphics.html",
    "href": "72_3xGraphics.html",
    "title": "27  3x Graphics",
    "section": "",
    "text": "Visualization is central to statistics and data science. It is used to check data, explore data, and communicate results.\nR has powerful graphical capabilities built in to the core language. It contains two largely separate graphics systems: ‘base’ graphics in the graphics package, inherited from the S language, and ‘grid’ graphics in the grid package: a “rewrite of the graphics layout capabilities”. There is limited support for interaction between the two. In practice, for a given application, choose one or the other. There are no high level functions for the grid graphics system built into the base R distribution, but a few very popular packages have been built on top of it. Both graphics systems can produce beautiful, layered, high quality graphics. It is possible to build functions using either system to produce most, if not all, types of plots."
  },
  {
    "objectID": "72_3xGraphics.html#base-graphics",
    "href": "72_3xGraphics.html#base-graphics",
    "title": "27  3x Graphics",
    "section": "27.1 Base graphics",
    "text": "27.1 Base graphics\nCommon R plotting functions like plot, barplot, boxplot, heatmap, etc. are built ontop of base graphics (Murrell 2018). Their default arguments provide a minimalist output, but can be tweaked extensively. An advantage of base graphics is they are very fast and relatively easy to extend.\nThe par function allows setting or querying graphical parameters of the base graphics system. Have a look at its documentation (?par).\nSome graphical parameters can only be set with a call to par prior to using a base plotting function. However, many parameters can also be passed using the ... construct of each base plotting function.\nSome common base graphical parameters:\n\npch: Point character\ncol: Color\ncex: Character expansion, i.e. relative size\nbty: Box type\nxlab: x-axis label\nylab: y-axis label\nmain: Main title\n\nAlways make sure that your plotting characters, axis labels and titles are legible. You must avoid, at all costs, ever using a huge graph with tiny letters spread over a whole slide in a presentation.\n\ncex: Character expansion for the plotting characters\ncex.axis: cex for axis annotation\ncex.lab: cex for x and y labels\ncex.main: cex for main title\n\nNote: All of these can be set either with a call to par() prior to plotting or passed as arguments in a plotting command, like plot().\nHowever, there is one important distinction: cex set with par() (which defaults to 1), sets the baseline and all other cex parameters multiply it. However, cex set within plot() stil multiplies cex set with par(), but only affectts the plotting character size."
  },
  {
    "objectID": "72_3xGraphics.html#grid-graphics",
    "href": "72_3xGraphics.html#grid-graphics",
    "title": "27  3x Graphics",
    "section": "27.2 Grid graphics",
    "text": "27.2 Grid graphics\nThe two most popular packages built on top of the grid package are:\n\nlattice (Sarkar 2008)\nggplot2 (Wickham 2011)\n\n\n27.2.1 ggplot2\nggplot2, created by Hadley Wickham (Wickham 2011), follows the Grammar of Graphics approach of Leland Wilkinson (Wilkinson 2012) and has a very different syntax than base functions.\nThe general idea is to start by defining the data and then add and/or modify graphical elements in a stepwise manner, which allows one to build complex and layered visualizations. A simplified interface to ggplot graphics is provided in the qplot() function of ggplot2 (but you should avoid it and use learn to use the ggplot() command which is fun and much more flexible and useful to know)"
  },
  {
    "objectID": "72_3xGraphics.html#rd-party-apis",
    "href": "72_3xGraphics.html#rd-party-apis",
    "title": "27  3x Graphics",
    "section": "27.3 3rd party APIs",
    "text": "27.3 3rd party APIs\nThere are also third party libraries with R APIs that provide even more modern graphic capabilities to the R user:\n\nplotly (Sievert et al. 2017)\nrbokeh\n\nBoth build interactive plots, which can be viewed in a web browser or exported to bitmap graphics, and both also follow the grammar of graphics paradigm, and therefore follow similar syntax to ggplot2.\nThe rtemis package (Gennatas 2017) provides visualization functions built on top of base graphics (for speed and extendability) and plotly (for interactivity):\n\nmplot3 static graphics (base)\ndplot3 interactive graphics (plotly)\n\nLet’s go over the most common plot types using base graphics, rtemis (`mplot3, dplot3), ggplot2, and plotly.\nThis is meant to get you started but is barely scratching the surface. There is extensive functionality included in each plotting library and you should consult the respective documentation for details."
  },
  {
    "objectID": "72_3xGraphics.html#box-plot",
    "href": "72_3xGraphics.html#box-plot",
    "title": "27  3x Graphics",
    "section": "27.4 Box plot",
    "text": "27.4 Box plot\nLet’s create some synthetic data.\n\nset.seed(2019)\nx <- as.data.frame(matrix(rnorm(200*4), 200))\ncolnames(x) <- c(\"mango\", \"banana\", \"tangerine\", \"sugar\")\n\n\n27.4.1 base\n\nboxplot(x)\n\n\n\n\n\nboxplot(x, col = \"steelblue4\")\n\n\n\n\n\n\n27.4.2 mplot3\n\nmplot3_box(x)\n\n\n\n\n\n\n27.4.3 dplot3\n\ndplot3_box(x)\n\n\n\n\n\n\n\n27.4.4 ggplot2\nAgain, ggplot requires an explicit categorical x-axis. In this case, this means we need to convert our dataset from wide to long.\nHere we use tidyr’s pivot_longer() function, since it is part of the same cult known as the tidyverse. You can instead use the builtin reshape() function or any wide-to-long operation of your choosing, e.g. data.table’s melt().\n\nlibrary(tidyr)\nx.long <- pivot_longer(x, 1:4, \n                       names_to = \"Fruit\", \n                       values_to = \"Feature\")\nx.long\n# A tibble: 800 × 2\n   Fruit     Feature\n   <chr>       <dbl>\n 1 mango      0.739 \n 2 banana     0.721 \n 3 tangerine -1.26  \n 4 sugar     -0.0187\n 5 mango     -0.515 \n 6 banana    -0.395 \n 7 tangerine  0.258 \n 8 sugar     -0.0301\n 9 mango     -1.64  \n10 banana     0.983 \n# … with 790 more rows\n\np <- ggplot(x.long, aes(Fruit, Feature)) + geom_boxplot()\np\n\n\n\n\nAdd some color:\n\n(p <- ggplot(x.long, aes(Fruit, Feature)) +\n   geom_boxplot(fill = c(\"#44A6AC66\", \"#F4A36266\", \"#3574A766\", \"#C23A7066\"),\n                colour = c(\"#44A6ACFF\", \"#F4A362FF\", \"#3574A7FF\", \"#C23A70FF\")))\n\n\n\n\n\n\n27.4.5 plotly\nIn plotly, we can use a loop to add each column’s boxplot one at a time. In the following example, we turn off the legend, since the names also appear below each boxplot:\n\nplt <- plot_ly(type = \"box\")\nfor (i in seq_along(x)) {\n  plt <- add_trace(plt, y = x[, i], name = colnames(x)[i])\n}\nplt |> layout(showlegend = F)"
  },
  {
    "objectID": "72_3xGraphics.html#histogram",
    "href": "72_3xGraphics.html#histogram",
    "title": "27  3x Graphics",
    "section": "27.5 Histogram",
    "text": "27.5 Histogram\n\nset.seed(2020)\na <- rnorm(500)\n\n\n27.5.1 base\n\nhist(a)\n\n\n\nhist(a, col = \"#18A3AC66\")\n\n\n\nhist(a, col = \"#18A3AC99\", border = \"white\", main = \"\", breaks = 30)\n\n\n\n\n\n\n27.5.2 mplot3\n\nmplot3_x(a, \"histogram\")\n\n\n\nmplot3_x(a, \"histogram\", hist.breaks = 30)\n\n\n\n\n\n\n27.5.3 dplot3\n\ndplot3_x(a, \"hist\")\n\n\n\n\ndplot3_x(a, \"hist\", hist.n.bins = 40)\n\n\n\n\n\n\n\n27.5.4 ggplot2\n\n(p <- ggplot(mapping = aes(a)) + geom_histogram())\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n(p <- ggplot(mapping = aes(a)) + geom_histogram(binwidth = .2))\n\n\n\n(p <- ggplot(mapping = aes(a)) +\n    geom_histogram(binwidth = .2, fill = \"#18A3AC99\"))\n\n\n\n\n\n\n27.5.5 plotly\n\nplt <- plot_ly(x = a, type = \"histogram\") |> \n  layout(bargap = .1)\nplt\n\n\n\n\n\n\n27.5.5.1 Grouped\n\nmplot3_x(iris$Petal.Length, \"hist\", group = iris$Species, hist.breaks = 10)\n\n\n\n\n\ndplot3_x(iris$Sepal.Length, \"hist\", group = iris$Species)\n\n\n\n\n\nor “ridge”-mode:\n\ndplot3_x(iris$Sepal.Length, \"hist\", group = iris$Species,\n         mode = \"ridge\")\n\n\n\n\n\n\nggplot(iris, aes(x = Sepal.Length, fill = Species)) + \n  geom_histogram(binwidth = .1)"
  },
  {
    "objectID": "72_3xGraphics.html#density-plot",
    "href": "72_3xGraphics.html#density-plot",
    "title": "27  3x Graphics",
    "section": "27.6 Density plot",
    "text": "27.6 Density plot\nThere is no builtin density plot, but you can get x and y coordinates from the density function and add a polygon:\n\n27.6.1 base\n\n.density <- density(iris$Sepal.Length)\nclass(.density)\n\n[1] \"density\"\n\nplot(.density$x, .density$y,\n     type = \"l\", yaxs = \"i\")\n\n\n\nplot(.density$x, .density$y,\n     type = 'l', yaxs = \"i\",\n     bty = \"n\",\n     xlab = \"\",  ylab = \"Density\",\n     col = \"#18A3AC66\",\n     main = \"Sepal Length Density\")\npolygon(c(.density$x, rev(.density$x)), c(.density$y, rep(0, length(.density$y))),\n        col = \"#18A3AC66\", border = NA)\n\n\n\n\n\n\n27.6.2 mplot3\n\nmplot3_x(iris$Sepal.Length, 'density')\n\n\n\n\n\n\n27.6.3 dplot3\n\ndplot3_x(iris$Sepal.Length)\n\n\n\n\n\n\n\n27.6.4 ggplot2\n\nggplot(iris, aes(x = Sepal.Length)) + geom_density()\n\n\n\n\nAdd color:\n\nggplot(iris, aes(x = Sepal.Length)) + geom_density(color = \"#18A3AC66\", fill = \"#18A3AC66\")\n\n\n\n\n\n27.6.4.1 Grouped\n\nmplot3_x(iris$Sepal.Length, group = iris$Species)\n\n\n\n\n\ndplot3_x(iris$Sepal.Length, group = iris$Species)\n\n\n\n\n\n\n(ggplot(iris, aes(Sepal.Length, color = Species, fill = Species)) + \n  geom_density(alpha = .5) +\n  scale_color_manual(values = c(\"#44A6AC\", \"#F4A362\", \"#3574A7\")) +\n  scale_fill_manual(values = c(\"#44A6AC\", \"#F4A362\", \"#3574A7\")) +\n  labs(x = \"Sepal Length\", y = \"Density\"))"
  },
  {
    "objectID": "72_3xGraphics.html#barplot",
    "href": "72_3xGraphics.html#barplot",
    "title": "27  3x Graphics",
    "section": "27.7 Barplot",
    "text": "27.7 Barplot\n\nschools <- data.frame(UCSF = 4, Stanford = 7, Penn = 12)\n\n\n27.7.1 base\n\nbarplot(as.matrix(schools))\n\n\n\nbarplot(as.matrix(schools), col = \"dodgerblue3\")\n\n\n\n\n\n\n27.7.2 mplot3\n\nmplot3_bar(schools)\n\n\n\n\n\n\n27.7.3 dplot3\n\ndplot3_bar(schools)\n\n\n\n\n\n\n\n27.7.4 ggplot2\nggplot requires an explicit column in the data that define the categorical x-axis:\n\nschools.df <- data.frame(University = colnames(schools),\n                         N_schools = as.numeric(schools[1, ]))\nggplot(schools.df, aes(University, N_schools)) +\n  geom_bar(stat = \"identity\", color = \"#18A3AC\", fill = \"#18A3AC\")\n\n\n\n\n\n\n27.7.5 plotly\n\nplt <- plot_ly(x = names(schools),\n               y = unlist(schools),\n               name = \"Schools\",\n               type = \"bar\")\nplt\n\n\n\n\n\nNote that for the above to work, y needs to be a vector, and since x was a data.frame of one line, we use unlist() to convert to a vector."
  },
  {
    "objectID": "72_3xGraphics.html#scatterplot",
    "href": "72_3xGraphics.html#scatterplot",
    "title": "27  3x Graphics",
    "section": "27.8 Scatterplot",
    "text": "27.8 Scatterplot\n\n27.8.1 base\nA default base graphics plot is rather minimalist:\n\nplot(iris$Sepal.Length, iris$Petal.Length)\n\n\n\n\nBy tweaking a few parameters, we get a perhaps prettier result:\n\nplot(iris$Sepal.Length, iris$Petal.Length,\n     pch = 16,\n     col = \"#18A3AC66\",\n     cex = 1.4,\n     bty = \"n\",\n     xlab = \"Sepal Length\", ylab = \"Petal Length\")\n\n\n\n\n\n\n27.8.2 mplot3\n\nmplot3_xy(iris$Sepal.Length, iris$Petal.Length)\n\n\n\n\ndplot3 provides similar functionality to mplot3, built on top of plotly. Notice how you can interact with the plot using the mouse:\n\n\n27.8.3 dplot3\n\ndplot3_xy(iris$Sepal.Length, iris$Petal.Length)\n\n\n\n\n\n\n\n27.8.4 ggplot2\nNote: The name of the package is ggplot2, the name of the function is ggplot.\n\nggplot(iris, aes(Sepal.Length, Petal.Length)) + geom_point()\n\n\n\n\n\n\n27.8.5 plotly\n\np <- plot_ly(iris, x = ~Sepal.Length, y = ~Petal.Length) %>% \n  add_trace(type = \"scatter\", mode = \"markers\")\np\n\n\n\n\n\n\n\n27.8.6 Grouped\nIn mplot3 and dplot3, add a group argument:\n\nmplot3_xy(iris$Sepal.Length, iris$Petal.Length,\n          group = iris$Species)\n\n\n\n\n\ndplot3_xy(iris$Sepal.Length, iris$Petal.Length,\n          group = iris$Species)\n\n\n\n\n\nIn ggplot2, specify color within aes.\nggplot() plots can be assigned to an object. Print the object to view it.\n\np <- ggplot(iris, aes(Sepal.Length, Petal.Length, color = Species)) +\n  geom_point()\np\n\n\n\n\nIn plotly define the color argument:\n\np <- plot_ly(iris, x = ~Sepal.Length, y = ~Petal.Length, color = ~Species) %>% \n  add_trace(type = \"scatter\", mode = \"markers\")\np"
  },
  {
    "objectID": "72_3xGraphics.html#scatterplot-with-fit",
    "href": "72_3xGraphics.html#scatterplot-with-fit",
    "title": "27  3x Graphics",
    "section": "27.9 Scatterplot with fit",
    "text": "27.9 Scatterplot with fit\n\n27.9.1 mplot3\nIn mplot3_xy(), define the algorithm to use to fit a curve, with fit. se.fit allows plotting the standard error bar (if it can be provided by the algorithm in fit)\n\nmplot3_xy(iris$Sepal.Length, iris$Petal.Length,\n          fit = \"gam\", se.fit = T)\n\n\n\n\nPassing a group argument, automatically fits separate models:\n\nmplot3_xy(iris$Sepal.Length, iris$Petal.Length,\n          fit = \"gam\", se.fit = T,\n          group = iris$Species)\n\n\n\n\n\n\n27.9.2 dplot3\nSame syntax as mplot3_xy() above:\n\ndplot3_xy(iris$Sepal.Length, iris$Petal.Length,\n          fit = \"gam\", se.fit = T)\n\n\n\n\n\n\ndplot3_xy(iris$Sepal.Length, iris$Petal.Length,\n          fit = \"gam\", se.fit = T,\n          group = iris$Species)\n\n\n\n\n\n\n\n27.9.3 ggplot2\nIn ggplot(), add a geom_smooth:\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Length)) +\n  geom_point() +\n  geom_smooth(method = 'gam')\n\n`geom_smooth()` using formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nTo group, again, use color:\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Length, color = Species)) +\n  geom_point() +\n  geom_smooth(method = 'gam')\n\n`geom_smooth()` using formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n27.9.4 plotly\nIn plot_ly(), add_lines():\n\nlibrary(mgcv)\nmod.gam <- gam(Petal.Length ~ s(Sepal.Length), data = iris)\nplot_ly(iris, x = ~Sepal.Length) %>%\n  add_trace(y = ~Petal.Length, type = \"scatter\", mode = \"markers\") %>% \n  add_lines(y = mod.gam$fitted.values)\n\n\n\n\n\nTo get fit by group, you add all elements one after the other - one way would be this:\n\niris.bySpecies <- split(iris, iris$Species)\ngam.fitted <- lapply(iris.bySpecies, function(i) {\n  gam(Petal.Length ~ s(Sepal.Length), data = i)$fitted\n})\nindex <- lapply(iris.bySpecies, function(i) order(i$Sepal.Length))\ncol <- c(\"#44A6AC\", \"#F4A362\", \"#3574A7\")\n.names <- names(iris.bySpecies)\np <- plot_ly()\nfor (i in seq_along(iris.bySpecies)) {\n  p <- add_trace(p, x = ~Sepal.Length, y = ~Petal.Length, \n                 type = \"scatter\", mode = \"markers\",\n                 data = iris.bySpecies[[i]],\n                 name = .names[i],\n                 color = col[i])\n}\nfor (i in seq_along(iris.bySpecies)) {\n  p <- add_lines(p, x = iris.bySpecies[[i]]$Sepal.Length[index[[i]]],\n                 y = gam.fitted[[i]][index[[i]]], \n                 # type = \"scatter\", mode = \"markers\",\n                 data = iris.bySpecies[[i]],\n                 name = paste(.names[i], \"GAM fit\"),\n                 color = col[i])\n}\np\n\n\n\n\n\nIt’s a lot of work, and that’s why dplot3 exists."
  },
  {
    "objectID": "72_3xGraphics.html#heatmap",
    "href": "72_3xGraphics.html#heatmap",
    "title": "27  3x Graphics",
    "section": "27.10 Heatmap",
    "text": "27.10 Heatmap\nLet’s create some synthetic correlation data:\n\nset.seed(2020)\nx <- matrix(rnorm(400), 20)\nx.cor <- cor(x)\n\n\n27.10.1 base\nR has a great builtin heatmap function, which supports hierarchical clustering and plots the dendrogram in the margins by default:\n\nheatmap(x.cor)\n\n\n\n\nIt may be a little surprising that clustering is on by default. To disable row and column dendrograms, set Rowv and Colv to NA:\n\nheatmap(x.cor, Rowv = NA, Colv = NA)\n\n\n\n\n\n\n27.10.2 mplot3\nmplot3 adds a colorbar to the side of the heatmap. Notice there are 10 circles above and 10 circles below zero to represent 10% increments.\n\nmplot3_heatmap(x.cor)\n\n\n\n\n\n\n27.10.3 dplot3\n\ndplot3_heatmap(x.cor)\n\n\n\n\n\n\n\n27.10.4 ggplot2\nggplot does not have a builtin heatmap function per se, but you can use geom_tile to build one. It also needs a data frame input in long form once again:\n\nx.cor.dat <- as.data.frame(x.cor)\ncolnames(x.cor.dat) <- rownames(x.cor.dat) <- paste0(\"V\", seq(20))\ncolnames(x.cor) <- rownames(x.cor) <- paste0(\"V\", seq(20))\nx.cor.long <- data.frame(NodeA = rownames(x.cor)[row(x.cor)],\n                         NodeB = colnames(x.cor)[col(x.cor)],\n                         Weight = c(x.cor))\n(p <- ggplot(x.cor.long, aes(NodeA, NodeB, fill = Weight)) +\n    geom_tile() + coord_equal())\n\n\n\n\n\n\n\n\nGennatas, Efstathios Dimitrios. 2017. “Towards Precision Psychiatry: Gray Matter Development and Cognition in Adolescence.”\n\n\nMurrell, Paul. 2018. R Graphics. CRC Press.\n\n\nSarkar, Deepayan. 2008. Lattice: Multivariate Data Visualization with r. New York: Springer. http://lmdvr.r-forge.r-project.org.\n\n\nSievert, Carson, Chris Parmer, Toby Hocking, Scott Chamberlain, Karthik Ram, Marianne Corvellec, and Pedro Despouy. 2017. “Plotly: Create Interactive Web Graphics via ‘Plotly. Js’.” R Package Version 4 (1): 110.\n\n\nWickham, Hadley. 2011. “Ggplot2.” Wiley Interdisciplinary Reviews: Computational Statistics 3 (2): 180–85.\n\n\nWilkinson, Leland. 2012. “The Grammar of Graphics.” In Handbook of Computational Statistics, 375–414. Springer."
  },
  {
    "objectID": "74_Colors.html",
    "href": "74_Colors.html",
    "title": "28  Colors in R",
    "section": "",
    "text": "Colors in R can be defined in many different ways:"
  },
  {
    "objectID": "74_Colors.html#color-names",
    "href": "74_Colors.html#color-names",
    "title": "28  Colors in R",
    "section": "28.1 Color names",
    "text": "28.1 Color names\nThere is a long list of color names R understands, and can be listed using colors().\nThey can be passed directly as characters.\nShades of gray are provided as gray0/grey0 (white) to gray100/grey100 (black).\nAbsurdly wide PDFs with all built-in R colors, excluding the grays/greys, are available sorted alphabeticaly and sorted by increasing Red and decreasing Green and Blue values"
  },
  {
    "objectID": "74_Colors.html#hexadecimal-codes",
    "href": "74_Colors.html#hexadecimal-codes",
    "title": "28  Colors in R",
    "section": "28.2 Hexadecimal codes",
    "text": "28.2 Hexadecimal codes\nHexadecimal color codes are characters starting with the pound sign, followed by 4 pairs of hex codes representing Red, Green, Blue, and Alpha values. Since RGB values go from 0 to 255, hex goes from 00 to FF. You can convert decimal to hex using as.hexmode:\n\nas.hexmode(0)\n\n[1] \"0\"\n\nas.hexmode(127)\n\n[1] \"7f\"\n\nas.hexmode(255)\n\n[1] \"ff\"\n\n\nThe last two values for the alpha setting are optional: if not included, defaults to max (opaque)"
  },
  {
    "objectID": "74_Colors.html#rgb",
    "href": "74_Colors.html#rgb",
    "title": "28  Colors in R",
    "section": "28.3 RGB",
    "text": "28.3 RGB\n\nrgb(0, 0, 1)\n\n[1] \"#0000FF\"\n\n\nNote the default maxColorValue = 1, set to 255 to use the usual RGB range of 0 to 255:\n\nrgb(0, 0, 255, maxColorValue = 255)\n\n[1] \"#0000FF\""
  },
  {
    "objectID": "74_Colors.html#hsv",
    "href": "74_Colors.html#hsv",
    "title": "28  Colors in R",
    "section": "28.4 HSV",
    "text": "28.4 HSV\nColor can also be parameterized using the hue, saturation, and value system (HSV). Each range from 0 to 1.\nSimplistically: Hue controls the color. Saturation 1 is max color and 0 is white. Value 1 is max color and 0 is black.\n\nhsv(1, 1, 1)\n\n[1] \"#FF0000\"\n\n\nIn the following plot, the values around the polar plot represent hue. Moving inwards to the center, saturation changes from 1 to 0.\n\nmplot_hsv()\n\nWarning in x - lwidths * xpad: longer object length is not a multiple of shorter\nobject length\n\n\nWarning in x + lwidths * xpad: longer object length is not a multiple of shorter\nobject length\n\n\nWarning in y - bheights * ypad: longer object length is not a multiple of\nshorter object length\n\n\nWarning in y + theights * ypad: longer object length is not a multiple of\nshorter object length\n\n\n\n\nmplot_hsv(v = .5)\n\nWarning in x - lwidths * xpad: longer object length is not a multiple of shorter\nobject length\n\n\nWarning in x + lwidths * xpad: longer object length is not a multiple of shorter\nobject length\n\n\nWarning in y - bheights * ypad: longer object length is not a multiple of\nshorter object length\n\n\nWarning in y + theights * ypad: longer object length is not a multiple of\nshorter object length"
  },
  {
    "objectID": "80_Profiling.html",
    "href": "80_Profiling.html",
    "title": "29  Benchmarking & Profiling",
    "section": "",
    "text": "Benchmarking is the process of timing the execution of code for the purpose of comparison. For example, you can compare the execution time of a program in two different systems, e.g. a laptop and a high performance server. Another common case is to compare the performance of two different programs that produce the same output on the same computer.\nProfiling refers to timing the different steps of a program to identify bottlenecks and potential targets for optimization."
  },
  {
    "objectID": "80_Profiling.html#system.time-time-the-execution-of-an-expression",
    "href": "80_Profiling.html#system.time-time-the-execution-of-an-expression",
    "title": "29  Benchmarking & Profiling",
    "section": "29.1 system.time(): Time the execution of an expression",
    "text": "29.1 system.time(): Time the execution of an expression\nThe base package’s system.time() function allows you to measure the execution time of an R expression.\n\nsystem.time(rnorm(9999999))\n\n   user  system elapsed \n  0.281   0.009   0.290 \n\n\n“elapsed” time is real time in seconds.\n“user” and “system” are time used by the CPU on different types of tasks (see ?proc.time).\nAs always, you can pass any R expression within curly brackets:\n\nx <- rnorm(9999)\nsystem.time({\n    for (i in 2:9999) {\n      x[i]\n      x[i] <- x[i]^3\n    }\n})\n\n   user  system elapsed \n  0.002   0.000   0.001 \n\n\nYou can use replicate() to get a measure of time over multiple executions and average it:\n\nset.seed(2020)\nx <- matrix(rnorm(500000), 5000)\ny <- 12 + x[, 3] + x[, 5]^2 + x[, 7]^3 + rnorm(5000)\nfit.glm <- function(x, y) glm.fit(x, y)\n    \nfit.glm_time10 <- replicate(10, system.time(fit.glm(x, y))[[1]])\n\n\nboxplot(fit.glm_time10)"
  },
  {
    "objectID": "80_Profiling.html#compare-execution-times-with-microbenchmarkmicrobenchmark",
    "href": "80_Profiling.html#compare-execution-times-with-microbenchmarkmicrobenchmark",
    "title": "29  Benchmarking & Profiling",
    "section": "29.2 Compare execution times with microbenchmark::microbenchmark()",
    "text": "29.2 Compare execution times with microbenchmark::microbenchmark()\nThe microbenchmark package’s microbenchmark() function allows you to time the execution of multiple expressions with sub-millisecond accuracy. It will execute each command a number of times as defined by the times argument (default = 100), and output statistics of execution time per expression in nanoseconds. Using plot() on the output produces a boxplot comparing the time distributions.\n\n# install.packages(\"microbenchmark\")\nlibrary(microbenchmark)\n\n\n29.2.1 Example: loop over matrix vs. data.frame\nLet’s create xmat, a 500 by 5 matrix and, xdf a data.frame with the same data.\n\nset.seed(2021)\nxmat <- matrix(rnorm(500*5), 5)\nxdf <- as.data.frame(xmat)\n\nIf you wanted to square either of them, you would just use ^2. Here, we create a function specifically to demonstrate the difference in working on a numeric matrix vs. a data.frame by using a nested loop that replaces each element one at a time.\n\nsilly_square <- function(x) {\n  for (i in seq_len(NROW(x))) {\n    for (j in seq_len(NCOL(x))) {\n      x[i, j] <- x[i, j]^2\n    }\n  }\n}\n\n\nmat_df_sq <- microbenchmark(silly_square_mat = silly_square(xmat),\n               silly_square_df = silly_square(xdf),\n               mat_squared = xmat^2,\n               df_squared = xdf^2)\n\nWarning in microbenchmark(silly_square_mat = silly_square(xmat), silly_square_df\n= silly_square(xdf), : less accurate nanosecond times to avoid potential integer\noverflows\n\nclass(mat_df_sq)\n\n[1] \"microbenchmark\" \"data.frame\"    \n\n\nPrint microbenchmark’s output:\n\nmat_df_sq\n\nUnit: microseconds\n             expr       min         lq        mean     median         uq\n silly_square_mat   101.762   107.8505   134.46770   110.8025   116.5425\n  silly_square_df 40087.791 41718.3405 43750.75232 43158.0965 44394.7385\n      mat_squared     1.107     1.6605     2.79948     3.0340     3.5260\n       df_squared  9086.666  9933.1110 11010.92679 10485.0940 11836.9870\n       max neval\n  1997.561   100\n 64110.716   100\n     5.412   100\n 16969.039   100\n\n\nNotice how a) either operation is much faster on a matrix vs. a data.frame and b) vectorized squaring with ^2 is much faster than the nested loop as expected.\nThere is a plot() method for microbenchmark objects:\n\nplot(mat_df_sq)\n\n\n\n\n\n\n29.2.2 Example: Group means\nLet’s perform a simple mean-by-group operation and compare three different approaches. As an example, we use the flights dataset from the nycflights13 package which includes data on 336,776 flights that departed from NY area airports in 2013. The data comes as a tibble, and we create data.frame and data.table versions.th\n\nlibrary(data.table)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:data.table':\n\n    between, first, last\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(nycflights13)\nclass(flights)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\ndim(flights)\n\n[1] 336776     19\n\nflights_df <- as.data.frame(flights)\nflights_dt <- as.data.table(flights)\n\nCompare performance of the same operation using different functions:\n\nbase R aggregate() with formula input\nbase R aggregate() with list input\nbase R tapply()\n\n\nflights_aggregate_formula <- function() {\n  aggregate(arr_delay ~ carrier, \n            data = flights_df,\n            mean, na.rm = TRUE)\n}\n  \nflights_aggregate <- function() {\n  aggregate(flights_df$arr_delay, \n            by = list(flights_df$carrier), \n            mean, na.rm = TRUE)\n}\n\nflights_tapply <- function() {\n  tapply(flights_df$arr_delay, \n         flights_df$carrier, \n         mean, na.rm = TRUE)\n}\n\ngroupmean_3x <- microbenchmark(\n  aggregate_formula = flights_aggregate_formula(),\n  aggregate = flights_aggregate(),\n  tapply = flights_tapply()\n  )\n\n\ngroupmean_3x\n\nUnit: milliseconds\n              expr       min        lq     mean   median       uq       max\n aggregate_formula 44.204683 50.173791 55.97023 52.99789 57.48840 101.97487\n         aggregate 38.253287 41.948760 45.79623 44.92173 47.27468  77.83227\n            tapply  8.372077  9.325921 11.45583 10.16460 11.43785  46.09351\n neval\n   100\n   100\n   100\n\n\n\nplot(groupmean_3x)"
  },
  {
    "objectID": "80_Profiling.html#profile-a-function-with-profvis",
    "href": "80_Profiling.html#profile-a-function-with-profvis",
    "title": "29  Benchmarking & Profiling",
    "section": "29.3 Profile a function with profvis()",
    "text": "29.3 Profile a function with profvis()\nThe profvis package’s profvis() function provides an interactive output to visualize the time spent in different calls within a program.\n\nlibrary(profvis)\nprofvis({\n  hf <- read.csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00519/heart_failure_clinical_records_dataset.csv\")\n  str(hf)\n  lt5 <- which(sapply(hf, \\(i) length(unique(i))) < 5)\n  for (i in lt5) hf[, i] <- factor(hf[, i])\n  index_numeric <- which(sapply(hf, is.numeric))\n  par(mfrow = c(1, length(index_numeric)))\n  for (i in index_numeric) boxplot(hf[, i])\n  par(mfrow = c(1, 1))\n})\n\n'data.frame':   299 obs. of  13 variables:\n $ age                     : num  75 55 65 50 65 90 75 60 65 80 ...\n $ anaemia                 : int  0 0 0 1 1 1 1 1 0 1 ...\n $ creatinine_phosphokinase: int  582 7861 146 111 160 47 246 315 157 123 ...\n $ diabetes                : int  0 0 0 0 1 0 0 1 0 0 ...\n $ ejection_fraction       : int  20 38 20 20 20 40 15 60 65 35 ...\n $ high_blood_pressure     : int  1 0 0 0 0 1 0 0 0 1 ...\n $ platelets               : num  265000 263358 162000 210000 327000 ...\n $ serum_creatinine        : num  1.9 1.1 1.3 1.9 2.7 2.1 1.2 1.1 1.5 9.4 ...\n $ serum_sodium            : int  130 136 129 137 116 132 137 131 138 133 ...\n $ sex                     : int  1 1 1 1 0 1 1 1 0 1 ...\n $ smoking                 : int  0 0 1 0 0 1 0 1 0 1 ...\n $ time                    : int  4 6 7 7 8 8 10 10 10 10 ...\n $ DEATH_EVENT             : int  1 1 1 1 1 1 1 1 1 1 ..."
  },
  {
    "objectID": "82_Optimization.html",
    "href": "82_Optimization.html",
    "title": "30  Optimization",
    "section": "",
    "text": "R provides a general purpose optimization tool, optim(). You can use it to estimate parameters that minimize any defined function.\nSupervised and unsupervised learning involves defining a loss function to minimize or an objective function to minimize or maximize.\nTo learn how optim() works, let’s write a simple function that returns linear coefficients by minimizing squared error."
  },
  {
    "objectID": "82_Optimization.html#data",
    "href": "82_Optimization.html#data",
    "title": "30  Optimization",
    "section": "30.1 Data",
    "text": "30.1 Data\n\nset.seed(2020)\nx <- sapply(seq(10), function(i) rnorm(500))\ny <- 12 + 1.5 * x[, 3] + 3.2 * x[, 7] + .5 * x[, 9] + rnorm(500)"
  },
  {
    "objectID": "82_Optimization.html#glm-glm-s_glm",
    "href": "82_Optimization.html#glm-glm-s_glm",
    "title": "30  Optimization",
    "section": "30.2 GLM (glm, s_GLM)",
    "text": "30.2 GLM (glm, s_GLM)\n\nyx.glm <- glm(y ~ x)\nsummary(yx.glm)\n\n\nCall:\nglm(formula = y ~ x)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-2.38739  -0.67391   0.00312   0.65531   3.08524  \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 11.979070   0.043252 276.962   <2e-16 ***\nx1           0.061798   0.040916   1.510   0.1316    \nx2          -0.003873   0.043271  -0.090   0.9287    \nx3           1.488113   0.042476  35.034   <2e-16 ***\nx4           0.031115   0.044015   0.707   0.4800    \nx5           0.034217   0.043664   0.784   0.4336    \nx6           0.034716   0.042189   0.823   0.4110    \nx7           3.183398   0.040605  78.399   <2e-16 ***\nx8          -0.034252   0.043141  -0.794   0.4276    \nx9           0.541219   0.046550  11.627   <2e-16 ***\nx10          0.087120   0.044000   1.980   0.0483 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.9207315)\n\n    Null deviance: 7339.42  on 499  degrees of freedom\nResidual deviance:  450.24  on 489  degrees of freedom\nAIC: 1390.5\n\nNumber of Fisher Scoring iterations: 2\n\n\nOr, using rtemis:\n\nmod.glm <- s_GLM(x, y)\n\n[2022-05-10 17:28:35 s_GLM] Hello, egenn \n\n.:Regression Input Summary\nTraining features: 500 x 10 \n Training outcome: 500 x 1 \n Testing features: Not available\n  Testing outcome: Not available\n\n[2022-05-10 17:28:36 s_GLM] Training GLM... \n\n.:GLM Regression Training Summary\n    MSE = 0.90 (93.87%)\n   RMSE = 0.95 (75.23%)\n    MAE = 0.77 (74.88%)\n      r = 0.97 (p = 5.3e-304)\n   R sq = 0.94\n\n\n\n\n\n[2022-05-10 17:28:37 s_GLM] Run completed in 0.03 minutes (Real: 1.63; User: 0.43; System: 0.04) \n\n\n\nsummary(mod.glm$mod)\n\n\nCall:\nglm(formula = .formula, family = family, data = df.train, weights = .weights, \n    na.action = na.action)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-2.38739  -0.67391   0.00312   0.65531   3.08524  \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 11.979070   0.043252 276.962   <2e-16 ***\nV1           0.061798   0.040916   1.510   0.1316    \nV2          -0.003873   0.043271  -0.090   0.9287    \nV3           1.488113   0.042476  35.034   <2e-16 ***\nV4           0.031115   0.044015   0.707   0.4800    \nV5           0.034217   0.043664   0.784   0.4336    \nV6           0.034716   0.042189   0.823   0.4110    \nV7           3.183398   0.040605  78.399   <2e-16 ***\nV8          -0.034252   0.043141  -0.794   0.4276    \nV9           0.541219   0.046550  11.627   <2e-16 ***\nV10          0.087120   0.044000   1.980   0.0483 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.9207315)\n\n    Null deviance: 7339.42  on 499  degrees of freedom\nResidual deviance:  450.24  on 489  degrees of freedom\nAIC: 1390.5\n\nNumber of Fisher Scoring iterations: 2"
  },
  {
    "objectID": "82_Optimization.html#optim",
    "href": "82_Optimization.html#optim",
    "title": "30  Optimization",
    "section": "30.3 optim",
    "text": "30.3 optim\nBasic usage of optim to find values of parameters that minimize a function:\n\nDefine a list of initial parameter values\nDefine a loss function whose first argument is the above list of initial parameter values\nPass parameter list and objective function to optim\n\nIn the following example, we wrap these three steps in a function called linearcoeffs, which will output the linear coefficients that minimize squared error, given a matrix/data.frame of features x and an outcome y. We also specify the optimization method to be used (See ?base::optim for details):\n\nlinearcoeffs <- function(x, y, method = \"BFGS\") {\n  \n  # 1. List of initial parameter values\n  params <- as.list(c(mean(y), rep(0, NCOL(x))))\n  names(params) <- c(\"Intercept\", paste0(\"Coefficient\", seq(NCOL(x))))\n  \n  # 2. Loss function: first argument is parameter list\n  loss <- function(params, x, y) {\n    estimated <- c(params[[1]] + x %*% unlist(params[-1]))\n    mean((y - estimated)^2)\n  }\n  \n  # 3. optim!\n  coeffs <- optim(params, loss, x = x, y = y, method = method)\n  \n  # The values that minimize the loss function are stored in $par\n  coeffs$par\n}\n\n\ncoeffs.optim <- linearcoeffs(x, y)\nestimated.optim <- cbind(1, x) %*% coeffs.optim\nmplot3_fit(y, estimated.optim)\n\n\n\ncoeffs.glm <- mod.glm$mod$coefficients\nestimated.glm <- cbind(1, x) %*% coeffs.glm\nmplot3_fit(y, estimated.glm)\n\n\n\n\n\nmplot3_fit(coeffs.glm, coeffs.optim)"
  },
  {
    "objectID": "84_HypothesisTesting.html",
    "href": "84_HypothesisTesting.html",
    "title": "31  Common statistical tests",
    "section": "",
    "text": "R includes a large number of functions to perform statistical hypothesis testing in the built-in stats package. This chapter includes a brief overview of the syntax for some common tests along with code to produce relevant plots of your data."
  },
  {
    "objectID": "84_HypothesisTesting.html#correlation-test",
    "href": "84_HypothesisTesting.html#correlation-test",
    "title": "31  Common statistical tests",
    "section": "31.1 Correlation test",
    "text": "31.1 Correlation test\n\nset.seed(2020)\nx <- rnorm(100)\ny1 <- .1 * x + rnorm(100)\ny2 <- .3 * x + rnorm(100)\ny3 <- x + rnorm(100)/5\ny4 <- x^2 + rnorm(100)\n\nScatterplot with linear fit:\n\nplot(x, y1,\n     col = \"#00000077\",\n     pch = 16, bty = \"none\")\nabline(lm(y1 ~ x), col = \"red\", lwd = 2)\n\n\n\n\n\nplot(x, y2,\n     col = \"#00000077\",\n     pch = 16, bty = \"none\")\nabline(lm(y3 ~ x), col = \"red\", lwd = 2)\n\n\n\n\n\nplot(x, y3,\n     col = \"#00000077\",\n     pch = 16, bty = \"none\")\nabline(lm(y3 ~ x), col = \"red\", lwd = 2)\n\n\n\n\nScatterplot with a LOESS fit\n\nscatter.smooth(x, y4,\n               col = \"#00000077\",\n               pch = 16, bty = \"none\",\n               lpars = list(col = \"red\", lwd = 2))\n\n\n\n\n\ncor.test(x, y1)\n\n\n    Pearson's product-moment correlation\n\ndata:  x and y1\nt = 0.66018, df = 98, p-value = 0.5107\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.1315978  0.2595659\nsample estimates:\n       cor \n0.06654026 \n\ncor.test(x, y2)\n\n\n    Pearson's product-moment correlation\n\ndata:  x and y2\nt = 3.3854, df = 98, p-value = 0.001024\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1357938 0.4889247\nsample estimates:\n      cor \n0.3235813 \n\ncor.test(x, y3)\n\n\n    Pearson's product-moment correlation\n\ndata:  x and y3\nt = 53.689, df = 98, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9754180 0.9888352\nsample estimates:\n      cor \n0.9834225 \n\ncor.test(x, y4)\n\n\n    Pearson's product-moment correlation\n\ndata:  x and y4\nt = 0.66339, df = 98, p-value = 0.5086\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.1312793  0.2598681\nsample estimates:\n       cor \n0.06686289"
  },
  {
    "objectID": "84_HypothesisTesting.html#students-t-test",
    "href": "84_HypothesisTesting.html#students-t-test",
    "title": "31  Common statistical tests",
    "section": "31.2 Student’s t-test",
    "text": "31.2 Student’s t-test\n\nset.seed(2021)\nx0 <- rnorm(500)\nx1 <- rnorm(500, mean = .7)\n\nFor all tests of differences in means, a boxplot is a good way to visualize. It accepts individual vectors, list or data.frame of vectors, or a formula to split a vector into groups by a factor.\n\nboxplot(x0, x1,\n        col = \"#05204999\", border = \"#052049\",\n        boxwex = .3, names = c(\"x0\", \"x1\"))\n\n\n\n\n\n31.2.1 One sample t-test\n\nt.test(x0)\n\n\n    One Sample t-test\n\ndata:  x0\nt = 0.093509, df = 499, p-value = 0.9255\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.08596896  0.09456106\nsample estimates:\n  mean of x \n0.004296046 \n\n\n\nt.test(x1)\n\n\n    One Sample t-test\n\ndata:  x1\nt = 15.935, df = 499, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.6322846 0.8101311\nsample estimates:\nmean of x \n0.7212079 \n\n\n\nboxplot(extra ~ group, sleep,\n        col = \"#05204999\", border = \"#052049\",\n        boxwex = .3)\n\n\n\n\n\n\n31.2.2 Two-sample T-test\nBoth t.test() and wilcox.test() (below) either accept input as two vectors, t.test(x, y) or a formula of the form t.test(x ~ group). The paired argument allows us to define a paired test. Since the sleep dataset includes measurements on the same cases in two conditions, we set paired = TRUE.\n\nt.test(extra ~ group, sleep, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  extra by group\nt = -4.0621, df = 9, p-value = 0.002833\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -2.4598858 -0.7001142\nsample estimates:\nmean difference \n          -1.58"
  },
  {
    "objectID": "84_HypothesisTesting.html#wilcoxon-test",
    "href": "84_HypothesisTesting.html#wilcoxon-test",
    "title": "31  Common statistical tests",
    "section": "31.3 Wilcoxon test",
    "text": "31.3 Wilcoxon test\nData from R Documentation:\n\n## Hollander & Wolfe (1973), 29f.\n## Hamilton depression scale factor measurements in 9 patients with\n##  mixed anxiety and depression, taken at the first (x) and second\n##  (y) visit after initiation of a therapy (administration of a\n##  tranquilizer).\nx <- c(1.83,  0.50,  1.62,  2.48, 1.68, 1.88, 1.55, 3.06, 1.30)\ny <- c(0.878, 0.647, 0.598, 2.05, 1.06, 1.29, 1.06, 3.14, 1.29)\ndepression <- data.frame(first = x, second = y, change = y - x)\n\n\n31.3.1 One-sample Wilcoxon\n\nwilcox.test(depression$change)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  depression$change\nV = 5, p-value = 0.03906\nalternative hypothesis: true location is not equal to 0\n\n\n\n\n31.3.2 Two-sample Wilcoxon rank sum test (unpaired)\na.k.a Mann-Whitney U test a.k.a. Mann–Whitney–Wilcoxon (MWW) a.k.a. Wilcoxon–Mann–Whitney test\n\nx1 <- rnorm(500, 3, 1.5)\nx2 <- rnorm(500, 5, 2)\nwilcox.test(x1, x2)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  x1 and x2\nW = 47594, p-value < 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n\n31.3.3 Two-sample Wilcoxon signed-rank test (paired)\n\nwilcox.test(x, y, paired = TRUE)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  x and y\nV = 40, p-value = 0.03906\nalternative hypothesis: true location shift is not equal to 0\n\n\n\nwilcox.test(x, y, paired = TRUE, alternative = \"greater\")\n\n\n    Wilcoxon signed rank exact test\n\ndata:  x and y\nV = 40, p-value = 0.01953\nalternative hypothesis: true location shift is greater than 0"
  },
  {
    "objectID": "84_HypothesisTesting.html#analysis-of-variance",
    "href": "84_HypothesisTesting.html#analysis-of-variance",
    "title": "31  Common statistical tests",
    "section": "31.4 Analysis of Variance",
    "text": "31.4 Analysis of Variance\n\nset.seed(20)\nBP_drug <- data.frame(Group = factor(rep(c(\"Placebo\", \"Drug_A\", \"Drug_B\"), each = 20),\n                                     levels = c(\"Placebo\", \"Drug_A\", \"Drug_B\")),\n                  SBP = c(rnorm(20, 140, 2.2), rnorm(20, 132, 2.1), rnorm(20, 138, 2)))\n\n\nboxplot(SBP ~ Group, BP_drug,\n        col = \"#05204999\", border = \"#052049\",\n        boxwex = .3)\n\n\n\n\n\nSBP_aov <- aov(SBP ~ Group, BP_drug)\nSBP_aov\n\nCall:\n   aov(formula = SBP ~ Group, data = BP_drug)\n\nTerms:\n                   Group Residuals\nSum of Squares  728.2841  264.4843\nDeg. of Freedom        2        57\n\nResidual standard error: 2.154084\nEstimated effects may be unbalanced\n\n\n\nsummary(SBP_aov)\n\n            Df Sum Sq Mean Sq F value Pr(>F)    \nGroup        2  728.3   364.1   78.48 <2e-16 ***\nResiduals   57  264.5     4.6                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe analysis of variance p-value is highly significant, but doesn’t tell us which levels of the Group factor are significantly different from each other. The boxplot already gives us a pretty good idea, but we can follow up with a pairwise t-test\n\n31.4.1 Pot-hoc pairwise t-tests\n\npairwise.t.test(BP_drug$SBP, BP_drug$Group,\n                p.adj = \"holm\")\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  BP_drug$SBP and BP_drug$Group \n\n       Placebo Drug_A \nDrug_A 2.9e-16 -      \nDrug_B 0.065   1.7e-13\n\nP value adjustment method: holm \n\n\nThe pairwise tests suggest that the difference between Placebo and Drug_A and between Drug_a and Drug_b are highly significant, while difference between Placebo and Drub_B is not (p = 0.065)."
  },
  {
    "objectID": "84_HypothesisTesting.html#kruskal-wallis-test",
    "href": "84_HypothesisTesting.html#kruskal-wallis-test",
    "title": "31  Common statistical tests",
    "section": "31.5 Kruskal-Wallis test",
    "text": "31.5 Kruskal-Wallis test\nKruskal-Wallis rank sum test of the null that the location parameters of the distribution of x are the same in each group (sample). The alternative is that they differ in at least one. It is a generalization of the Wilcoxon test to multiple independent samples.\nFrom the R Documentation:\n\n## Hollander & Wolfe (1973), 116.\n## Mucociliary efficiency from the rate of removal of dust in normal\n##  subjects, subjects with obstructive airway disease, and subjects\n##  with asbestosis.\nx <- c(2.9, 3.0, 2.5, 2.6, 3.2) # normal subjects\ny <- c(3.8, 2.7, 4.0, 2.4)      # with obstructive airway disease\nz <- c(2.8, 3.4, 3.7, 2.2, 2.0) # with asbestosis\nkruskal.test(list(x, y, z))\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  list(x, y, z)\nKruskal-Wallis chi-squared = 0.77143, df = 2, p-value = 0.68\n\n\n\nboxplot(Ozone ~ Month, data = airquality,\n        col = \"#05204999\", border = \"#052049\",\n        boxwex = .3)\n\n\n\n\n\nkruskal.test(Ozone ~ Month, data = airquality)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  Ozone by Month\nKruskal-Wallis chi-squared = 29.267, df = 4, p-value = 6.901e-06"
  },
  {
    "objectID": "84_HypothesisTesting.html#chi-squared-test",
    "href": "84_HypothesisTesting.html#chi-squared-test",
    "title": "31  Common statistical tests",
    "section": "31.6 Chi-squared Test",
    "text": "31.6 Chi-squared Test\nPearson’s chi-squared test for count data\nSome synthetic data:\n\nset.seed(2021)\nset.seed(2021)\nCohort <- factor(sample(c(\"Control\", \"Case\"), 500, TRUE),\n                 levels = c(\"Control\", \"Case\"))\nSex <- factor(\n  sapply(seq(Cohort), \\(i) sample(c(\"Male\", \"Female\"), 1,\n                                  prob = if (Cohort[i] == \"Control\") c(1, 1) else c(2, 1))))\ndat <- data.frame(Cohort, Sex)\nhead(dat)\n\n   Cohort    Sex\n1 Control   Male\n2    Case   Male\n3    Case   Male\n4    Case Female\n5 Control Female\n6    Case   Male\n\n\nYou can lot count data using a mosaic plot, with either a table or formula input:\n\nmosaicplot(table(Cohort, Sex),\n           color = c(\"orchid\", \"skyblue\"),\n           border = NA,\n           main = \"Cohort x Sex\")\n\n\n\n\n\nmosaicplot(Cohort ~ Sex, dat,\n           color = c(\"orchid\", \"skyblue\"),\n           border = NA,\n           main = \"Cohort x Sex\")\n\n\n\n\nchisq.test() accepts either two factors, or a table:\n\ncohort_sex_chisq <- chisq.test(dat$Cohort, dat$Sex)\ncohort_sex_chisq\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  dat$Cohort and dat$Sex\nX-squared = 18.015, df = 1, p-value = 2.192e-05\n\n\n\ncohort_sex_chisq <- chisq.test(table(dat$Cohort, dat$Sex))\ncohort_sex_chisq\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table(dat$Cohort, dat$Sex)\nX-squared = 18.015, df = 1, p-value = 2.192e-05"
  },
  {
    "objectID": "84_HypothesisTesting.html#fishers-exact-test",
    "href": "84_HypothesisTesting.html#fishers-exact-test",
    "title": "31  Common statistical tests",
    "section": "31.7 Fisher’s exact test",
    "text": "31.7 Fisher’s exact test\nFisher’s exact test for count data\nWorking on the same data as above, fisher.test() also accepts either two factors or a table as input:\n\ncohort_sex_fisher <- fisher.test(dat$Cohort, dat$Sex)\ncohort_sex_fisher\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  dat$Cohort and dat$Sex\np-value = 1.512e-05\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 1.516528 3.227691\nsample estimates:\nodds ratio \n  2.207866 \n\n\n\ncohort_sex_fisher <- fisher.test(table(dat$Cohort, dat$Sex))\ncohort_sex_fisher\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  table(dat$Cohort, dat$Sex)\np-value = 1.512e-05\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 1.516528 3.227691\nsample estimates:\nodds ratio \n  2.207866"
  },
  {
    "objectID": "84_HypothesisTesting.html#f-test-to-compare-two-variances",
    "href": "84_HypothesisTesting.html#f-test-to-compare-two-variances",
    "title": "31  Common statistical tests",
    "section": "31.8 F Test to compare two variances",
    "text": "31.8 F Test to compare two variances\n\nx1 <- rnorm(500, sd = 1)\nx2 <- rnorm(400, sd = 1.5)\nvar.test(x1, x2)\n\n\n    F test to compare two variances\n\ndata:  x1 and x2\nF = 0.43354, num df = 499, denom df = 399, p-value < 2.2e-16\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.3594600 0.5218539\nsample estimates:\nratio of variances \n         0.4335363 \n\n\n\nboxplot(x1, x2,\n        col = \"#05204999\", border = \"#052049\",\n        boxwex = .3)\n\n\n\n\nFrom R Documentation:\n\nx <- rnorm(50, mean = 0, sd = 2)\ny <- rnorm(30, mean = 1, sd = 1)\nvar.test(x, y)                  # Do x and y have the same variance?\n\n\n    F test to compare two variances\n\ndata:  x and y\nF = 5.4776, num df = 49, denom df = 29, p-value = 5.817e-06\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n  2.752059 10.305597\nsample estimates:\nratio of variances \n          5.477573 \n\nvar.test(lm(x ~ 1), lm(y ~ 1))  # same\n\n\n    F test to compare two variances\n\ndata:  lm(x ~ 1) and lm(y ~ 1)\nF = 5.4776, num df = 49, denom df = 29, p-value = 5.817e-06\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n  2.752059 10.305597\nsample estimates:\nratio of variances \n          5.477573"
  },
  {
    "objectID": "84_HypothesisTesting.html#bartlett-test-of-homogeneity-of-variances",
    "href": "84_HypothesisTesting.html#bartlett-test-of-homogeneity-of-variances",
    "title": "31  Common statistical tests",
    "section": "31.9 Bartlett test of homogeneity of variances",
    "text": "31.9 Bartlett test of homogeneity of variances\nPerforms Bartlett’s test of the null that the variances in each of the groups (samples) are the same.\nFrom the R Documentation:\n\nplot(count ~ spray, data = InsectSprays)\n\n\n\n\n\nbartlett.test(InsectSprays$count, InsectSprays$spray)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  InsectSprays$count and InsectSprays$spray\nBartlett's K-squared = 25.96, df = 5, p-value = 9.085e-05\n\n\n\nbartlett.test(count ~ spray, data = InsectSprays)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  count by spray\nBartlett's K-squared = 25.96, df = 5, p-value = 9.085e-05"
  },
  {
    "objectID": "84_HypothesisTesting.html#fligner-killeen-test-of-homogeneity-of-variances",
    "href": "84_HypothesisTesting.html#fligner-killeen-test-of-homogeneity-of-variances",
    "title": "31  Common statistical tests",
    "section": "31.10 Fligner-Killeen test of homogeneity of variances",
    "text": "31.10 Fligner-Killeen test of homogeneity of variances\nPerforms a Fligner-Killeen (median) test of the null that the variances in each of the groups (samples) are the same.\n\nboxplot(count ~ spray, data = InsectSprays)\n\n\n\n# works the same if you do plot(count ~ spray, data = InsectSprays)\nfligner.test(InsectSprays$count, InsectSprays$spray)\n\n\n    Fligner-Killeen test of homogeneity of variances\n\ndata:  InsectSprays$count and InsectSprays$spray\nFligner-Killeen:med chi-squared = 14.483, df = 5, p-value = 0.01282\n\nfligner.test(count ~ spray, data = InsectSprays)\n\n\n    Fligner-Killeen test of homogeneity of variances\n\ndata:  count by spray\nFligner-Killeen:med chi-squared = 14.483, df = 5, p-value = 0.01282"
  },
  {
    "objectID": "84_HypothesisTesting.html#ansari-bradley-test",
    "href": "84_HypothesisTesting.html#ansari-bradley-test",
    "title": "31  Common statistical tests",
    "section": "31.11 Ansari-Bradley test",
    "text": "31.11 Ansari-Bradley test\nPerforms the Ansari-Bradley two-sample test for a difference in scale parameters.\n\nramsay <- c(111, 107, 100, 99, 102, 106, 109, 108, 104, 99,\n            101, 96, 97, 102, 107, 113, 116, 113, 110, 98)\njung.parekh <- c(107, 108, 106, 98, 105, 103, 110, 105, 104,\n            100, 96, 108, 103, 104, 114, 114, 113, 108, 106, 99)\nansari.test(ramsay, jung.parekh)\n\nWarning in ansari.test.default(ramsay, jung.parekh): cannot compute exact p-\nvalue with ties\n\n\n\n    Ansari-Bradley test\n\ndata:  ramsay and jung.parekh\nAB = 185.5, p-value = 0.1815\nalternative hypothesis: true ratio of scales is not equal to 1\n\n\n\nx <- rnorm(40, sd = 1.5)\ny <- rnorm(40, sd = 2.5)\nansari.test(x, y)\n\n\n    Ansari-Bradley test\n\ndata:  x and y\nAB = 963, p-value = 0.005644\nalternative hypothesis: true ratio of scales is not equal to 1"
  },
  {
    "objectID": "84_HypothesisTesting.html#mood-two-sample-test-of-scale",
    "href": "84_HypothesisTesting.html#mood-two-sample-test-of-scale",
    "title": "31  Common statistical tests",
    "section": "31.12 Mood two-sample test of scale",
    "text": "31.12 Mood two-sample test of scale\n\nmood.test(x, y)\n\n\n    Mood two-sample test of scale\n\ndata:  x and y\nZ = -2.7363, p-value = 0.006213\nalternative hypothesis: two.sided"
  },
  {
    "objectID": "84_HypothesisTesting.html#kolmogorov-smirnoff-test",
    "href": "84_HypothesisTesting.html#kolmogorov-smirnoff-test",
    "title": "31  Common statistical tests",
    "section": "31.13 Kolmogorov-Smirnoff test",
    "text": "31.13 Kolmogorov-Smirnoff test\nPerform a one- or two-sample Kolmogorov-Smirnov test Null: x and y were drawn from the same continuous distribution.\n\nx1 <- rnorm(200, 0, 1)\nx2 <- rnorm(200, -.5, 1.5)\nks.test(x1, x2)\n\n\n    Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  x1 and x2\nD = 0.35, p-value = 4.579e-11\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "84_HypothesisTesting.html#shapiro-wilk-test-of-normality",
    "href": "84_HypothesisTesting.html#shapiro-wilk-test-of-normality",
    "title": "31  Common statistical tests",
    "section": "31.14 Shapiro-Wilk test of normality",
    "text": "31.14 Shapiro-Wilk test of normality\n\nset.seed(2021)\nx <- rnorm(2000)\ny1 <- .7 * x\ny2 <- x + x^3\n\n\n31.14.1 Q-Q Plot\n\nqqplot(rnorm(300), y1, pch = 16, col = \"#00000077\")\nqqline(y1, col = \"red\", lwd = 2)\n\n\n\n\n\nqqplot(rnorm(300), y2, pch = 16, col = \"#00000077\")\nqqline(y2, col = \"red\", lwd = 2)"
  },
  {
    "objectID": "84_HypothesisTesting.html#shapiro-wilk-test",
    "href": "84_HypothesisTesting.html#shapiro-wilk-test",
    "title": "31  Common statistical tests",
    "section": "31.15 Shapiro-Wilk test",
    "text": "31.15 Shapiro-Wilk test\n\nshapiro.test(y1)\n\n\n    Shapiro-Wilk normality test\n\ndata:  y1\nW = 0.99952, p-value = 0.9218\n\nshapiro.test(y2)\n\n\n    Shapiro-Wilk normality test\n\ndata:  y2\nW = 0.72936, p-value < 2.2e-16"
  },
  {
    "objectID": "84_HypothesisTesting.html#statsresources",
    "href": "84_HypothesisTesting.html#statsresources",
    "title": "31  Common statistical tests",
    "section": "31.16 Resources",
    "text": "31.16 Resources\n\nRegression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models"
  },
  {
    "objectID": "85_GLM.html",
    "href": "85_GLM.html",
    "title": "32  Introduction to the GLM",
    "section": "",
    "text": ".:rtemis 0.91 🌊 aarch64-apple-darwin20 (64-bit)\n  Defaults\n  |   Theme: whitegrid\n  |    Font: Fira Sans\n  | Palette: rtCol1\n  |    Plan: multicore\n  |   Cores: 8/10 available\n  Resources\n  | Documentation: https://rtemis.lambdamd.org\n  |       Learn R: https://class.lambdamd.org/pdsr\n  | rtemis themes: https://egenn.lambdamd.org/software/#rtemis_themes\n  |          Cite: `citation(\"rtemis\")`\n  Setup\n  | Enable progress reporting: `progressr::handlers(global = TRUE)`"
  },
  {
    "objectID": "85_GLM.html#generalized-linear-model-glm",
    "href": "85_GLM.html#generalized-linear-model-glm",
    "title": "32  Introduction to the GLM",
    "section": "32.1 Generalized Linear Model (GLM)",
    "text": "32.1 Generalized Linear Model (GLM)\nThe Generalized Linear Model is one of the most popular and important models in statistics.\nIt fits a model of the form:\n\\[y = β_0 + β_1 x_1 + β_2 x_2 + ... β_n x_n + ε\\]\nwhere:\n\n\\(y\\) is the dependent variable, i.e. outcome of interest\n\\(x_1\\) to \\(x_n\\) are the independent variables, a.k.a. covariates, a.k.a. predictors\n\\(β_0\\) is the intercept\n\\(β_1\\) to \\(β_n\\) are the coefficients\n\\(ε\\) is the error\n\nIn matrix notation:\n\\[y = Χβ + ε\\]\nLet’s look at an example using the GLM for regression. We’ll use the diabetes dataset from the mlbench package as an example.\n\ndata(PimaIndiansDiabetes2, package = 'mlbench')\nstr(PimaIndiansDiabetes2)\n\n'data.frame':   768 obs. of  9 variables:\n $ pregnant: num  6 1 8 1 0 5 3 10 2 8 ...\n $ glucose : num  148 85 183 89 137 116 78 115 197 125 ...\n $ pressure: num  72 66 64 66 40 74 50 NA 70 96 ...\n $ triceps : num  35 29 NA 23 35 NA 32 NA 45 NA ...\n $ insulin : num  NA NA NA 94 168 NA 88 NA 543 NA ...\n $ mass    : num  33.6 26.6 23.3 28.1 43.1 25.6 31 35.3 30.5 NA ...\n $ pedigree: num  0.627 0.351 0.672 0.167 2.288 ...\n $ age     : num  50 31 32 21 33 30 26 29 53 54 ...\n $ diabetes: Factor w/ 2 levels \"neg\",\"pos\": 2 1 2 1 2 1 2 1 2 2 ...\n\n\nWe fit a model predicting glucose level from all other covariates:\n\nmod <- glm(glucose ~ ., family = \"gaussian\", data = PimaIndiansDiabetes2)\nmod\n\n\nCall:  glm(formula = glucose ~ ., family = \"gaussian\", data = PimaIndiansDiabetes2)\n\nCoefficients:\n(Intercept)     pregnant     pressure      triceps      insulin         mass  \n    75.5742      -0.2168       0.1826       0.0277       0.1174      -0.0896  \n   pedigree          age  diabetespos  \n     0.1955       0.3700      21.6502  \n\nDegrees of Freedom: 391 Total (i.e. Null);  383 Residual\n  (376 observations deleted due to missingness)\nNull Deviance:      372400 \nResidual Deviance: 192000   AIC: 3561\n\nclass(mod)\n\n[1] \"glm\" \"lm\" \n\n\nThe glm() function accepts a formula that defines the model.\nThe formula hp ~ . means “regress hp on all other variables”. The family argument defines we are performing regression and the data argument points to the data frame where the covariates used in the formula are found.\nFor a gaussian output, we can also use the lm() function. There are minor differences in the output created, but the model is the same:\n\nmod_lm <- lm(glucose ~ ., data = PimaIndiansDiabetes2)\nmod_lm\n\n\nCall:\nlm(formula = glucose ~ ., data = PimaIndiansDiabetes2)\n\nCoefficients:\n(Intercept)     pregnant     pressure      triceps      insulin         mass  \n    75.5742      -0.2168       0.1826       0.0277       0.1174      -0.0896  \n   pedigree          age  diabetespos  \n     0.1955       0.3700      21.6502  \n\nclass(mod_lm)\n\n[1] \"lm\"\n\n\n\n32.1.1 Summary\nGet summary of the model using summary():\n\nsummary(mod)\n\n\nCall:\nglm(formula = glucose ~ ., family = \"gaussian\", data = PimaIndiansDiabetes2)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-73.274  -14.043   -2.433   13.004   78.112  \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 75.57419    8.08972   9.342  < 2e-16 ***\npregnant    -0.21685    0.48754  -0.445   0.6567    \npressure     0.18262    0.10014   1.824   0.0690 .  \ntriceps      0.02770    0.14665   0.189   0.8503    \ninsulin      0.11740    0.01027  11.433  < 2e-16 ***\nmass        -0.08960    0.22836  -0.392   0.6950    \npedigree     0.19555    3.40567   0.057   0.9542    \nage          0.36997    0.16183   2.286   0.0228 *  \ndiabetespos 21.65020    2.75623   7.855 4.07e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 501.4034)\n\n    Null deviance: 372384  on 391  degrees of freedom\nResidual deviance: 192037  on 383  degrees of freedom\n  (376 observations deleted due to missingness)\nAIC: 3560.6\n\nNumber of Fisher Scoring iterations: 2\n\n\nNote how R prints stars next to covariates whose p-values falls within certain limits, described right below the table of estimates.\nAlso notice that categorical variables of n levels get n-1 separate coefficients; the first level is considered the baseline. Therefore, make sure to set up factors appropriately before modeling to ensure the correct level is used as baseline.\n\n\n32.1.2 Coefficients\n\ncoefficients(mod)\n\n(Intercept)    pregnant    pressure     triceps     insulin        mass \n75.57418905 -0.21684536  0.18261621  0.02769953  0.11739871 -0.08959822 \n   pedigree         age diabetespos \n 0.19554841  0.36997121 21.65020282 \n\n# or\nmod$coefficients\n\n(Intercept)    pregnant    pressure     triceps     insulin        mass \n75.57418905 -0.21684536  0.18261621  0.02769953  0.11739871 -0.08959822 \n   pedigree         age diabetespos \n 0.19554841  0.36997121 21.65020282 \n\n\n\n\n32.1.3 Fitted values\n\nfitted(mod) |> head()\n\n       4        5        7        9       14       15 \n104.3669 134.0163 123.8123 191.4744 227.1301 147.0313 \n\n# or\nmod$fitted.values |> head()\n\n       4        5        7        9       14       15 \n104.3669 134.0163 123.8123 191.4744 227.1301 147.0313 \n\n\n\n\n32.1.4 Residuals\n\nresiduals(mod) |> head()\n\n         4          5          7          9         14         15 \n-15.366923   2.983712 -45.812340   5.525562 -38.130138  18.968718 \n\n# or\nmod$residuals |> head()\n\n         4          5          7          9         14         15 \n-15.366923   2.983712 -45.812340   5.525562 -38.130138  18.968718 \n\n\n\n\n32.1.5 p-values\nTo extract the p-values of the intercept and each coefficient, we use coef() on summary(). The final (4th) column lists the p-values:\n\ncoef(summary(mod))\n\n               Estimate Std. Error     t value     Pr(>|t|)\n(Intercept) 75.57418905 8.08972421  9.34199821 7.916237e-19\npregnant    -0.21684536 0.48753958 -0.44477489 6.567337e-01\npressure     0.18261621 0.10014453  1.82352656 6.900293e-02\ntriceps      0.02769953 0.14664887  0.18888336 8.502843e-01\ninsulin      0.11739871 0.01026851 11.43288871 3.047384e-26\nmass        -0.08959822 0.22835576 -0.39236241 6.950087e-01\npedigree     0.19554841 3.40566786  0.05741852 9.542418e-01\nage          0.36997121 0.16183342  2.28612371 2.279239e-02\ndiabetespos 21.65020282 2.75623039  7.85500474 4.071061e-14\n\n\n\n\n32.1.6 Plot linear fit\nYou use lines() to add a line on top of a scatterplot drawn with plot().\nlines() accepts x and y vectors of coordinates:\n\nset.seed(2020)\nx <- rnorm(500)\ny <- .73 * x + .5 * rnorm(500)\nxy.fit <- lm(y~x)$fitted\nplot(x, y, pch = 16, col = \"#18A3AC99\")\nlines(x, xy.fit, col = \"#178CCB\", lwd = 2)\n\n\n\n\nIn rtemis, you can use argument fit to use any supported algorithm (see modSelect()) to estimate the fit:\n\nmplot3_xy(x, y, fit = \"glm\")\n\n\n\n\n\n\n32.1.7 Logistic Regression\nFor logistic regression, i.e. classification, you can use glm() with family = binomial\nUsing the same dataset, let’s predict diabetes status:\n\ndiabetes_mod <- glm(diabetes ~ ., PimaIndiansDiabetes2, \n                    family = \"binomial\")\ndiabetes_mod\n\n\nCall:  glm(formula = diabetes ~ ., family = \"binomial\", data = PimaIndiansDiabetes2)\n\nCoefficients:\n(Intercept)     pregnant      glucose     pressure      triceps      insulin  \n -1.004e+01    8.216e-02    3.827e-02   -1.420e-03    1.122e-02   -8.253e-04  \n       mass     pedigree          age  \n  7.054e-02    1.141e+00    3.395e-02  \n\nDegrees of Freedom: 391 Total (i.e. Null);  383 Residual\n  (376 observations deleted due to missingness)\nNull Deviance:      498.1 \nResidual Deviance: 344  AIC: 362\n\n\n\nsummary(diabetes_mod)\n\n\nCall:\nglm(formula = diabetes ~ ., family = \"binomial\", data = PimaIndiansDiabetes2)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.7823  -0.6603  -0.3642   0.6409   2.5612  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.004e+01  1.218e+00  -8.246  < 2e-16 ***\npregnant     8.216e-02  5.543e-02   1.482  0.13825    \nglucose      3.827e-02  5.768e-03   6.635 3.24e-11 ***\npressure    -1.420e-03  1.183e-02  -0.120  0.90446    \ntriceps      1.122e-02  1.708e-02   0.657  0.51128    \ninsulin     -8.253e-04  1.306e-03  -0.632  0.52757    \nmass         7.054e-02  2.734e-02   2.580  0.00989 ** \npedigree     1.141e+00  4.274e-01   2.669  0.00760 ** \nage          3.395e-02  1.838e-02   1.847  0.06474 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 498.10  on 391  degrees of freedom\nResidual deviance: 344.02  on 383  degrees of freedom\n  (376 observations deleted due to missingness)\nAIC: 362.02\n\nNumber of Fisher Scoring iterations: 5"
  },
  {
    "objectID": "85_GLM.html#mass-univariate-analysis",
    "href": "85_GLM.html#mass-univariate-analysis",
    "title": "32  Introduction to the GLM",
    "section": "32.2 Mass-univariate analysis",
    "text": "32.2 Mass-univariate analysis\nThere are many cases where we have a large number of predictors and, along with any other number of tests or models, we may want to regress our outcome of interest on each covariate, one at a time.\nLet’s create some synthetic data with 1000 cases and 100 covariates\nThe outcome is generated using just 4 of those 100 covariates and has added noise.\n\nset.seed(2020)\nn_col <- 100\nn_row <- 1000\nx <- as.data.frame(lapply(seq(n_col), function(i) rnorm(n_row)),\n                   col.names = paste0(\"Feature_\", seq(n_col)))\ndim(x)\n\n[1] 1000  100\n\ny <- .7 + x[, 10] + .3 * x[, 20] + 1.3 * x[, 30] + x[, 50] + rnorm(500)\n\nLet’s fit a linear model regressing y on each column of x using lm:\n\nmod.xy.massuni <- lapply(seq(x), function(i) lm(y ~ x[, i]))\nlength(mod.xy.massuni)\n\n[1] 100\n\nnames(mod.xy.massuni) <- paste0(\"mod\", seq(x))\n\nTo extract p-values for each model, we must find where exactly to look.\nLet’s look into the first model:\n\n(ms1 <- summary(mod.xy.massuni$mod1))\n\n\nCall:\nlm(formula = y ~ x[, i])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.5402 -1.4881 -0.0618  1.4968  5.8152 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.61800    0.06878   8.985   <2e-16 ***\nx[, i]       0.08346    0.06634   1.258    0.209    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.174 on 998 degrees of freedom\nMultiple R-squared:  0.001584,  Adjusted R-squared:  0.0005831 \nF-statistic: 1.583 on 1 and 998 DF,  p-value: 0.2086\n\nms1$coefficients\n\n              Estimate Std. Error  t value     Pr(>|t|)\n(Intercept) 0.61800326 0.06878142 8.985032 1.266204e-18\nx[, i]      0.08346393 0.06634074 1.258110 2.086464e-01\n\n\nThe p-values for each feature is stored in row 1, column 4 fo the coefficients matrix. Let’s extract all of them:\n\nmod.xy.massuni.pvals <- sapply(mod.xy.massuni, function(i) summary(i)$coefficients[2, 4])\n\nLet’s see which variable are significant at the 0.05:\n\nwhich(mod.xy.massuni.pvals < .05)\n\n mod5 mod10 mod12 mod20 mod28 mod30 mod42 mod50 mod61 mod65 mod72 mod82 mod85 \n    5    10    12    20    28    30    42    50    61    65    72    82    85 \nmod91 mod94 mod99 \n   91    94    99 \n\n\n…and which are significant at the 0.01 level:\n\nwhich(mod.xy.massuni.pvals < .01)\n\nmod10 mod20 mod28 mod30 mod50 \n   10    20    28    30    50"
  },
  {
    "objectID": "85_GLM.html#correction-for-multiple-comparisons",
    "href": "85_GLM.html#correction-for-multiple-comparisons",
    "title": "32  Introduction to the GLM",
    "section": "32.3 Correction for multiple comparisons",
    "text": "32.3 Correction for multiple comparisons\nWe’ve performed a large number of tests and before reporting the results, we need to control for multiple comparisons.\nTo do that, we use R’s p.adjust() function. It adjusts a vector of p-values to account for multiple comparisons using one of multiple methods. The default, and recommended, is the Holm method. It ensures that FWER < α, i.e. controls the family-wise error rate, a.k.a. the probability of making one or more false discoveries (Type I errors)\n\nmod.xy.massuni.pvals.holm_adjusted <- p.adjust(mod.xy.massuni.pvals)\n\nNow, let’s see which features’ p-values survive the magical .05 threshold:\n\nwhich(mod.xy.massuni.pvals.holm_adjusted < .05)\n\nmod10 mod20 mod30 mod50 \n   10    20    30    50 \n\n\nThese are indeed the correct features (not surprisingly, still reassuringly)."
  },
  {
    "objectID": "85_GLM.html#glmresources",
    "href": "85_GLM.html#glmresources",
    "title": "32  Introduction to the GLM",
    "section": "32.4 Resources",
    "text": "32.4 Resources\n\nRegression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models"
  },
  {
    "objectID": "86_Resampling.html",
    "href": "86_Resampling.html",
    "title": "33  Resampling",
    "section": "",
    "text": "Resampling refers to a collection of techniques for selecting cases from a sample. It is central to many machine learning algorithms and pipelines. The two core uses of resampling in predictive modeling / machinbe learning are model selection (a.k.a. tuning) and model assessment. By convention, we use the terms training and validation set when refering to model selection, and training and testing set when refering to model assessment. The terminology is unfortunately not intuitive and has led to much confusion. Some people reverse the terms, but we use the terms training, validation, and testing as they are used in the Elements of Statistical Learning (p. 222, Second edition, 12th printing)."
  },
  {
    "objectID": "86_Resampling.html#model-selection-and-assessment",
    "href": "86_Resampling.html#model-selection-and-assessment",
    "title": "33  Resampling",
    "section": "33.1 Model Selection and Assessment",
    "text": "33.1 Model Selection and Assessment\n\nModel Selection aka Hyperparameter tuning\n\nResamples of the training set are drawn creating multiple training and validation sets. For each resample, a combination of hyperparameters is used to train a model. The mean validation-set error across resamples is calculated. The combination of hyperparameters with the minimum loss on average across validation-set resamples is selected to train the full training sample.\n\nModel assessment\n\nResamples of the full sample are drawn, resulting into multiple training - testing sets. A model is trained on each training set and its performance assessed on the corresponding test set. Model performance is averaged across all test sets.\nNested resampling or nested crossvalidation is the procedure where 1. and 2. are nested so that hyperparameter tuning (resampling of the training set) is performed within each of multiple training resamples and performance is tested in each corresponding test set. [elevate] performs automatic nested resampling and is one of the core supervised learning functions in rtemis.\n\n\n\n\n\n10-fold crossvalidation"
  },
  {
    "objectID": "86_Resampling.html#the-resample-function",
    "href": "86_Resampling.html#the-resample-function",
    "title": "33  Resampling",
    "section": "33.2 The resample function",
    "text": "33.2 The resample function\nThe resample() function is responsible for all resampling in rtemis.\n\nx <- rnorm(500)\nres <- resample(x)\n.:Resampling Parameters\n    n.resamples: 10 \n      resampler: strat.sub \n   stratify.var: y \n        train.p: 0.75 \n   strat.n.bins: 4 \n[2022-06-27 15:22:34 resample] Created 10 stratified subsamples \n\nclass(res)\n\n[1] \"resample\" \"list\"    \n\n\nIt outputs a list which is an S3 object of class resample, with print and plot methods.\n\nres\n.:rtemis resample object \n              N: 10 \n           type: strat.sub \n        train.p: 0.75 \n   strat.n.bins: 4 \n\nplot(res)\n\n\n\n\nThe teal-colored lines represent the training cases selected for each resample, the white are testing cases (held out).\nresample() supports 5 types of resampling:\n\nk-fold crossvalidation (Stratified)\n\nYou split the cases into k sets (folds). Each set is used once as the validation or testing set. This means each cases is left out exactly once and there is no overlap between different validation/test sets. In rtemis, the folds are also stratified by default on the outcome unless otherwise chosen. Stratification tries to maintain the full sample’s distribution in both training and left-out sets. This is crucial for non-normally distributed continuous outcomes or imbalanced datasets. 10 is a common value for k, called 10-fold. Note that the size of the training and left-out sets depends on the sample size.\n\nres.10fold <- resample(x, 10, \"kfold\")\n.:Resampling Parameters\n    n.resamples: 10 \n      resampler: kfold \n   stratify.var: y \n   strat.n.bins: 4 \n[2022-06-27 15:22:34 resample] Created 10 independent folds \n\n\n\nStratified subsampling\n\nDraw n.resamples stratified samples from the data given a certain probability (train.p) that each case belongs to the training set. Since you are randomly sampling from the full sample each time, there will be overlap in the test set cases, but you control the training-to-testing ratio and number of resamples independently, unlike in k-fold resampling.\n\nres.25ss <- resample(x, 25, \"strat.sub\")\n.:Resampling Parameters\n    n.resamples: 25 \n      resampler: strat.sub \n   stratify.var: y \n        train.p: 0.75 \n   strat.n.bins: 4 \n[2022-06-27 15:22:34 resample] Created 25 stratified subsamples \n\n\n\nBootstrap\n\nThe bootstrap: random sampling with replacement. Since cases are replicated, you should use bootstrap as the outer resampler if you will also have inner resampling for tuning, since the same case may end up in both training and validation sets.\n\nres.100boot <- resample(x, 100, \"bootstrap\")\n.:Resampling Parameters\n   n.resamples: 100 \n     resampler: bootstrap \n[2022-06-27 15:22:34 resample] Created 100 bootstrap resamples \n\n\n\nStratified Bootstrap\n\nThis is stratified subsampling with random replication of cases to match the length of the original sample. Same as the bootstrap, do not use if you will be further resampling each resample.\n\nres.100sboot <- resample(x, 100, \"strat.boot\")\n.:Resampling Parameters\n     n.resamples: 100 \n       resampler: strat.boot \n    stratify.var: y \n         train.p: 0.75 \n    strat.n.bins: 4 \n   target.length: 500 \n[2022-06-27 15:22:34 resample] Created 100 stratified bootstraps \n\n\n\nLeave-One-Out-Crossvalidation (LOOCV)\n\nThis is k-fold crossvalidation where \\(k = N\\), where \\(N\\) is number of data points/cases in the whole sample. It has been included for experimentation and completenes, but it is not recommended either for model selection or assessment over the other resampling methods.\n\nres.loocv <- resample(x, resampler = \"loocv\")\n.:Resampling Parameters\n   n.resamples: 500 \n     resampler: loocv \n[2022-06-27 15:22:34 resample] Created 500 independent folds (LOOCV)"
  },
  {
    "objectID": "86_Resampling.html#example-stratified-vs-random-sampling-in-a-binomial-distribution",
    "href": "86_Resampling.html#example-stratified-vs-random-sampling-in-a-binomial-distribution",
    "title": "33  Resampling",
    "section": "33.3 Example: Stratified vs random sampling in a binomial distribution",
    "text": "33.3 Example: Stratified vs random sampling in a binomial distribution\nImagine y is the outcome of interest where events occur with a probability of .1 - a common scenario in many fields.\n\nset.seed(2020)\nx <- rbinom(100, 1, .1)\nmplot3_x(x)\n\n\n\nfreq <- table(x)\nprob <- freq[2] / sum(freq)\nres.nonstrat <- lapply(seq(10), function(i) sample(seq(x), .75*length(x)))\nres.strat <- resample(x)\n.:Resampling Parameters\n    n.resamples: 10 \n      resampler: strat.sub \n   stratify.var: y \n        train.p: 0.75 \n   strat.n.bins: 4 \n[2022-06-27 15:22:34 strat.sub] Using max n bins possible = 2 \n[2022-06-27 15:22:34 resample] Created 10 stratified subsamples \n\nprob.nonstrat <- sapply(seq(10), function(i) {\n  freq <- table(x[res.nonstrat[[i]]])\n  freq[2]/sum(freq)\n})\nprob.strat <- sapply(seq(10), function(i) {\n  freq <- table(x[res.strat[[i]]])\n  freq[2]/sum(freq)\n})\nprob.nonstrat\n\n         1          1          1          1          1          1          1 \n0.09333333 0.08000000 0.08000000 0.06666667 0.06666667 0.10666667 0.10666667 \n         1          1          1 \n0.10666667 0.09333333 0.08000000 \n\nsd(prob.nonstrat)\n\n[1] 0.0156505\n\nprob.strat\n\n         1          1          1          1          1          1          1 \n0.08108108 0.08108108 0.08108108 0.08108108 0.08108108 0.08108108 0.08108108 \n         1          1          1 \n0.08108108 0.08108108 0.08108108 \n\nsd(prob.strat)\n\n[1] 0\n\n\nAs expected, the random sampling resulted in different event probability in each resample, whereas stratified subsampling maintained a constant probability across resamples."
  },
  {
    "objectID": "87_Supervised.html",
    "href": "87_Supervised.html",
    "title": "34  Supervised Learning",
    "section": "",
    "text": "This is a very brief introduction to machine learning using the rtemis package. rtemis includes functions for:"
  },
  {
    "objectID": "87_Supervised.html#installation",
    "href": "87_Supervised.html#installation",
    "title": "34  Supervised Learning",
    "section": "34.1 Installation",
    "text": "34.1 Installation\ninstall the remotes package, if you don’t have it:\n\ninstall.packages(\"remotes\")\n\nInstall rtemis:\n\nremotes::install_github(\"egenn/rtemis\")\n\nrtemis uses a large number of packages under the hood. Since you would not need to use all of them, they are not installed by default. Each time an rtemis function is called, a dependency check is run and a message is printed if any packages need to be installed.\nFor this short tutorial, start by installing ranger, if it is not already installed:\n\ninstall.packages(\"ranger\")\n\nLoad rtemis:\n\nlibrary(rtemis)\n\n  .:rtemis 0.91 🌊 aarch64-apple-darwin20 (64-bit)\n\n  Defaults\n  |   Theme: whitegrid\n  |    Font: Fira Sans\n  | Palette: rtCol1\n  |    Plan: multicore\n  |   Cores: 8/10 available\n\n  Resources\n  | Documentation: https://rtemis.lambdamd.org\n  |       Learn R: https://class.lambdamd.org/pdsr\n  | rtemis themes: https://egenn.lambdamd.org/software/#rtemis_themes\n  |          Cite: `citation(\"rtemis\")`\n\n  Setup\n  | Enable progress reporting: `progressr::handlers(global = TRUE)`"
  },
  {
    "objectID": "87_Supervised.html#data-input-for-supervised-learning",
    "href": "87_Supervised.html#data-input-for-supervised-learning",
    "title": "34  Supervised Learning",
    "section": "34.2 Data Input for Supervised Learning",
    "text": "34.2 Data Input for Supervised Learning\nAll rtemis supervised learning functions begin with s. (“supervised”).\nThey accept the same first four arguments:\nx, y, x.test, y.test\nbut are flexible and allow you to also provide combined (x, y) and (x.test, y.test) data frames, as explained below.\n\n34.2.1 Scenario 1 (x.train, y.train, x.test, y.test)\nIn the most straightforward case, provide each featureset and outcome individually:\n\nx: Training set features\ny: Training set outcome\nx.test: Testing set features (Optional)\ny.test: Testing set outcome (Optional)\n\n\nx <- rnormmat(200, 10, seed = 2019)\nw <- rnorm(10)\ny <- x %*% w + rnorm(200)\nres <- resample(y, seed = 2020)\n.:Resampling Parameters\n    n.resamples: 10 \n      resampler: strat.sub \n   stratify.var: y \n        train.p: 0.75 \n   strat.n.bins: 4 \n[2022-05-10 18:10:21 resample] Created 10 stratified subsamples \n\nx.train <- x[res$Subsample_1, ]\nx.test <- x[-res$Subsample_1, ]\ny.train <- y[res$Subsample_1]\ny.test <- y[-res$Subsample_1]\n\n\nmod_glm <- s_GLM(x.train, y.train, x.test, y.test)\n[2022-05-10 18:10:21 s_GLM] Hello, egenn \n\n.:Regression Input Summary\nTraining features: 147 x 10 \n Training outcome: 147 x 1 \n Testing features: 53 x 10 \n  Testing outcome: 53 x 1 \n\n[2022-05-10 18:10:21 s_GLM] Training GLM... \n\n.:GLM Regression Training Summary\n    MSE = 0.84 (91.88%)\n   RMSE = 0.92 (71.51%)\n    MAE = 0.75 (69.80%)\n      r = 0.96 (p = 5.9e-81)\n   R sq = 0.92\n\n.:GLM Regression Testing Summary\n    MSE = 1.22 (89.03%)\n   RMSE = 1.10 (66.88%)\n    MAE = 0.90 (66.66%)\n      r = 0.94 (p = 2.5e-26)\n   R sq = 0.89\n\n\n\n\n[2022-05-10 18:10:22 s_GLM] Run completed in 0.03 minutes (Real: 1.62; User: 0.43; System: 0.04) \n\n\n\n\n34.2.2 Scenario 2: (x.train, x.test)\nYou can provide training and testing sets as a single data.frame each, where the last column is the outcome. Now x is the full training data and y the full testing data:\n\nx: data.frame(x.train, y.train)\ny: data.frame(x.test, y.test)\n\n\nx <- rnormmat(200, 10, seed = 2019)\nw <- rnorm(10)\ny <- x %*% w + rnorm(200)\ndat <- data.frame(x, y)\nres <- resample(dat, seed = 2020)\n[2022-05-10 18:10:22 resample] Input contains more than one columns; will stratify on last \n.:Resampling Parameters\n    n.resamples: 10 \n      resampler: strat.sub \n   stratify.var: y \n        train.p: 0.75 \n   strat.n.bins: 4 \n[2022-05-10 18:10:22 resample] Created 10 stratified subsamples \n\ndat_train <- dat[res$Subsample_1, ]\ndat_test <- dat[-res$Subsample_1, ]\n\n\nmod_glm <- s_GLM(dat_train, dat_test)\n[2022-05-10 18:10:22 s_GLM] Hello, egenn \n\n.:Regression Input Summary\nTraining features: 147 x 10 \n Training outcome: 147 x 1 \n Testing features: 53 x 10 \n  Testing outcome: 53 x 1 \n\n[2022-05-10 18:10:22 s_GLM] Training GLM... \n\n.:GLM Regression Training Summary\n    MSE = 0.84 (91.88%)\n   RMSE = 0.92 (71.51%)\n    MAE = 0.75 (69.80%)\n      r = 0.96 (p = 5.9e-81)\n   R sq = 0.92\n\n.:GLM Regression Testing Summary\n    MSE = 1.22 (89.03%)\n   RMSE = 1.10 (66.88%)\n    MAE = 0.90 (66.66%)\n      r = 0.94 (p = 2.5e-26)\n   R sq = 0.89\n\n\n\n\n[2022-05-10 18:10:22 s_GLM] Run completed in 4.8e-04 minutes (Real: 0.03; User: 0.02; System: 4e-03) \n\n\nThe dataPrepare() function will check data dimensions and determine whether data was input as separate feature and outcome sets or combined and ensure the correct number of cases and features was provided.\nIn either scenario, Regression will be performed if the outcome is numeric and Classification if the outcome is a factor."
  },
  {
    "objectID": "87_Supervised.html#regression",
    "href": "87_Supervised.html#regression",
    "title": "34  Supervised Learning",
    "section": "34.3 Regression",
    "text": "34.3 Regression\n\n34.3.1 Check Data with checkData()\n\nx <- rnormmat(500, 50, seed = 2019)\nw <- rnorm(50)\ny <- x %*% w + rnorm(500)\ndat <- data.frame(x, y)\nres <- resample(dat)\n[2022-05-10 18:10:23 resample] Input contains more than one columns; will stratify on last \n.:Resampling Parameters\n    n.resamples: 10 \n      resampler: strat.sub \n   stratify.var: y \n        train.p: 0.75 \n   strat.n.bins: 4 \n[2022-05-10 18:10:23 resample] Created 10 stratified subsamples \n\ndat_train <- dat[res$Subsample_1, ]\ndat_test <- dat[-res$Subsample_1, ]\n\n\ncheckData(x)\n x: A matrix with 500 rows and 50 features\n\n  Data types________________\n  * 50 continuous features\n  * 0 integer features\n  * 0 categorical features\n  * 0 character features\n  * 0 date features\n\n  Issues____________________\n  * 0 constant features\n  * 0 duplicated cases\n  * 0 features include 'NA' values\n\n  Recommendations___________\n  * Everything looks good\n\n\n\n\n34.3.2 Single Model\n\nmod <- s_GLM(dat_train, dat_test)\n[2022-05-10 18:10:23 s_GLM] Hello, egenn \n\n.:Regression Input Summary\nTraining features: 374 x 50 \n Training outcome: 374 x 1 \n Testing features: 126 x 50 \n  Testing outcome: 126 x 1 \n\n[2022-05-10 18:10:23 s_GLM] Training GLM... \n\n.:GLM Regression Training Summary\n    MSE = 1.02 (97.81%)\n   RMSE = 1.01 (85.18%)\n    MAE = 0.81 (84.62%)\n      r = 0.99 (p = 1.3e-310)\n   R sq = 0.98\n\n.:GLM Regression Testing Summary\n    MSE = 0.98 (97.85%)\n   RMSE = 0.99 (85.35%)\n    MAE = 0.76 (85.57%)\n      r = 0.99 (p = 2.7e-105)\n   R sq = 0.98\n\n\n\n\n[2022-05-10 18:10:23 s_GLM] Run completed in 7.8e-04 minutes (Real: 0.05; User: 0.03; System: 0.01) \n\n\n\n\n34.3.3 Crossvalidated Model\n\nmod <- elevate(dat, mod = \"glm\")\n[2022-05-10 18:10:23 elevate] Hello, egenn \n\n.:Regression Input Summary\nTraining features: 500 x 50 \n Training outcome: 500 x 1 \n[2022-05-10 18:10:23 elevate] Using future framework \n[2022-05-10 18:10:23 resLearn] Outer resampling plan set to sequential \n[2022-05-10 18:10:23 resLearn] Training Generalized Linear Model on 10 stratified subsamples... \n\n.:elevate GLM\nN repeats = 1 \nN resamples = 10 \nResampler = strat.sub \nMean MSE of 10 resamples in each repeat = 1.22\nMean MSE reduction in each repeat =  97.50%\n\n\n\n\n\n[2022-05-10 18:10:23 elevate] Run completed in 0.01 minutes (Real: 0.39; User: 0.33; System: 0.02) \n\n\nUse the describe() function to get a summary in (plain) English:\n\nmod$describe()\n\nRegression was performed using Generalized Linear Model. Model generalizability was assessed using 10 stratified subsamples. The mean R-squared across all testing set resamples was 0.97.\n\n\n\nmod$plot()"
  },
  {
    "objectID": "87_Supervised.html#classification",
    "href": "87_Supervised.html#classification",
    "title": "34  Supervised Learning",
    "section": "34.4 Classification",
    "text": "34.4 Classification\n\n34.4.1 Check Data\n\ndata(Sonar, package = 'mlbench')\ncheckData(Sonar)\n Sonar: A data.frame with 208 rows and 61 features\n\n  Data types________________\n  * 60 continuous features\n  * 0 integer features\n  * 1 categorical feature, which is not ordered\n  * 0 character features\n  * 0 date features\n\n  Issues____________________\n  * 0 constant features\n  * 0 duplicated cases\n  * 0 features include 'NA' values\n\n  Recommendations___________\n  * Everything looks good\n\nres <- resample(Sonar)\n[2022-05-10 18:10:24 resample] Input contains more than one columns; will stratify on last \n.:Resampling Parameters\n    n.resamples: 10 \n      resampler: strat.sub \n   stratify.var: y \n        train.p: 0.75 \n   strat.n.bins: 4 \n[2022-05-10 18:10:24 strat.sub] Using max n bins possible = 2 \n[2022-05-10 18:10:24 resample] Created 10 stratified subsamples \n\nsonar_train <- Sonar[res$Subsample_1, ]\nsonar_test <- Sonar[-res$Subsample_1, ]\n\n\n\n34.4.2 Single model\n\nmod <- s_RANGER(sonar_train, sonar_test)\n[2022-05-10 18:10:24 s_RANGER] Hello, egenn \n\n[2022-05-10 18:10:24 dataPrepare] Imbalanced classes: using Inverse Probability Weighting \n\n.:Classification Input Summary\nTraining features: 155 x 60 \n Training outcome: 155 x 1 \n Testing features: 53 x 60 \n  Testing outcome: 53 x 1 \n\n.:Parameters\n   n.trees: 1000 \n      mtry: NULL \n\n[2022-05-10 18:10:24 s_RANGER] Training Random Forest (ranger) Classification with 1000 trees... \n\n.:RANGER Classification Training Summary\n                   Reference \n        Estimated  M   R   \n                M  83   0\n                R   0  72\n\n                   Overall  \n      Sensitivity  1      \n      Specificity  1      \nBalanced Accuracy  1      \n              PPV  1      \n              NPV  1      \n               F1  1      \n         Accuracy  1      \n              AUC  1      \n\n  Positive Class:  M \n\n.:RANGER Classification Testing Summary\n                   Reference \n        Estimated  M   R   \n                M  25  11\n                R   3  14\n\n                   Overall  \n      Sensitivity  0.8929 \n      Specificity  0.5600 \nBalanced Accuracy  0.7264 \n              PPV  0.6944 \n              NPV  0.8235 \n               F1  0.7812 \n         Accuracy  0.7358 \n              AUC  0.8643 \n\n  Positive Class:  M \n\n\n\n\n[2022-05-10 18:10:24 s_RANGER] Run completed in 2.3e-03 minutes (Real: 0.14; User: 0.20; System: 0.05) \n\n\n\n\n34.4.3 Crossvalidated Model\n\nmod <- elevate(Sonar)\n[2022-05-10 18:10:24 elevate] Hello, egenn \n\n.:Classification Input Summary\nTraining features: 208 x 60 \n Training outcome: 208 x 1 \n[2022-05-10 18:10:24 elevate] Using future framework \n[2022-05-10 18:10:24 resLearn] Outer resampling plan set to sequential \n[2022-05-10 18:10:24 resLearn] Training Random Forest (ranger) on 10 stratified subsamples... \n\n.:elevate RANGER\nN repeats = 1 \nN resamples = 10 \nResampler = strat.sub \nMean Balanced Accuracy of 10 test sets in each repeat = 0.83\n\n\n\n\n[2022-05-10 18:10:25 elevate] Run completed in 0.02 minutes (Real: 1.18; User: 2.06; System: 0.40) \n\n\n\nmod$describe()\n\nClassification was performed using Random Forest (ranger). Model generalizability was assessed using 10 stratified subsamples. The mean Balanced Accuracy across all testing set resamples was 0.83.\n\n\n\nmod$plot()\n\n\n\n\n\nmod$plotROC()\n\n\n\n\n\nmod$plotPR()\n\n\n\n\n\n\n34.4.4 Evaluation of a binary classifier\n\n\n\n\n\nEvaluation metrics for a binary classifier"
  },
  {
    "objectID": "87_Supervised.html#understanding-overfitting",
    "href": "87_Supervised.html#understanding-overfitting",
    "title": "34  Supervised Learning",
    "section": "34.5 Understanding Overfitting",
    "text": "34.5 Understanding Overfitting\nOverfitting occurs when a model fits noise in the outcome. To make this clear, consider the following example:\nAssume a random variable x:\n\nset.seed(2020)\nx <- sort(rnorm(500))\n\nand a data-generating function fn():\n\nfn <- function(x) 12 + x^5\n\nThe true y is therefore equal to fn(x):\n\ny_true <- fn(x)\n\nHowever, assume y is recorded with some noise, in this case gaussian:\n\ny_noise <- fn(x) + rnorm(500, sd = sd(y_true))\n\nWe plot:\n\nmplot3_xy(x, list(y_noise = y_noise, y_true = y_true))\n\n\n\n\nWe want to find a model that best approximates y_true, but we only know y_noise.\nA maximally overfitted model would model noise perfectly:\n\nmplot3_xy(x, list(Overfitted_model = y_noise, Ideal_model = y_true), type = \"l\")\n\n\n\n\nAn example of an SVM set up to overfit heavily:\n\nmplot3_xy(x, y_noise, fit = \"svm\",\n          fit.params = list(kernel = \"radial\", cost = 100, gamma = 100))\n\n\n\n\nAn example of a good approximation of fn using a GAM with penalized splines:\n\nmplot3_xy(x, y_noise, fit = \"gam\")"
  },
  {
    "objectID": "87_Supervised.html#rtemis-documentation",
    "href": "87_Supervised.html#rtemis-documentation",
    "title": "34  Supervised Learning",
    "section": "34.6 rtemis Documentation",
    "text": "34.6 rtemis Documentation\nFor more information on using rtemis, see the rtemis online documentation and vignettes"
  },
  {
    "objectID": "88_Unsupervised.html",
    "href": "88_Unsupervised.html",
    "title": "35  Unsupervised Learning",
    "section": "",
    "text": "Unsupervised learning aims to learn relationships within a dataset without focusing at a particular outcome. You will often hear of unsupervised learning being performed on unlabeled data. To be clear, it means it does not use the labels to guide learning - whether labels are available or not. You might, for example, perform unsupervised learning ahead of supervised learning as we shall see later. Unsupervised learning includes a number of approaches, most of which can be divided into two categories:\nIn rtemis, clustering algorithms begin with c_ and decomposition/dimensionality reduction algorithms begin with d_"
  },
  {
    "objectID": "88_Unsupervised.html#decomposition",
    "href": "88_Unsupervised.html#decomposition",
    "title": "35  Unsupervised Learning",
    "section": "35.1 Decomposition / Dimensionality Reduction",
    "text": "35.1 Decomposition / Dimensionality Reduction\nUse decomSelect() to get a listing of available decomposition algorithms:\n\ndecomSelect()\n\n.:decomSelect\nrtemis supports the following decomposition algorithms:\n\n    Name                                   Description\n     CUR                      CUR Matrix Approximation\n   H2OAE                               H2O Autoencoder\n H2OGLRM                H2O Generalized Low-Rank Model\n     ICA                Independent Component Analysis\n  ISOMAP                                        ISOMAP\n    KPCA           Kernel Principal Component Analysis\n     LLE                      Locally Linear Embedding\n     MDS                      Multidimensional Scaling\n     NMF             Non-negative Matrix Factorization\n     PCA                  Principal Component Analysis\n    SPCA           Sparse Principal Component Analysis\n     SVD                  Singular Value Decomposition\n    TSNE   t-distributed Stochastic Neighbor Embedding\n    UMAP Uniform Manifold Approximation and Projection\n\n\nWe can further divide decomposition algorithms into linear (e.g. PCA, ICA, NMF) and nonlinear dimensionality reduction, (also called manifold learning, like LLE and tSNE).\n\n35.1.1 Principal Component Analysic (PCA)\n\nx <- iris[, 1:4]\niris_PCA <- d_PCA(x)\n\n[2022-05-10 18:13:50 d_PCA] Hello, egenn \n[2022-05-10 18:13:50 d_PCA] ||| Input has dimensions 150 rows by 4 columns, \n[2022-05-10 18:13:50 d_PCA]     interpreted as 150 cases with 4 features. \n[2022-05-10 18:13:50 d_PCA] Performing Principal Component Analysis... \n[2022-05-10 18:13:50 d_PCA] Run completed in 4e-04 minutes (Real: 0.02; User: 0.01; System: 1e-03) \n\nmplot3_xy(iris_PCA$projections.train[, 1], \n          iris_PCA$projections.train[, 2], \n          group = iris$Species,\n          xlab = \"1st PCA component\", \n          ylab = \"2nd PCA component\", \n          main = \"PCA on iris\")\n\n\n\n\n\n\n35.1.2 Independent Component Analysis (ICA)\n\niris_ICA <- d_ICA(x, k = 2)\n\n[2022-05-10 18:13:51 d_ICA] Hello, egenn \n[2022-05-10 18:13:51 d_ICA] ||| Input has dimensions 150 rows by 4 columns, \n[2022-05-10 18:13:51 d_ICA]     interpreted as 150 cases with 4 features. \n[2022-05-10 18:13:51 d_ICA] Running Independent Component Analysis... \n[2022-05-10 18:13:51 d_ICA] Run completed in 4.2e-04 minutes (Real: 0.02; User: 3e-03; System: 1e-03) \n\nmplot3_xy(iris_ICA$projections.train[, 1], \n          iris_ICA$projections.train[, 2], \n          group = iris$Species,\n          xlab = \"1st ICA component\", \n          ylab = \"2nd ICA component\", \n          main = \"ICA on iris\")\n\n\n\n\n\n\n35.1.3 Non-negative Matrix Factorization (NMF)\n\niris_NMF <- d_NMF(x, k = 2)\n\n[2022-05-10 18:13:51 d_NMF] Hello, egenn \n[2022-05-10 18:13:52 d_NMF] ||| Input has dimensions 150 rows by 4 columns, \n[2022-05-10 18:13:52 d_NMF]     interpreted as 150 cases with 4 features. \n[2022-05-10 18:13:52 d_NMF] Running Non-negative Matrix Factorization... \n[2022-05-10 18:13:53 d_NMF] Run completed in 0.02 minutes (Real: 1.22; User: 1.02; System: 0.05) \n\nmplot3_xy(iris_NMF$projections.train[, 1], \n          iris_NMF$projections.train[, 2], \n          group = iris$Species,\n          xlab = \"1st NMF component\", \n          ylab = \"2nd NMF component\", \n          main = \"NMF on iris\")"
  },
  {
    "objectID": "88_Unsupervised.html#clustering",
    "href": "88_Unsupervised.html#clustering",
    "title": "35  Unsupervised Learning",
    "section": "35.2 Clustering",
    "text": "35.2 Clustering\nUse clustSelect() to get a listing of available clustering algorithms:\n\nclustSelect()\n\n.:clustSelect\nrtemis supports the following clustering algorithms:\n\n      Name                                                 Description\n    CMEANS                                    Fuzzy C-means Clustering\n    DBSCAN Density-based spatial clustering of applications with noise\n       EMC                         Expectation Maximization Clustering\n    HARDCL                                   Hard Competitive Learning\n    HOPACH     Hierarchical Ordered Partitioning And Collapsing Hybrid\n H2OKMEANS                                      H2O K-Means Clustering\n    KMEANS                                          K-Means Clustering\n MEANSHIFT                                       Mean Shift Clustering\n      NGAS                                       Neural Gas Clustering\n       PAM                                 Partitioning Around Medoids\n      PAMK               Partitioning Around Medoids with k estimation\n      SPEC                                         Spectral Clustering\n\n\nLet’s cluster iris and we shall also use an NMF decomposition as we saw above to project to 2 dimensions.\nWe’ll use two of the most popular clustering algorithms, K-means and PAM, aka K-medoids.\n\nx <- iris[, 1:4]\niris_NMF <- d_NMF(x, k = 2)\n\n[2022-05-10 18:13:53 d_NMF] Hello, egenn \n[2022-05-10 18:13:53 d_NMF] ||| Input has dimensions 150 rows by 4 columns, \n[2022-05-10 18:13:53 d_NMF]     interpreted as 150 cases with 4 features. \n[2022-05-10 18:13:53 d_NMF] Running Non-negative Matrix Factorization... \n[2022-05-10 18:13:53 d_NMF] Run completed in 4.5e-03 minutes (Real: 0.27; User: 0.26; System: 0.01) \n\n\n\n35.2.1 K-Means\n\niris.KMEANS <- c_KMEANS(x, k = 3)\n\n[2022-05-10 18:13:53 c_KMEANS] Hello, egenn \n[2022-05-10 18:13:53 c_KMEANS] Performing K-means Clustering with k = 3... \n[2022-05-10 18:13:53 c_KMEANS] Run completed in 1.8e-03 minutes (Real: 0.11; User: 0.09; System: 0.01) \n\nmplot3_xy(iris_NMF$projections.train[, 1], iris_NMF$projections.train[, 2],\n          group = iris.KMEANS$clusters.train,\n          xlab = \"1st NMF component\", \n          ylab = \"2nd NMF component\", \n          main = \"KMEANS on iris\")\n\n\n\n\n\n\n35.2.2 Partitioning Around Medoids with k estimation (PAMK)\n\niris_PAMK <- c_PAMK(x, krange = 3:10)\n\n[2022-05-10 18:13:53 c_PAMK] Hello, egenn \n[2022-05-10 18:13:54 c_PAMK] Partitioning Around Medoids... \n[2022-05-10 18:13:54 c_PAMK] Estimated optimal number of clusters: 3 \n[2022-05-10 18:13:54 c_PAMK] Run completed in 0.01 minutes (Real: 0.49; User: 0.38; System: 0.02) \n\nmplot3_xy(iris_NMF$projections.train[, 1], iris_NMF$projections.train[, 2],\n          group = iris_PAMK$clusters.train,\n          xlab = \"1st NMF component\", \n          ylab = \"2nd NMF component\", \n          main = \"PAMK on iris\")"
  },
  {
    "objectID": "94_GitHubIntro.html",
    "href": "94_GitHubIntro.html",
    "title": "36  Git & GitHub: the basics",
    "section": "",
    "text": "git is famously powerful and notoriously complex. This is a very brief introduction to a very small subset of git’s functionality. Multiple online resources can help you delve into git in considerably more depth.\nFirst, some important definitions:"
  },
  {
    "objectID": "94_GitHubIntro.html#installing-git",
    "href": "94_GitHubIntro.html#installing-git",
    "title": "36  Git & GitHub: the basics",
    "section": "36.1 Installing git",
    "text": "36.1 Installing git\nCheck if you system already includes an installation of git. If not you can download it from the official git website"
  },
  {
    "objectID": "94_GitHubIntro.html#basic-git-usage",
    "href": "94_GitHubIntro.html#basic-git-usage",
    "title": "36  Git & GitHub: the basics",
    "section": "36.2 Basic git usage",
    "text": "36.2 Basic git usage\nIn the system terminal, all git commanda begin with git and are followed by a command name:\n\n36.2.1 Cloning (“Downloading”)\nDownload a repository to your computer for the first time. Replace “user” with the username and “repo” with the repository name.\n\ngit clone https://github.com/user/repo.git\n\nThis will clone the remote repository to a folder name ‘repo’. You can optionally provide a different folder name after the URL.\nTo update a previously cloned repository:\n\ngit pull\n\n\n\n36.2.2 Pushing (“Uploading”)\nGet info on local changes to repository:\n\ngit status\n\nWorking locally, stage new or modified files:\n\ngit add /path/to/file\n\nStill working locally, commit changes with an informative message:\n\ngit commit -m Fixed this or added that\n\n(Note that the previous steps did not require an internet connection - this one does) Push one or multiple commits to remote repository:\n\ngit push\n\n\n\n36.2.3 Collaborating\nThe main way of contributing to a project is by a) making a new “branch” of the repository, b) making your edits, and c) either merging to master yourself or requesting your edits be merged by the owner/s of the repository. This allows multiple people to work on the codebase without getting in each other’s way.\n\n\n36.2.4 Branching and merging\nScenario: you are working on your own project, hosted on its own repository. You want to develop a new feature, which may take some time to code and test before you make it part of your official project code.\n* Create a new branch, e.g. devel * Work in your new branch until all testing is successful * Merge back to master branch\nAlways from your system terminal, from within a directory in your repository: Create a new branch:\n\ngit branch devel\n\nSwitch to your new branch:\n\ngit checkout devel\n\nWork on your code, using git add/commit/push as per usual.\nWhen you are done testing and are happy to merge back to master:\n\ngit checkout master\ngit merge devel\ngit push\n\nAll the commits performed while you were working in the devel branch will be included in that last git push from master.\n\n\n36.2.5 Pull request\nScenario: You are contributing to a repository along with other collaborators. You want to suggest a new feature is added to the code:\n\nCreate a new branch, e.g. mynewfeature\nWork in new branch until you are ready happy to share and testing is complete\nGo on to the repository website, select your branch and perform a “Pull request” asking that the changes in your mynewfeature branch are merged into master\nThe repository owner/s will review the request and can merge"
  },
  {
    "objectID": "94_GitHubIntro.html#gists",
    "href": "94_GitHubIntro.html#gists",
    "title": "36  Git & GitHub: the basics",
    "section": "36.3 Gists",
    "text": "36.3 Gists\nGitHub also offers a very convenient pastebin-like service called Gist, which lets you quickly and easily share code snippets.\nTo share some R code using a gist:\n\nVist the gist site.\nWrite in/copy-paste some code\nAdd a name including a .R suffix at the top left of the entry box\nCopy-paste the URL to share with others"
  },
  {
    "objectID": "94_GitHubIntro.html#gitresources",
    "href": "94_GitHubIntro.html#gitresources",
    "title": "36  Git & GitHub: the basics",
    "section": "36.4 Git Resources",
    "text": "36.4 Git Resources\nGit and GitHub are very powerful and flexible, with a great deal of functionality. Some resources to learn (a great deal) more:\n\nGit cheat sheet\nGitHub guides # Pro Git Book by Scott Chacon and Ben Straub"
  },
  {
    "objectID": "94_GitHubIntro.html#git-and-github-for-open-and-reproducible-science",
    "href": "94_GitHubIntro.html#git-and-github-for-open-and-reproducible-science",
    "title": "36  Git & GitHub: the basics",
    "section": "36.5 Git and GitHub for open and reproducible science",
    "text": "36.5 Git and GitHub for open and reproducible science\nIt is recommended to create a new GitHub repository for each new research project. It may be worthwhile creating a new repository when it’s time to publish a paper, to include all final working code that should accompany the publication (and e.g. exclude all trial-and-error, testing, etc. code). As Always, make sure to follow journal requirements for reporting data deposition (includes code) and accessibility."
  },
  {
    "objectID": "94_GitHubIntro.html#applications-with-builtin-git-support",
    "href": "94_GitHubIntro.html#applications-with-builtin-git-support",
    "title": "36  Git & GitHub: the basics",
    "section": "36.6 Applications with builtin git support",
    "text": "36.6 Applications with builtin git support\nMany applications support git, and allow you to pull / add / commit / push and more directly from the app using their GUI.\nA couple of interest for the R user:\n\nRStudio Out trusty IDE has a Git panel enabled when a project is in a directory that’s part of a git repository\nThe Atom editor GitHub’s own feature-packed text editor is naturally built around git and GitHub support. It offers its own package manager with access to a large and growing ecosystem of packages. Packages are available that transform Atom to a very capable and customizable IDE."
  },
  {
    "objectID": "95_TerminalIntro.html",
    "href": "95_TerminalIntro.html",
    "title": "37  Introduction to the system shell",
    "section": "",
    "text": "This is a very brief introduction to some of the most commonly used shell commands.\nA shell is a command line interface allowing access to an operating system’s services. Multiple different shells exist. The most popular is probably bash, which is the default in most Linux installations. In MacOS, the default shell switched form bash to zsh in 2019 with the release of Catalina. In Windows, various shells are available through the Windows Subsystem for Linux.\nThe commands listed here will work similarly in all/most shells."
  },
  {
    "objectID": "95_TerminalIntro.html#common-shell-commands",
    "href": "95_TerminalIntro.html#common-shell-commands",
    "title": "37  Introduction to the system shell",
    "section": "37.1 Common shell commands",
    "text": "37.1 Common shell commands\nThe first thing to look for in a new environment is the help system. In the shell, this is accessed with man:\n\nman: Print the manual pages\n\n\nman man\n\n\npwd: Print working directory (the directory you are currently in)\n\n\npwd\n\n\ncd: Set working directory to /path/to/dir\n\n\ncd /path/to/dir\n\n\nmv: Move file from /current/dir/ to /new/dir\n\n\nmv /current/dir/file /new/dir\n\n\nmv: Rename file to newfilename\n\n\nmv /current/dir/file /current/dir/newfilename\n\n\ncp: Make a copy of file from currentPath into altPath\n\n\ncp /currentPath/file /altPath/file\n\n\nmkdir: Create a new directory named ‘newdir’\n\n\nmkdir /path/to/newdir\n\n\nrmdir: Remove (i.e. delete) uselessFile\n\n\n\n\n\nrm: Remove (i.e. delete) uselessFile\n\n\nrm /path/to/uselessFile\n\n\ncat: Print contents of file to the console\n\n\ncat /path/to/file\n\n\nuname: Get system information\n\n\nuname -a\n\n\nwhoami: When you forget the basics\n\n\nwhoami"
  },
  {
    "objectID": "95_TerminalIntro.html#running-system-commands-within-r",
    "href": "95_TerminalIntro.html#running-system-commands-within-r",
    "title": "37  Introduction to the system shell",
    "section": "37.2 Running system commands within R",
    "text": "37.2 Running system commands within R\nYou can execute any system command within R using the system() command:\n\nsystem(\"uname -a\")"
  },
  {
    "objectID": "98_Resources.html",
    "href": "98_Resources.html",
    "title": "38  Resources",
    "section": "",
    "text": "The R Manuals include a number of resources, including:\n\nIntroduction to R\nCRAN task views offer curated lists of packages by topic"
  },
  {
    "objectID": "98_Resources.html#r-markdown",
    "href": "98_Resources.html#r-markdown",
    "title": "38  Resources",
    "section": "38.2 R markdown",
    "text": "38.2 R markdown\n\nR Markdown: The Definitive Guide by Yihui Xie, J. J. Allaire, Garrett Grolemund\nbookdown: Authoring Books and Technical Documents with R Markdown: how to make websites like this one you are on right now"
  },
  {
    "objectID": "98_Resources.html#documentation",
    "href": "98_Resources.html#documentation",
    "title": "38  Resources",
    "section": "38.3 Documentation",
    "text": "38.3 Documentation\n\nDocumentation with roxygen2"
  },
  {
    "objectID": "98_Resources.html#r-for-data-science",
    "href": "98_Resources.html#r-for-data-science",
    "title": "38  Resources",
    "section": "38.4 R for data science",
    "text": "38.4 R for data science\n\nR Programming for Data Science by Roger D. Peng, based mostly on base R, and also covers the basics of dplyr.\nR for Data Science by Hadley Wickham & Garrett Grolemund, based on the tidyverse\nData wrangling, exploration, and analysis with R"
  },
  {
    "objectID": "98_Resources.html#graphics",
    "href": "98_Resources.html#graphics",
    "title": "38  Resources",
    "section": "38.5 Graphics",
    "text": "38.5 Graphics\n\n38.5.1 ggplot2\n\nggplot2\n\n\n\n38.5.2 Plotly\n\nPlotly R API\nInteractive web-based data visualization with R, plotly, and shiny"
  },
  {
    "objectID": "98_Resources.html#advanced-r",
    "href": "98_Resources.html#advanced-r",
    "title": "38  Resources",
    "section": "38.6 Advanced R",
    "text": "38.6 Advanced R\n\nEfficient R Programming by Colin Gillespie & Robin Lovelace\nHigh performance functions with Rcpp"
  },
  {
    "objectID": "98_Resources.html#git-and-github",
    "href": "98_Resources.html#git-and-github",
    "title": "38  Resources",
    "section": "38.7 Git and GitHub",
    "text": "38.7 Git and GitHub\n\nGitHub guides\nPro Git Book by Scott Chacon and Ben Straub"
  },
  {
    "objectID": "98_Resources.html#machine-learning",
    "href": "98_Resources.html#machine-learning",
    "title": "38  Resources",
    "section": "38.8 Machine Learning",
    "text": "38.8 Machine Learning\n\nAn Introduction to Statistical Learning offers an accessible view of core learning algorithms, without being math-heavy.\nElements of Statistical Learning offers a deeper and more extensive view on learning algorithms.\nMachine Learning with rtemis"
  },
  {
    "objectID": "98_Resources.html#getting-help",
    "href": "98_Resources.html#getting-help",
    "title": "38  Resources",
    "section": "38.9 Getting help",
    "text": "38.9 Getting help\nStack Overflow is a massively popular Q&A site for programmers, part of the wider Stack Exchange network. Many R-related web searches will bring up posts in Stack Overflow. You can view all questions tagged with “r”.\nWhen posting a question in any setting, it is strongly recommended to provide a minimal reproducible example (MRE). Stack Overflow provides guidelines on how to create an MRE."
  },
  {
    "objectID": "200_CrashCourse.html#introduction-format",
    "href": "200_CrashCourse.html#introduction-format",
    "title": "Crash Course",
    "section": "Introduction & Format",
    "text": "Introduction & Format\nThis is a brief introduction to the R programming language for health data science. It covers basic commands to allow you to read in data, perform common manipulations, plot data and run common tests.\nR is a programming language developed specifically for statistical computing and graphics.\nIt is often mis-characterized as a “statistical package”, similar to SPSS, for example, but as a full programming language it has far more extensive functionality.\nFor a more thorough coverage of the topic, see the Programming for Data Science in R book. Links to book chapters will be provided throughout these notes for those interested in reading up further into particular topics.\nR can be used to perform most, if not all, operations in statistics, data science, machine learning. This is not a crash course in statistics, data science, or machine learning, but an introduction to the language itself.\nIf you have any questions, write them down and make sure to ask them during each section’s Q&A."
  },
  {
    "objectID": "200_CrashCourse.html#introduction-to-programming",
    "href": "200_CrashCourse.html#introduction-to-programming",
    "title": "Crash Course",
    "section": "Introduction to Programming",
    "text": "Introduction to Programming\nEveryone can code\nEveryone can learn how to program. Whether it takes you a minute or a little longer to learn how to write the code neessary to perform a certain, you can master it. Do not worry about comparing yourself to others.\nAs with more or less everything, a) motivation is key and b) you get good with practice.\nYou are here because, presumably, you have important questions to answet using data.\nKnowing even a little code can give you the power to work with your own data without depending fully on someone else. At the same time, it makes collaborating with other scientistics, clinicians, statisticians, etc. much more effective & efficient.\nYou don’t learn to code by reading books or slides, you learn to code by doing. Ideally, work on data you are interested in, trying to asnwet questions you care about.\nLearning to code can be exciting and frustrating. It’s similar to learning to play an instrument - like the guitar. At first, it may seem unnatural and annoying, but you can get better at it rather quickly and it’s very rewarding and satisfying.\nIt is important to be able to read & write code.\nCoding needs logic & creativity\nProgramming is based on logic and learning to code helps structure your thoughts, your experiments, your reasoning.\nProgramming languages are human constructs. They developed to answer important needs. They develop with time, as needs evolve. Many design choices are explained historically, a few may be more arbitrary.\nEverything is a series of simple, manageable steps\nRemember this: the shortest and simplest piece of code up to the longest and most complex is made of a sequence of relatively simple steps.\nAlways be clear about what you want to achieve first, then break it down step-by-step in code. Each step is relatively easy to figure out and check that it is working as desired. A common mistake is to write multiple steps in one go, and then have a hard time figuring out where an error occurs or why.\nTo get from A to B using code there are virtually always multiple different paths. That can be confusing or perhaps frustrating, but it is also exciting. Programming to the uninitiated may seem a a rigid exercise but it is highly creative. Remember that there objective and subjective differences to consider when designing a code to take you from A to B. Suppose you have two approaches that have the same input and produce the same output. An objective difference would be how fast each completes the task and how many lines of code or number of function calls it requires. A subjective difference would be the programming style / syntax used / whether the code is “elegant” - a pretty broad topic.\nErrors happen and they are not all the same\nErrors in code happen all the time, it is part of the process. But, not all errors are the same, far from it. One crucial difference is coding errors that:\n\nstop execution of code and produce an error message. This is the best case scenario because it can’t go unnoticed.\ndo not stop execution of code but produce a warning. These warnings are too often ignored. They may be serios or trivial, but must be investigated.\ndo not stop execution and produce no warnings. This is the worst kind of error since it is silent. These are very common and the only way to recognize them are to check the output.\n\nDetails matter (a lot)\nA lot of beginner and non-beginner mistakes occur because a variable or function name is misspelled.\nALWAYS, ALWAYS, ALWAYS READ ERROR AND WARNING MESSAGES. The all-caps is because this is a) essential and b) far too often ignored.\nAlways check yourself\nRemember: the most important thing is to ensure you produce correct results at each step. Don’t place writing smart or elegant code above writing correct code. Spend time reviewing your code. Ideally, if possible, have one or more other people review your code.\nDocument everything\nMake a habit from the very beginning to always use comments in your code to explai what you are trying to achieve and why. You will often need to revisit your code after some time has passed. Life will be very hard if it’s not clear what is happening and why.\nProgramming is largely a team sport. A lot of code is written collaboratively or is used by people other than the author. Again, comprehensive documentation is super important.\nHelp is at your fingertips\nWhether you are just starting out or you are a seasoned programmer, you have many sources of information to help you troubleshoot or learn new skills.\n\nUse the built-in documentation! Builtin help files, written by the code author, are almost always the best place to start. Their quality will vary, but they are often sufficient to learn how to use a function properly.\nProgramming is largely an online activity. All documentation and source code (for open source projects) is available online. Most errors or difficulties you encounter have been encountered many times before by others. A very large number of Q&A sites, blogs, forums are a web search away. Copy-pasting an error message into a search engine will often result in multiple hits."
  },
  {
    "objectID": "200_CrashCourse.html#the-r-language",
    "href": "200_CrashCourse.html#the-r-language",
    "title": "Crash Course",
    "section": "The R language",
    "text": "The R language\n\nThe S statistical programming language was developed in 1976 at Bell Labs by John Chambers and others “to turn ideas into software, quickly and faithfully”.\nR is an open source implementation of S developed by Ross Ihaka and Robert Gentleman at the University of Auckland, New Zealand; initial version released in 1995.\nSupported by the R Foundation for Statistical Computing, developed by the R Core Team and the contributions of many others.\nOfficial part of the Free Software Foundation’s GNU project available under GNU GPL v2.\nLatest version 4.1.3 released 2022-03-10 (as of writing)"
  },
  {
    "objectID": "200_CrashCourse.html#free-open-source-software",
    "href": "200_CrashCourse.html#free-open-source-software",
    "title": "Crash Course",
    "section": "Free Open Source Software",
    "text": "Free Open Source Software\nFree Open Source Software (FOSS) is software that is “free” and “open source” - what does that really mean?\nWhat is Free Software?\n\n“Free software is software that gives you, the user, the freedom to share, study and modify it. We call this free software because the user is free.”\n\n— Free Software Foundation\n\n\nWhat is Open Source Software\n\n“Open source software is software with source code that anyone can inspect, modify, and enhance.”\n\n— opensource.com\n\n\nWhy is FOSS important?\nThere are many advantage to FOSS, in general. Some of those, that are highly relevant in biomedical research and clinical applications include the promotion of inclusion, transparency, and trustworthiness."
  },
  {
    "objectID": "200_CrashCourse.html#rstudio-integrated-development-environment",
    "href": "200_CrashCourse.html#rstudio-integrated-development-environment",
    "title": "Crash Course",
    "section": "RStudio Integrated Development Environment",
    "text": "RStudio Integrated Development Environment\nRStudio offers a popular, feature-full Integrated Development Environment (IDE) for R.\nMore advanced users can use Visual Studio Code with the R Extension for a similar R IDE experience together with all the extra functionality an convenience of VS Code."
  },
  {
    "objectID": "200_CrashCourse.html#the-r-core-language-package-ecosystem",
    "href": "200_CrashCourse.html#the-r-core-language-package-ecosystem",
    "title": "Crash Course",
    "section": "The R core language & package ecosystem",
    "text": "The R core language & package ecosystem\nR boasts extensive quantitative and statistical functionality in the base system.\nThis functionality is extended through a vast ecosystem of external packages.\n\nCRAN: The Comprehensive R Archive Network (https://cran.r-project.org/): 19001 packages\nBioconductor: Bioinformatics-related packages and more (https://www.bioconductor.org/): 2083+ packages\nGitHub: The largest source code host (>200M repositories; https://github.com/): Likely hosts most of the above and quite a few more. Also hosts a copy of the entire CRAN."
  },
  {
    "objectID": "200_CrashCourse.html#reading-in-data",
    "href": "200_CrashCourse.html#reading-in-data",
    "title": "Crash Course",
    "section": "Reading in Data",
    "text": "Reading in Data\nWe shall use a heart failure dataset as an example. It is freely available at the UCI repository: “https://archive.ics.uci.edu/ml/machine-learning-databases/00519/heart_failure_clinical_records_dataset.csv”\n\nCSV\n\ndat <- read.csv(\"~/icloud/Data/UCI/heart_failure_clinical_records_dataset.csv\")\n\nThe head() function prints the first few lines of an object:\n\nhead(dat)\n\n  age anaemia creatinine_phosphokinase diabetes ejection_fraction\n1  75       0                      582        0                20\n2  55       0                     7861        0                38\n3  65       0                      146        0                20\n4  50       1                      111        0                20\n5  65       1                      160        1                20\n6  90       1                       47        0                40\n  high_blood_pressure platelets serum_creatinine serum_sodium sex smoking time\n1                   1    265000              1.9          130   1       0    4\n2                   0    263358              1.1          136   1       0    6\n3                   0    162000              1.3          129   1       1    7\n4                   0    210000              1.9          137   1       0    7\n5                   0    327000              2.7          116   0       0    8\n6                   1    204000              2.1          132   1       1    8\n  DEATH_EVENT\n1           1\n2           1\n3           1\n4           1\n5           1\n6           1\n\n\nThe read.csv() function read the contents of the CSV file into an R object known as a data.frame. This is essentially a table like a spreadsheet, where each row represents a case (e.g. a subject, patient, etc.) and each columnn represents a variable (e.g. Patient ID, Age, Sex, Dx, etc.)\n\n\nXLSX\n\ndat_too <- openxlsx::read.xlsx(\"~/icloud/Data/UCI/heart_failure_clinical_records_dataset.xlsx\")\n\n\nhead(dat_too)\n\n  age anaemia creatinine_phosphokinase diabetes ejection_fraction\n1  75       0                      582        0                20\n2  55       0                     7861        0                38\n3  65       0                      146        0                20\n4  50       1                      111        0                20\n5  65       1                      160        1                20\n6  90       1                       47        0                40\n  high_blood_pressure platelets serum_creatinine serum_sodium sex smoking time\n1                   1    265000              1.9          130   1       0    4\n2                   0    263358              1.1          136   1       0    6\n3                   0    162000              1.3          129   1       1    7\n4                   0    210000              1.9          137   1       0    7\n5                   0    327000              2.7          116   0       0    8\n6                   1    204000              2.1          132   1       1    8\n  DEATH_EVENT\n1           1\n2           1\n3           1\n4           1\n5           1\n6           1"
  },
  {
    "objectID": "200_CrashCourse.html#inspect-summarize-data",
    "href": "200_CrashCourse.html#inspect-summarize-data",
    "title": "Crash Course",
    "section": "Inspect & summarize data",
    "text": "Inspect & summarize data\nGet data dimensions:\n\ndim(dat)\n\n[1] 299  13\n\n\nLook at the data structure, including data types:\n\nstr(dat)\n\n'data.frame':   299 obs. of  13 variables:\n $ age                     : num  75 55 65 50 65 90 75 60 65 80 ...\n $ anaemia                 : int  0 0 0 1 1 1 1 1 0 1 ...\n $ creatinine_phosphokinase: int  582 7861 146 111 160 47 246 315 157 123 ...\n $ diabetes                : int  0 0 0 0 1 0 0 1 0 0 ...\n $ ejection_fraction       : int  20 38 20 20 20 40 15 60 65 35 ...\n $ high_blood_pressure     : int  1 0 0 0 0 1 0 0 0 1 ...\n $ platelets               : num  265000 263358 162000 210000 327000 ...\n $ serum_creatinine        : num  1.9 1.1 1.3 1.9 2.7 2.1 1.2 1.1 1.5 9.4 ...\n $ serum_sodium            : int  130 136 129 137 116 132 137 131 138 133 ...\n $ sex                     : int  1 1 1 1 0 1 1 1 0 1 ...\n $ smoking                 : int  0 0 1 0 0 1 0 1 0 1 ...\n $ time                    : int  4 6 7 7 8 8 10 10 10 10 ...\n $ DEATH_EVENT             : int  1 1 1 1 1 1 1 1 1 1 ...\n\n\nGet summary of dataset:\n\nsummary(dat)\n\n      age           anaemia       creatinine_phosphokinase    diabetes     \n Min.   :40.00   Min.   :0.0000   Min.   :  23.0           Min.   :0.0000  \n 1st Qu.:51.00   1st Qu.:0.0000   1st Qu.: 116.5           1st Qu.:0.0000  \n Median :60.00   Median :0.0000   Median : 250.0           Median :0.0000  \n Mean   :60.83   Mean   :0.4314   Mean   : 581.8           Mean   :0.4181  \n 3rd Qu.:70.00   3rd Qu.:1.0000   3rd Qu.: 582.0           3rd Qu.:1.0000  \n Max.   :95.00   Max.   :1.0000   Max.   :7861.0           Max.   :1.0000  \n ejection_fraction high_blood_pressure   platelets      serum_creatinine\n Min.   :14.00     Min.   :0.0000      Min.   : 25100   Min.   :0.500   \n 1st Qu.:30.00     1st Qu.:0.0000      1st Qu.:212500   1st Qu.:0.900   \n Median :38.00     Median :0.0000      Median :262000   Median :1.100   \n Mean   :38.08     Mean   :0.3512      Mean   :263358   Mean   :1.394   \n 3rd Qu.:45.00     3rd Qu.:1.0000      3rd Qu.:303500   3rd Qu.:1.400   \n Max.   :80.00     Max.   :1.0000      Max.   :850000   Max.   :9.400   \n  serum_sodium        sex            smoking            time      \n Min.   :113.0   Min.   :0.0000   Min.   :0.0000   Min.   :  4.0  \n 1st Qu.:134.0   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.: 73.0  \n Median :137.0   Median :1.0000   Median :0.0000   Median :115.0  \n Mean   :136.6   Mean   :0.6488   Mean   :0.3211   Mean   :130.3  \n 3rd Qu.:140.0   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:203.0  \n Max.   :148.0   Max.   :1.0000   Max.   :1.0000   Max.   :285.0  \n  DEATH_EVENT    \n Min.   :0.0000  \n 1st Qu.:0.0000  \n Median :0.0000  \n Mean   :0.3211  \n 3rd Qu.:1.0000  \n Max.   :1.0000"
  },
  {
    "objectID": "200_CrashCourse.html#data-types",
    "href": "200_CrashCourse.html#data-types",
    "title": "Crash Course",
    "section": "Data types",
    "text": "Data types\nA vector in R is a collection of items of the same type (e.g. numbers or characters) of any length, including 1 (i.e. there is no distinction between a scalar and a vector).\nData types in R are essentially different types of vectors.\nR includes a number of builtin data types. Some of the most common are:\n\nnumeric (e.g. 1.2, 5.9, 11.4)\ncharacter (e.g. “SF”, “SD”)\nlogical (e.g. “TRUE”, “FALSE”)\n\nTo create a new vector you can use the assignment operator <- or =.\n\na <- 4\n\nYou can print the contents of an object just by typing its name in the console:\n\na\n\n[1] 4\n\n\nis the same as:\n\nprint(a)\n\n[1] 4\n\n\nA comment beging with #. Anything placed after this will not be executed. Use comments to document every step in your code.\nUse c() to combine multiple values:\n\nb <- c(3, 5, 7)\n\n\nb\n\n[1] 3 5 7\n\n\nTo create a character vector, use single or double quotes around each element:\n\ndept <- c(\"ED\", \"Neuro\", \"Peds\")\n\n\ndept\n\n[1] \"ED\"    \"Neuro\" \"Peds\""
  },
  {
    "objectID": "200_CrashCourse.html#data-structures",
    "href": "200_CrashCourse.html#data-structures",
    "title": "Crash Course",
    "section": "Data Structures",
    "text": "Data Structures\nR includes multiple different data structures. Think of a data structure as a container that holds one or more vectors of data.\nThe data.frame is one of the most common data structures for statistics, because it can hold vectors of different kinds, e.g. numeric, categorical, and character.\n\n\n\nRead more about data structures\n\n\n\n\nFactors\nFactors in R are used to store categorical variables and therefore have many important uses in statistics / data science / machine learning.\nLet’s convert binary categorical variables in our dataset to factors:\n\ndat$anaemia <- factor(dat$anaemia)\ndat$diabetes <- factor(dat$diabetes)\ndat$high_blood_pressure <- factor(dat$high_blood_pressure)\ndat$sex <- factor(dat$sex)\ndat$smoking <- factor(dat$smoking)\ndat$DEATH_EVENT <- factor(dat$DEATH_EVENT)\n\n\n\n\nRead more about factors"
  },
  {
    "objectID": "200_CrashCourse.html#working-with-data.frames",
    "href": "200_CrashCourse.html#working-with-data.frames",
    "title": "Crash Course",
    "section": "Working with data.frames",
    "text": "Working with data.frames\nOne way to select a column of a data.frame by name, is to use the $ notation. Note, we use head() to avoid printing the entire variable.\n\nhead(dat$age)\n\n[1] 75 55 65 50 65 90"
  },
  {
    "objectID": "200_CrashCourse.html#functions-in-r",
    "href": "200_CrashCourse.html#functions-in-r",
    "title": "Crash Course",
    "section": "Functions in R",
    "text": "Functions in R\nR includes a very large number of functions in the base language, which allow you to do a whole lot of data cleaning & manipulation, plotting, and modeling.\nA function is called by typing its name, followed by a parenthesis with or without arguments.\nFor example, to get the mean of the b vector from above:\n\nmean(b)\n\n[1] 5\n\n\n\n\n\nLearn how to write your own functions"
  },
  {
    "objectID": "200_CrashCourse.html#summarize-data",
    "href": "200_CrashCourse.html#summarize-data",
    "title": "Crash Course",
    "section": "Summarize data",
    "text": "Summarize data\nA lot of statistical functionality is built in to the language. You can easily get summary statistics of variables using functions like mean(), median(), range(), max(), min().\n\nContinuous variables\n\nmean(dat$age)\n\n[1] 60.83389\n\nmedian(dat$age)\n\n[1] 60\n\nmin(dat$age)\n\n[1] 40\n\nmax(dat$age)\n\n[1] 95\n\nsummary(dat$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  40.00   51.00   60.00   60.83   70.00   95.00 \n\n\n\n\nCategorical variables\nThe table() function gives you the counts for each level/unique value of a variable:\n\ntable(dat$sex)\n\n\n  0   1 \n105 194 \n\ntable(dat$smoking)\n\n\n  0   1 \n203  96"
  },
  {
    "objectID": "200_CrashCourse.html#plots",
    "href": "200_CrashCourse.html#plots",
    "title": "Crash Course",
    "section": "Plots",
    "text": "Plots\nR has powerful and extensive support for graphics built in to the core language.\nHere, we look at how to produce some common and important plot types:\n\nHistogram\nDraw a histogram using hist(x)\n\nhist(dat$age, col = \"lightseagreen\")\n\n\n\n\n\n\nBoxplot\nDraw a boxplot using boxplot(x)\n\nboxplot(dat$ejection_fraction, col = \"lightseagreen\")\n\n\n\n\nYou can use a simple formula notation to draw boxplots grouped by a categorical variable using ~ symbol:continuous variable ~ grouping variable\n\nboxplot(dat$serum_sodium ~ dat$smoking, col = \"lightseagreen\")\n\n\n\n\n\n\nScatter plot\nDraw a scatter plot using plot(x, y)\n\nplot(dat$age, dat$serum_sodium, col = \"lightseagreen\")\n\n\n\n\n\nplot(dat$age, dat$serum_sodium, col = \"lightseagreen\")"
  },
  {
    "objectID": "200_CrashCourse.html#hypothesis-testing",
    "href": "200_CrashCourse.html#hypothesis-testing",
    "title": "Crash Course",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nT-test\nAre the means of two groups significantly different? We use a simple formula notation as with the boxplot above to split values by group:\n\nt.test(dat$serum_sodium ~ dat$sex)\n\n\n    Welch Two Sample t-test\n\ndata:  dat$serum_sodium by dat$sex\nt = 0.45176, df = 184.61, p-value = 0.652\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.8565791  1.3653665\nsample estimates:\nmean in group 0 mean in group 1 \n       136.7905        136.5361 \n\n\n\n\nChi-squared test\nTest for association between two categorical variables:\n\nchisq.test(dat$smoking, dat$DEATH_EVENT)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  dat$smoking and dat$DEATH_EVENT\nX-squared = 0.0073315, df = 1, p-value = 0.9318\n\n\n\nsmoking_sex <- chisq.test(dat$smoking, dat$sex)\nsmoking_sex\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  dat$smoking and dat$sex\nX-squared = 57.463, df = 1, p-value = 3.444e-14\n\n\nYou can print the observed frequencies:\n\nsmoking_sex$observed\n\n           dat$sex\ndat$smoking   0   1\n          0 101 102\n          1   4  92\n\n\nand the expected frequencies:\n\nsmoking_sex$expected\n\n           dat$sex\ndat$smoking        0         1\n          0 71.28763 131.71237\n          1 33.71237  62.28763"
  },
  {
    "objectID": "200_CrashCourse.html#saving-data",
    "href": "200_CrashCourse.html#saving-data",
    "title": "Crash Course",
    "section": "Saving data",
    "text": "Saving data\n\nCSV\nYou can write R objects to CSV file using `write.csv()’. These can be read directly into any program or language that can handle data.\n\nwrite.csv(dat, \"~/Data/dat.csv\")\n\n\n\nRDS\nYou can also directly save any R object as an “RDS” file. These can be read into R. The advantage is that they are compressed and therefore may take a lot less space, and will maintain any type conversion you have performed.\n\nsaveRDS(dat, \"~/Data/dat.rds\")"
  },
  {
    "objectID": "200_CrashCourse.html#builtin-documentation",
    "href": "200_CrashCourse.html#builtin-documentation",
    "title": "Crash Course",
    "section": "Builtin Documentation",
    "text": "Builtin Documentation\nAfter you’ve successfully installed R and RStudio, one of the first things to know is how to access and search the builtin documentation.\n\nGet help on a specific item\nIf you know the name of what you’re looking for (an R function most commonly, but possibly also the name of a dataset, or a package itself), just type ? followed by the name of said function, dataset, etc. in the R prompt:\n\n?sample\n\nIn RStudio, the above example will bring up the documentation for the sample function in the dedicated “Help” window, commonly situated at the bottom right (but can be moved by the user freely). If you are running R directly at the system shell, the same information is printed directly at the console.\nTry running the above example on your system.\n\n\nSearch the docs\nIf you do not know the name of what you are looking for, you can use double question marks, ??, followed by your query (this is short for the help.search command that provides a number of arguments you can look up using ?help.search):\n\n??bootstrap"
  },
  {
    "objectID": "99_References.html",
    "href": "99_References.html",
    "title": "References",
    "section": "",
    "text": "Bengtsson, Henrik. 2019. matrixStats: Functions That Apply to Rows\nand Columns of Matrices (and to Vectors). https://CRAN.R-project.org/package=matrixStats.\n\n\nBuuren, S van, and Karin Groothuis-Oudshoorn. 2010. “Mice:\nMultivariate Imputation by Chained Equations in r.” Journal\nof Statistical Software, 1–68.\n\n\nChambers, John M. 1998. Programming with Data: A Guide to the s\nLanguage. Springer Science & Business Media.\n\n\nGennatas, Efstathios Dimitrios. 2017. “Towards Precision\nPsychiatry: Gray Matter Development and Cognition in\nAdolescence.”\n\n\nMack, Christina, Zhaohui Su, and Daniel Westreich. 2018. “Managing\nMissing Data in Patient Registries: Addendum to Registries for\nEvaluating Patient Outcomes: A User’s Guide, [Internet].”\n\n\nMurrell, Paul. 2018. R Graphics. CRC Press.\n\n\nSarkar, Deepayan. 2008. Lattice: Multivariate Data Visualization\nwith r. New York: Springer. http://lmdvr.r-forge.r-project.org.\n\n\nSievert, Carson, Chris Parmer, Toby Hocking, Scott Chamberlain, Karthik\nRam, Marianne Corvellec, and Pedro Despouy. 2017. “Plotly: Create\nInteractive Web Graphics via ‘Plotly. Js’.” R\nPackage Version 4 (1): 110.\n\n\nStekhoven, Daniel J, and Peter Bühlmann. 2012.\n“MissForest—Non-Parametric Missing Value Imputation for Mixed-Type\nData.” Bioinformatics 28 (1): 112–18.\n\n\nWickham, Hadley. 2011. “Ggplot2.” Wiley\nInterdisciplinary Reviews: Computational Statistics 3 (2): 180–85.\n\n\nWilkinson, Leland. 2012. “The Grammar of Graphics.” In\nHandbook of Computational Statistics, 375–414. Springer.\n\n\nWright, Marvin N, and Andreas Ziegler. 2015. “Ranger: A Fast\nImplementation of Random Forests for High Dimensional Data in c++ and\nr.” arXiv Preprint arXiv:1508.04409."
  },
  {
    "objectID": "04_Packages.html",
    "href": "04_Packages.html",
    "title": "4  R packages",
    "section": "",
    "text": "The Comprehensive R Archive Network (CRAN) is the official R package repository and currently hosts 16271 packages (as of 2020-09-13). To install a package from CRAN, use the builtin install.packages command:\n\ninstall.packages('glmnet')\n\n\n\n\nold.packages()\n\n\n\n\nIf you don’t set ask = FALSE, you will have to accept each package update separately.\n\nupdate.packages(ask = FALSE)"
  },
  {
    "objectID": "04_Packages.html#github",
    "href": "04_Packages.html#github",
    "title": "4  R packages",
    "section": "4.2 GitHub",
    "text": "4.2 GitHub\nGitHub contains a large number of R packages, some of which also exist in CRAN, but the GitHub version may be updated a lot more frequently. To install from GitHub, you need to have the remotes package from CRAN first:\n\ninstall.packages(\"remotes\")\n\n\nremotes::install_github(\"user/repo\")\n\nNote: Running remotes::install_github(\"user/repo\") will not reinstall a previously installed package, unless it has been updated."
  },
  {
    "objectID": "04_Packages.html#bioconductor",
    "href": "04_Packages.html#bioconductor",
    "title": "4  R packages",
    "section": "4.3 Bioconductor",
    "text": "4.3 Bioconductor\nBioconductor is a repository which includes tools for the analysis and comprehension of high-throughput genomic data, among others. To install package from Bioconductor, first install the BiocManager package from CRAN:\n\ninstall.packages(\"BiocManager\")\n\nand then use that similar to the builtin install.packages:\n\nBiocManager::install(\"packageName\")"
  },
  {
    "objectID": "04_Packages.html#installed-packages",
    "href": "04_Packages.html#installed-packages",
    "title": "4  R packages",
    "section": "4.4 Installed packages",
    "text": "4.4 Installed packages\nList all R packages installed on your system with installed.packages() (the following block has not been run to prevent a very long output)\n\ninstalled.packages()\n\nList attached packages with search():\n\nsearch()\n\n [1] \".GlobalEnv\"        \"tools:quarto\"      \"package:stats\"    \n [4] \"package:graphics\"  \"package:grDevices\" \"package:utils\"    \n [7] \"package:datasets\"  \"package:methods\"   \"Autoloads\"        \n[10] \"package:base\"     \n\n\nList attached packages with their system path:\n\nsearchpaths()\n\n [1] \".GlobalEnv\"                                                                    \n [2] \"tools:quarto\"                                                                  \n [3] \"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/stats\"    \n [4] \"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/graphics\" \n [5] \"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/grDevices\"\n [6] \"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/utils\"    \n [7] \"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/datasets\" \n [8] \"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/methods\"  \n [9] \"Autoloads\"                                                                     \n[10] \"/Library/Frameworks/R.framework/Resources/library/base\""
  },
  {
    "objectID": "04_Packages.html#dependencies",
    "href": "04_Packages.html#dependencies",
    "title": "4  R packages",
    "section": "4.5 Dependencies",
    "text": "4.5 Dependencies\nMost R packages, whether in CRAN, Bioconductor, or GitHub, themselves rely on other packages to run. These are called dependencies. Many of these dependencies get installed automatically when you call install.packages() or remotes::install_github(), etc. This depends largely on whether they are essential for the new package to work. Some packages, especially if they provide a large number of functions that may not all be used by all users, may make some dependencies optional. In that cases, if you try to execute a specific function that depends on uninstalled packages you may get a warning or error or some type of message indicating that you need to install further packages."
  },
  {
    "objectID": "03_IDEs.html",
    "href": "03_IDEs.html",
    "title": "3  IDEs",
    "section": "",
    "text": "An Integrated Development Environment (IDE) is a software application that offers extensive functionality for programmers, including ability to read, write, and execute code, develop and test software packages, etc.\nIDEs that support R usually also allow viewing plots or launching web applications within the same environment. An IDE can make working in R easier, more productive, and, importantly, more fun."
  },
  {
    "objectID": "03_IDEs.html#rstudio",
    "href": "03_IDEs.html#rstudio",
    "title": "3  IDEs",
    "section": "3.1 RStudio",
    "text": "3.1 RStudio\nRStudio is a very popular Integrated Development Environment (IDE) for R. This is the recommended environment for beginners. Make sure to keep your installation up-to-date; new features are added often.\nIt is recommended to set up a new RStudio project for each data project:\nRStudio projects allows you to organize your work. Each project keeps track of your workspace, open source files, working directory, and history.\nTo create a new RStudio Project click on File > New Project… from the main menu or the “Create a project” icon (second from top-left usually) in the RStudio toolbar."
  },
  {
    "objectID": "03_IDEs.html#vs-code",
    "href": "03_IDEs.html#vs-code",
    "title": "3  IDEs",
    "section": "3.2 VS Code",
    "text": "3.2 VS Code\nVisual Studio Code, a.k.a. VS Code is a source code editor and one of the most popular IDEs across different languages. The VS Code marketplace includes a very large number of extensions.\nThe vscode-R extension allows using VS Code as an R IDE. To use it, you need to install the languageserver and rlang packages:\ninstall.packages(c(\"languageserver\", \"rlang\"))\nThe httpgd graphics device is recommended. Install it using:\nremotes::install_github(\"nx10/httpgd\")\nand enable it in the extension settings (“Plot: Use httpgd”)\nThe Remote - SSH extension allows using a local VS Code installation (e.g. on your laptop) and executing code (R, Python, etc.) on a remote server on which you have SSH access."
  },
  {
    "objectID": "03_IDEs.html#jupyter-lab",
    "href": "03_IDEs.html#jupyter-lab",
    "title": "3  IDEs",
    "section": "3.3 Jupyter Lab",
    "text": "3.3 Jupyter Lab\nJupyter is a popular notebook interface, which supports multiple programming languages, including R.\nJupyterLab is the “next-generation web-based user interface for Project Jupyter”.\nThere are different ways to install jupyter and jupyter-lab.\nOne way is:\n\nInstall miniforge\nUse conda to install jupyterlab:\n\n\nconda install jupyterlab\n\n\nInstall the R kernel:\n\n\nconda install r-irkernel   \n\n\nInstall the IRkernel R packages:\n\n\ninstall.packages('IRkernel')\nIRkernel::installspec()\n\n\nStart jupyter-lab:\n\n\njupyter-lab"
  },
  {
    "objectID": "200_CrashCourse.html",
    "href": "200_CrashCourse.html",
    "title": "Crash Course",
    "section": "",
    "text": "This is for the UCSF DCR Intro to R"
  },
  {
    "objectID": "07_DataTypes.html#base-types",
    "href": "07_DataTypes.html#base-types",
    "title": "6  Data Types",
    "section": "6.1 Base types",
    "text": "6.1 Base types\n\n\n\nAll data in R is stored in vectors, whether stand-alone, i.e. a 1-D collection of items of the same type (e.g. numeric, character, etc.), or within another data structure (e.g. a data.frame, list).\n\n\nData types in R are essentially different types of vectors.\n\n\n\nR includes a number of builtin data types.\nThese are defined by the R core team: users cannot define their own data types.\nUsers can define their own classes - see Classes and Object-Oriented Programming.\nSome of the most popular data types in R are:\n\nLogical (i.e. TRUE or FALSE, a.k.a. Boolean)\nNumeric, integer\nNumeric, double\nCharacter\nEnvironment\nClosure (essentially, function; technically, a function and an environment)\n\n\n\n\nMany errors in R occur because a variable is, or gets coerced to, the wrong type by accident.\n\n\n\n\n\n\nCheck variable types with typeof() and/or str()."
  },
  {
    "objectID": "07_DataTypes.html#assignment",
    "href": "07_DataTypes.html#assignment",
    "title": "6  Data Types",
    "section": "6.2 Assignment",
    "text": "6.2 Assignment\nUse <- for all assignments:\n\nx <- 3\n# You can add comments within code blocks using the usual \"#\" prefix\n\n\n\n\nIn RStudio the keyboard shortcut for the assignment operator <- is Option - (MacOS) or Alt - (Windows).\n\n\n\nTyping the name of an object…\n\nx\n\n[1] 3\n\n\n…is equivalent to printing it\n\nprint(x)\n\n[1] 3\n\n\nYou can also place any assignment in parentheses and this will perform the assignment and print the object:\n\n(x <- 3)\n\n[1] 3\n\n\n\n\n\nWhile you could use the equal sign ‘=’ for assignment, you should only use it to pass arguments to functions.\n\n\n\nYou can assign the same value to multiple objects - this can be useful when initializing variables.\n\nx <- z <- init <- 0\nx\n\n[1] 0\n\nz\n\n[1] 0\n\ninit\n\n[1] 0\n\n\nExcitingly, R allows assignment in the opposite direction as well:\n\n10 -> x\nx\n\n[1] 10\n\n\nWe shall see later that the -> assignment can be convenient at the end of a pipe.\nYou can even do this, which is fun (?) but unlikely to be useful:\n\nx <- 7 -> z\nx\n\n[1] 7\n\nz\n\n[1] 7"
  },
  {
    "objectID": "07_DataTypes.html#combine-values",
    "href": "07_DataTypes.html#combine-values",
    "title": "6  Data Types",
    "section": "6.3 Combine values",
    "text": "6.3 Combine values\nUse c() to combine multiple values into a vector:\n\nx <- c(-12, 3.5, 104)\nx\n\n[1] -12.0   3.5 104.0"
  },
  {
    "objectID": "07_DataTypes.html#initialize---coerce---test-types",
    "href": "07_DataTypes.html#initialize---coerce---test-types",
    "title": "6  Data Types",
    "section": "6.4 Initialize - coerce - test (types)",
    "text": "6.4 Initialize - coerce - test (types)\nThe following summary table lists the functions to initialize, coerce (=convert), and test the core data types, which are shown in more detail in the following paragraphs:\n\n\n\nInitialize\nCoerce\nTest\n\n\n\n\nlogical(n)\nas.logical(x)\nis.logical(x)\n\n\ninteger(n)\nas.integer(x)\nis.integer(x)\n\n\ndouble(n)\nas.double(x)\nis.double(x)\n\n\nnumeric(n)\nas.numeric(x)\nis.numeric(x)\n\n\ncharacter(n)\nas.character(x)\nis.character(x)\n\n\n\nNote: numeric and double functions on lines 3 and 4 above are equivalent. (Try printing numeric and double in the console)"
  },
  {
    "objectID": "07_DataTypes.html#logical",
    "href": "07_DataTypes.html#logical",
    "title": "6  Data Types",
    "section": "6.5 Logical",
    "text": "6.5 Logical\nIf you are writing code, (it’s good practice to) use TRUE and FALSE.\nOn the console, you can abbreviate to T and F, they are the same.\n\na <- c(TRUE, FALSE)\na <- c(T, F)\nx <- 4\nb <- x > 10\nb\n\n[1] FALSE\n\nstr(b)\n\n logi FALSE\n\ntypeof(b)\n\n[1] \"logical\""
  },
  {
    "objectID": "07_DataTypes.html#integer",
    "href": "07_DataTypes.html#integer",
    "title": "6  Data Types",
    "section": "6.6 Integer",
    "text": "6.6 Integer\nCreate a range of integers using colon notation start:end:\n\n(x <- 11:15)\n\n[1] 11 12 13 14 15\n\ntypeof(x)\n\n[1] \"integer\"\n\nstr(x)\n\n int [1:5] 11 12 13 14 15\n\n\nNote that assigning an integer defaults to type double:\n\nx <- 1\ntypeof(x)\n\n[1] \"double\"\n\nstr(x)\n\n num 1\n\n\nYou can force it to be stored as integer by adding an L suffix:\n\nx <- 1L\ntypeof(x)\n\n[1] \"integer\"\n\nstr(x)\n\n int 1\n\n\n\nx <- c(1L, 3L, 5L)\nstr(x)\n\n int [1:3] 1 3 5"
  },
  {
    "objectID": "07_DataTypes.html#double",
    "href": "07_DataTypes.html#double",
    "title": "6  Data Types",
    "section": "6.7 Double",
    "text": "6.7 Double\n\nx <- c(1.2, 3.4, 10.987632419834556)\nx\n\n[1]  1.20000  3.40000 10.98763\n\ntypeof(x)\n\n[1] \"double\"\n\nstr(x)\n\n num [1:3] 1.2 3.4 11"
  },
  {
    "objectID": "07_DataTypes.html#character",
    "href": "07_DataTypes.html#character",
    "title": "6  Data Types",
    "section": "6.8 Character",
    "text": "6.8 Character\nA character vector consists of one or more elements, each of which consists of one or more actual characters, i.e. it is not a vector of single characters. (The length of a character vector is the number of individual elements, and is not related to the number of characters in each element)\n\nx <- \"word\"\ntypeof(x)\n\n[1] \"character\"\n\nlength(x)\n\n[1] 1\n\n\n\n(x <- c(\"a\", \"b\", \"gamma\", \"delta\"))\n\n[1] \"a\"     \"b\"     \"gamma\" \"delta\"\n\ntypeof(x)\n\n[1] \"character\"\n\nlength(x)\n\n[1] 4"
  },
  {
    "objectID": "07_DataTypes.html#environment",
    "href": "07_DataTypes.html#environment",
    "title": "6  Data Types",
    "section": "6.9 Environment",
    "text": "6.9 Environment\nDefining your own environments is probably for advanced use only:\n\nx <- new.env()\nx$name <- \"Guava\"\nx$founded <- 2020\nx\n\n<environment: 0x11c992668>\n\ntypeof(x)\n\n[1] \"environment\""
  },
  {
    "objectID": "07_DataTypes.html#closure-function",
    "href": "07_DataTypes.html#closure-function",
    "title": "6  Data Types",
    "section": "6.10 Closure (function)",
    "text": "6.10 Closure (function)\nClosures are functions - they contain their own variable definitions.\nRead more on functions.\n\nsquare <- function(x) x^2\nsquare(3)\n\n[1] 9\n\ntypeof(square)\n\n[1] \"closure\""
  },
  {
    "objectID": "07_DataTypes.html#initialize-vectors",
    "href": "07_DataTypes.html#initialize-vectors",
    "title": "6  Data Types",
    "section": "6.11 Initialize vectors",
    "text": "6.11 Initialize vectors\nYou can create / initialize vectors of specific type with the vector command and specifying a mode or directly by calling the relevant function:\n\n(xl <- vector(mode = \"logical\", length = 10))\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n(xd <- vector(mode = \"double\", length = 10))\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\n(xn <- vector(mode = \"numeric\", length = 10)) # same as \"double\"\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\n(xi <- vector(mode = \"integer\", length = 10))\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\n(xc <- vector(mode = \"character\", length = 10))\n\n [1] \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\n\n\nThese are aliases of the vector command above (print their source code to see for yourself)\n\nxl <- logical(10)\nxd <- double(10)\nxn <- numeric(10) # same as double\nxi <- integer(10)\nxc <- character(10)"
  },
  {
    "objectID": "07_DataTypes.html#explicit-coercion",
    "href": "07_DataTypes.html#explicit-coercion",
    "title": "6  Data Types",
    "section": "6.12 Explicit coercion",
    "text": "6.12 Explicit coercion\nWe can explicitly convert objects of one type to a different type using as.* functions:\n\n(x <- c(1.2, 2.3, 3.4))\n\n[1] 1.2 2.3 3.4\n\n(x <- as.logical(x))\n\n[1] TRUE TRUE TRUE\n\n(x <- as.double(x))\n\n[1] 1 1 1\n\n(x <- as.numeric(x))\n\n[1] 1 1 1\n\n(x <- as.integer(x))\n\n[1] 1 1 1\n\n(x <- as.character(x))\n\n[1] \"1\" \"1\" \"1\"\n\n\nLogical vectors are converted to 1s and 0s as expected:\nTRUE becomes 1 and FALSE becomes 0\n\nx <- c(TRUE, TRUE, FALSE)\nas.numeric(x)\n\n[1] 1 1 0\n\n\nNote that converting from numeric to logical anything other than zero is TRUE:\n\nx <- seq(-2, 2, .5)\nas.logical(x)\n\n[1]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n\n\nNot all conversions are possible.\nThere is no meaningful/consistent way to convert a character vector to numeric.\nThe following outputs NA values and prints a (helpful) error message.\n\nx <- c(\"mango\", \"banana\", \"tangerine\")\nas.numeric(x)\n\nWarning: NAs introduced by coercion\n\n\n[1] NA NA NA"
  },
  {
    "objectID": "07_DataTypes.html#implicit-coercion",
    "href": "07_DataTypes.html#implicit-coercion",
    "title": "6  Data Types",
    "section": "6.13 Implicit coercion",
    "text": "6.13 Implicit coercion\nRemember, the language generally tries to make life easier. Sometimes this means it will automatically coerce one class to another to allow requested operations.\nFor example, you can get the sum of a logical vector.\nIt will automatically be converted to numeric as we saw earlier.\n\nx <- c(TRUE, TRUE, FALSE)\nsum(x)\n\n[1] 2\n\n\nOn the other hand, you cannot sum a factor, for example.\nYou get an error with an explanation:\n\nx <- factor(c(\"mango\", \"banana\", \"mango\"))\nsum(x)\n\nError in Summary.factor(structure(c(2L, 1L, 2L), levels = c(\"banana\", : 'sum' not meaningful for factors\n\n\n\n\n\nNote: We had to add error = TRUE in the Rmarkdown’s code block’s options (not visible in the HTML output), because otherwise compilation of the Rmarkdown document would stop at the error.\n\n\n\nIf for some reason it made sense, you could explicitly coerce to numeric and then sum:\n\nx <- factor(c(\"mango\", \"banana\", \"mango\"))\nsum(as.numeric(x))\n\n[1] 5"
  },
  {
    "objectID": "07_DataTypes.html#na-missing-values",
    "href": "07_DataTypes.html#na-missing-values",
    "title": "6  Data Types",
    "section": "6.14 NA: Missing Values",
    "text": "6.14 NA: Missing Values\nMissing values in any data type - logical, integer, double, or character - are coded using NA.\nTo check for the presence of NA values, use is.na():\n\n(x <- c(1.2, 5.3, 4.8, NA, 9.6))\n\n[1] 1.2 5.3 4.8  NA 9.6\n\nis.na(x)\n\n[1] FALSE FALSE FALSE  TRUE FALSE\n\n\n\n(x <- c(\"mango\", \"banana\", NA, \"sugar\", \"ackee\"))\n\n[1] \"mango\"  \"banana\" NA       \"sugar\"  \"ackee\" \n\nis.na(x)\n\n[1] FALSE FALSE  TRUE FALSE FALSE\n\n\n\n(x <- c(T, T, F, T, F, F, NA))\n\n[1]  TRUE  TRUE FALSE  TRUE FALSE FALSE    NA\n\nis.na(x)\n\n[1] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n\n\nis.na() works similarly on matrices:\n\nx <- matrix(1:20, 5)\nx[4, 3] <- NA\nis.na(x)\n\n      [,1]  [,2]  [,3]  [,4]\n[1,] FALSE FALSE FALSE FALSE\n[2,] FALSE FALSE FALSE FALSE\n[3,] FALSE FALSE FALSE FALSE\n[4,] FALSE FALSE  TRUE FALSE\n[5,] FALSE FALSE FALSE FALSE\n\n\n\n\n\nNote that is.na() returns a response for each element (i.e. is vectorized) in contrast to is.numeric(), is.logical(), etc. It makes sense, since the latter are chacking the type of a whole object, while the former is checking individual elements.\n\n\n\nanyNA() is a very useful function to check if there an any NA values in an object:\n\nanyNA(x)\n\n[1] TRUE\n\n\n\n\n\nAny operations on an NA results in NA\n\n\n\n\nx <- c(1.2, 5.3, 4.8, NA, 9.6)\nx*2\n\n[1]  2.4 10.6  9.6   NA 19.2\n\n\nMultiple functions that accept as input an object with multiple values (a vector, a matrix, a data.frame, etc.) will return NA if any element is NA:\n\nmean(x)\n\n[1] NA\n\nmedian(x)\n\n[1] NA\n\nsd(x)\n\n[1] NA\n\nmin(x)\n\n[1] NA\n\nmax(x)\n\n[1] NA\n\nrange(x)\n\n[1] NA NA\n\n\nFirst, make sure NA values represent legitimate missing data and not some error.\nThen, decide how you want to handle it.\nIn all of the above commands you can pass na.rm = TRUE to ignore NA values:\n\nmean(x, na.rm = TRUE)\n\n[1] 5.225\n\nmedian(x, na.rm = TRUE)\n\n[1] 5.05\n\nsd(x, na.rm = TRUE)\n\n[1] 3.441293\n\nmin(x, na.rm = TRUE)\n\n[1] 1.2\n\nmax(x, na.rm = TRUE)\n\n[1] 9.6\n\nrange(x, na.rm = TRUE)\n\n[1] 1.2 9.6\n\n\nMore generally, you can use na.exclude() to exclude NA values from R objects. This can be very useful for function that do not include a na.rm or similar argument to handle NA values.\n\nx <- c(1, 2, NA, 4)\nna.exclude(x)\n\n[1] 1 2 4\nattr(,\"na.action\")\n[1] 3\nattr(,\"class\")\n[1] \"exclude\"\n\n\nOn a data.frame, na.exclude() excludes rows with any NAs:\n\ndf <- data.frame(a = c(1, 2, NA, 4),\n                 b = c(11, NA, 13, 14))\nna.exclude(df)\n\n  a  b\n1 1 11\n4 4 14\n\n\nThe chapter on Handling Missing Data describes some approaches to handling missing data in the context of statistics or modeling, commonly supervised learning."
  },
  {
    "objectID": "07_DataTypes.html#nan-not-a-number",
    "href": "07_DataTypes.html#nan-not-a-number",
    "title": "6  Data Types",
    "section": "6.15 NaN: Not a number",
    "text": "6.15 NaN: Not a number\nNaN is a special case of NA and can be the result of undefined mathematical operations:\n\na <- log(-4)\n\nWarning in log(-4): NaNs produced\n\n\nNote that class() returns “numeric”:\n\nclass(a)\n\n[1] \"numeric\"\n\n\nTo test for NaNs, use:\n\nis.nan(a)\n\n[1] TRUE\n\n\nNaNs are also NA:\n\nis.na(a)\n\n[1] TRUE\n\n\nBut the opposite is not true:\n\nis.nan(NA)\n\n[1] FALSE\n\n\n\n\n\nNaN can be considered a subtype of NA, as such: is.na(NaN) is TRUE, but is.nan(NA) is FALSE."
  },
  {
    "objectID": "07_DataTypes.html#null-the-empty-object",
    "href": "07_DataTypes.html#null-the-empty-object",
    "title": "6  Data Types",
    "section": "6.16 NULL: the empty object",
    "text": "6.16 NULL: the empty object\nThe NULL object represents an empty object.\n\n\n\nNULL means empty, not missing, and is therefore entirely different from NA\n\n\n\nNULL shows up for example when initializing a list:\n\na <- vector(\"list\", 4)\na\n\n[[1]]\nNULL\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\n[[4]]\nNULL\n\n\nand it can be replaced normally:\n\na[[1]] <- 3\na\n\n[[1]]\n[1] 3\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\n[[4]]\nNULL\n\n\n\n6.16.1 Replacing with NULL\nYou cannot replace one or more elements of a vector/matrix/array with NULL because NULL has length 0 and replacement requires object of equal length:\n\na <- 11:15\na\n\n[1] 11 12 13 14 15\n\na[1] <- NULL\n\nError in a[1] <- NULL: replacement has length zero\n\n\nHowever, in lists and therefore also data frames, replacing an element with NULL removes that element:\n\nal <- list(alpha = 11:15,\n           beta = rnorm(10),\n           gamma = c(\"mango\", \"banana\", \"tangerine\"))\nal\n\n$alpha\n[1] 11 12 13 14 15\n\n$beta\n [1] -0.8016487 -0.7860426 -0.5259926  0.2113334  0.5138032 -0.5592976\n [7]  0.8519664 -0.5732299  0.4055569  0.6355769\n\n$gamma\n[1] \"mango\"     \"banana\"    \"tangerine\"\n\nal[[2]] <- NULL\nal\n\n$alpha\n[1] 11 12 13 14 15\n\n$gamma\n[1] \"mango\"     \"banana\"    \"tangerine\"\n\n\nFinally, NULL is often used as the default value in a function’s argument. The function definition must then determine what the default behavior/value should be."
  },
  {
    "objectID": "Preface.html",
    "href": "Preface.html",
    "title": "1  Preface",
    "section": "",
    "text": "Throughout this book you will see boxes with R code followed by its output, if any. The code (or input) is decorated with a teal border on the left to separate it from its output, like in the following example:\n\nx <- rnorm(200)\nx[1:20]\n\n [1] -0.34000492 -0.19121165 -0.28505888 -3.37786596 -0.63246323  1.91527794\n [7]  0.35995719  1.82124951  0.62459497  0.60281351 -0.48072925 -0.70539676\n[13] -0.35515658 -2.65903095  1.08978524  0.26585378  0.14115796  0.92783336\n[19] -0.06879746  0.78760204\n\n\nNotice that R adds numbers in brackets in the beginning of each row. This happens when R prints the contents of a vector. The number is the integer index of the first element in that row. Therefore, the first one is always [1] and the number of the subsequent rows depends on how many elements fit in each line. If the output is a single element, it will still have [1] in front of it.\nAlso notice that if we enclose the assignment operation of a variable in parentheses, this prints the resulting value of the variable. Therefore, this:\n\n(y <- 4)\n\n[1] 4\n\n\nis equivalent to:\n\ny <- 4\ny\n\n[1] 4\n\n\nCurrently, this site uses Fira Code to display source code, which supports multiple character ligatures that make code prettier / easier to read. \n\n\n\n\n\nLigated versions of some common character combinations as they should appear in this site\n\n\n\n\nNote that if you mouse over the input code box, a clickable “Copy to clipboard” appears on the top right of the box allowing you to copy paste into an R session or file.  Lastly, you will see the following informational boxes at times:\n\n\n\nNote\n\n\n\n\n\n\nImportant\n\n\n\n\n\n\nCaution\n\n\n\nThis book was created using Quarto, ported from the previous version which used bookdown."
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "2  Introduction",
    "section": "",
    "text": "R is a modern implementation of the S language and part of the GNU Project.\nR is an interpreted language, allowing interactive work with data. It is written in C, Fortran, and R itself.\nSome of R’s strengths:\n\nThe base language (i.e. what is included in R when you first install it) comes loaded with functionality for\n\ndata cleaning and manipulation\nstatistical testing and modeling\npowerful graphics\n\nThe vast ecosystem of third party packages brings unparalleled functionality for statistics, epidemiology, machine learning, visualization, image processing and much more. This includes specialized packages for many biomedical applications.\n\nSee also: What is R? on the R Project website.\nThis book was compiled using R version 4.2.1 (2022-06-23).\nMake sure you have the latest version by visiting the R project website\nIt’s a good idea to keep a log of the version of R and installed packages when beginning a new project. An easy way to do this is to save the output of sessionInfo():\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] fansi_1.0.3       digest_0.6.29     jsonlite_1.8.0    magrittr_2.0.3   \n [5] evaluate_0.15     rlang_1.0.4       stringi_1.7.6     cli_3.3.0        \n [9] rmarkdown_2.14    tools_4.2.1       stringr_1.4.0     htmlwidgets_1.5.4\n[13] xfun_0.31         yaml_2.3.5        fastmap_1.1.0     compiler_4.2.1   \n[17] htmltools_0.5.2   knitr_1.39       \n\n\n\n\n\n\nChambers, John M. 1998. Programming with Data: A Guide to the s Language. Springer Science & Business Media."
  },
  {
    "objectID": "IDEs.html",
    "href": "IDEs.html",
    "title": "\n3  IDEs\n",
    "section": "",
    "text": "An Integrated Development Environment (IDE) is a software application that offers extensive functionality for programmers, including ability to read, write, and execute code, develop and test software packages, etc.\nIDEs that support R usually also allow viewing plots or launching web applications within the same environment. An IDE can make working in R easier, more productive, and, importantly, more fun."
  },
  {
    "objectID": "IDEs.html#rstudio",
    "href": "IDEs.html#rstudio",
    "title": "3  IDEs",
    "section": "3.1 RStudio",
    "text": "3.1 RStudio\nRStudio is the most popular Integrated Development Environment IDE dedicated to R. This is the recommended environment for beginners. Make sure to keep your installation up-to-date; new features are added often.\nIt is recommended to set up a new RStudio project for each data project:\nRStudio projects allows you to organize your work. Each project keeps track of your working directory, workspace, history, and source documents.\nTo create a new RStudio Project click on File > New Project… from the main menu or the “Create a project” icon (second from top-left usually) in the RStudio toolbar."
  },
  {
    "objectID": "IDEs.html#vs-code",
    "href": "IDEs.html#vs-code",
    "title": "\n3  IDEs\n",
    "section": "\n3.2 VS Code",
    "text": "3.2 VS Code\nVisual Studio Code, a.k.a. VS Code is a source code editor and one of the most popular IDEs across different languages. The VS Code marketplace includes a very large number of extensions.\nThe vscode-R extension allows using VS Code as an R IDE. To use it, you need to install the languageserver and rlang packages:\ninstall.packages(c(\"languageserver\", \"rlang\"))\nThe httpgd graphics device is recommended. Install it using:\nremotes::install_github(\"nx10/httpgd\")\nand enable it in the extension settings (“Plot: Use httpgd”)\nThe Remote - SSH extension allows using a local VS Code installation (e.g. on your laptop) and executing code (R, Python, etc.) on a remote server on which you have SSH access.  VS Code’s Jupyter extension allows you to open and run jupyter notebooks."
  },
  {
    "objectID": "IDEs.html#jupyter-lab",
    "href": "IDEs.html#jupyter-lab",
    "title": "3  IDEs",
    "section": "3.3 Jupyter Lab",
    "text": "3.3 Jupyter Lab\nJupyter is a popular notebook interface, which supports multiple programming languages, including R.\nJupyterLab is the “next-generation web-based user interface for Project Jupyter”.\nThere are different ways to install jupyter and jupyter-lab.\nOne way is:\n\nInstall miniforge\nUse conda to install jupyterlab:\n\n\nconda install jupyterlab\n\n\nInstall the R kernel:\n\n\nconda install r-irkernel   \n\n\nInstall the IRkernel R packages:\n\n\ninstall.packages('IRkernel')\nIRkernel::installspec()\n\n\nStart jupyter-lab:\n\n\njupyter-lab"
  },
  {
    "objectID": "Packages.html",
    "href": "Packages.html",
    "title": "\n4  R packages\n",
    "section": "",
    "text": "The Comprehensive R Archive Network (CRAN) is the official R package repository and currently hosts 18391 packages (as of 2022-07-19). To install a package from CRAN, use the builtin install.packages() command:\n\ninstall.packages(\"glmnet\")\n\n\n\nold.packages()\n\n\nIf you don’t set ask = FALSE, you will have to accept each package update separately.\n\nupdate.packages(ask = FALSE)"
  },
  {
    "objectID": "Packages.html#github",
    "href": "Packages.html#github",
    "title": "\n4  R packages\n",
    "section": "\n4.2 GitHub",
    "text": "4.2 GitHub\nGitHub contains a large number of R packages, some of which also exist in CRAN, but the GitHub version may be updated (a lot) more frequently. To install from GitHub, you need to have the remotes package from CRAN first:\n\ninstall.packages(\"remotes\")\n\n\nremotes::install_github(\"user/repo\")\n\nNote: Running remotes::install_github(\"user/repo\") will not reinstall a previously installed package, unless it has been updated."
  },
  {
    "objectID": "Packages.html#bioconductor",
    "href": "Packages.html#bioconductor",
    "title": "\n4  R packages\n",
    "section": "\n4.3 Bioconductor",
    "text": "4.3 Bioconductor\nBioconductor is a repository which includes tools for the analysis and comprehension of high-throughput genomic data, among others. To install packages from Bioconductor, first install the BiocManager package from CRAN. See Bioconductor’s latest installation instructions here; learn more about BiocManager here\n\ninstall.packages(\"BiocManager\")\n\nFor R versions 4.2+, proceed by installing Bioconductor:\n\nBiocManager::install(version = \"3.15\")\n\nNow, you can use BiocManager::install() to install Bioconductor packages:\n\nBiocManager::install(\"packageName\")"
  },
  {
    "objectID": "Packages.html#installed-packages",
    "href": "Packages.html#installed-packages",
    "title": "\n4  R packages\n",
    "section": "\n4.4 Installed packages",
    "text": "4.4 Installed packages\nList all R packages installed on your system with installed.packages()\n\ninstalled.packages()\n\nList attached packages with search():\n\nsearch()\n\n [1] \".GlobalEnv\"        \"tools:quarto\"      \"package:stats\"    \n [4] \"package:graphics\"  \"package:grDevices\" \"package:utils\"    \n [7] \"package:datasets\"  \"package:methods\"   \"Autoloads\"        \n[10] \"package:base\"     \n\n\nList attached packages with their system path:\n\nsearchpaths()\n\n [1] \".GlobalEnv\"                                                                    \n [2] \"tools:quarto\"                                                                  \n [3] \"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/stats\"    \n [4] \"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/graphics\" \n [5] \"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/grDevices\"\n [6] \"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/utils\"    \n [7] \"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/datasets\" \n [8] \"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/methods\"  \n [9] \"Autoloads\"                                                                     \n[10] \"/Library/Frameworks/R.framework/Resources/library/base\""
  },
  {
    "objectID": "Packages.html#dependencies",
    "href": "Packages.html#dependencies",
    "title": "\n4  R packages\n",
    "section": "\n4.5 Dependencies",
    "text": "4.5 Dependencies\nMost R packages, whether in CRAN, Bioconductor, or GitHub, themselves rely on other packages to run. These are called dependencies. Many of these dependencies get installed automatically when you call install.packages() or remotes::install_github(), etc. This depends largely on whether they are essential for the new package to work. Some packages, especially if they provide a large number of functions that may not all be used by all users, may make some dependencies optional. In that cases, if you try to execute a specific function that depends on uninstalled packages you may get a warning or error or some type of message indicating that you need to install further packages."
  },
  {
    "objectID": "Packages.html#builtin-documentation",
    "href": "Packages.html#builtin-documentation",
    "title": "\n4  R packages\n",
    "section": "\n4.6 Builtin Documentation",
    "text": "4.6 Builtin Documentation\nAfter you’ve successfully installed R and RStudio, one of the first things to know is how to access and search the builtin documentation.\n\n4.6.1 Get help on a specific item\nIf you know the name of what you’re looking for (an R function most commonly, but possibly also the name of a dataset, or a package itself), just type ? followed by the name of said function, dataset, etc. in the R prompt:\n\n?sample\n\nIn RStudio, the above example will bring up the documentation for the sample function in the dedicated “Help” window, commonly situated at the bottom right (but can be moved by the user freely). If you are running R directly at the system shell, the same information is printed directly at the console.\nTry running the above example on your system.\n\n4.6.2 Search the docs\nIf you do not know the name of what you are looking for, you can use double question marks, ??, followed by your query (this is short for the help.search command that provides a number of arguments you can look up using ?help.search):\n\n??bootstrap"
  },
  {
    "objectID": "BasicOps.html",
    "href": "BasicOps.html",
    "title": "5  Basic operations",
    "section": "",
    "text": "First, before even learning about data types and structures, it may be worth looking at some of the basic mathematical and statistical operations in R."
  },
  {
    "objectID": "BasicOps.html#arithmetic",
    "href": "BasicOps.html#arithmetic",
    "title": "5  Basic operations",
    "section": "\n5.1 Arithmetic",
    "text": "5.1 Arithmetic\n\nx <- 10\ny <- 3\n\nStandard arithmetic operations are as expected:\n\nx + y\n\n[1] 13\n\nx - y\n\n[1] 7\n\nx * y\n\n[1] 30\n\nx / y\n\n[1] 3.333333\n\n\nExponentiation uses ^: (The caret (^) is likely the most common but not the only symbol used for exponentiation across programming languages)\n\nx^3\n\n[1] 1000\n\n\nSquare root is sqrt():\n\nsqrt(81)\n\n[1] 9\n\n\nNatural logs with log():\n\nlog(12)\n\n[1] 2.484907\n\n\nBase 10 log with log10():\n\nlog10(1000)\n\n[1] 3\n\n\nInteger division i.e. Divide and forget the remainder\n\nx %/% y\n\n[1] 3\n\n\ni.e. how many times the denominator fits in the numerator, without taking fractions of the denominator. It can be applied on decimals the same way:\n\n9.5 %/% 3.1\n\n[1] 3\n\n\nModulo operation i.e. Divide and return just the remainder\n\nx %% y\n\n[1] 1\n\n\n\nx <- (-10:10)[-11]\ny <- sample((-10:10)[-11], 20)\nx - (x %/% y) * y == x %% y\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[16] TRUE TRUE TRUE TRUE TRUE"
  },
  {
    "objectID": "BasicOps.html#logical-operations",
    "href": "BasicOps.html#logical-operations",
    "title": "5  Basic operations",
    "section": "\n5.2 Logical operations",
    "text": "5.2 Logical operations\nLogical AND: &\n\nT & T\n\n[1] TRUE\n\n\n\nT & F\n\n[1] FALSE\n\n\nLogical OR: |\n\nT | F\n\n[1] TRUE\n\n\nLogical negation: !\n\nx <- TRUE\n!x\n\n[1] FALSE\n\n\nExclusive OR: xor()\nXOR evaluates to TRUE when two logicals are different,\ni.e. one or the other is TRUE but not both.\n\na <- c(T, T, T, F, F, F)\nb <- c(F, F, T, F, T, T)\na & b\n\n[1] FALSE FALSE  TRUE FALSE FALSE FALSE\n\na | b\n\n[1]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE\n\nxor(a, b)\n\n[1]  TRUE  TRUE FALSE FALSE  TRUE  TRUE\n\n\nTest all elements of an object are TRUE with all():\n\nall(a)\n\n[1] FALSE\n\n\nTest if any element is TRUE with any():\n\nany(a)\n\n[1] TRUE"
  },
  {
    "objectID": "BasicOps.html#common-descriptive-stats",
    "href": "BasicOps.html#common-descriptive-stats",
    "title": "5  Basic operations",
    "section": "\n5.3 Common descriptive stats",
    "text": "5.3 Common descriptive stats\nFirst, let’s use the rnorm() function to draw 200 numbers at random from a normal distribution:\n\nx <- rnorm(200)\n\nBasic descriptive stat operations -\nmean, median, standard deviation, minimum, maximum, and range:\n\nmean(x)\n\n[1] 0.02334843\n\nmedian(x)\n\n[1] -0.008372719\n\nsd(x)\n\n[1] 1.030247\n\nmin(x)\n\n[1] -2.558759\n\nmax(x)\n\n[1] 2.590201\n\nrange(x)\n\n[1] -2.558759  2.590201\n\n\n\nsummary(x)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-2.558759 -0.769125 -0.008373  0.023348  0.748230  2.590201"
  },
  {
    "objectID": "DataTypes.html#base-types",
    "href": "DataTypes.html#base-types",
    "title": "6  Data Types & Vectors",
    "section": "\n6.1 Base types",
    "text": "6.1 Base types\nThe simplest and most fundamental object in R is the vector: a one-dimensional collection of elements of the same data type, e.g. numbers, characters, etc. (known as an “atomic” vector).  For example, a numeric vector may consist of elements 12, 14, 20, and a character vector may consist of elements \"x\", \"y\", \"z\".  Vectors can exist as stand-alone objects, or they can exist within other data structures, e.g. data.frames, lists, etc.  This chapter covers different atomic vectors, and the next covers data structures (Chapter 7).  R includes a number of builtin data types. These are defined by the R core team - users cannot define their own data types.  Users can, however, define their own classes (Chapter 24).  The main/most common data types in R are:\n\n\nNumeric, including integer and double\n\nCharacter\n\nLogical (i.e. TRUE or FALSE, a.k.a. Boolean)\n\nOther data types include environments and closures i.e. functions (Chapter 15)."
  },
  {
    "objectID": "DataTypes.html#assignment",
    "href": "DataTypes.html#assignment",
    "title": "6  Data Types & Vectors",
    "section": "\n6.2 Assignment",
    "text": "6.2 Assignment\nUse <- for all assignments:\n\nx <- 3\n# You can add comments within code blocks using the usual \"#\" prefix\n\n\n\n\nIn RStudio the keyboard shortcut for the assignment operator <- is Option - (MacOS) or Alt - (Windows).\n\n\n\nTyping the name of an object…\n\nx\n\n[1] 3\n\n\n…is equivalent to printing it\n\nprint(x)\n\n[1] 3\n\n\nYou can also place any assignment in parentheses and this will perform the assignment and print the object:\n\n(x <- 3)\n\n[1] 3\n\n\n\n\n\nYou can use either <- or = for assignment. Many R syntax guides you should only use it to pass arguments to functions.\n\n\n\nYou can assign the same value to multiple objects - this can be useful when initializing variables.\n\nx <- z <- init <- 0\nx\n\n[1] 0\n\nz\n\n[1] 0\n\ninit\n\n[1] 0\n\n\nExcitingly, R allows assignment in the opposite direction as well:\n\n10 -> x\nx\n\n[1] 10\n\n\nWe shall see later that the -> assignment can be convenient at the end of a pipe.\nYou can even do the following - which is fun, if not particularly useful:\n\nx <- 7 -> z\nx\n\n[1] 7\n\nz\n\n[1] 7\n\n\n\n\n\nIt’s good practice to use clear and descriptive names for all objects you create.\n\n\nFor multi-word names, snake case is a good option:\n\n\nadmission_date, age_at_onset, etc."
  },
  {
    "objectID": "DataTypes.html#combine-values",
    "href": "DataTypes.html#combine-values",
    "title": "6  Data Types",
    "section": "6.3 Combine values",
    "text": "6.3 Combine values\nUse c() to combine multiple values into a vector:\n\nx <- c(-12, 3.5, 104)\nx\n\n[1] -12.0   3.5 104.0"
  },
  {
    "objectID": "DataTypes.html#initialize---coerce---test-types",
    "href": "DataTypes.html#initialize---coerce---test-types",
    "title": "6  Data Types & Vectors",
    "section": "6.7 Initialize - coerce - test (types)",
    "text": "6.7 Initialize - coerce - test (types)\nThe following summary table lists the functions to initialize, coerce (=convert), and test the core data types, which are shown in more detail in the following paragraphs:\n\n\n\nInitialize\nCoerce\nTest\n\n\n\n\nlogical(n)\nas.logical(x)\nis.logical(x)\n\n\ninteger(n)\nas.integer(x)\nis.integer(x)\n\n\ndouble(n)\nas.double(x)\nis.double(x)\n\n\nnumeric(n)\nas.numeric(x)\nis.numeric(x)\n\n\ncharacter(n)\nas.character(x)\nis.character(x)\n\n\n\nNote: numeric and double functions on lines 3 and 4 above are equivalent. (Try printing numeric and double in the console)"
  },
  {
    "objectID": "DataTypes.html#logical",
    "href": "DataTypes.html#logical",
    "title": "6  Data Types",
    "section": "6.7 Logical",
    "text": "6.7 Logical\nIf you are writing code, (it’s good practice to) use TRUE and FALSE.\nOn the console, you can abbreviate to T and F, they are the same.\n\na <- c(TRUE, FALSE)\na <- c(T, F)\nx <- 4\nb <- x > 10\nb\n\n[1] FALSE\n\nstr(b)\n\n logi FALSE\n\ntypeof(b)\n\n[1] \"logical\""
  },
  {
    "objectID": "DataTypes.html#integer",
    "href": "DataTypes.html#integer",
    "title": "6  Data Types",
    "section": "6.8 Integer",
    "text": "6.8 Integer\nCreate a range of integers using colon notation start:end:\n\n(x <- 11:15)\n\n[1] 11 12 13 14 15\n\ntypeof(x)\n\n[1] \"integer\"\n\nstr(x)\n\n int [1:5] 11 12 13 14 15\n\n\nNote that assigning an integer defaults to type double:\n\nx <- 1\ntypeof(x)\n\n[1] \"double\"\n\nstr(x)\n\n num 1\n\n\nYou can force it to be stored as integer by adding an L suffix:\n\nx <- 1L\ntypeof(x)\n\n[1] \"integer\"\n\nstr(x)\n\n int 1\n\n\n\nx <- c(1L, 3L, 5L)\nstr(x)\n\n int [1:3] 1 3 5"
  },
  {
    "objectID": "DataTypes.html#double",
    "href": "DataTypes.html#double",
    "title": "6  Data Types & Vectors",
    "section": "6.10 Double",
    "text": "6.10 Double\n\nx <- c(1.2, 3.4, 10.987632419834556)\nx\n\n[1]  1.20000  3.40000 10.98763\n\ntypeof(x)\n\n[1] \"double\"\n\nstr(x)\n\n num [1:3] 1.2 3.4 11"
  },
  {
    "objectID": "DataTypes.html#character",
    "href": "DataTypes.html#character",
    "title": "6  Data Types",
    "section": "6.10 Character",
    "text": "6.10 Character\nA character vector consists of one or more elements, each of which consists of one or more actual characters, i.e. it is not a vector of single characters. (The length of a character vector is the number of individual elements, and is not related to the number of characters in each element)\n\nx <- \"word\"\ntypeof(x)\n\n[1] \"character\"\n\nlength(x)\n\n[1] 1\n\n\n\n(x <- c(\"a\", \"b\", \"gamma\", \"delta\"))\n\n[1] \"a\"     \"b\"     \"gamma\" \"delta\"\n\ntypeof(x)\n\n[1] \"character\"\n\nlength(x)\n\n[1] 4"
  },
  {
    "objectID": "DataTypes.html#environment",
    "href": "DataTypes.html#environment",
    "title": "6  Data Types & Vectors",
    "section": "6.12 Environment",
    "text": "6.12 Environment\nDefining your own environments is probably for advanced use only:\n\nx <- new.env()\nx$name <- \"Guava\"\nx$founded <- 2020\nx\n\n<environment: 0x120aa30b0>\n\ntypeof(x)\n\n[1] \"environment\""
  },
  {
    "objectID": "DataTypes.html#closure-function",
    "href": "DataTypes.html#closure-function",
    "title": "6  Data Types & Vectors",
    "section": "6.13 Closure (function)",
    "text": "6.13 Closure (function)\nClosures are functions - they contain their own variable definitions.\nRead more on functions.\n\nsquare <- function(x) x^2\nsquare(3)\n\n[1] 9\n\ntypeof(square)\n\n[1] \"closure\""
  },
  {
    "objectID": "DataTypes.html#initialize-vectors",
    "href": "DataTypes.html#initialize-vectors",
    "title": "6  Data Types & Vectors",
    "section": "6.6 Initialize vectors",
    "text": "6.6 Initialize vectors\nInitializing a vector or other data structure is the process by which you create an object of a certain size with some initial values, e.g. all zeros or all NAs, in order to replace with other values later.\nThis is usually computationaly more efficient than starting with a small object and appending to it multiple times.  You can create / initialize vectors of specific type with the vector command and specifying a mode or directly by calling the relevant function:\n\n(xl <- vector(mode = \"logical\", length = 10))\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n(xd <- vector(mode = \"double\", length = 10))\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\n(xn <- vector(mode = \"numeric\", length = 10)) # same as \"double\"\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\n(xi <- vector(mode = \"integer\", length = 10))\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\n(xc <- vector(mode = \"character\", length = 10))\n\n [1] \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\n\n\nThese are aliases of the vector command above (print their source code to see for yourself)\n\n(xl <- logical(10))\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n(xd <- double(10))\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\n(xn <- numeric(10)) # same as double\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\n(xi <- integer(10))\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\n(xc <- character(10))\n\n [1] \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\""
  },
  {
    "objectID": "DataTypes.html#explicit-coercion",
    "href": "DataTypes.html#explicit-coercion",
    "title": "6  Data Types & Vectors",
    "section": "\n6.7 Explicit coercion",
    "text": "6.7 Explicit coercion\nWe can explicitly convert vector of one type to a different type using as.* functions:\n\nx <- c(1.2, 2.3, 3.4)\nas.logical(x)\n\n[1] TRUE TRUE TRUE\n\nas.double(x)\n\n[1] 1.2 2.3 3.4\n\nas.numeric(x)\n\n[1] 1.2 2.3 3.4\n\nas.integer(x)\n\n[1] 1 2 3\n\nas.character(x)\n\n[1] \"1.2\" \"2.3\" \"3.4\"\n\n\nLogical vectors are converted to 1s and 0s as expected:\nTRUE becomes 1 and FALSE becomes 0\n\nx <- c(TRUE, TRUE, FALSE)\nas.numeric(x)\n\n[1] 1 1 0\n\n\nNote that when converting from numeric to logical, anything other than zero is TRUE:\n\nx <- seq(-2, 2, .5)\nx\n\n[1] -2.0 -1.5 -1.0 -0.5  0.0  0.5  1.0  1.5  2.0\n\nas.logical(x)\n\n[1]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n\n\nNot all conversions are possible.\nThere is no meaningful/consistent way to convert a character vector to numeric.\nThe following outputs NA values and prints a helpful error message.\n\nx <- c(\"mango\", \"banana\", \"tangerine\")\nas.numeric(x)\n\nWarning: NAs introduced by coercion\n\n\n[1] NA NA NA"
  },
  {
    "objectID": "DataTypes.html#implicit-coercion",
    "href": "DataTypes.html#implicit-coercion",
    "title": "6  Data Types & Vectors",
    "section": "\n6.8 Implicit coercion",
    "text": "6.8 Implicit coercion\nRemember, the language generally tries to make life easier. Sometimes this means it will automatically coerce one class to another to allow requested operations.  For example, you can get the sum of a logical vector.\nIt will automatically be converted to numeric as we saw earlier.\n\nx <- c(TRUE, TRUE, FALSE)\nsum(x)\n\n[1] 2\n\n\nOn the other hand, you cannot sum a factor, for example.\nYou get an error with an explanation:\n\nx <- factor(c(\"mango\", \"banana\", \"mango\"))\nsum(x)\n\nError in Summary.factor(structure(c(2L, 1L, 2L), levels = c(\"banana\", : 'sum' not meaningful for factors\n\n\n\n\n\nNote: We had to add error = TRUE in the Rmarkdown’s code block’s options (not visible in the HTML output), because otherwise compilation of the Rmarkdown document would stop at the error.\n\n\n\nIf for some reason it made sense, you could explicitly coerce to numeric and then sum:\n\nx <- factor(c(\"mango\", \"banana\", \"mango\"))\nsum(as.numeric(x))\n\n[1] 5\n\n\n\n\n\nMany errors in R occur because a variable is, or gets coerced to, the wrong type or class by accident."
  },
  {
    "objectID": "DataTypes.html#na-missing-values",
    "href": "DataTypes.html#na-missing-values",
    "title": "6  Data Types & Vectors",
    "section": "6.11 NA: Missing Values",
    "text": "6.11 NA: Missing Values\nMissing values in any data type - logical, integer, double, or character - are coded using NA.\nTo check for the presence of NA values, use is.na():\n\n(x <- c(1.2, 5.3, 4.8, NA, 9.6))\n\n[1] 1.2 5.3 4.8  NA 9.6\n\nis.na(x)\n\n[1] FALSE FALSE FALSE  TRUE FALSE\n\n\n\n(x <- c(\"mango\", \"banana\", NA, \"sugar\", \"ackee\"))\n\n[1] \"mango\"  \"banana\" NA       \"sugar\"  \"ackee\" \n\nis.na(x)\n\n[1] FALSE FALSE  TRUE FALSE FALSE\n\n\n\n(x <- c(T, T, F, T, F, F, NA))\n\n[1]  TRUE  TRUE FALSE  TRUE FALSE FALSE    NA\n\nis.na(x)\n\n[1] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n\n\nis.na() works similarly on matrices:\n\nx <- matrix(1:20, 5)\nx[4, 3] <- NA\nis.na(x)\n\n      [,1]  [,2]  [,3]  [,4]\n[1,] FALSE FALSE FALSE FALSE\n[2,] FALSE FALSE FALSE FALSE\n[3,] FALSE FALSE FALSE FALSE\n[4,] FALSE FALSE  TRUE FALSE\n[5,] FALSE FALSE FALSE FALSE\n\n\n\n\n\nNote that is.na() returns a response for each element (i.e. is vectorized) in contrast to is.numeric(), is.logical(), etc. It makes sense, since the latter are chacking the type of a whole object, while the former is checking individual elements.\n\n\n\nanyNA() is a very useful function to check if there an any NA values in an object:\n\nanyNA(x)\n\n[1] TRUE\n\n\n\n\n\nAny operations on an NA results in NA\n\n\n\n\nx <- c(1.2, 5.3, 4.8, NA, 9.6)\nx*2\n\n[1]  2.4 10.6  9.6   NA 19.2\n\n\nMultiple functions that accept as input an object with multiple values (a vector, a matrix, a data.frame, etc.) will return NA if any element is NA:\n\nmean(x)\n\n[1] NA\n\nmedian(x)\n\n[1] NA\n\nsd(x)\n\n[1] NA\n\nmin(x)\n\n[1] NA\n\nmax(x)\n\n[1] NA\n\nrange(x)\n\n[1] NA NA\n\n\nFirst, make sure NA values represent legitimate missing data and not some error.\nThen, decide how you want to handle it.\nIn all of the above commands you can pass na.rm = TRUE to ignore NA values:\n\nmean(x, na.rm = TRUE)\n\n[1] 5.225\n\nmedian(x, na.rm = TRUE)\n\n[1] 5.05\n\nsd(x, na.rm = TRUE)\n\n[1] 3.441293\n\nmin(x, na.rm = TRUE)\n\n[1] 1.2\n\nmax(x, na.rm = TRUE)\n\n[1] 9.6\n\nrange(x, na.rm = TRUE)\n\n[1] 1.2 9.6\n\n\nMore generally, you can use na.exclude() to exclude NA values from R objects. This can be very useful for function that do not include a na.rm or similar argument to handle NA values.\n\nx <- c(1, 2, NA, 4)\nna.exclude(x)\n\n[1] 1 2 4\nattr(,\"na.action\")\n[1] 3\nattr(,\"class\")\n[1] \"exclude\"\n\n\nOn a data.frame, na.exclude() excludes rows with any NAs:\n\ndf <- data.frame(a = c(1, 2, NA, 4),\n                 b = c(11, NA, 13, 14))\nna.exclude(df)\n\n  a  b\n1 1 11\n4 4 14\n\n\nThe chapter on Handling Missing Data describes some approaches to handling missing data in the context of statistics or modeling, commonly supervised learning."
  },
  {
    "objectID": "DataTypes.html#nan-not-a-number",
    "href": "DataTypes.html#nan-not-a-number",
    "title": "6  Data Types & Vectors",
    "section": "\n6.10 NaN: Not a number",
    "text": "6.10 NaN: Not a number\nNaN is a special case of NA and can be the result of undefined mathematical operations:\n\na <- log(-4)\n\nWarning in log(-4): NaNs produced\n\n\nNote that class() returns “numeric”:\n\nclass(a)\n\n[1] \"numeric\"\n\n\nTo test for NaNs, use:\n\nis.nan(a)\n\n[1] TRUE\n\n\nNaNs are also NA:\n\nis.na(a)\n\n[1] TRUE\n\n\nBut the opposite is not true:\n\nis.nan(NA)\n\n[1] FALSE\n\n\n\n\n\nNaN can be considered a subtype of NA, as such: is.na(NaN) is TRUE, but is.nan(NA) is FALSE."
  },
  {
    "objectID": "DataTypes.html#null-the-empty-object",
    "href": "DataTypes.html#null-the-empty-object",
    "title": "6  Data Types & Vectors",
    "section": "\n6.11 NULL: The empty object",
    "text": "6.11 NULL: The empty object\nThe NULL object represents an empty object.\n\n\n\nNULL means empty, not missing, and is therefore entirely different from NA\n\n\n\nNULL shows up for example when initializing a list:\n\na <- vector(\"list\", 4)\na\n\n[[1]]\nNULL\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\n[[4]]\nNULL\n\n\nand it can be replaced normally:\n\na[[1]] <- 3\na\n\n[[1]]\n[1] 3\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\n[[4]]\nNULL\n\n\n\n6.11.1 Replacing with NULL\nYou cannot replace one or more elements of a vector/matrix/array with NULL because NULL has length 0 and replacement requires object of equal length:\n\na <- 11:15\na\n\n[1] 11 12 13 14 15\n\na[1] <- NULL\n\nError in a[1] <- NULL: replacement has length zero\n\n\nHowever, in lists and therefore also data frames, replacing an element with NULL removes that element:\n\nal <- list(alpha = 11:15,\n           beta = rnorm(10),\n           gamma = c(\"mango\", \"banana\", \"tangerine\"))\nal\n\n$alpha\n[1] 11 12 13 14 15\n\n$beta\n [1] -0.92178769  1.05197460  0.08311876 -0.70358632  0.75480304 -1.04660027\n [7] -0.67645019  0.06251899 -0.40265422 -0.81982079\n\n$gamma\n[1] \"mango\"     \"banana\"    \"tangerine\"\n\nal[[2]] <- NULL\nal\n\n$alpha\n[1] 11 12 13 14 15\n\n$gamma\n[1] \"mango\"     \"banana\"    \"tangerine\"\n\n\nFinally, NULL is often used as the default value in a function’s argument. The function definition must then determine what the default behavior/value should be."
  },
  {
    "objectID": "DataStructures.html",
    "href": "DataStructures.html",
    "title": "7  Data Structures",
    "section": "",
    "text": "There are 5 main data structures in R: \n\n\n\n\n\n\n\n\nData Structure\nDimensionality\nContents\nNotes\n\n\n\nVector\n1-D\nhomogeneous\nthe “base” object\n\n\nMatrix\n2-D\nhomogeneous\na vector with 2 dimensions\n\n\nArray\nN-D\nhomogeneous\na vector with N dimensions\n\n\nList\n1-D; can be nested\nheterogeneous\na collection of any R objects, each of any length\n\n\nData frame\n2-D\nheterogeneous\na special kind of list: a collection of (column) vectors of any type, all of the same length\n\n\n\nVectors are homogeneous data structures which means all of their elements have to be of the same type (see Chapter 6), e.g. integer, double, character, logical.\nMatrices and arrays are vectors with more dimensions, and as such, are also homogeneous.\nLists are the most flexible. Their elements can be any R objects, including lists, and therefore can be nested.\nData frames are a special kind of list. Their elements are one or more vectors, which can be of any type, and form columns. Therefore a data.frame is a two-dimensional data structure where rows typically correspond to cases (e.g. individuals) and columns represent variables. As such, data.frames are the most common data structure for statistical analysis. \n\n\n\n\nR Data Structure summary - Best to read through this chapter first and then refer back to this figure\n\n\n\n\n\n\n\nCheck object class with class() Check object class and contents’ types with str()"
  },
  {
    "objectID": "DataStructures.html#initialize---coerce---test-structures",
    "href": "DataStructures.html#initialize---coerce---test-structures",
    "title": "7  Data Structures",
    "section": "7.1 Initialize - coerce - test (structures)",
    "text": "7.1 Initialize - coerce - test (structures)\nThe following summary table lists the functions to initialize, coerce (=convert), and test the core data structures, which are shown in more detail in the following paragraphs:\n\n\n\nInitialize\nCoerce\nTest\n\n\n\n\nvector(n)\nas.vector(x)\nis.vector(x)\n\n\nmatrix(n)\nas.matrix(x)\nis.matrix(x)\n\n\narray(n)\nas.array(x)\nis.array(x)\n\n\nlist(n)\nas.list(x)\nis.list(x)\n\n\ndata.frame(n)\nas.data.frame(x)\nis.data.frame(x)"
  },
  {
    "objectID": "DataStructures.html#vectors",
    "href": "DataStructures.html#vectors",
    "title": "7  Data Structures",
    "section": "\n7.2 Vectors",
    "text": "7.2 Vectors\nA vector is the most basic & fundamental data structure in R. Other data structures are made up of one or more vectors.\n\nx <- c(1, 3, 5, 7)\nx\n\n[1] 1 3 5 7\n\nclass(x)\n\n[1] \"numeric\"\n\ntypeof(x)\n\n[1] \"double\"\n\n\nA vector has length() but no dim():\n\nlength(x)\n\n[1] 4\n\ndim(x)\n\nNULL\n\n\n\n7.2.1 Initializing a vector\nSee Initializing vectors"
  },
  {
    "objectID": "DataStructures.html#matrices",
    "href": "DataStructures.html#matrices",
    "title": "7  Data Structures",
    "section": "\n7.3 Matrices",
    "text": "7.3 Matrices\nA matrix is a vector with 2 dimensions.\nTo create a matrix, you pass a vector to the matrix() function and specify number of rows using nrow and/or number of columns using ncol:\n\nx <- matrix(21:50,\n            nrow = 10, ncol = 3)\nx\n\n      [,1] [,2] [,3]\n [1,]   21   31   41\n [2,]   22   32   42\n [3,]   23   33   43\n [4,]   24   34   44\n [5,]   25   35   45\n [6,]   26   36   46\n [7,]   27   37   47\n [8,]   28   38   48\n [9,]   29   39   49\n[10,]   30   40   50\n\nclass(x)\n\n[1] \"matrix\" \"array\" \n\n\nA matrix has length (length(x)) equal to the number of all (i, j) elements or nrow * ncol (if i is the row index and j is the column index) and dimensions (dim(x)) as expected:\n\nlength(x)\n\n[1] 30\n\ndim(x)\n\n[1] 10  3\n\nnrow(x)\n\n[1] 10\n\nncol(x)\n\n[1] 3\n\n\n\n7.3.1 Construct by row vs. by column\nBy default, vectors are constructed by column (byrow = FALSE)\n\nx <- matrix(1:20, nrow = 10, ncol = 2, byrow = FALSE)\nx\n\n      [,1] [,2]\n [1,]    1   11\n [2,]    2   12\n [3,]    3   13\n [4,]    4   14\n [5,]    5   15\n [6,]    6   16\n [7,]    7   17\n [8,]    8   18\n [9,]    9   19\n[10,]   10   20\n\n\nYou can set the byrow argument to TRUE to fill the matrix by row instead:\n\nx <- matrix(1:20, nrow = 10, ncol = 2, byrow = TRUE)\nx\n\n      [,1] [,2]\n [1,]    1    2\n [2,]    3    4\n [3,]    5    6\n [4,]    7    8\n [5,]    9   10\n [6,]   11   12\n [7,]   13   14\n [8,]   15   16\n [9,]   17   18\n[10,]   19   20\n\n\n\n7.3.2 Initialize a matrix\n\nx <- matrix(NA, nrow = 6, ncol = 4)\nx\n\n     [,1] [,2] [,3] [,4]\n[1,]   NA   NA   NA   NA\n[2,]   NA   NA   NA   NA\n[3,]   NA   NA   NA   NA\n[4,]   NA   NA   NA   NA\n[5,]   NA   NA   NA   NA\n[6,]   NA   NA   NA   NA\n\nx <- matrix(0, nrow = 6, ncol = 4)\nx\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    0    0    0\n[2,]    0    0    0    0\n[3,]    0    0    0    0\n[4,]    0    0    0    0\n[5,]    0    0    0    0\n[6,]    0    0    0    0\n\n\n\n7.3.3 Bind vectors by column or by row\nUse cbind (“column-bind”) to convert a set of input vectors to columns of a matrix. The vectors must be of the same length:\n\nx <- cbind(1:10, 11:20, 41:50)\nx\n\n      [,1] [,2] [,3]\n [1,]    1   11   41\n [2,]    2   12   42\n [3,]    3   13   43\n [4,]    4   14   44\n [5,]    5   15   45\n [6,]    6   16   46\n [7,]    7   17   47\n [8,]    8   18   48\n [9,]    9   19   49\n[10,]   10   20   50\n\nclass(x)\n\n[1] \"matrix\" \"array\" \n\n\nSimilarly, you can use rbind (“row-bind”) to convert a set of input vectors to rows of a matrix. The vectors again must be of the same length:\n\nx <- rbind(1:10, 11:20, 41:50)\nx\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    1    2    3    4    5    6    7    8    9    10\n[2,]   11   12   13   14   15   16   17   18   19    20\n[3,]   41   42   43   44   45   46   47   48   49    50\n\nclass(x)\n\n[1] \"matrix\" \"array\" \n\n\n\n7.3.4 Combine matrices\ncbind() and rbind() can be used to combine two or more matrices together - or vector and matrices:\n\ncbind(matrix(1, 5, 2), matrix(2, 5, 4))\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    1    1    2    2    2    2\n[2,]    1    1    2    2    2    2\n[3,]    1    1    2    2    2    2\n[4,]    1    1    2    2    2    2\n[5,]    1    1    2    2    2    2"
  },
  {
    "objectID": "DataStructures.html#arrays",
    "href": "DataStructures.html#arrays",
    "title": "7  Data Structures",
    "section": "\n7.4 Arrays",
    "text": "7.4 Arrays\nArrays are vectors with dimensions.\nYou can have 1D, 2D or any-D, i.e. ND arrays.\n\n7.4.1 1D array\nA 1D array is just like a vector but of class array and with dim(x) equal to length(x) (remember, vectors have only length(x) and undefined dim(x)):\n\nx <- 1:10\nxa <- array(1:10, dim = 10)\nclass(x)\n\n[1] \"integer\"\n\nis.vector(x)\n\n[1] TRUE\n\nlength(x)\n\n[1] 10\n\ndim(x)\n\nNULL\n\nclass(xa)\n\n[1] \"array\"\n\nis.vector(xa)\n\n[1] FALSE\n\nlength(xa)\n\n[1] 10\n\ndim(xa)\n\n[1] 10\n\n\nIt is rather unlikely you will need to use a 1D array instead of a vector.\n\n7.4.2 2D array\nA 2D array is a matrix:\n\nx <- array(1:40, dim = c(10, 4))\nclass(x)\n\n[1] \"matrix\" \"array\" \n\ndim(x)\n\n[1] 10  4\n\n\n\n7.4.3 ND array\nYou can build an N-dimensional array:\n\nx <- array(1:60, dim = c(5, 4, 3))\nx\n\n, , 1\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    6   11   16\n[2,]    2    7   12   17\n[3,]    3    8   13   18\n[4,]    4    9   14   19\n[5,]    5   10   15   20\n\n, , 2\n\n     [,1] [,2] [,3] [,4]\n[1,]   21   26   31   36\n[2,]   22   27   32   37\n[3,]   23   28   33   38\n[4,]   24   29   34   39\n[5,]   25   30   35   40\n\n, , 3\n\n     [,1] [,2] [,3] [,4]\n[1,]   41   46   51   56\n[2,]   42   47   52   57\n[3,]   43   48   53   58\n[4,]   44   49   54   59\n[5,]   45   50   55   60\n\nclass(x)\n\n[1] \"array\"\n\n\nYou can provide names for each dimensions using the dimnames argument. It accepts a list where each elements is a character vector of legth equal to the dimension length. Using the same example as above, we pass three character vector of length 5, 4, and 3 to match the length of the dimensions:\n\nx <- array(1:60,\n            dim = c(5, 4, 3),\n            dimnames = list(letters[1:5],\n                            c(\"alpha\", \"beta\", \"gamma\", \"delta\"),\n                            c(\"x\", \"y\", \"z\")))\n\n3D arrays can be used to represent color images. Here, just for fun, we use rasterImage() to show how you would visualize such an image:\n\nx <- array(sample(1:255, 432, TRUE), dim = c(12, 12, 3))\npar(\"pty\")\n\n[1] \"m\"\n\npar(pty = \"s\")\nplot(NULL, NULL,\n     xlim = c(0, 100), ylim = c(0, 100),\n     axes = F, ann = F, pty = \"s\")\nrasterImage(x/255, 0, 0, 100, 100)"
  },
  {
    "objectID": "DataStructures.html#lists",
    "href": "DataStructures.html#lists",
    "title": "7  Data Structures",
    "section": "\n7.5 Lists",
    "text": "7.5 Lists\nTo define a list, we use list() to pass any number of objects.\nIf these objects are passed as named arguments, the names will be used as element names:\n\nx <- list(one = 1:4,\n          two = sample(seq(0, 100, .1), 10),\n          three = c(\"mango\", \"banana\", \"tangerine\"),\n          four = median)\nclass(x)\n\n[1] \"list\"\n\nstr(x)\n\nList of 4\n $ one  : int [1:4] 1 2 3 4\n $ two  : num [1:10] 17.4 48.2 58.8 39.7 97.3 18.1 74.6 36.1 22 30.9\n $ three: chr [1:3] \"mango\" \"banana\" \"tangerine\"\n $ four :function (x, na.rm = FALSE, ...)  \n\nlength(x)\n\n[1] 4\n\n\n\n7.5.1 Nested lists\nSince each element can be any object, we can build nested lists:\n\nx <- list(alpha = letters[sample(26, 4)],\n          beta = sample(12),\n          gamma = list(i = rnorm(10),\n                       j = runif(10),\n                       j = seq(0, 1000, length.out = 10)))\nx\n\n$alpha\n[1] \"x\" \"e\" \"m\" \"u\"\n\n$beta\n [1]  8  5  4  9 10  7 12  1  6  3  2 11\n\n$gamma\n$gamma$i\n [1]  0.64452730 -0.04058204 -0.54127821  0.88899570  0.37553280  1.79328196\n [7] -0.12137978  0.76619205 -0.37580713  0.86967357\n\n$gamma$j\n [1] 0.84588505 0.40399447 0.49876210 0.02761738 0.72345641 0.36397560\n [7] 0.32729380 0.68235205 0.62625841 0.89212092\n\n$gamma$j\n [1]    0.0000  111.1111  222.2222  333.3333  444.4444  555.5556  666.6667\n [8]  777.7778  888.8889 1000.0000\n\n\nIn the example above, alpha, beta, and gamma, are x’s elements. Notice how the length of the list refers to the number of these top-level elements:\n\nlength(x)\n\n[1] 3\n\n\n\n7.5.2 Initialize a list\nWhen setting up experiments, it can be very convenient to set up and empty list, where results will be stored (e.g. using a for-loop)\n\nx <- vector(\"list\", 4)\nx\n\n[[1]]\nNULL\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\n[[4]]\nNULL\n\nlength(x)\n\n[1] 4\n\n\n\n7.5.3 Combine lists\nYou can combine lists with c() (just like vectors):\n\nl1 <- list(q = 11:14, r = letters[11:14])\nl2 <- list(s = LETTERS[21:24], t = 100:97)\nx <- c(l1, l2)\nx\n\n$q\n[1] 11 12 13 14\n\n$r\n[1] \"k\" \"l\" \"m\" \"n\"\n\n$s\n[1] \"U\" \"V\" \"W\" \"X\"\n\n$t\n[1] 100  99  98  97\n\nlength(x)\n\n[1] 4"
  },
  {
    "objectID": "DataStructures.html#dataframestruc",
    "href": "DataStructures.html#dataframestruc",
    "title": "7  Data Structures",
    "section": "\n7.7 Data frames",
    "text": "7.7 Data frames\n\n\n\nA data frames is a special type of list where each element has the same length and forms a column, resulting in a 2D structure. Unlike matrices, each column can contain a different data type.\n\n\n\ndata.frames are usually created with named elements:\n\nx <- data.frame(Feat_1 = 1:5,\n                Feat_2 = rnorm(5),\n                Feat_3 = paste0(\"rnd_\", sample(seq(100), 5)))\nx\n\n  Feat_1     Feat_2 Feat_3\n1      1 -1.1738976 rnd_86\n2      2 -0.3750615 rnd_50\n3      3  0.5128854 rnd_40\n4      4  1.0546336 rnd_92\n5      5 -0.5312238 rnd_18\n\nclass(x)\n\n[1] \"data.frame\"\n\nstr(x)\n\n'data.frame':   5 obs. of  3 variables:\n $ Feat_1: int  1 2 3 4 5\n $ Feat_2: num  -1.174 -0.375 0.513 1.055 -0.531\n $ Feat_3: chr  \"rnd_86\" \"rnd_50\" \"rnd_40\" \"rnd_92\" ...\n\nclass(x$Feat_1)\n\n[1] \"integer\"\n\n\nUnlike a matrix, the elements of a data.frame are the columns, not the individual values in each position. Therefore the length of a data.frame is equal to the number of columns:\n\nmat <- matrix(1:100, 10)\nlength(mat)\n\n[1] 100\n\ndf <- as.data.frame(mat)\nlength(df)\n\n[1] 10"
  },
  {
    "objectID": "DataStructures.html#attributes",
    "href": "DataStructures.html#attributes",
    "title": "7  Data Structures",
    "section": "\n7.11 Attributes",
    "text": "7.11 Attributes\nR objects may have some builtin attributes but you can add arbitrary attributes to any R object. These are used to store additional information, sometimes called metadata.\n\n7.11.1 Print all attributes\nTo print an object’s attributes, use attributes:\n\nattributes(iris)\n\n$names\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"     \n\n$class\n[1] \"data.frame\"\n\n$row.names\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n[109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n[127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n[145] 145 146 147 148 149 150\n\n\nThis returns a named list. In this case we got names, class, and row.names of the iris data frame.\n\n7.11.2 Get or set specific attributes\nYou can assign new attributes using attr:\n\n(x <- c(1:10))\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nattr(x, \"name\") <- \"Very special vector\"\n\nPrinting the vector after adding a new attribute, prints the attribute name and value underneath the vector itself:\n\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\nattr(,\"name\")\n[1] \"Very special vector\"\n\n\nOur trusty str function will print attributes as well\n\nstr(x)\n\n int [1:10] 1 2 3 4 5 6 7 8 9 10\n - attr(*, \"name\")= chr \"Very special vector\"\n\n\n\n7.11.2.1 A matrix is a vector - a closer look\nLet’s see how a matrix is literally just a vector with assigned dimensions.\nStart with a vector of length 20:\n\nx <- 1:20\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\n\n\nThe vector has no attributes - yet:\n\nattributes(x)\n\nNULL\n\n\nTo convert to a matrix, we would normally pass our vector to the matrix() function and define number of rows and/or columns:\n\nxm <- matrix(x, 5)\nxm\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    6   11   16\n[2,]    2    7   12   17\n[3,]    3    8   13   18\n[4,]    4    9   14   19\n[5,]    5   10   15   20\n\nattributes(xm)\n\n$dim\n[1] 5 4\n\n\nJust for demonstration, let’s instead directly add a dimension attribute to our vector:\n\nattr(x, \"dim\") <- c(5, 4)\nx\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    6   11   16\n[2,]    2    7   12   17\n[3,]    3    8   13   18\n[4,]    4    9   14   19\n[5,]    5   10   15   20\n\nclass(x)\n\n[1] \"matrix\" \"array\" \n\n\nJust like that, we have a matrix."
  },
  {
    "objectID": "Factors.html",
    "href": "Factors.html",
    "title": "9  Factors",
    "section": "",
    "text": "Factors in R are used to store categorical variables and therefore have many important uses in statistics / data science / machine learning.\nFor example, factors can be used to store information on sex, race, diagnosis, treatment group, etc.\nIf you are new to R and to factors, begin with the following introductory-level section."
  },
  {
    "objectID": "Factors.html#factors-introductory",
    "href": "Factors.html#factors-introductory",
    "title": "9  Factors",
    "section": "\n9.1 Factors (introductory)",
    "text": "9.1 Factors (introductory)\n\n\n\nFactors in R are a special type of vector.\n\n\n\n\nEach element can take a value from a set of values known as the factor’s levels\n\n\n\n\nA factor’s levels are stored in a particular order, which affects how that factor is treated by some functions.\n\n\n\n\nYou can specify whether the order of the levels defines a quantitative relationship such that level1 < level2, etc.\n\n\n\n\n\nYou can create a factor by passing a numeric or character vector to factor() or to as.factor().\nThe difference is that as.factor() does not accept any arguments while factor() does.\nLet’s start with a character vector that includes three unique values - “a”, “b”, and “c”:\n\nx <- c(\"a\", \"c\", \"b\", \"b\", \"a\", \"a\", \"b\", \"c\")\nx\n\n[1] \"a\" \"c\" \"b\" \"b\" \"a\" \"a\" \"b\" \"c\"\n\n\nAssume that “a”, “b”, and “c” define three different groups and we want to convert this character vector to a factor.as.factor() and factor() without any arguments produce the same output:\n\nxf <- factor(x)\nxf\n\n[1] a c b b a a b c\nLevels: a b c\n\nclass(xf)\n\n[1] \"factor\"\n\nxftoo <- as.factor(x)\nxftoo\n\n[1] a c b b a a b c\nLevels: a b c\n\nclass(xftoo)\n\n[1] \"factor\"\n\n\nNotice that when a factor is printed in the R console:\n\nThe elements are printed without double quotes around them, differentiating them from character vectors.\nThe factor levels are printed below the vector values.\n\ntable() is a very useful function for factors - it gives the counts for each level:\n\ntable(xf)\n\nxf\na b c \n3 3 2 \n\n\nLet’s look at a different example. We define a factor to identify cases and controls:\n\ng <- factor(c(\"case\", \"control\", \"control\", \"case\", \"control\"))\ng\n\n[1] case    control control case    control\nLevels: case control\n\n\nBy default, the levels are ordered alphabeticaly.\n\n9.1.1 Set the order of factor levels\nYou can define the order of the factor levels with the level argument of the factor() function.\n(For example, the first factor level is used as the baseline in some statistical operations, e.g. glm(), in which case the “control” level should be first.)\n\ng <- factor(c(\"case\", \"control\", \"control\", \"case\", \"control\"),\n            levels = c(\"control\", \"case\"))\ng\n\n[1] case    control control case    control\nLevels: control case\n\n\nThe levels argument can include values not present in the input data vector. This may be used for example when some known categories are not present in your sample, but may be added in the future, or you want specifically show they are absent in a table or a plot, etc.\n\ng <- factor(c(\"Type I\", \"Type III\", \"Type III\", \"Type I\"),\n            levels = c(\"Type I\", \"Type II\", \"Type III\"))\ng\n\n[1] Type I   Type III Type III Type I  \nLevels: Type I Type II Type III\n\n\nOn the other hand, if the levels argument is specified and does not include one or more of the values present in the input data vector, the corresponding elements become NA:\n\ng <- factor(c(\"case\", \"control\", \"undefined\", \"control\", \"case\", \"control\", \"undefined\"),\n            levels = c(\"control\", \"case\"))\ng\n\n[1] case    control <NA>    control case    control <NA>   \nLevels: control case\n\n\n\n9.1.2 Define level labels\nYou can define level names or labels other than the values in the input vector using the labels argument:\nAssume you started with the following character vector\n\nx <- c(\"female\", \"female\", \"male\", \"female\", \"male\")\n\nYou can attach different labels to each level rather than default to “female” and “male” by passing a character vector to the labels arguments:\n\nxf <- factor(x, labels = c(\"F\", \"M\"))\nxf\n\n[1] F F M F M\nLevels: F M\n\n\nThe order of names in the labels argument must match the order of levels. In the above example, the levels default to c(\"female\", \"male\") because they are sorted alphabeticaly if not specified. Otherwise, we can specify both the levels and labels arguments to define both order of levels and provide new labels:\n\nxf <- factor(x, levels = c(\"male\", \"female\"), labels = c(\"M\", \"F\"))\nxf\n\n[1] F F M F M\nLevels: M F\n\n\n\n9.1.3 Change level labels of existing factor\nWe can change the level labels of a factor object\n\nusing the factor() command in the same way we create a factor from a character or other vector\nusing the levels() command\n\nStart with a factor of two groups and change labels using the factor() command:\n\nxf <- factor(c(\"GroupA\", \"GroupB\", \"GroupB\", \"GroupA\"))\nxf <- factor(xf, labels = c(\"A\", \"B\"))\nxf\n\n[1] A B B A\nLevels: A B\n\n\nStart with the same factor and change labels using the levels() command.\nThis is similar to using colnames() on a data.frame and is much faster than using factor() as above:\n\nxf <- factor(c(\"GroupA\", \"GroupB\", \"GroupB\", \"GroupA\"))\nlevels(xf) <- c(\"A\", \"B\")\nxf\n\n[1] A B B A\nLevels: A B\n\n\n\n\n\nThe levels() command changes the names of the levels. It cannot be used to change the order of levels, which must be done with factor()"
  },
  {
    "objectID": "Factors.html#factors-advanced",
    "href": "Factors.html#factors-advanced",
    "title": "9  Factors",
    "section": "\n9.2 Factors (advanced)",
    "text": "9.2 Factors (advanced)\n\n\n\nA factor is a vector A factor contains three crucial pieces of information:\n\n\n\n\nThe underlying integer vector\n\n\n\n\nThe mapping of integers to labels\n\n\n\n\nWhether the factor is ordered\n\n\n\n\n\nLet’s unpack these.\nBegin with a simple factor:\n\nx <- factor(c(\"female\", \"female\", \"female\", \"male\", \"male\"))\nx\n\n[1] female female female male   male  \nLevels: female male\n\n\nInternally, the command sees there are two distinct labels, female and male, and defaults to assigning integer numbers alphabetically, in this case female has been mapped to ‘1’ and male to ‘2’.\nPrinting a factor prints the vector of labels followed by the levels, i.e. the unique labels.\n\n9.2.1 The underlying integer vector\n\nEach level is assigned an integer. (Internally, this is the “data” that forms the elements of a factor vector). You don’t see these integers unless you convert the factor to numeric (as.numeric()) or look at the (truncated) output of str()\n\nas.numeric(x)\n\n[1] 1 1 1 2 2\n\n\n\n9.2.2 The mapping of integers to labels\n\nThis defines which integer is mapped to which label, i.e. whether 1 is mapped to male or female. You can store the same information regardless which one you choose to call 1 and 2.\nTo get the mapping you can use levels(). It prints the labels in order:\n\nlevels(x)\n\n[1] \"female\" \"male\"  \n\n\nAgain, this means that female is mapped to 1 and male is mapped to 2.\n\nstr(x)\n\n Factor w/ 2 levels \"female\",\"male\": 1 1 1 2 2\n\n\nThe above tells you that x is a factor,\nit has two levels labeled as “female” and “male”, in that order, i.e. female is level 1 and male is level 2.\nThe last part shows that the first five elements (in this case the whole vector) consists of three elements of level 1 (female) followed by 2 elements of level 2 (male)\n\n9.2.2.1 Setting new level labels\nYou can use the levels() command with an assignment to assign new labels to a factor (same syntax to how you use rownames() or colnames() to assign new row or column names to a matrix or data frame)\n\nxf <- factor(sample(c(\"patient_status_positive\", \"patient_status_negative\"), 10, T),\n             levels = c(\"patient_status_positive\", \"patient_status_negative\"))\nxf\n\n [1] patient_status_negative patient_status_negative patient_status_positive\n [4] patient_status_negative patient_status_positive patient_status_negative\n [7] patient_status_negative patient_status_negative patient_status_negative\n[10] patient_status_negative\nLevels: patient_status_positive patient_status_negative\n\n\n\nlevels(xf)\n\n[1] \"patient_status_positive\" \"patient_status_negative\"\n\nlevels(xf) <- c(\"positive\", \"negative\")\nxf\n\n [1] negative negative positive negative positive negative negative negative\n [9] negative negative\nLevels: positive negative\n\n\n\n9.2.2.2 Defining the mapping of labels to integers\nIf you want to define the mapping of labels to their integer representation (and not default to them sorted alphabeticaly), you use the levels arguments of the factor() function.\nThe vector passed to the levels arguments must include at least all unique values passed to factor(), otherwise you will get NA values\nWithout defining levels they are assigned alphabeticaly:\n\nx <- factor(c(\"alpha\", \"alpha\", \"gamma\", \"delta\", \"delta\"))\nx\n\n[1] alpha alpha gamma delta delta\nLevels: alpha delta gamma\n\n\nDefine levels:\n\nx <- factor(c(\"alpha\", \"alpha\", \"gamma\", \"delta\", \"delta\"),\n levels = c(\"alpha\", \"gamma\", \"delta\"))\nx\n\n[1] alpha alpha gamma delta delta\nLevels: alpha gamma delta\n\n\nThe table command has a number of useful applications, in it simplest form, it tabulates number of elements with each unique value found in a vector:\n\ntable(x)\n\nx\nalpha gamma delta \n    2     1     2 \n\n\nIf you forget (or choose to exclude) a level, all occurences are replaced by NA:\n\nx <- factor(c(\"alpha\", \"alpha\", \"gamma\", \"delta\", \"delta\"),\n levels = c(\"alpha\", \"gamma\"))\nx\n\n[1] alpha alpha gamma <NA>  <NA> \nLevels: alpha gamma\n\n\nIf you know that more levels exist, even if no examples are present in your sample, you can includes these extra levels:\n\nx <- factor(c(\"alpha\", \"alpha\", \"gamma\", \"delta\", \"delta\"),\n levels = c(\"alpha\", \"beta\", \"gamma\", \"delta\"))\nx\n\n[1] alpha alpha gamma delta delta\nLevels: alpha beta gamma delta\n\n\n\ntable(x)\n\nx\nalpha  beta gamma delta \n    2     0     1     2 \n\n\n\n9.2.3 Is the factor ordered\n\nWe looked at how you can define the order of levels using the levels argument in factor(), which affects the integer mapping to each label.\nThis can affect how some applications treat the different levels.\nOn top of the order of the mapping, you can further define if there is a quantitative relationship among levels of the form level 1 < level 2 < ... < level n. This, in turn, can affect how the factor is treated by some functions, like some functions that fit statistical models.\n\n\n\nAll factors’ levels appear in some order or other.\n\n\nAn ordered factor indicates that its levels have a quantitative relationship of the form level 1 < level 2 < … < level n.\n\n\n\nFirst an unordered factor:\n\ndat <- sample(c(\"small\", \"medium\", \"large\"), 10, TRUE)\nx <- factor(dat)\nx\n\n [1] medium small  large  small  large  large  medium medium small  medium\nLevels: large medium small\n\n\nTo make the above into an ordered factor, we need to define the order of the levels with the levels arguments and also specify that it is ordered with the ordered argument:\n\nx <- factor(dat,\n            levels = c(\"small\", \"medium\", \"large\"),\n            ordered = TRUE)\nx\n\n [1] medium small  large  small  large  large  medium medium small  medium\nLevels: small < medium < large\n\n\nNote how the levels now include the < sign between levels to indicate the ordering.\n\n9.2.4 Change order of levels or labels\nWe’ve seen how to create a factor with defined order of levels and how to change level labels already. Because these are prone to serious accidents, let’s look at them again, together.\nTo change the order of levels of an existing factor use factor():\n\nx <- factor(c(\"target\", \"target\", \"control\", \"control\", \"control\"))\nx\n\n[1] target  target  control control control\nLevels: control target\n\n\nChange the order so that target is first (i.e. corresponds to 1:\n\nx <- factor(x, levels = c(\"target\", \"control\"))\nx\n\n[1] target  target  control control control\nLevels: target control\n\n\nTo change the labels of the levels use levels():\n\nx\n\n[1] target  target  control control control\nLevels: target control\n\nlevels(x) <- c(\"hit\", \"decoy\")\nx\n\n[1] hit   hit   decoy decoy decoy\nLevels: hit decoy\n\n\n\n\n\nChanging the levels of a factor with levels() does not change the internal integer representation but changes every element’s label.\n\n\n\n\n9.2.5 Fatal error to avoid\nExample scenario: You receive a dataset for classification where the outcome is a factor of 1s and 0s:\n\noutcome <- factor(c(1, 1, 0, 0, 0, 1, 0))\noutcome\n\n[1] 1 1 0 0 0 1 0\nLevels: 0 1\n\n\nSome classification procedures expect the first level to be the ‘positive’ outcome, so you decide to reorder the levels.\nYou mistakenly use levels() instead of factor(x, levels=c(...)) hoping to achieve this.\nYou end up flipping all the outcome values.\n\nlevels(outcome) <- c(\"1\", \"0\")\noutcome\n\n[1] 0 0 1 1 1 0 1\nLevels: 1 0\n\n\nAll zeros became ones and ones became zeros.\nYour model does the exact opposite of what you intended.\n\n9.2.6 Factor to numeric\nWhile it often makes sense to have factors with words for labels, they can be any character and that includes numbers (i.e. numbers which are treated as labels)\n\nf <- factor(c(3, 7, 7, 9, 3, 3, 9))\nf\n\n[1] 3 7 7 9 3 3 9\nLevels: 3 7 9\n\n\nThis behaves just like any other factor with all the rules we learned above.\nThere is a very easy trap to fall into, if you ever decide to convert such a factor to numeric.\nThe first thing that usually comes to mind is to use as.numeric().\n\n# !don't do this!\nas.numeric(f)\n\n[1] 1 2 2 3 1 1 3\n\n\nBut! We already know this will return the integer index, it will not return the labels as numbers.\nBy understanding the internal representation of the factor, i.e. that a factor is an integer vector indexing a set of labels, you can convert labels to numeric exactly by indexing the set of labels:\n\nlevels(f)[f]\n\n[1] \"3\" \"7\" \"7\" \"9\" \"3\" \"3\" \"9\"\n\n\nThe above suggests that used as an index within the brackets, f is coerced to integer, therefore to understand the above:\n\nlevels(f)\n\n[1] \"3\" \"7\" \"9\"\n\nlevels(f)[as.integer(f)]\n\n[1] \"3\" \"7\" \"7\" \"9\" \"3\" \"3\" \"9\"\n\n# same as\nlevels(f)[f]\n\n[1] \"3\" \"7\" \"7\" \"9\" \"3\" \"3\" \"9\"\n\n\nA different way around this that may be less confusing is to simply convert the factor to character and then to numeric:\n\nas.numeric(as.character(f))\n\n[1] 3 7 7 9 3 3 9"
  },
  {
    "objectID": "Factors.html#summary",
    "href": "Factors.html#summary",
    "title": "9  Factors",
    "section": "\n9.3 Summary",
    "text": "9.3 Summary\n\n\n\n\n\nFactors in R are essentially integer vectors with labels.\n\n\n\n\nA factor’s internal integer values range from 1 to the number of levels, i.e.  categories.\n\n\n\n\nEach integer corresponds to a label.\n\n\n\n\nTo set order of levels: factor(x, levels = levels_in_desired_order) to order levels\n\n\n\n\nTo change labels: levels(x) <- newlabels or factor(x, labels = newlabels)\n\n\n\n\n\n\n\n\nTo avoid confusion, do not use numbers as level labels, if possible."
  },
  {
    "objectID": "Indexing.html",
    "href": "Indexing.html",
    "title": "\n8  Indexing\n",
    "section": "",
    "text": "An index is used to pick elements of a data structure (i.e. a vector, matrix, array, list, data frame, etc.). You can select (or exclude) one or multiple elements at a time.  There are three types of index vectors you can use in R to identify elements of an object:\nInteger indexing in R is 1-based, meaning the first item of a vector is in position 1. In contrast, many programming languages use 0-based indexing where the first element is in the 0th position, the second in the 1st, and the nth in the n-1 position.  To understand indexing, make sure you are very comfortable with the core R data structures: vectors, matrices, arrays, lists, and data.frames. What is indexing used for?\nIndexing can be used to get values from an object or to set values in an object.  The main indexing operator in R is the square bracket [].\nLists use double square brackets [[]]"
  },
  {
    "objectID": "Indexing.html#indexvectors",
    "href": "Indexing.html#indexvectors",
    "title": "\n8  Indexing\n",
    "section": "\n8.1 Vectors",
    "text": "8.1 Vectors\nStart with a simple vector:\n\nx <- 15:24\nx\n\n [1] 15 16 17 18 19 20 21 22 23 24\n\n\n\n8.1.1 Integer Index\nGet the 5th element of a vector:\n\nx[5]\n\n[1] 19\n\n\nGet elements 6 through 9 of the same vector:\n\nx[6:9]\n\n[1] 20 21 22 23\n\n\nAn integer index can be used to reverse order of elements:\n\nx[5:3]\n\n[1] 19 18 17\n\n\nNote that an integer index can be used to repeat elements. This is often done by accident, when someone passes the wrong vector as an index, so beware.\n\nx[c(1, 1, 1, 4)]\n\n[1] 15 15 15 18\n\n\n\n8.1.2 Logical Index\nLogical indexes are usually created as the output of a logical operation, i.e. an elementwise comparison.  Select elements with value greater than 19:\n\nidl <- x > 19\nx[idl]\n\n[1] 20 21 22 23 24\n\n\nLogical vectors can be created directly in the brackets:\n\nx[x > 19]\n\n[1] 20 21 22 23 24\n\n\n\n8.1.3 Extract vs. Replace\n\nx <- c(24, 32, 41, 37, 999, 999, 999)\n\nIndexing allows you to access specific elements, for example to perform calculations on them.\nGet the mean of elements 1 through 4:\n\nmean(x[1:4])\n\n[1] 33.5\n\n\nYou can combine indexing with assignment to replace elements of an object.\nReplace values in elements 1:4 with their log:\n\nx[1:4] <- log(x[1:4])\nx\n\n[1]   3.178054   3.465736   3.713572   3.610918 999.000000 999.000000 999.000000\n\n\nReplace elements that are equal to 999 with NA:\n\nx[x == 999] <- NA\nx\n\n[1] 3.178054 3.465736 3.713572 3.610918       NA       NA       NA"
  },
  {
    "objectID": "Indexing.html#indexmatrices",
    "href": "Indexing.html#indexmatrices",
    "title": "\n8  Indexing\n",
    "section": "\n8.2 Matrices",
    "text": "8.2 Matrices\nReminder:\n\nA matrix is a 2D vector and contains elements of the same type (numeric, integer, character, etc.).\nA data frame is a 2D list and each column can contain a different data type.\n\nTo index a 2D structure, whether a matrix or data frame, we use the form: [row, column].\nThe following indexing operations are therefore the same whether applied on a matrix or a data frame:\n\nmat <- matrix(21:60, 10)\ncolnames(mat) <- paste0(\"Feature_\", seq(ncol(mat)))\nrownames(mat) <- paste0(\"Row_\", seq(nrow(mat)))\nmat\n\n       Feature_1 Feature_2 Feature_3 Feature_4\nRow_1         21        31        41        51\nRow_2         22        32        42        52\nRow_3         23        33        43        53\nRow_4         24        34        44        54\nRow_5         25        35        45        55\nRow_6         26        36        46        56\nRow_7         27        37        47        57\nRow_8         28        38        48        58\nRow_9         29        39        49        59\nRow_10        30        40        50        60\n\ndf <- as.data.frame(mat)\ndf\n\n       Feature_1 Feature_2 Feature_3 Feature_4\nRow_1         21        31        41        51\nRow_2         22        32        42        52\nRow_3         23        33        43        53\nRow_4         24        34        44        54\nRow_5         25        35        45        55\nRow_6         26        36        46        56\nRow_7         27        37        47        57\nRow_8         28        38        48        58\nRow_9         29        39        49        59\nRow_10        30        40        50        60\n\n\nTo get the contents of the fifth row, second column:\n\nmat[5, 2]\n\n[1] 35\n\ndf[5, 2]\n\n[1] 35\n\n\nWe show the following on matrices, but they work just the same on data.frames.\nIf you want to select an entire row or an entire column, you leave the row or column index blank, but you must include a comma:\nGet the first row:\n\nmat[1, ]\n\nFeature_1 Feature_2 Feature_3 Feature_4 \n       21        31        41        51 \n\n\nGet the second column:\n\nmat[, 2]\n\n Row_1  Row_2  Row_3  Row_4  Row_5  Row_6  Row_7  Row_8  Row_9 Row_10 \n    31     32     33     34     35     36     37     38     39     40 \n\n\nNote that colnames and rownames where added to the matrix above for convenience - if they are absent, there are no labels above each element.\nYou can define ranges for both rows and columns:\n\nmat[6:7, 2:4]\n\n      Feature_2 Feature_3 Feature_4\nRow_6        36        46        56\nRow_7        37        47        57\n\n\nYou can return rows and/or columns reversed if desired:\n\nmat[7:6, 4:2]\n\n      Feature_4 Feature_3 Feature_2\nRow_7        57        47        37\nRow_6        56        46        36\n\n\nYou can use vectors to specify any combination of rows and columns.\nGet rows 2, 4, and 7 of columns 1, 4, and 3:\n\nmat[c(2, 4, 7), c(1, 4, 3)]\n\n      Feature_1 Feature_4 Feature_3\nRow_2        22        52        42\nRow_4        24        54        44\nRow_7        27        57        47\n\n\nSince a matrix is a vector with 2 dimensions, you can also index the underlying vector directly. Regardless of whether a matrix was created by row or by column (default), the data is stored and acceesed by column. You can see that by converting the matrix to a 1D vector:\n\nas.vector(mat)\n\n [1] 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45\n[26] 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60\n\n\nsame as:\n\nc(mat)\n\n [1] 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45\n[26] 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60\n\n\nFor example, ‘mat’ has 10 rows and 4 columns, therefore the 11th element is in row 1 column 2\n\nmat[11]\n\n[1] 31\n\n\nis the same as:\n\nmat[1, 2]\n\n[1] 31\n\n\nThis only works with matrices, not data.frames.\n\n8.2.1 Matrix of indexes\nThis is quite less common, but potentially useful. It allows you to specify a series of individual [i, j] indexes, i.e. is a way to select multiple individual non-contiguous elements\n\nidm <- matrix(c(2, 4, 7, 4, 3, 1), 3)\nidm\n\n     [,1] [,2]\n[1,]    2    4\n[2,]    4    3\n[3,]    7    1\n\n\nAn n-by-2 matrix can be used to index as a length n vector of [row, colum] indexes. Therefore, the above matrix, will return elements [2, 4], [4, 3], [7, 1]:\n\nmat[idm]\n\n[1] 52 44 27\n\n\n\n8.2.2 Logical index\nSelect all rows with values greater than 15 on the second column:\nThe logical index for this operation is:\n\nmat[, 2] > 15\n\n Row_1  Row_2  Row_3  Row_4  Row_5  Row_6  Row_7  Row_8  Row_9 Row_10 \n  TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE \n\n\nIt can be used directly to index the matrix:\n\nmat[mat[, 2] > 15, ]\n\n       Feature_1 Feature_2 Feature_3 Feature_4\nRow_1         21        31        41        51\nRow_2         22        32        42        52\nRow_3         23        33        43        53\nRow_4         24        34        44        54\nRow_5         25        35        45        55\nRow_6         26        36        46        56\nRow_7         27        37        47        57\nRow_8         28        38        48        58\nRow_9         29        39        49        59\nRow_10        30        40        50        60\n\n\nIndexing a matrix or a data.frame can return either a smaller matrix/data.frame or a vector.\nIn general, objects in R are returned in their most simple form unless otherwise specified. This means that if you extract a column or a row, you get a vector:\nGet the third column:\n\nmat[, 3]\n\n Row_1  Row_2  Row_3  Row_4  Row_5  Row_6  Row_7  Row_8  Row_9 Row_10 \n    41     42     43     44     45     46     47     48     49     50 \n\nclass(mat[, 3])\n\n[1] \"integer\"\n\n\nYou can specify drop = FALSE to stop R from dropping the unused dimension and return a matrix or data.frame of a single column:\n\nmat[, 3, drop = FALSE]\n\n       Feature_3\nRow_1         41\nRow_2         42\nRow_3         43\nRow_4         44\nRow_5         45\nRow_6         46\nRow_7         47\nRow_8         48\nRow_9         49\nRow_10        50\n\ndf[, 3, drop = FALSE]\n\n       Feature_3\nRow_1         41\nRow_2         42\nRow_3         43\nRow_4         44\nRow_5         45\nRow_6         46\nRow_7         47\nRow_8         48\nRow_9         49\nRow_10        50\n\n\nCheck it is still a matrix or data.frame:\n\nclass(mat[, 3, drop = FALSE])\n\n[1] \"matrix\" \"array\" \n\nclass(df[, 3, drop = FALSE])\n\n[1] \"data.frame\""
  },
  {
    "objectID": "Indexing.html#indexlists",
    "href": "Indexing.html#indexlists",
    "title": "\n8  Indexing\n",
    "section": "\n8.3 Lists",
    "text": "8.3 Lists\nReminder: A list can contain elements of different classes and of different lengths:\n\nx <- list(one = 1001:1004,\n          two = sample(seq(0, 100, .1), 10),\n          three = c(\"Neuro\", \"Cardio\", \"Radio\"),\n          four = median)\nx\n\n$one\n[1] 1001 1002 1003 1004\n\n$two\n [1] 73.7 72.6 13.8 66.1 42.4 45.4  0.5 51.7 34.8 36.4\n\n$three\n[1] \"Neuro\"  \"Cardio\" \"Radio\" \n\n$four\nfunction (x, na.rm = FALSE, ...) \nUseMethod(\"median\")\n<bytecode: 0x126428598>\n<environment: namespace:stats>\n\n\n\n8.3.1 Extract single list element:\nYou can access a single list element using:\n\nUsing double brackets [[ with either name or integer position\n\n\n$ followed by name of the element (therefore only works if elements are named)\n\nFor example, to access the third element:\n\nx$three\n\n[1] \"mango\"     \"banana\"    \"tangerine\"\n\n\nsame as:\n\nx[[3]]\n\n[1] \"mango\"     \"banana\"    \"tangerine\"\n\n\nsame as:\n\nx[[\"three\"]]\n\n[1] \"mango\"     \"banana\"    \"tangerine\"\n\n\nTo access a list element programmaticaly, i.e. using a name or integer index stored in a variable, only the bracket notation works. Therefore, programmatically, you would always use double brackets to access different elements:\n\nidi <- 3\nidc <- \"three\"\nx[[idi]]\n\n[1] \"mango\"     \"banana\"    \"tangerine\"\n\nx[[idc]]\n\n[1] \"mango\"     \"banana\"    \"tangerine\"\n\n\n\n8.3.2 Extract one or more list elements as a list:\nYou can extract one or more list elements as a pruned list using single bracket [ notation. Similar to indexing of a vector, this can be either a logical, integer, or character vector:\n\nx[3]\n\n$three\n[1] \"Neuro\"  \"Cardio\" \"Radio\" \n\nx[\"three\"]\n\n$three\n[1] \"Neuro\"  \"Cardio\" \"Radio\" \n\nx[c(F, F, T, F)]\n\n$three\n[1] \"Neuro\"  \"Cardio\" \"Radio\" \n\n\nExtract multiple elements:\n\nx[2:3]\n\n$two\n [1] 73.7 72.6 13.8 66.1 42.4 45.4  0.5 51.7 34.8 36.4\n\n$three\n[1] \"Neuro\"  \"Cardio\" \"Radio\" \n\n# same as\nx[c(\"two\", \"three\")]\n\n$two\n [1] 73.7 72.6 13.8 66.1 42.4 45.4  0.5 51.7 34.8 36.4\n\n$three\n[1] \"Neuro\"  \"Cardio\" \"Radio\" \n\n# same as\nx[c(F, T, T, F)]\n\n$two\n [1] 73.7 72.6 13.8 66.1 42.4 45.4  0.5 51.7 34.8 36.4\n\n$three\n[1] \"Neuro\"  \"Cardio\" \"Radio\" \n\n\n\n8.3.3 Recursive indexing of list\nGiven the following list:\n\nx <- list(PIDN = 2001:2020,\n          Dept = c(\"Neuro\", \"Cardio\", \"Radio\"),\n          Age = rnorm(20, 57, 1.3))\n\nWe can access the 3rd element of the 2nd element:\n\nx[[2]][3]\n\n[1] \"Radio\"\n\n\nor\n\nx[[c(2, 3)]]\n\n[1] \"Radio\"\n\n\nThis is called recursive partitioning and is perhaps more often used by accident, when one instead wanted to extrant the 2nd and 3rd elements:\n\nx[c(2, 3)]\n\n$Dept\n[1] \"Neuro\"  \"Cardio\" \"Radio\" \n\n$Age\n [1] 58.09782 56.81164 57.76611 57.72478 60.06583 55.23224 55.37520 59.00544\n [9] 54.50458 56.29461 57.91026 57.31950 56.46564 57.88263 56.38345 56.09201\n[17] 57.73912 57.96733 56.80198 56.61300\n\n\n\n8.3.4 Flatten list\nYou can convert a list to one lone vector containing all the individual components of the original list using unlist(). Notice how names are automatically created based on the original structure:\n\nx <- list(alpha = sample(seq(100), 10),\n          beta = sample(seq(100), 10),\n          gamma = sample(seq(100), 10))\nx\n\n$alpha\n [1] 56 37 70 73 69 90  4 76 36 81\n\n$beta\n [1] 54 60 14 49 55 98 28 83 37 26\n\n$gamma\n [1] 42  9 80 77 82 14 60 41 16 68\n\nunlist(x)\n\n alpha1  alpha2  alpha3  alpha4  alpha5  alpha6  alpha7  alpha8  alpha9 alpha10 \n     56      37      70      73      69      90       4      76      36      81 \n  beta1   beta2   beta3   beta4   beta5   beta6   beta7   beta8   beta9  beta10 \n     54      60      14      49      55      98      28      83      37      26 \n gamma1  gamma2  gamma3  gamma4  gamma5  gamma6  gamma7  gamma8  gamma9 gamma10 \n     42       9      80      77      82      14      60      41      16      68 \n\n\nIf you want to drop the names, you can use set the use.names argument to FALSE or wrap the above in unname():\n\nunlist(x, use.names = F)\n\n [1] 56 37 70 73 69 90  4 76 36 81 54 60 14 49 55 98 28 83 37 26 42  9 80 77 82\n[26] 14 60 41 16 68\n\n# same as\nunname(unlist(x))\n\n [1] 56 37 70 73 69 90  4 76 36 81 54 60 14 49 55 98 28 83 37 26 42  9 80 77 82\n[26] 14 60 41 16 68"
  },
  {
    "objectID": "Indexing.html#indexdfs",
    "href": "Indexing.html#indexdfs",
    "title": "\n8  Indexing\n",
    "section": "\n8.4 Data frames",
    "text": "8.4 Data frames\nWe’ve saw above that a data frame can be indexed in many ways similar to a matrix, i.e. by defining rows and columns. At the same time, we know that a data frame is a rectangular list. Like a list, its elements are vectors of any type (integer, double, character, factor, and more) but, unlike a list, they have to be of the same length. A data frame can also be indexed the same way as a list and similar to list indexing, notice that some methods return a smaller data frame, while others return vectors.\n\n\n\nYou can index a data frame using all the ways you can index a list and all the ways you can index a matrix.\n\n\n\nLet’s create a simple data frame:\n\nx <- data.frame(Feat_1 = 21:25,\n                Feat_2 = rnorm(5),\n                Feat_3 = paste0(\"rnd_\", sample(seq(100), 5)))\nx\n\n  Feat_1      Feat_2 Feat_3\n1     21  0.32941401 rnd_84\n2     22  0.05228479 rnd_72\n3     23  1.89495104 rnd_67\n4     24 -0.08797606 rnd_82\n5     25  0.68797668 rnd_24\n\n\n\n8.4.1 Extract single column as a vector\nJust like in a list, using double brackets [[ or the $ operator returns an element, i.e. a vector:\n\nx$Feat_2\n\n[1]  0.32941401  0.05228479  1.89495104 -0.08797606  0.68797668\n\n\n\nx[[2]]\n\n[1]  0.32941401  0.05228479  1.89495104 -0.08797606  0.68797668\n\n\n\nx[, 2]\n\n[1]  0.32941401  0.05228479  1.89495104 -0.08797606  0.68797668\n\n\n\n8.4.2 Extract “one or more” columns as a data.frame\nAccessing a column by name using square brackets, returns a single-column data.frame:\n\nx[\"Feat_2\"]\n\n       Feat_2\n1  0.32941401\n2  0.05228479\n3  1.89495104\n4 -0.08797606\n5  0.68797668\n\n\nAccessing a column by [row, column] either by position or name, return a vector by default:\n\nx[, 2]\n\n[1]  0.32941401  0.05228479  1.89495104 -0.08797606  0.68797668\n\n\n\nx[, \"Feat_2\"]\n\n[1]  0.32941401  0.05228479  1.89495104 -0.08797606  0.68797668\n\n\nAs we saw earlier, we can specify drop = FALSE to return a data.frame:\n\nclass(x[, 2, drop = FALSE])\n\n[1] \"data.frame\"\n\nclass(x[, \"Feat_2\", drop = FALSE])\n\n[1] \"data.frame\"\n\n\nAs in lists, all indexing and slicing operations, with the exception of the $ notation, work with a variable holding either a column name of or an integer location:\n\nidi <- 2\nidc <- \"Feat_2\"\nx[idi]\n\n       Feat_2\n1  0.32941401\n2  0.05228479\n3  1.89495104\n4 -0.08797606\n5  0.68797668\n\nx[idc]\n\n       Feat_2\n1  0.32941401\n2  0.05228479\n3  1.89495104\n4 -0.08797606\n5  0.68797668\n\nx[[idi]]\n\n[1]  0.32941401  0.05228479  1.89495104 -0.08797606  0.68797668\n\nx[[idc]]\n\n[1]  0.32941401  0.05228479  1.89495104 -0.08797606  0.68797668\n\nx[, idi]\n\n[1]  0.32941401  0.05228479  1.89495104 -0.08797606  0.68797668\n\nx[, idc]\n\n[1]  0.32941401  0.05228479  1.89495104 -0.08797606  0.68797668\n\nx[, idi, drop = F]\n\n       Feat_2\n1  0.32941401\n2  0.05228479\n3  1.89495104\n4 -0.08797606\n5  0.68797668\n\nx[, idc, drop = F]\n\n       Feat_2\n1  0.32941401\n2  0.05228479\n3  1.89495104\n4 -0.08797606\n5  0.68797668\n\n\nExtracting multiple columns returns a data frame:\n\nx[, 2:3]\n\n       Feat_2 Feat_3\n1  0.32941401 rnd_84\n2  0.05228479 rnd_72\n3  1.89495104 rnd_67\n4 -0.08797606 rnd_82\n5  0.68797668 rnd_24\n\nclass(x[, 2:3])\n\n[1] \"data.frame\"\n\n\n\n8.4.3 Extract rows\nUnlike indexing a row of a matrix, indexing a row of a data.frame returns a single-row data.frame, since it contains multiple columns of potentially different types:\n\nx[1, ]\n\n  Feat_1   Feat_2 Feat_3\n1     21 0.329414 rnd_84\n\nclass(x[1, ])\n\n[1] \"data.frame\"\n\n\nConvert into a list using c():\n\nc(x[1, ])\n\n$Feat_1\n[1] 21\n\n$Feat_2\n[1] 0.329414\n\n$Feat_3\n[1] \"rnd_84\"\n\nclass(c(x[1, ]))\n\n[1] \"list\"\n\n\nConvert into a (named) vector using unlist():\n\nunlist(x[1, ])\n\n             Feat_1              Feat_2              Feat_3 \n               \"21\" \"0.329414009589926\"            \"rnd_84\" \n\nclass(unlist(x[1, ]))\n\n[1] \"character\"\n\n\n\n8.4.4 Logical index\n\nx[x$Feat_1 > 22, ]\n\n  Feat_1      Feat_2 Feat_3\n3     23  1.89495104 rnd_67\n4     24 -0.08797606 rnd_82\n5     25  0.68797668 rnd_24"
  },
  {
    "objectID": "Indexing.html#logical---integer-indexing",
    "href": "Indexing.html#logical---integer-indexing",
    "title": "\n8  Indexing\n",
    "section": "\n8.5 Logical <-> Integer indexing",
    "text": "8.5 Logical <-> Integer indexing\nIn this chapter, we have learned how to use both integer and logical indexes.\n\n\n\n\n\nA logical index needs to be of the same dimensions as the object it is indexing (unless you really want to recycle values - see chapter on vectorization): you are specifying whether to include or exclude each element\n\n\n\n\nAn integer index will be shorter than the object it is indexing: you are specifying which subset of elements to include (or with a - in front, which elements to exclude)\n\n\n\n\n\nIt’s easy to convert between the two types.\nFor example, start with a sequence of integers:\n\nx <- 21:30\nx\n\n [1] 21 22 23 24 25 26 27 28 29 30\n\n\nLet’s create a logical index based on two inequalities:\n\nlogical_index <- x > 23 & x < 28\nlogical_index\n\n [1] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n\n\n\n8.5.1 Logical to integer index with which():\n\n\n\nThe common mistake is to attempt to convert a logical index to an integer index using as.integer(). This results in a vector of 1’s and 0’s, NOT an integer index.which() converts a logical index to an integer index.\n\n\n\nwhich() literally gives the position of all TRUE elements in a vector, thus converting a logical to an integer index:\n\ninteger_index <- which(logical_index)\ninteger_index\n\n[1] 4 5 6 7\n\n\ni.e. positions 4, 5, 6, 7 of the logical_index are TRUE\n\n\n\nA logical and an integer index are equivalent if they select the exact same elements\n\n\n\nLet’s check than when used to index x, they both return the same result:\n\nx[logical_index]\n\n[1] 24 25 26 27\n\nx[integer_index]\n\n[1] 24 25 26 27\n\nall(x[logical_index] == x[integer_index])\n\n[1] TRUE\n\n\n\n8.5.2 Integer to logical index\nOn the other hand, if we want to convert an integer index to a logical index, we can begin with a logical vector of the same length or dimension as the object we want to index with all FALSE values:\n\nlogical_index_too <- vector(length = length(x))\nlogical_index_too\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nAnd use the integer index to replace the corresponding elements to TRUE:\n\nlogical_index_too[integer_index] <- TRUE\nlogical_index_too\n\n [1] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n\n\nThis, of course, is the same as the logical index we started with.\n\nall(logical_index == logical_index_too)\n\n[1] TRUE"
  },
  {
    "objectID": "Indexing.html#exclude-cases-using-an-index",
    "href": "Indexing.html#exclude-cases-using-an-index",
    "title": "\n8  Indexing\n",
    "section": "\n8.6 Exclude cases using an index",
    "text": "8.6 Exclude cases using an index\nVery often, we want to use an index, whether logical or integer, to exclude cases instead of to select cases. To do that with a logical integer, we simply use an exclamation point in front of the index to negate each element (convert each TRUE to FALSE and each FALSE to TRUE):\n\nlogical_index\n\n [1] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n\n!logical_index\n\n [1]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n\n\n\nx[!logical_index]\n\n[1] 21 22 23 28 29 30\n\n\nTo exclude elements using an integer index, R allows you to use negative indexing:\n\nx[-integer_index]\n\n[1] 21 22 23 28 29 30\n\n\n\n\n\nTo get the complement of an index, you negate a logical index (!logical_index) or you subtract an integer index (-integer_index):"
  },
  {
    "objectID": "Vectorization.html",
    "href": "Vectorization.html",
    "title": "11  Vectorized Operations",
    "section": "",
    "text": "Most built-in R functions are vectorized and many functions from external packages are as well.\nVectorization is very efficient: it can save both human (your) time and machine time.\nIn many cases, applying a function on all elements simultaneously may seem obvious or expected behavior, but since not all functions are vectorized, make sure to check the documentation (and/or check using a simple example)"
  },
  {
    "objectID": "Vectorization.html#operations-between-vectors-of-equal-length",
    "href": "Vectorization.html#operations-between-vectors-of-equal-length",
    "title": "11  Vectorized Operations",
    "section": "\n11.1 Operations between vectors of equal length",
    "text": "11.1 Operations between vectors of equal length\nSuch operations are applied between corresponding elements of each vector:\n\nx <- 1:10\nz <- 11:20\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nz\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\nx + z\n\n [1] 12 14 16 18 20 22 24 26 28 30\n\n\ni.e. the above is equal to c(x[1] + z[1], x[2] + z[2], ..., x[n] + z[n])"
  },
  {
    "objectID": "Vectorization.html#operations-between-a-vector-and-a-scalar",
    "href": "Vectorization.html#operations-between-a-vector-and-a-scalar",
    "title": "11  Vectorized Operations",
    "section": "\n11.2 Operations between a vector and a scalar",
    "text": "11.2 Operations between a vector and a scalar\nIn this cases, the scalar is repeated to match the length of the vector, i.e. it is recycled:\n\n(x + 10)\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\n(x * 2)\n\n [1]  2  4  6  8 10 12 14 16 18 20\n\n(x / 10)\n\n [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n\n(x ^ 2)\n\n [1]   1   4   9  16  25  36  49  64  81 100"
  },
  {
    "objectID": "Vectorization.html#operations-between-vectors-of-unequal-length-value-recycling",
    "href": "Vectorization.html#operations-between-vectors-of-unequal-length-value-recycling",
    "title": "11  Vectorized Operations",
    "section": "\n11.3 Operations between vectors of unequal length: value recycling\n",
    "text": "11.3 Operations between vectors of unequal length: value recycling\n\nOperations between a vector and a scalar are a special case of operations between vectors of unequal length. Whenever you perform an operation between two objects of different length, the shorter object’s elements are recycled:\n\nx + c(2:1)\n\n [1]  3  3  5  5  7  7  9  9 11 11\n\n\n\n\n\nOperations between objects of unequal length can occur by mistake. If the shorter object’s length is a multiple of the longer object’s length, there will be no error or warning, as above. Otherwise, there is a warning (which may be confusing at first) BUT recycling still happens and is highly unlikely to be intentional:\n\n\n\n\nx + c(1, 3, 9)\n\nWarning in x + c(1, 3, 9): longer object length is not a multiple of shorter\nobject length\n\n\n [1]  2  5 12  5  8 15  8 11 18 11"
  },
  {
    "objectID": "Vectorization.html#vectorized-matrix-operations",
    "href": "Vectorization.html#vectorized-matrix-operations",
    "title": "11  Vectorized Operations",
    "section": "\n11.4 Vectorized matrix operations",
    "text": "11.4 Vectorized matrix operations\nOperations between matrices are similarly vectorized, i.e. performed between corresponding elements:\n\na <- matrix(1:4, 2)\nb <- matrix(11:14, 2)\na\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nb\n\n     [,1] [,2]\n[1,]   11   13\n[2,]   12   14\n\na + b\n\n     [,1] [,2]\n[1,]   12   16\n[2,]   14   18\n\na * b\n\n     [,1] [,2]\n[1,]   11   39\n[2,]   24   56\n\na / b\n\n           [,1]      [,2]\n[1,] 0.09090909 0.2307692\n[2,] 0.16666667 0.2857143"
  },
  {
    "objectID": "Vectorization.html#vectorized-functions",
    "href": "Vectorization.html#vectorized-functions",
    "title": "11  Vectorized Operations",
    "section": "\n11.5 Vectorized functions",
    "text": "11.5 Vectorized functions\nSome examples of common mathematical operations that are vectorized:\n\nlog(x)\n\n [1] 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379 1.7917595 1.9459101\n [8] 2.0794415 2.1972246 2.3025851\n\nsqrt(x)\n\n [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427\n [9] 3.000000 3.162278\n\nsin(x)\n\n [1]  0.8414710  0.9092974  0.1411200 -0.7568025 -0.9589243 -0.2794155\n [7]  0.6569866  0.9893582  0.4121185 -0.5440211\n\ncos(x)\n\n [1]  0.5403023 -0.4161468 -0.9899925 -0.6536436  0.2836622  0.9601703\n [7]  0.7539023 -0.1455000 -0.9111303 -0.8390715"
  },
  {
    "objectID": "Vectorization.html#ifelse",
    "href": "Vectorization.html#ifelse",
    "title": "11  Vectorized Operations",
    "section": "\n11.6 ifelse()\n",
    "text": "11.6 ifelse()\n\nifelse() is vectorized and can be a great and compact alternative to a more complex expression:\n\na <- 1:10\ny <- ifelse(a > 5, 11:20, 21:30)\ny\n\n [1] 21 22 23 24 25 16 17 18 19 20\n\n\nSo what did this do?\nIt is equivalent to:\n\nidl <- a > 5\nyes <- 11:20\nno <- 21:30\nout <- vector(\"numeric\", 10)\nfor (i in seq(a)) {\n  if (idl[i]) {\n    out[i] <- yes[i]\n  } else {\n    out[i] <- no[i]\n  }\n}\nout\n\n [1] 21 22 23 24 25 16 17 18 19 20\n\n\ni.e.\n\nCreate a logical index using test\n\nfor each element i in test:\n\nif the element i is TRUE, return yes[i], else no[i]\n\n\n\n\nFor another example, lets take integers 1:11 and square the odd ones and cube the even ones. We use the modulo operation %% to test if each element is odd or even:\n\nx <- 1:11\nxsc <- ifelse(x %% 2 == 0, c(1:11)^3, c(1:11)^2)\nxsc\n\n [1]    1    8    9   64   25  216   49  512   81 1000  121"
  },
  {
    "objectID": "InputOutput.html#r-datasets",
    "href": "InputOutput.html#r-datasets",
    "title": "10  Data Input/Output",
    "section": "\n10.1 R datasets",
    "text": "10.1 R datasets\n\n10.1.1 Datasets included with R (in package ‘datasets’)\nList built-in datasets with data() and no arguments:\n\ndata()\n\nThese built-in datasets are normally readily available in the R console (because the datasets package is automatically loaded)\nYou can check if this is the case using search()\n\nsearch()\n\n [1] \".GlobalEnv\"        \"tools:quarto\"      \"package:stats\"    \n [4] \"package:graphics\"  \"package:grDevices\" \"package:utils\"    \n [7] \"package:datasets\"  \"package:methods\"   \"Autoloads\"        \n[10] \"package:base\"     \n\n\n\n10.1.2 Datasets included with other packages\nList a dataset included with some R package:\n\ndata(package = \"glmnet\")\ndata(package = \"MASS\")\ndata(package = \"mlbench\")\n\nLoad a dataset from some R package:\n\ndata(Sonar, package = \"mlbench\")\n\nNote: quotes around “Sonar” in the data() command above are optional."
  },
  {
    "objectID": "InputOutput.html#system-commands",
    "href": "InputOutput.html#system-commands",
    "title": "10  Data Input/Output",
    "section": "\n10.2 System commands",
    "text": "10.2 System commands\nGet working directory with getwd()\n\ngetwd()\n\nYou can set a different working directory with setwd()\nList files in current directory:\n\ndir()\n\nYou can execute a command of you operating system (OS) -i.e. MacOS, Linux, Windows- from within R using the system() function:\n\nsystem(\"uname -a\")\n\nNote: See issue here"
  },
  {
    "objectID": "InputOutput.html#data-io",
    "href": "InputOutput.html#data-io",
    "title": "10  Data Input/Output",
    "section": "\n10.3 Data I/O",
    "text": "10.3 Data I/O\n\n\n\n\nCommon Data Input/Output commands in R\n\n\n\n\n\n10.3.1 Read local CSV\nread.table() is the core function that reads data from formatted text files in R, where cases correspond to lines and variables to columns. Its many arguments allow to read different formats. read.csv() is an alias for read.table() that defaults to commas as separators and dots for decimal points. (Run read.csv in the console to print its source read the documentation with ?read.table).  Some important arguments for read.table() listed here with their default values for read.csv():\n\n\nsep = \",\": Character that separate entries. Default is a comma; use “ for tab-separated files (default setting in read.delim())\n\ndec = \".\": Character for the decimal point. Default is a dot; in some cases where a comma is used as the decimal point, the entry separator sep may be a semicolon (default setting in read.csv2())\n\nna.strings = \"NA\": Character vector of strings to be coded as “NA”\n\ncolClasses = NA: Either a character vector defining each column’s type (e.g. c(“character”, “numeric”, “numeric”) recycled as necessary or a named vector defining specific columns’ types (e.g. c(ICD9 = “character”, Sex = “factor”, SBP = “numeric”, DOB = “Date”)). Unspecified columns automatically determined. Note: Set a column to “NULL” (with quotes) to exclude column.\n\n\nmen <-  read.csv(\"../Data/pone.0204161.s001.csv\")\n\n\n10.3.2 Read data from the web\nread.csv() can directly read an online file. In the second example below, we also define that missing data is coded with ? using the na.strings argument:\n\nparkinsons <- read.csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data\")\n\nsleep <- read.csv(\"https://www.openml.org/data/get_csv/53273/sleep.arff\",\n                  na.strings = \"?\")\n\nThe above files are read from two very popular online data repositories. Confusingly, neither file ends in .csv, but they both work with read.csv(). You can always look at the plain text file to determine if it can work with read.table()/read.csv() and what settings to use.\n\n10.3.3 Read zipped data from the web\n\n10.3.3.1 using gzcon() and csv.read()\n\nread.table() /read.csv() also accepts a “connection” as input.\nHere we define a connection to a zipped file by nesting gzcon() and url():\n\ncon <- gzcon(url(\"https://github.com/EpistasisLab/pmlb/raw/master/datasets/breast_cancer_wisconsin/breast_cancer_wisconsin.tsv.gz\"),\n             text = TRUE)\n\nWe read the connection and specify the file is tab-separated, or call read.delim():\n\nbcw <- read.csv(con, header = TRUE, sep = \"\\t\")\n\n#same as\nbcw <- read.delim(con, header = TRUE)\n\n\n10.3.3.2 using data.table’s fread()\n\nYou can also use data.table’s fread(), which will directly handle zipped files:\n\nlibrary(data.table)\nbcw2 <- fread(\"https://github.com/EpistasisLab/penn-ml-benchmarks/raw/master/datasets/classification/breast-cancer-wisconsin/breast-cancer-wisconsin.tsv.gz\")\n\nIf you want to stick to using data frames, set the argument data.table to FALSE:\n\nbcw2 <- fread(\"https://github.com/EpistasisLab/penn-ml-benchmarks/raw/master/datasets/classification/breast-cancer-wisconsin/breast-cancer-wisconsin.tsv.gz\",\n              data.table = FALSE)\n\n\n10.3.4 Write to CSV\nUse the write.csv() function to write an R object (usually data frame or matrix) to a CSV file. Setting row.names = FALSE is usually a good idea. (Instead of storing data in rownames, it’s usually best to create a new column.)\n\nwrite.csv(iris, \"../Data/iris.csv\", row.names = FALSE)\n\nNote that in this case we did not need to save row names (which are just integers 1 to 150 and would add a useless extra column in the output)\n\n10.3.5 Read .xslx using openxlsx::read.xlsx()\n\nAs an example, we can read the csv we saved earlier into Excel and then save it as a .xlsx file.\n\niris.path <- normalizePath(\"../Data/iris.xlsx\")\niris2 <- openxlsx::read.xlsx(iris.path)\n\nNote: openxlsx::read.xlsx() does not work with a relative path like \"./Data/iris.xlsc\". Therefore we used the normalizePath() function to give us the full path of the file without having to type it out.\nCheck that the data is still identical:\n\nall(iris == iris2)\n\n\n10.3.6 Write an R object to RDS\nYou can write any R object directly to file so that you can recover it at any time, share it, etc. Remember that since a list can contain any number of objects of any type, you can save any collection of objects as an RDS file. For multiple objects, see also the save.image() command below.\n\nsaveRDS(iris, \"iris.rds\")\n\nTo load an object saved in an rds file, assign it to an object using readRDS():\n\niris_fromFile <- readRDS(\"iris.rds\")\nall(iris == iris_fromFile)\n\n\n10.3.7 Write multiple R objects to RData file using save()\n\n\nmat1 <- sapply(seq_len(10), function(i) rnorm(500))\nmat2 <- sapply(seq_len(10), function(i) rnorm(500))\nsave(mat1, mat2, file = \"./mat.RData\")\n\nNote: we will learn how to use sapply() later under “Loop functions”\nTo load the variables in the .RData file you saved, use the load() command:\n\nload(\"./Rmd/mat.RData\")\n\nNote that load() adds the objects to your workspace using with their original names. You do not assign them to a new object, unlike with the readRDS() call above.\n\n10.3.8 Write your entire workspace to a RData image using save.image()\n\nYou can save your entire workspace to a RData file using the save.image() function.\n\nsave.image(\"workspace_10_05_2020.RData\")\n\nSame as above, to re-load the workspace saved in the .RData file, use the load() command:\n\nload(\"workspace_10_05_2020.RData\")"
  },
  {
    "objectID": "ControlFlow.html",
    "href": "ControlFlow.html",
    "title": "12  Control flow",
    "section": "",
    "text": "Code is often not executed linearly (i.e. line-by-line). Control flow (or flow of control) operations define the order in which code segments are executed.\nExecution is often conditional (if - else or switch).\nSegments of code may be repeated multiple times (for) or as long as certain conditions are met (while).\nControl flow operations form some of the fundamental building blocks of programs. Each operation is very simple - combine enough of them and you can build up to any amount of complexity."
  },
  {
    "objectID": "ControlFlow.html#if---else",
    "href": "ControlFlow.html#if---else",
    "title": "12  Control flow",
    "section": "\n12.1 if - else:",
    "text": "12.1 if - else:\n\na <- 4\nif (a < 10) {\n  cat(\"a is not that big\")\n} else {\n  cat(\"a is not too small\")\n}\n\na is not that big"
  },
  {
    "objectID": "ControlFlow.html#if---else-if---else",
    "href": "ControlFlow.html#if---else-if---else",
    "title": "12  Control flow",
    "section": "\n12.2 if - else if - else:",
    "text": "12.2 if - else if - else:\n\na <- sample(seq(-2, 2, .5), 1)\na\n\n[1] 1\n\nif (a > 0) {\n  result <- \"positive\"\n} else if (a == 0) {\n  result <- \"zero\"\n} else {\n  result <- \"negative\"\n}\nresult\n\n[1] \"positive\""
  },
  {
    "objectID": "ControlFlow.html#conditional-assignment-with-if---else",
    "href": "ControlFlow.html#conditional-assignment-with-if---else",
    "title": "12  Control flow",
    "section": "\n12.3 Conditional assignment with if - else:",
    "text": "12.3 Conditional assignment with if - else:\nYou can use an if statement as part of an assignment:\n\na <- 8\ny <- if (a > 5) {\n  10\n} else {\n  0\n}"
  },
  {
    "objectID": "ControlFlow.html#conditional-assignment-with-ifelse",
    "href": "ControlFlow.html#conditional-assignment-with-ifelse",
    "title": "12  Control flow",
    "section": "\n12.4 Conditional assignment with ifelse:",
    "text": "12.4 Conditional assignment with ifelse:\n\na <- 3\n(y <- ifelse(a > 5, 10, 0))\n\n[1] 0\n\n\nifelse is vectorized:\n\na <- 1:10\n(y <- ifelse(a > 7, a^2, a))\n\n [1]   1   2   3   4   5   6   7  64  81 100"
  },
  {
    "objectID": "ControlFlow.html#for-loops",
    "href": "ControlFlow.html#for-loops",
    "title": "12  Control flow",
    "section": "12.5 for loops",
    "text": "12.5 for loops\n\n\n\nUse for loops to repeat execution of a block of code a certain number of times.\n\n\n\nThe for loop syntax is for (var in vector) expression.\nThe expression is usually surrounded by curly brackets and can include any number of lines, any amount of code:\n\nfor (i in 1:3) {\n  print(\"I love coffee\")\n}\n\n[1] \"I love coffee\"\n[1] \"I love coffee\"\n[1] \"I love coffee\"\n\n\nThe loop executes for length(vector) times.\nAt iteration i, var = vector[i].\nYou will often use the value of var inside the loop (but you don’t have to):\n\nfor (i in seq(10)) {\n  cat(i^2, \"\\n\")\n}\n\n1 \n4 \n9 \n16 \n25 \n36 \n49 \n64 \n81 \n100 \n\n\nletters is a built-in constant that includes all 26 lowercase letters of the Roman alphabet; LETTERS similarly includes all 26 uppercase letters.\n\nfor (letter in letters[1:5]) {\n  cat(letter, \"is a letter!\\n\")\n}\n\na is a letter!\nb is a letter!\nc is a letter!\nd is a letter!\ne is a letter!\n\n\n\n12.5.1 Working on data within a for loop\nA common scenario involves working on a data object, whether a vector, matrix, list, data.frame, and performing an operation on each elements, one at a time. While a lot of these operations are often performed using loop functions instead, for loops can certainly be used.\nYou can start by initializing an object of the appropriate class and dimensions to hold the output. Then, each iteration of the for loop will assign its output to the corresponding element/s of this object.\nIn the following example we transform the mtcars built-in dataset’s features to z-scores. The built-in command scale() will do this for quickly and conveniently, this is for demonstration purposes:\nFirst, initialize the output to be the desired class and dimensions:\n\nclass(mtcars)\n\n[1] \"data.frame\"\n\ndim(mtcars)\n\n[1] 32 11\n\nmtcars_z <- data.frame(matrix(0, 32, 11))\ncolnames(mtcars_z) <- colnames(mtcars)\n\nor, it is much simpler to just make a copy of mtcars to be overwritten by the for loop later:\n\nmtcars_z <- mtcars\n\nStandardization involves subtracting the mean and dividing by the standard deviation.\nHere is the for loop - we iterate through each column and assign the transformed data:\n\nfor (i in 1:ncol(mtcars)) {\n  mtcars_z[, i] <- (mtcars[, i] - mean(mtcars[, i])) / sd(mtcars[, i])\n}\n\nLet’s compare to the output of the scale() command by print the first 3 rows and columns of each:\n\nmtcars_z2 <- as.data.frame(scale(mtcars))\nmtcars_z[1:3, 1:3]\n\n                    mpg        cyl       disp\nMazda RX4     0.1508848 -0.1049878 -0.5706198\nMazda RX4 Wag 0.1508848 -0.1049878 -0.5706198\nDatsun 710    0.4495434 -1.2248578 -0.9901821\n\nmtcars_z2[1:3, 1:3]\n\n                    mpg        cyl       disp\nMazda RX4     0.1508848 -0.1049878 -0.5706198\nMazda RX4 Wag 0.1508848 -0.1049878 -0.5706198\nDatsun 710    0.4495434 -1.2248578 -0.9901821\n\n\nNote that we wrapped scale() around as.data.frame() because it outputs a matrix.\nWe can check that all elements are the same with all():\n\nall(mtcars_z == mtcars_z2)\n\n[1] FALSE\n\n\n\n\n12.5.2 Nested for loops\n\na <- matrix(1:9, 3)\nfor (i in seq(3)) {\n  for (j in seq(3)) {\n    cat(\"  a[\", i, \",\", j, \"] is \", a[i, j], \"\\n\", sep = \"\")\n  }\n}\n\n  a[1,1] is 1\n  a[1,2] is 4\n  a[1,3] is 7\n  a[2,1] is 2\n  a[2,2] is 5\n  a[2,3] is 8\n  a[3,1] is 3\n  a[3,2] is 6\n  a[3,3] is 9\n\n\n\n\n12.5.3 Printing within a for loop\nIn the R console objects get printed just by typing their name:\n\na <- 4\na\n\n[1] 4\n\n# same as\nprint(a)\n\n[1] 4\n\n\nThis “automatic printing” does not happen within a for loop, so you simply use print() (or cat() as preferred):\nThe following loop does not print out anything:\n\na <- 0\nfor (i in 1:4) {\n  a <- a + i^2\n  a\n}\n\nbut this does:\n\na <- 0\nfor (i in 1:4) {\n  a <- a + i^2\n  print(a)\n}\n\n[1] 1\n[1] 5\n[1] 14\n[1] 30"
  },
  {
    "objectID": "ControlFlow.html#select-one-of-multiple-alternatives-with-switch",
    "href": "ControlFlow.html#select-one-of-multiple-alternatives-with-switch",
    "title": "12  Control flow",
    "section": "\n12.6 Select one of multiple alternatives with switch\n",
    "text": "12.6 Select one of multiple alternatives with switch\n\nInstead of using multiple if - else if statements, we can build a more compact call using switch. (It is best suited for options that are of type character, rather than numeric)\n\ny <- sample(letters[seq(8)], 1)\ny\n\n[1] \"c\"\n\noutput <- switch(y,                      # 1. Some expression\n                 a = \"Well done\",        # 2. The possible values of the expression, unquoted\n                 b = \"Not bad\",          #    followed by the `=` and the conditional output\n                 c = \"Nice try\",\n                 d = \"Not a nice try\",\n                 e = \"This is bad\",\n                 f = \"Fail\",\n                 \"This is not even a possible grade\") # 3. An optional last argument is the default\n                                                      #    value, if there is no match above\noutput\n\n[1] \"Nice try\"\n\n\n\na <- rnorm(1)\na\n\n[1] 0.9914272\n\nout <- switch(as.integer(a > 0),\n              `1` = \"Input is positive\",\n              `0` = \"Input is not positive\")\nout\n\n[1] \"Input is positive\"\n\n\n\na <- rnorm(1)\na\n\n[1] 2.050325\n\nout <- switch(as.character(a > 0),\n              `TRUE` = \"Input is positive\",\n              `FALSE` = \"Input is not positive\")\nout\n\n[1] \"Input is positive\"\n\n\n\n12.6.1 switch example: HTTP Status Codes\n\nstatus <- sample(400:410, 1)\nstatus\n\n[1] 408\n\nresponse <- switch(as.character(status),\n                   `400` = \"Bad Request\",\n                   `401` = \"Unauthorized\",\n                   `402` = \"Payment Required\",\n                   `403` = \"Forbidden\",\n                   `404` = \"Not Found\",\n                   `405` = \"Method Not Allowed\",\n                   `406` = \"Not Acceptable\",\n                   `407` = \"Proxy Authentication Required\",\n                   `408` = \"Request Timeout\",\n                   `409` = \"Conflict\",\n                   `410` = \"Gone\")\nresponse\n\n[1] \"Request Timeout\""
  },
  {
    "objectID": "ControlFlow.html#while-loops",
    "href": "ControlFlow.html#while-loops",
    "title": "12  Control flow",
    "section": "\n12.7 while loops",
    "text": "12.7 while loops\n\na <- 10\nwhile (a > 0) {\n  a <- a - 1\n  cat(\"a is equal to\", a, \"\\n\")\n}\n\na is equal to 9 \na is equal to 8 \na is equal to 7 \na is equal to 6 \na is equal to 5 \na is equal to 4 \na is equal to 3 \na is equal to 2 \na is equal to 1 \na is equal to 0 \n\ncat(\"when all is said and done, a is\", a)\n\nwhen all is said and done, a is 0"
  },
  {
    "objectID": "ControlFlow.html#break-stops-execution-of-a-loop",
    "href": "ControlFlow.html#break-stops-execution-of-a-loop",
    "title": "12  Control flow",
    "section": "\n12.8 break stops execution of a loop:",
    "text": "12.8 break stops execution of a loop:\n\nfor (i in seq(10)) {\n  if (i == 5) break\n  cat(i, \"squared is\", i^2, \"\\n\")\n}\n\n1 squared is 1 \n2 squared is 4 \n3 squared is 9 \n4 squared is 16"
  },
  {
    "objectID": "ControlFlow.html#next-skips-the-current-iteration",
    "href": "ControlFlow.html#next-skips-the-current-iteration",
    "title": "12  Control flow",
    "section": "\n12.9 next skips the current iteration:",
    "text": "12.9 next skips the current iteration:\n\nfor (i in seq(7)) {\n  if (i == 5) next\n  cat(i, \"squared is\", i^2, \"\\n\")\n}\n\n1 squared is 1 \n2 squared is 4 \n3 squared is 9 \n4 squared is 16 \n6 squared is 36 \n7 squared is 49"
  },
  {
    "objectID": "ControlFlow.html#repeat-loops",
    "href": "ControlFlow.html#repeat-loops",
    "title": "12  Control flow",
    "section": "\n12.10 repeat loops",
    "text": "12.10 repeat loops\nA repeat block initiates an infinite loop and you must use break to exit.\n\ni <- 10\nrepeat {\n i <- i - 1\n if (i == 0) break\n cat(\"i is\", i, \"\\n\")\n}\n\ni is 9 \ni is 8 \ni is 7 \ni is 6 \ni is 5 \ni is 4 \ni is 3 \ni is 2 \ni is 1"
  },
  {
    "objectID": "Summarize.html",
    "href": "Summarize.html",
    "title": "13  Summarizing Data",
    "section": "",
    "text": "Let’s read in a dataset from OpenML:"
  },
  {
    "objectID": "Summarize.html#get-summary-of-an-r-object-with-summary",
    "href": "Summarize.html#get-summary-of-an-r-object-with-summary",
    "title": "13  Summarizing Data",
    "section": "13.1 Get summary of an R object with summary()",
    "text": "13.1 Get summary of an R object with summary()\nR includes summary() methods for a number of different objects.\n\nsummary(heart)\n\n      age            sex             chest_pain           trestbps    \n Min.   :28.00   Length:294         Length:294         Min.   : 92.0  \n 1st Qu.:42.00   Class :character   Class :character   1st Qu.:120.0  \n Median :49.00   Mode  :character   Mode  :character   Median :130.0  \n Mean   :47.83                                         Mean   :132.6  \n 3rd Qu.:54.00                                         3rd Qu.:140.0  \n Max.   :66.00                                         Max.   :200.0  \n                                                       NA's   :1      \n      chol           fbs              restecg             thalach     \n Min.   : 85.0   Length:294         Length:294         Min.   : 82.0  \n 1st Qu.:209.0   Class :character   Class :character   1st Qu.:122.0  \n Median :243.0   Mode  :character   Mode  :character   Median :140.0  \n Mean   :250.8                                         Mean   :139.1  \n 3rd Qu.:282.5                                         3rd Qu.:155.0  \n Max.   :603.0                                         Max.   :190.0  \n NA's   :23                                            NA's   :1      \n    exang              oldpeak          slope                 ca     \n Length:294         Min.   :0.0000   Length:294         Min.   :0    \n Class :character   1st Qu.:0.0000   Class :character   1st Qu.:0    \n Mode  :character   Median :0.0000   Mode  :character   Median :0    \n                    Mean   :0.5861                      Mean   :0    \n                    3rd Qu.:1.0000                      3rd Qu.:0    \n                    Max.   :5.0000                      Max.   :0    \n                                                        NA's   :291  \n     thal               num           \n Length:294         Length:294        \n Class :character   Class :character  \n Mode  :character   Mode  :character"
  },
  {
    "objectID": "Summarize.html#fast-builtin-column-and-row-operations",
    "href": "Summarize.html#fast-builtin-column-and-row-operations",
    "title": "13  Summarizing Data",
    "section": "13.2 Fast builtin column and row operations",
    "text": "13.2 Fast builtin column and row operations\nWe saw in Loop Functions how we can apply functions on rows, columns, or other subsets of our data. R has optimized builtin functions for some very common operations, with self-explanatory names:\n\ncolSums(): column sums\nrowSums(): row sums\ncolMeans(): column means\nrowMeans(): row means\n\n\na <- matrix(1:20, 5)\na\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    6   11   16\n[2,]    2    7   12   17\n[3,]    3    8   13   18\n[4,]    4    9   14   19\n[5,]    5   10   15   20\n\n\n\ncolSums(a)\n\n[1] 15 40 65 90\n\n# same as\napply(a, 2, sum)\n\n[1] 15 40 65 90\n\n\n\nrowSums(a)\n\n[1] 34 38 42 46 50\n\n# same as\napply(a, 1, sum)\n\n[1] 34 38 42 46 50\n\n\n\ncolMeans(a)\n\n[1]  3  8 13 18\n\n# same as\napply(a, 2, mean)\n\n[1]  3  8 13 18\n\n\n\nrowMeans(a)\n\n[1]  8.5  9.5 10.5 11.5 12.5\n\n# same as\napply(a, 1, mean)\n\n[1]  8.5  9.5 10.5 11.5 12.5"
  },
  {
    "objectID": "Summarize.html#optimized-matrix-operations-with-matrixstats",
    "href": "Summarize.html#optimized-matrix-operations-with-matrixstats",
    "title": "13  Summarizing Data",
    "section": "13.3 Optimized matrix operations with matrixStats",
    "text": "13.3 Optimized matrix operations with matrixStats\nWhile the builtin operations above are already optimized and faster than the equivalent calls, the matrixStats package (Bengtsson 2019) offers a number of futher optimized matrix operations, including drop-in replacements of the above. These should be prefered when dealing with bigger data:\n\nlibrary(matrixStats)\ncolSums2(a)\n\n[1] 15 40 65 90\n\nrowSums2(a)\n\n[1] 34 38 42 46 50\n\ncolMeans2(a)\n\n[1]  3  8 13 18\n\nrowMeans2(a)\n\n[1]  8.5  9.5 10.5 11.5 12.5\n\n\nNote: matrixStats provides replacement functions named almost identically to their base counterpart - so they are easy to find - but are different - so they don’t mask the base functions (this is important and good software design)."
  },
  {
    "objectID": "Summarize.html#see-alos",
    "href": "Summarize.html#see-alos",
    "title": "13  Summarizing Data",
    "section": "13.4 See alos",
    "text": "13.4 See alos\naggregate() for grouped summary statistics\n\n\n\n\nBengtsson, Henrik. 2019. matrixStats: Functions That Apply to Rows and Columns of Matrices (and to Vectors). https://CRAN.R-project.org/package=matrixStats."
  },
  {
    "objectID": "Aggregate.html",
    "href": "Aggregate.html",
    "title": "14  Aggregate",
    "section": "",
    "text": "aggregate() is a powerful way to apply functions on splits of your data. It can replicate functionality of the *apply() family, but can be more flexible. This may come with a performance penalty, only noticeable with big data, in which case it is recommended to use data.table for fast group-by data summarization.\naggregate() can work either with a formula notation or directly on data.frames and vectors. We show how to perform each operation below with either approach. The formula interface might be easier to work with interactivly on the console. While you can code with the formula interface, the regular approach is a lot more straightforward to do so.\nFor this example, we get the penguin data:\nSee example below for 1 or multiple variables by 1 or more groups using either the formula interface, or working directly on objects with $ indexing or using with():"
  },
  {
    "objectID": "Aggregate.html#single-variable-by-single-group",
    "href": "Aggregate.html#single-variable-by-single-group",
    "title": "14  Aggregate",
    "section": "14.1 Single variable by single group",
    "text": "14.1 Single variable by single group\nNote that the formula method defaults to na.action = na.omit\nUsing the formula interface:\n\naggregate(bill_length_mm ~ species,\n          penguins, mean)\n\n    species bill_length_mm\n1    Adelie       38.79139\n2 Chinstrap       48.83382\n3    Gentoo       47.50488\n\n\nDirectly working with vectors:\n\naggregate(penguins$bill_length_mm,\n          by = list(penguins$species),\n          mean, na.rm = T)\n\n    Group.1        x\n1    Adelie 38.79139\n2 Chinstrap 48.83382\n3    Gentoo 47.50488\n\n\nUsing with():\n\nwith(penguins,\n     aggregate(bill_length_mm,\n               by = list(species),\n               mean, na.rm = TRUE))\n\n    Group.1        x\n1    Adelie 38.79139\n2 Chinstrap 48.83382\n3    Gentoo 47.50488"
  },
  {
    "objectID": "Aggregate.html#multiple-variables-by-single-group",
    "href": "Aggregate.html#multiple-variables-by-single-group",
    "title": "14  Aggregate",
    "section": "14.2 Multiple variables by single group",
    "text": "14.2 Multiple variables by single group\n\naggregate(cbind(bill_length_mm, flipper_length_mm) ~ species,\n          penguins, mean)\n\n    species bill_length_mm flipper_length_mm\n1    Adelie       38.79139          189.9536\n2 Chinstrap       48.83382          195.8235\n3    Gentoo       47.50488          217.1870\n\n\n\naggregate(penguins[, c(\"bill_length_mm\", \"flipper_length_mm\")],\n          by = list(penguins$species),\n          mean, na.rm = TRUE)\n\n    Group.1 bill_length_mm flipper_length_mm\n1    Adelie       38.79139          189.9536\n2 Chinstrap       48.83382          195.8235\n3    Gentoo       47.50488          217.1870\n\n\n\nwith(penguins,\n     aggregate(cbind(bill_length_mm, flipper_length_mm),\n               by = list(species),\n               mean, na.rm = TRUE))\n\n    Group.1 bill_length_mm flipper_length_mm\n1    Adelie       38.79139          189.9536\n2 Chinstrap       48.83382          195.8235\n3    Gentoo       47.50488          217.1870"
  },
  {
    "objectID": "Aggregate.html#single-variable-by-multiple-groups",
    "href": "Aggregate.html#single-variable-by-multiple-groups",
    "title": "14  Aggregate",
    "section": "14.3 Single variable by multiple groups",
    "text": "14.3 Single variable by multiple groups\n\naggregate(bill_length_mm ~ species + island, penguins, mean)\n\n    species    island bill_length_mm\n1    Adelie    Biscoe       38.97500\n2    Gentoo    Biscoe       47.50488\n3    Adelie     Dream       38.50179\n4 Chinstrap     Dream       48.83382\n5    Adelie Torgersen       38.95098\n\n\n\naggregate(penguins$bill_length_mm,\n          by = list(penguins$species, penguins$island),\n          mean, na.rm = TRUE)\n\n    Group.1   Group.2        x\n1    Adelie    Biscoe 38.97500\n2    Gentoo    Biscoe 47.50488\n3    Adelie     Dream 38.50179\n4 Chinstrap     Dream 48.83382\n5    Adelie Torgersen 38.95098\n\n\n\nwith(penguins,\n     aggregate(bill_length_mm,\n               by = list(species, island),\n               mean, na.rm = TRUE))\n\n    Group.1   Group.2        x\n1    Adelie    Biscoe 38.97500\n2    Gentoo    Biscoe 47.50488\n3    Adelie     Dream 38.50179\n4 Chinstrap     Dream 48.83382\n5    Adelie Torgersen 38.95098"
  },
  {
    "objectID": "Aggregate.html#multiple-variables-by-multiple-groups",
    "href": "Aggregate.html#multiple-variables-by-multiple-groups",
    "title": "14  Aggregate",
    "section": "14.4 Multiple variables by multiple groups",
    "text": "14.4 Multiple variables by multiple groups\n\naggregate(cbind(bill_length_mm, flipper_length_mm) ~ species + island,\n          penguins, mean)\n\n    species    island bill_length_mm flipper_length_mm\n1    Adelie    Biscoe       38.97500          188.7955\n2    Gentoo    Biscoe       47.50488          217.1870\n3    Adelie     Dream       38.50179          189.7321\n4 Chinstrap     Dream       48.83382          195.8235\n5    Adelie Torgersen       38.95098          191.1961\n\n\n\naggregate(penguins[, c(\"bill_length_mm\", \"flipper_length_mm\")],\n          by = list(penguins$species, penguins$island),\n          mean, na.rm = TRUE)\n\n    Group.1   Group.2 bill_length_mm flipper_length_mm\n1    Adelie    Biscoe       38.97500          188.7955\n2    Gentoo    Biscoe       47.50488          217.1870\n3    Adelie     Dream       38.50179          189.7321\n4 Chinstrap     Dream       48.83382          195.8235\n5    Adelie Torgersen       38.95098          191.1961\n\n\n\nwith(penguins,\n     aggregate(cbind(bill_length_mm, flipper_length_mm),\n               by = list(species, island),\n               mean, na.rm = TRUE))\n\n    Group.1   Group.2 bill_length_mm flipper_length_mm\n1    Adelie    Biscoe       38.97500          188.7955\n2    Gentoo    Biscoe       47.50488          217.1870\n3    Adelie     Dream       38.50179          189.7321\n4 Chinstrap     Dream       48.83382          195.8235\n5    Adelie Torgersen       38.95098          191.1961"
  },
  {
    "objectID": "Aggregate.html#see-also",
    "href": "Aggregate.html#see-also",
    "title": "14  Aggregate",
    "section": "14.5 See also",
    "text": "14.5 See also\ntapply() for an alternative methods of applying function on subsets of a single variable (probably faster)"
  },
  {
    "objectID": "Functions.html",
    "href": "Functions.html",
    "title": "15  Functions",
    "section": "",
    "text": "Writing functions is a core part of programming.\nWhen should you write a function?\nWhenever you find yourself repeating pieces of code.\nWhy is it important?\nWriting functions helps reduce the total amount of code, which increases efficiency, reduces the chance of error, and can make code more readable.\n Functions in R are “first-class objects”.\nThis means they can be stored inside other objects (e.g. a list), they can be passed as arguments to other functions and can be returned as output from functions.  For example, you can use a command like apply(mat, 2, mean)  Functions in R are for the most part like mathematical functions: they have one or more inputs and one output. The inputs are known as the function arguments. If you want to return multiple outputs, you can return a list containing any number of R objects.  User-defined functions are refered to as “closures” in R. A closure is made of a function and its environment. Closures are distinct from primitive functions, which are internally implemented."
  },
  {
    "objectID": "Functions.html#simple-functions",
    "href": "Functions.html#simple-functions",
    "title": "15  Functions",
    "section": "15.1 Simple functions",
    "text": "15.1 Simple functions\nLet’s start with a very simple function: single argument with no default value:\n\nsquare <- function(x) {\n  x^2\n}\n\nsquare(3)\n\n[1] 9\n\n\nNotice above that x^2 is automatically returned by the function. It is the same as explicitly returning it with return():\n\nsquare <- function(x) {\n  out <- x^2\n  return(out)\n}\n\nsquare(4)\n\n[1] 16\n\n\nwhich is the same as:\n\nsquare <- function(x) {\n  out <- x^2\n  out\n}\n\nsquare(5)\n\n[1] 25\n\n\nA function returns either:\n\nan object passed to return()\nthe value of the last expression within the function definition such as out or x^2 above.\n\nreturn() is a way to end evaluation early:\n\nsquare.pos <- function(x) {\n  if (x > 0) {\n    return(x^2)\n  } else {\n    x\n  }\n  cat(\"The input was left unchanged\\n\")\n}\n\nx <- sample(-10:10, 1)\nx\n\n[1] -6\n\nsquare.pos(x)\n\nThe input was left unchanged\n\n\nMultiple arguments, with and without defaults:\n\nraise <- function(x, power = 2) {\n  x^power\n}\n\nx <- sample(10, 1)\nx\n\n[1] 3\n\nraise(x)\n\n[1] 9\n\nraise(x, power = 3)\n\n[1] 27\n\nraise(x, 3)\n\n[1] 27"
  },
  {
    "objectID": "Functions.html#argument-matching",
    "href": "Functions.html#argument-matching",
    "title": "15  Functions",
    "section": "15.2 Argument matching",
    "text": "15.2 Argument matching\nR will match unambiguous abbreviations of arguments:\n\nfn <- function(alpha = 2, beta = 3, gamma = 4) {\n  alpha * beta + gamma\n}\nfn(g = 2)\n\n[1] 8"
  },
  {
    "objectID": "Functions.html#arguments-with-prescribed-set-of-allowed-values",
    "href": "Functions.html#arguments-with-prescribed-set-of-allowed-values",
    "title": "15  Functions",
    "section": "15.3 Arguments with prescribed set of allowed values",
    "text": "15.3 Arguments with prescribed set of allowed values\nYou can match specific values for an argument using match.arg():\n\nmyfn <- function(type = c(\"alpha\", \"beta\", \"gamma\")) {\n  type <- match.arg(type)\n  cat(\"You have selected type '\", type, \"'\\n\", sep = \"\")\n}\n\nmyfn(\"a\")\n\nYou have selected type 'alpha'\n\nmyfn(\"b\")\n\nYou have selected type 'beta'\n\nmyfn(\"g\")\n\nYou have selected type 'gamma'\n\nmyfn(\"d\")\n\nError in match.arg(type): 'arg' should be one of \"alpha\", \"beta\", \"gamma\"\n\n\nAbove you see that partial matching using match.arg() was able to identify a valid option, and when there was no match, an informative error was printed.\nPartial matching is also automatically done on the argument names themselves, but it’s important to avoid depending on that.\n\nadsr <- function(attack = 100,\n                 decay = 250,\n                 sustain = 40,\n                 release = 1000) {\n  cat(\"Attack time:\", attack, \"ms\\n\",\n      \"Decay time:\", decay, \"ms\\n\",\n      \"Sustain level:\", sustain, \"\\n\",\n      \"Release time:\", release, \"ms\\n\")\n}\n\nadsr(50, s = 100, r = 500)\n\nAttack time: 50 ms\n Decay time: 250 ms\n Sustain level: 100 \n Release time: 500 ms"
  },
  {
    "objectID": "Functions.html#passing-extra-arguments-to-another-function-with-the-...-argument",
    "href": "Functions.html#passing-extra-arguments-to-another-function-with-the-...-argument",
    "title": "15  Functions",
    "section": "15.4 Passing extra arguments to another function with the ... argument",
    "text": "15.4 Passing extra arguments to another function with the ... argument\nMany functions include a ... argument at the end. Any arguments not otherwise matched are collected there. A common use for this is to pass them to another function:\n\ncplot <- function(x, y,\n                  cex = 1.5,\n                  pch = 16,\n                  col = \"#18A3AC\",\n                  bty = \"n\", ...) {\n  plot(x, y, cex = cex, pch = pch, col = col, bty = bty, ...)\n                  }\n\n... is also used for variable number of iputs, often as the first argument of a function. For example, look at the documentation of c, cat, cbind, rbind, paste\nNote: Any arguments after the ..., must be named fully, i.e. will not be partially matched."
  },
  {
    "objectID": "Functions.html#return-multiple-objects",
    "href": "Functions.html#return-multiple-objects",
    "title": "15  Functions",
    "section": "15.5 Return multiple objects",
    "text": "15.5 Return multiple objects\nR function can only return a single object. This is not much of a problem because you can simply put any collection of objects into a list and return it:\n\nlfn <- function(x, fn = square) {\n  xfn <- fn(x)\n  \n  list(x = x,\n       xfn = xfn,\n       fn = fn)\n}\n\nlfn(3)\n\n$x\n[1] 3\n\n$xfn\n[1] 9\n\n$fn\nfunction(x) {\n  out <- x^2\n  out\n}\n<bytecode: 0x11bd78030>"
  },
  {
    "objectID": "Functions.html#warnings-and-errors",
    "href": "Functions.html#warnings-and-errors",
    "title": "15  Functions",
    "section": "15.6 Warnings and errors",
    "text": "15.6 Warnings and errors\nYou can use warning(\"some warning message\") at any point inside a function to produce a warning message during execution. The message gets printed to the R console, but function execution is not stopped.\nOn the other hand, you can use stop(\"some error message\") to print an error message to console and stop function execution.\nThe following function (el10) calculates:\n\\[ e^{log_{10}(x)} \\]\n\nel10 <- function(x) {\n  exp(log10(x))\n}\n\nwhich is not defined for negative x. In this case, we could let R give a warning when it tries to compute log10(x):\n\nval1 <- el10(-3)\n\nWarning in el10(-3): NaNs produced\n\n\nWe could instead produce our own warning message:\n\nel10 <- function(x) {\n  if (x < 0) warning(\"x must be positive\")\n  exp(log10(x))\n}\nval2 <- el10(-3)\n\nWarning in el10(-3): x must be positive\n\n\nWarning in el10(-3): NaNs produced\n\nval2\n\n[1] NaN\n\n\nAs you see, the output (NaN) still gets returned.\nAlternatively, we can use stop() to end function execution:\n\nel10 <- function(x) {\n  if (x < 0) stop(\"x must be positive\")\n  exp(log10(x))\n}\nval3 <- el10(-3)\n\nError in el10(-3): x must be positive\n\n\nNote how, in this case, function evalutation is stopped and no value is returned."
  },
  {
    "objectID": "Functions.html#scoping",
    "href": "Functions.html#scoping",
    "title": "15  Functions",
    "section": "15.7 Scoping",
    "text": "15.7 Scoping\nFunctions exist in their own environment, i.e. contain their own variable definitions.\n\nx <- 3\ny <- 4\nfn <- function(x, y) {\n  x <- 10*x\n  y <- 20*y\n  cat(\"Inside the function, x = \", x, \" and y = \", y, \"\\n\")\n}\n\nfn(x, y)\n\nInside the function, x =  30  and y =  80 \n\ncat(\"Outside the function, x = \", x, \" and y = \", y, \"\\n\")\n\nOutside the function, x =  3  and y =  4 \n\n\nHowever, if a variable is referenced within a function but no local definition exists, the interpreter will look for the variable at the parent directory. It is best ensure all objects needed within a function are specified as arguments and passed appropriately when the function is called.\nIn the following example, x is only defined outside the function definition, but referenced within it.\n\nx <- 21\n\nitfn <- function(y, lr = 1) {\n  x + lr * y\n}\n\nitfn(3)\n\n[1] 24\n\n\n\n15.7.1 function vs. for loop\nLet’s z-score the built-in mtcars dataset once with a for loop and once with a custom function. This links back to the example seen earlier in the for loop section. In practice, this would be performed with the scale() command:\nWithin the for loop, we are assigning columns directly to the object initialized before the loop. In the following example, we use print(environment()) to print the environment outside and inside the loop function to show that it is the same. This is purely for demonstration:\n\n# initialize new object 'mtcars_z'\nmtcars_z <- mtcars\ncat(\"environment outside for loop is: \")\n\nenvironment outside for loop is: \n\nprint(environment())\n\n<environment: R_GlobalEnv>\n\n# z-score one column at a time in a for loop\nfor (i in 1:ncol(mtcars)) {\n  mtcars_z[, i] <- (mtcars[, i] - mean(mtcars[, i])) / sd(mtcars[, i])\n  cat(\"environment inside for loop is: \")\n  print(environment())\n}\n\nenvironment inside for loop is: <environment: R_GlobalEnv>\nenvironment inside for loop is: <environment: R_GlobalEnv>\nenvironment inside for loop is: <environment: R_GlobalEnv>\nenvironment inside for loop is: <environment: R_GlobalEnv>\nenvironment inside for loop is: <environment: R_GlobalEnv>\nenvironment inside for loop is: <environment: R_GlobalEnv>\nenvironment inside for loop is: <environment: R_GlobalEnv>\nenvironment inside for loop is: <environment: R_GlobalEnv>\nenvironment inside for loop is: <environment: R_GlobalEnv>\nenvironment inside for loop is: <environment: R_GlobalEnv>\nenvironment inside for loop is: <environment: R_GlobalEnv>\n\n\nIn contrast, all operations remain local within a function and the output must be returned:\n\nztransform <- function(x) {\n  cat(\"environment inside function body is: \")\n  print(environment())\n  z <- as.data.frame(sapply(mtcars, function(i) (i - mean(i))/sd(i)))\n  rownames(z) <- rownames(x)\n  z\n}\nmtcars_z2 <- ztransform(mtcars)\n\nenvironment inside function body is: <environment: 0x109a20a70>\n\ncat(\"environment outside function body is: \")\n\nenvironment outside function body is: \n\nprint(environment())\n\n<environment: R_GlobalEnv>\n\n\nNotice how the environment outside and inside the loop function is the same, it is the Global environemnt, but the environment within the function is different. That is why any objects created or changed within a function must be returned if we want to make them available."
  },
  {
    "objectID": "Functions.html#pipe",
    "href": "Functions.html#pipe",
    "title": "15  Functions",
    "section": "15.8 The pipe operator",
    "text": "15.8 The pipe operator\n\n\n\n\n\nIllustration of pipes in R\n\n\n\n\nA pipe operator was first introduced to R by the magrittr package with the %>% symbol. Note that a number of other packages that endorse the use of pipes export the pipe operator as well.\nStarting with R version 4.1, a native pipe operator is included with the |> symbol.\nA pipe allows writing f(x) as x |> f() (native pipe) or x %>% f (magrittr).\nNote that the native pipe requires parentheses, but magrittr works with or without them.\nA pipe is often used to:\n\navoid multiple temporary assignments in a multistep procedure, or\nas an alternative to nesting functions.\n\nSome packages and developers promote its use, others discourage it. You should try and see if/when it suits your needs.\nThe following:\n\nx <- f1(x)\nx <- f2(x)\nx <- f3(x)\n\nis equivalent to:\n\nx <- f3(f2(f1(x)))\n\nis equivalent to:\n\nx <- x |> f1() |> f2() |> f3()\n\n\niris[, -5] |>\n  split(iris$Species) |>\n  lapply(function(i) sapply(i, mean))\n\n$setosa\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n       5.006        3.428        1.462        0.246 \n\n$versicolor\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n       5.936        2.770        4.260        1.326 \n\n$virginica\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n       6.588        2.974        5.552        2.026 \n\n\nPipes are used extensively in the tidyverse packages and many other third-party packages.\nYou can learn more about the magrittr pipe operator in the vignette\n\n\n\nIn RStudio the keyboard shortcut for the pipe operator is Shift-Command-M (MacOS) or Ctrl-Shift-M (Windows)\n\n\n\n\n15.8.1 Differences between native pipe and magrittr\n\nnative pipe requires () after function name, magrittr works with or without them\n\n\nx <- rnorm(300)\n\n\nx |> mean()\n\n[1] -0.01902987\n\n\nbut this would fail:\n\nx |> mean\n\nwhile either works in magrittr\n\nlibrary(magrittr)\nx %>% mean()\n\n[1] -0.01902987\n\n\n\nx %>% mean\n\n[1] -0.01902987\n\n\n\nnative pipe by design only pipes its LHS to the first unnamed argument on the RHS. magrittr allows using a period . to pipe to any position on the RHS. The native pipe workaround is using an anonymous function (can use the new shorter syntax \\(x) instead of function(x))\n\ne.g.: Find the position of “r” in the latin alphabet\nIn this example, we want to pass the LHS to the second argument of grep().\nUsing native pipe, we name the first argument pattern and the LHS is passed to the first unnamed argument, i.e. the second (which is x, the character vector where matches are looked for)\n\nletters |> grep(pattern = \"r\")\n\n[1] 18\n\n\nwith magrittr you can use the dot notation to specify where to pipe into:\n\nletters %>% grep(\"r\", .)\n\n[1] 18\n\n\nFor demonstration, here’s the slightly involved way you would achieve this with an anonymous function and the native pipe. This may make sense for more complex calls.\n\nletters |> {\\(x) grep(\"r\", x)}()\n\n[1] 18"
  },
  {
    "objectID": "LoopFunctions.html",
    "href": "LoopFunctions.html",
    "title": "16  Loop Functions",
    "section": "",
    "text": "Loop functions are some of the most widely used R functions. They replace longer expressions created with a for loop, for example.\nThey can result in more compact and readable code and are often faster to execute than a for loop.\nBefore starting to use the above functions, we need to learn about anonymous functions, which are often used within the apply functions."
  },
  {
    "objectID": "LoopFunctions.html#apply",
    "href": "LoopFunctions.html#apply",
    "title": "16  Loop Functions",
    "section": "\n16.1 apply()\n",
    "text": "16.1 apply()\n\n\n\n\napply() applies a function over one or more dimensions of an array of 2 dimensions or more (this includes matrices) or a data frame:\n\n\napply(array, MARGIN, FUN)\n\n\n\nMARGIN can be an integer vector or character indicating the dimensions over which ‘FUN’ will be applied.\nBy convention, rows come first (just like in indexing), therefore:MARGIN = 1: apply function on each row MARGIN = 2: apply function on each column\nLet’s calculate the mean value of each of the first four columns of the iris dataset:\n\nx <- iris[, -5]\niris_column_mean <- apply(x, MARGIN = 2, FUN = mean) \niris_column_mean\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    5.843333     3.057333     3.758000     1.199333 \n\n\n\n\n\nHint: It is possibly easiest to think of the “MARGIN” as the dimension you want to keep. In the above case, we want the mean for each variable, i.e. we want to keep columns and collapse rows.\n\n\n\nThe above is equivalent to:\n\niris_column_mean <- numeric(ncol(x))\nnames(iris_column_mean) <- names(x)\n\nfor (i in seq(x)) {\n  iris_column_mean[i] <- mean(x[, i])\n}\niris_column_mean\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    5.843333     3.057333     3.758000     1.199333 \n\n\nIf you wanted to get the mean of the rows (makes little sense in this case):\n\nhead(apply(x, 1, mean))\n\n[1] 2.550 2.375 2.350 2.350 2.550 2.850\n\n\n\n\n\napply() only works on objects with defined (i.e. non-NULL) dim(), i.e. arrays.\n\n\n\n\n\n\nTry to think why you can’t use apply() to apply a function fn() on a vector v.\n\n\n…\n\n\n…\n\n\nBecause that would be fn(v)"
  },
  {
    "objectID": "LoopFunctions.html#lapply",
    "href": "LoopFunctions.html#lapply",
    "title": "16  Loop Functions",
    "section": "\n16.2 lapply()\n",
    "text": "16.2 lapply()\n\n\n\n\nlapply() applies a function on each element of its input and returns a list of the outputs.\n\n\n\nNote: The ‘elements’ of a data frame are its columns (remember, a data frame is a list with equal-length elements). The ‘elements’ of a matrix are each cell one by one, by column. Therefore lapply() has a very different effect on a data frame and a matrix. lapply() is commonly used to iterate over the columns of a data frame.\nlapply() is the only function of the *apply() family that always returns a list.\n\niris.median <- lapply(iris[, -5], median)\niris.median\n\n$Sepal.Length\n[1] 5.8\n\n$Sepal.Width\n[1] 3\n\n$Petal.Length\n[1] 4.35\n\n$Petal.Width\n[1] 1.3\n\n\nThe above is equivalent to:\n\niris.median <- vector(\"list\", 4)\nnames(iris.median) <- colnames(iris[, -5])\nfor (i in 1:4) {\n  iris.median[[i]] <- median(iris[, 1])\n}"
  },
  {
    "objectID": "LoopFunctions.html#sapply",
    "href": "LoopFunctions.html#sapply",
    "title": "16  Loop Functions",
    "section": "\n16.3 sapply()\n",
    "text": "16.3 sapply()\n\nsapply() is an alias for lapply(), followed by a call to simplify2array().\n(Check the source code for sapply() by typing sapply at the console and hitting Enter).\n\n\n\nUnlike lapply(), the output of sapply() is variable: it is the simplest R object that can hold the data type(s) resulting from the operations, i.e. a vector, matrix, data frame, or list.\n\n\n\n\niris.median <- sapply(iris[, -5], median)\niris.median\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n        5.80         3.00         4.35         1.30 \n\n\n\niris.summary <- data.frame(Mean = sapply(iris[, -5], mean),\n                           SD = sapply(iris[, -5], sd))\niris.summary\n\n                 Mean        SD\nSepal.Length 5.843333 0.8280661\nSepal.Width  3.057333 0.4358663\nPetal.Length 3.758000 1.7652982\nPetal.Width  1.199333 0.7622377"
  },
  {
    "objectID": "LoopFunctions.html#vapply",
    "href": "LoopFunctions.html#vapply",
    "title": "16  Loop Functions",
    "section": "\n16.4 vapply()\n",
    "text": "16.4 vapply()\n\nMuch less commonly used (possibly underused) than lapply() or sapply(), vapply() allows you to specify what the expected output looks like - for example a numeric vector of length 2, a character vector of length 1.\nThis can have two advantages:\n\nIt is safer against errors\nIt will sometimes be a little faster\n\nYou add the argument FUN.VALUE which must be of the correct type and length of the expected result of each iteration.\n\nvapply(iris[, -5], median, FUN.VALUE = .1)\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n        5.80         3.00         4.35         1.30 \n\n\nHere, each iteration returns the median of each column, i.e. a numeric vector of length 1. Therefore FUN.VALUE can be any numeric scalar.\nFor example, if we instead returned the range of each column, FUN.VALUE should be a numeric vector of length 2:\n\nvapply(iris[, -5], range, FUN.VALUE = rep(.1, 2))\n\n     Sepal.Length Sepal.Width Petal.Length Petal.Width\n[1,]          4.3         2.0          1.0         0.1\n[2,]          7.9         4.4          6.9         2.5\n\n\nIf FUN.VALUE does not match the returned value, we get an informative error:\n\nvapply(iris[, -5], range, FUN.VALUE = .1)\n\nError in vapply(iris[, -5], range, FUN.VALUE = 0.1): values must be length 1,\n but FUN(X[[1]]) result is length 2"
  },
  {
    "objectID": "LoopFunctions.html#tapply",
    "href": "LoopFunctions.html#tapply",
    "title": "16  Loop Functions",
    "section": "\n16.5 tapply()\n",
    "text": "16.5 tapply()\n\ntapply() is one way (of many) to apply a function on subgroups of data as defined by one or more factors.\nIn the following example, we calculate the mean Sepal.Length by species on the iris dataset:\n\nmean_Sepal.Length_by_Species <- tapply(iris$Sepal.Length, iris$Species, mean)\nmean_Sepal.Length_by_Species\n\n    setosa versicolor  virginica \n     5.006      5.936      6.588 \n\n\nThe above is equivalent to:\n\nspecies <- levels(iris$Species)\nmean_Sepal.Length_by_Species <- vector(\"numeric\", length(species))\nnames(mean_Sepal.Length_by_Species) <- species\n\nfor (i in seq(species)) {\n  mean_Sepal.Length_by_Species[i] <- \n    mean(iris$Sepal.Length[iris$Species == species[i]])\n}\nmean_Sepal.Length_by_Species\n\n    setosa versicolor  virginica \n     5.006      5.936      6.588"
  },
  {
    "objectID": "LoopFunctions.html#mapply",
    "href": "LoopFunctions.html#mapply",
    "title": "16  Loop Functions",
    "section": "\n16.6 mapply()\n",
    "text": "16.6 mapply()\n\nThe above functions all work well when you iterating over elements of a single object.mapply() allows you to execute a function that accepts two or more inputs, say fn(x, z) using the i-th element of each input, and will return:fn(x[1], z[1]), fn(x[2], z[2]), …, fn(x[n], z[n])\nLet’s create a simple function that accepts two numeric arguments, and two vectors length 5 each:\n\nraise <- function(x, power) x^power\nx <- 2:6\np <- 6:2\n\nUse mapply to raise each x to the corresponding p:\n\nout <- mapply(raise, x, p)\nout\n\n[1]  64 243 256 125  36\n\n\nThe above is equivalent to:\n\nout <- vector(\"numeric\", 5)\nfor (i in seq(5)) {\n  out[i] <- raise(x[i], p[i])\n}\nout\n\n[1]  64 243 256 125  36"
  },
  {
    "objectID": "LoopFunctions.html#iterating-over-a-sequence-instead-of-an-object",
    "href": "LoopFunctions.html#iterating-over-a-sequence-instead-of-an-object",
    "title": "16  Loop Functions",
    "section": "\n16.7 Iterating over a sequence instead of an object",
    "text": "16.7 Iterating over a sequence instead of an object\nWith lapply(), sapply() and vapply() there is a very simple trick that may often come in handy:\nInstead of iterating over elements of an object, you can iterate over an integer index of whichever elements you want to access and use it accordingly within the anonymous function.\nThis alternative approach is much closer to how we would use an integer sequence in a for loop.\nIt will be clearer through an example:\nGet the mean of the first four columns of iris:\n\n# original way: iterate through elements i.e. columns:\nsapply(iris[, -5], function(i) mean(i))\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    5.843333     3.057333     3.758000     1.199333 \n\n# alternative way: iterate over integer index of elements:\nsapply(1:4, function(i) mean(iris[, i]))\n\n[1] 5.843333 3.057333 3.758000 1.199333\n\n# equivalent to:\nfor (i in 1:4) {\n  mean(iris[, i])\n}\n\nNotice that in this approach, since you are not passing the object (iris, in the above example) as the input to lapply(), it needs to be accessed within the anonymous function."
  },
  {
    "objectID": "LoopFunctions.html#applying-on-matrices-vs.-data-frames",
    "href": "LoopFunctions.html#applying-on-matrices-vs.-data-frames",
    "title": "16  Loop Functions",
    "section": "\n16.8 *apply()ing on matrices vs. data frames",
    "text": "16.8 *apply()ing on matrices vs. data frames\nTo consolidate some of what was learned above, let’s focus on the difference between working on a matrix vs. a data frame.\nFirst, let’s create a matrix and a data frame with the same data:\n\namat <- matrix(21:70, 10)\ncolnames(amat) <- paste0(\"Feature_\", 1:ncol(amat))\namat\n\n      Feature_1 Feature_2 Feature_3 Feature_4 Feature_5\n [1,]        21        31        41        51        61\n [2,]        22        32        42        52        62\n [3,]        23        33        43        53        63\n [4,]        24        34        44        54        64\n [5,]        25        35        45        55        65\n [6,]        26        36        46        56        66\n [7,]        27        37        47        57        67\n [8,]        28        38        48        58        68\n [9,]        29        39        49        59        69\n[10,]        30        40        50        60        70\n\nadf <- as.data.frame(amat)\nadf\n\n   Feature_1 Feature_2 Feature_3 Feature_4 Feature_5\n1         21        31        41        51        61\n2         22        32        42        52        62\n3         23        33        43        53        63\n4         24        34        44        54        64\n5         25        35        45        55        65\n6         26        36        46        56        66\n7         27        37        47        57        67\n8         28        38        48        58        68\n9         29        39        49        59        69\n10        30        40        50        60        70\n\n\nWe’ve seen that with apply() we specify the dimension to operate on and it works the same way on both matrices and data frames:\n\napply(amat, 2, mean)\n\nFeature_1 Feature_2 Feature_3 Feature_4 Feature_5 \n     25.5      35.5      45.5      55.5      65.5 \n\napply(adf, 2, mean)\n\nFeature_1 Feature_2 Feature_3 Feature_4 Feature_5 \n     25.5      35.5      45.5      55.5      65.5 \n\n\nHowever, sapply() (and lapply(), vapply()) acts on each element of the object, therefore it is not meaningful to pass a matrix to it:\n\nsapply(amat, mean)\n\n [1] 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45\n[26] 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70\n\n\nThe above returns the mean of each element, i.e. the element itself, which is pointless.\nSince a data frame is a list, and its columns are its elements, it works great for column operations on data frames:\n\nsapply(adf, mean)\n\nFeature_1 Feature_2 Feature_3 Feature_4 Feature_5 \n     25.5      35.5      45.5      55.5      65.5 \n\n\nIf you want to use sapply() on a matrix, you could iterate over an integer sequence as shown in the previous section:\n\nsapply(1:ncol(amat), function(i) mean(amat[, i]))\n\n[1] 25.5 35.5 45.5 55.5 65.5\n\n\nThis is shown to help emphasize the differences between the function and the data structures. In practice, you would use apply() on a matrix."
  },
  {
    "objectID": "LoopFunctions.html#anonfns",
    "href": "LoopFunctions.html#anonfns",
    "title": "16  Loop Functions",
    "section": "\n16.9 Anonymous functions",
    "text": "16.9 Anonymous functions\nAnonymous functions are just like regular functions but they are not assigned to an object - i.e. they are not “named”.\nThey are usually passed as arguments to other functions to be used once, hence no need to name them.\nIn R, anonymous functions are often used with the apply family of functions.\nExample of a simple regular function:\n\nsquared <- function(x) {\n  x^2\n}\n\nBecause this is a short function definition, it can also be written in a single line without curly brackets:\n\nsquared <- function(x) x^2\n\nThe equivalent anonymous function is the same, but omitting the assignment:\n\nfunction(x) x^2\n\nfunction(x) x^2\n\n\nLet’s use the squared() function within sapply() to square the first four columns of the iris dataset. In these examples, we often wrap functions around head() which prints the first few lines of an object to avoid:\n\nhead(iris[, 1:4])\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1          5.1         3.5          1.4         0.2\n2          4.9         3.0          1.4         0.2\n3          4.7         3.2          1.3         0.2\n4          4.6         3.1          1.5         0.2\n5          5.0         3.6          1.4         0.2\n6          5.4         3.9          1.7         0.4\n\niris_sq <- sapply(iris[, 1:4], squared)\nhead(iris_sq)\n\n     Sepal.Length Sepal.Width Petal.Length Petal.Width\n[1,]        26.01       12.25         1.96        0.04\n[2,]        24.01        9.00         1.96        0.04\n[3,]        22.09       10.24         1.69        0.04\n[4,]        21.16        9.61         2.25        0.04\n[5,]        25.00       12.96         1.96        0.04\n[6,]        29.16       15.21         2.89        0.16\n\n\nLet’s do the same as above, but this time using an anonymous function:\n\niris_sqtoo <- sapply(iris[, 1:4], function(x) x^2)\nhead(iris_sqtoo)\n\n     Sepal.Length Sepal.Width Petal.Length Petal.Width\n[1,]        26.01       12.25         1.96        0.04\n[2,]        24.01        9.00         1.96        0.04\n[3,]        22.09       10.24         1.69        0.04\n[4,]        21.16        9.61         2.25        0.04\n[5,]        25.00       12.96         1.96        0.04\n[6,]        29.16       15.21         2.89        0.16\n\n\nThe entire anonymous function definition is passed in the function argument (FUN in the R documentation)."
  },
  {
    "objectID": "DataFrames.html",
    "href": "DataFrames.html",
    "title": "17  Data frames",
    "section": "",
    "text": "R’s data.frame is the main object used to hold data for statistical analysis, visualization, and predictive modeling.\nSee the Data frames section in the Data Structures chapter for basic info on data.frames."
  },
  {
    "objectID": "DataFrames.html#column-and-row-names",
    "href": "DataFrames.html#column-and-row-names",
    "title": "17  Data frames",
    "section": "17.1 Column and row names",
    "text": "17.1 Column and row names\nLet’s start with a simple example data.frame:\n\ndf <- data.frame(PID = c(111:119),\n                 Hospital = c(\"UCSF\", \"HUP\", \"Stanford\",\n                             \"Stanford\", \"UCSF\", \"HUP\", \n                             \"HUP\", \"Stanford\", \"UCSF\"),\n                Age = c(22, 34, 41, 19, 53, 21, 63, 22, 19),\n                Sex = c(1, 1, 0, 1, 0, 0, 1, 0, 0))\ndf\n\n  PID Hospital Age Sex\n1 111     UCSF  22   1\n2 112      HUP  34   1\n3 113 Stanford  41   0\n4 114 Stanford  19   1\n5 115     UCSF  53   0\n6 116      HUP  21   0\n7 117      HUP  63   1\n8 118 Stanford  22   0\n9 119     UCSF  19   0\n\n\nThe optional row.names argument (see data.frame usage in the R documentation) can be used to define row names at the time of the data frame creation. It accepts either - a single integer or a character specifying a column of the data.frame being created whose values should be used as row names, or - a vector of values (character or integer) of the row names to be used\nFor example, we can use the “PID” column:\n\ndf <- data.frame(PID = c(111:119),\n                 Hospital = c(\"UCSF\", \"HUP\", \"Stanford\",\n                             \"Stanford\", \"UCSF\", \"HUP\", \n                             \"HUP\", \"Stanford\", \"UCSF\"),\n                Age = c(22, 34, 41, 19, 53, 21, 63, 22, 19),\n                Sex = c(1, 1, 0, 1, 0, 0, 1, 0, 0),\n                row.names = \"PID\")\n\n\n\n\nIt is recommended to not use/depend on row names to identify or index data.frames, and instead include a column of case IDs.\n\n\n\nWe can get column names and row names with colnames() and rownames(), respectively:\n\ncolnames(df)\n\n[1] \"Hospital\" \"Age\"      \"Sex\"     \n\nrownames(df)\n\n[1] \"111\" \"112\" \"113\" \"114\" \"115\" \"116\" \"117\" \"118\" \"119\"\n\n\nTo set new column or row names use the form:\ncolnames(df) <- new.colnames\nrownames(df) <- new.rownames\nwhere new.colnames and new.rownames is a character vector.\nYou can rename all columns/rows or use indexing to replace specific names:\nRename all rows:\n\nrownames(df) <- paste0(\"Patient_\", 1:9)\ndf\n\n          Hospital Age Sex\nPatient_1     UCSF  22   1\nPatient_2      HUP  34   1\nPatient_3 Stanford  41   0\nPatient_4 Stanford  19   1\nPatient_5     UCSF  53   0\nPatient_6      HUP  21   0\nPatient_7      HUP  63   1\nPatient_8 Stanford  22   0\nPatient_9     UCSF  19   0\n\n\nRename first two columns:\n\ncolnames(df)[1:2] <- c(\"Center\", \"Age_at_Dx\")\ndf\n\n            Center Age_at_Dx Sex\nPatient_1     UCSF        22   1\nPatient_2      HUP        34   1\nPatient_3 Stanford        41   0\nPatient_4 Stanford        19   1\nPatient_5     UCSF        53   0\nPatient_6      HUP        21   0\nPatient_7      HUP        63   1\nPatient_8 Stanford        22   0\nPatient_9     UCSF        19   0"
  },
  {
    "objectID": "DataFrames.html#delete-columns-or-rows",
    "href": "DataFrames.html#delete-columns-or-rows",
    "title": "17  Data frames",
    "section": "17.2 Delete columns or rows",
    "text": "17.2 Delete columns or rows\nTo delete a data.frame column, set it to NULL:\n\ndf$Sex <- NULL\ndf\n\n            Center Age_at_Dx\nPatient_1     UCSF        22\nPatient_2      HUP        34\nPatient_3 Stanford        41\nPatient_4 Stanford        19\nPatient_5     UCSF        53\nPatient_6      HUP        21\nPatient_7      HUP        63\nPatient_8 Stanford        22\nPatient_9     UCSF        19\n\n\nTo delete a data.frame row, you can “index it out”.\nFor example, to remove the third and fifths rows of the above data.frame using an integer index:\n\ndf <- df[-c(3, 5), ]\ndf\n\n            Center Age_at_Dx\nPatient_1     UCSF        22\nPatient_2      HUP        34\nPatient_4 Stanford        19\nPatient_6      HUP        21\nPatient_7      HUP        63\nPatient_8 Stanford        22\nPatient_9     UCSF        19\n\n\nYou can similarly exclude a row using a logical index. Logical indexing occurs usually following some filtering condition.\nFor example, exclude patients under 20 years old:\n\ndf <- df[!df$Age < 20, ]\ndf\n\n            Center Age_at_Dx\nPatient_1     UCSF        22\nPatient_2      HUP        34\nPatient_6      HUP        21\nPatient_7      HUP        63\nPatient_8 Stanford        22"
  },
  {
    "objectID": "DataFrames.html#subset",
    "href": "DataFrames.html#subset",
    "title": "17  Data frames",
    "section": "17.3 subset()",
    "text": "17.3 subset()\nsubset() allows you to\n\nfilter cases that meet certain conditions using the subset argument\nselect columns using the select argument\n\n(head() returns the first few lines of a data frame. We use it to avoid printing too many lines)\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\niris_sl.gt.med <- subset(iris, Sepal.Length > median(Sepal.Length))\n\nNote: You can use the column name Sepal.Length directly, i.e. unquoted and you don’t need to use iris$Sepal.Length. This is called Non-Standard Evaluation (NSE).\n\nx <- data.frame(one = 1:10,\n                two = rnorm(10),\n                group = c(rep(\"alpha\", 4),  rep(\"beta\", 6)))\nsubset(x, subset = two > 0, select = two)\n\n          two\n1  0.69580902\n3  1.50594672\n4  0.81149830\n5  0.00194977\n8  0.11582764\n9  0.62433447\n10 0.28292049\n\nsubset(x, two > 0, -one)\n\n          two group\n1  0.69580902 alpha\n3  1.50594672 alpha\n4  0.81149830 alpha\n5  0.00194977  beta\n8  0.11582764  beta\n9  0.62433447  beta\n10 0.28292049  beta\n\nsubset(x, two > 0, two:one)\n\n          two one\n1  0.69580902   1\n3  1.50594672   3\n4  0.81149830   4\n5  0.00194977   5\n8  0.11582764   8\n9  0.62433447   9\n10 0.28292049  10\n\nsubset(x, two > 0, two:group)\n\n          two group\n1  0.69580902 alpha\n3  1.50594672 alpha\n4  0.81149830 alpha\n5  0.00194977  beta\n8  0.11582764  beta\n9  0.62433447  beta\n10 0.28292049  beta"
  },
  {
    "objectID": "DataFrames.html#split",
    "href": "DataFrames.html#split",
    "title": "17  Data frames",
    "section": "17.4 split()",
    "text": "17.4 split()\nSplit a data frame into multiple data frames by groups defined by a factor:\n\nx_by_group <- split(x, x$group)\nx_by_group\n\n$alpha\n  one        two group\n1   1  0.6958090 alpha\n2   2 -0.1740827 alpha\n3   3  1.5059467 alpha\n4   4  0.8114983 alpha\n\n$beta\n   one         two group\n5    5  0.00194977  beta\n6    6 -0.11111405  beta\n7    7 -1.09063363  beta\n8    8  0.11582764  beta\n9    9  0.62433447  beta\n10  10  0.28292049  beta"
  },
  {
    "objectID": "DataFrames.html#with",
    "href": "DataFrames.html#with",
    "title": "17  Data frames",
    "section": "17.5 with()",
    "text": "17.5 with()\nWithin a with() expression, you can access list elements or data.frame columns without quoting or using the $ operator:\n\nwith(x, one + two)\n\n [1]  1.695809  1.825917  4.505947  4.811498  5.001950  5.888886  5.909366\n [8]  8.115828  9.624334 10.282920\n\nwith(x, x[group == \"alpha\", ])\n\n  one        two group\n1   1  0.6958090 alpha\n2   2 -0.1740827 alpha\n3   3  1.5059467 alpha\n4   4  0.8114983 alpha\n\nwith(x, x[two > 0, ])\n\n   one        two group\n1    1 0.69580902 alpha\n3    3 1.50594672 alpha\n4    4 0.81149830 alpha\n5    5 0.00194977  beta\n8    8 0.11582764  beta\n9    9 0.62433447  beta\n10  10 0.28292049  beta"
  },
  {
    "objectID": "DataFrames.html#feature-transformation-with-transform",
    "href": "DataFrames.html#feature-transformation-with-transform",
    "title": "17  Data frames",
    "section": "17.6 Feature transformation with transform()",
    "text": "17.6 Feature transformation with transform()\nMake up some data:\n\ndat <- data.frame(Sex = c(0, 0, 1, 1, 0),\n                  Height = c(1.5, 1.6, 1.55, 1.73, 1.8),\n                  Weight = c(55, 70, 69, 76, 91))\n\n\ndat <- transform(dat, BMI = Weight/Height^2)\ndat\n\n  Sex Height Weight      BMI\n1   0   1.50     55 24.44444\n2   0   1.60     70 27.34375\n3   1   1.55     69 28.72008\n4   1   1.73     76 25.39343\n5   0   1.80     91 28.08642\n\n\ntransform() is probably not used too often, because it is trivial to do the same with direct assignment:\n\ndat$BMI <- dat$Weight/dat$Height^2\n\nbut can be useful when adding multiple variables and/or used in a pipe:\n\ndat |> \n  subset(Sex == 0) |> \n  transform(DeltaWeightFromMean = Weight - mean(Weight),\n            BMI = Weight/Height^2,\n            CI = Weight/Height^3)\n\n  Sex Height Weight      BMI DeltaWeightFromMean       CI\n1   0    1.5     55 24.44444                 -17 16.29630\n2   0    1.6     70 27.34375                  -2 17.08984\n5   0    1.8     91 28.08642                  19 15.60357"
  },
  {
    "objectID": "DataFrames.html#identify-and-remove-duplicated-row-with-duplicated-and-unique",
    "href": "DataFrames.html#identify-and-remove-duplicated-row-with-duplicated-and-unique",
    "title": "17  Data frames",
    "section": "17.7 Identify and remove duplicated row with duplicated() and unique()",
    "text": "17.7 Identify and remove duplicated row with duplicated() and unique()\nThe duplicated() function when applied on a data.frame returns a logical index specifying the location of duplicated rows - specifically, of row which are the duplicate of another row further up the data.frame. This means that if rows 20 and 23 are identical, duplicated() will return TRUE for row 23.\nOn the other hand, unique() will remove duplicate rows from a data.frame.\n\nx <- data.frame(ID = c(203, 808, 909, 707, 808),\n                Age = c(23, 44, 33, 42, 44))\n\n\nduplicated(x)\n\n[1] FALSE FALSE FALSE FALSE  TRUE\n\n\n\nunique(x)\n\n   ID Age\n1 203  23\n2 808  44\n3 909  33\n4 707  42"
  },
  {
    "objectID": "TableJoins.html",
    "href": "TableJoins.html",
    "title": "18  Table joins",
    "section": "",
    "text": "We often have data from separate sources that we want to combine into a single data.frame. Table joins allow you to specify how to perform such a merge.\nScenario: You have received two tables with clinical data. Each table contains a column with a unique identifier (ID) plus a number of variables which are unique to each table. You want to merge them into one big table so that for each ID you have all available variables. You want to make sure that the same ID number (e.g. 108) corresponds to the same case in both datasets, but not all IDs needs to be present in both datasets.\nLet’s make up some synthetic data:\nThere are four main types of join operations:"
  },
  {
    "objectID": "TableJoins.html#merge",
    "href": "TableJoins.html#merge",
    "title": "18  Table joins",
    "section": "18.1 merge()",
    "text": "18.1 merge()\nThe merge() command in R is used to perform table joins.\nSyntax:\nmerge(x, y, by)\nwhere x and y and the two data.frames to join, and by is the column name of the ID variable used to identify rows. If the two datasets’ ID column has a different name, e.g. “PatientID” in one and “PID” in the other, you can either rename one of them, or probably best, use the following syntax:\nmerge(x, y, by.x, by.y)\nwhere by.x should be the name of the ID column for the x dataset and by.y should be the name of the ID column for the y dataset.\nIf you do not specify by or by.x and by.y arguments, merge() defaults to using the intersection of column names of the two input datasets. Look at the merge()’s documentation:\nby = intersect(names(x), names(y))\nIn our example datasets above, this works as expected and identifies “PID” as the common column:\n\nintersect(names(a), names(b))\n\n[1] \"PID\""
  },
  {
    "objectID": "TableJoins.html#inner-join",
    "href": "TableJoins.html#inner-join",
    "title": "18  Table joins",
    "section": "18.2 Inner join",
    "text": "18.2 Inner join\nThe default arguments of merge() perform an inner join:\n\n(ab.inner <- merge(a, b))\n\n  PID Hospital Age Sex  V1 Department\n1 106      HUP  21   0 153  Neurology\n2 107      HUP  63   1  89  Radiology\n3 108 Stanford  22   0 112  Emergency\n4 109     UCSF  19   0 228 Cardiology\n\n# same as\n(ab.inner <- merge(a, b, by = \"PID\"))\n\n  PID Hospital Age Sex  V1 Department\n1 106      HUP  21   0 153  Neurology\n2 107      HUP  63   1  89  Radiology\n3 108 Stanford  22   0 112  Emergency\n4 109     UCSF  19   0 228 Cardiology\n\n# same as\n(ab.inner <- merge(a, b, all = FALSE))\n\n  PID Hospital Age Sex  V1 Department\n1 106      HUP  21   0 153  Neurology\n2 107      HUP  63   1  89  Radiology\n3 108 Stanford  22   0 112  Emergency\n4 109     UCSF  19   0 228 Cardiology\n\n\nNote that the resulting table only contains cases found in both datasets, i.e. IDs 106 through 109"
  },
  {
    "objectID": "TableJoins.html#outer-join",
    "href": "TableJoins.html#outer-join",
    "title": "18  Table joins",
    "section": "18.3 Outer join",
    "text": "18.3 Outer join\nYou can perform an outer join by specifying all = TRUE:\n\n(ab.outer <- merge(a, b, all = TRUE))\n\n   PID Hospital Age Sex  V1 Department\n1  101     UCSF  22   1  NA       <NA>\n2  102      HUP  34   1  NA       <NA>\n3  103 Stanford  41   0  NA       <NA>\n4  104 Stanford  19   1  NA       <NA>\n5  105     UCSF  53   0  NA       <NA>\n6  106      HUP  21   0 153  Neurology\n7  107      HUP  63   1  89  Radiology\n8  108 Stanford  22   0 112  Emergency\n9  109     UCSF  19   0 228 Cardiology\n10 110     <NA>  NA  NA  91    Surgery\n11 111     <NA>  NA  NA 190  Neurology\n12 112     <NA>  NA  NA 101 Psychiatry\n\n(ab.outer <- merge(a, b, by = \"PID\", all = TRUE))\n\n   PID Hospital Age Sex  V1 Department\n1  101     UCSF  22   1  NA       <NA>\n2  102      HUP  34   1  NA       <NA>\n3  103 Stanford  41   0  NA       <NA>\n4  104 Stanford  19   1  NA       <NA>\n5  105     UCSF  53   0  NA       <NA>\n6  106      HUP  21   0 153  Neurology\n7  107      HUP  63   1  89  Radiology\n8  108 Stanford  22   0 112  Emergency\n9  109     UCSF  19   0 228 Cardiology\n10 110     <NA>  NA  NA  91    Surgery\n11 111     <NA>  NA  NA 190  Neurology\n12 112     <NA>  NA  NA 101 Psychiatry\n\n\nNote that the resulting data frame contains all cases found in either dataset and missing values are represented with NA."
  },
  {
    "objectID": "TableJoins.html#left-outer-join",
    "href": "TableJoins.html#left-outer-join",
    "title": "18  Table joins",
    "section": "18.4 Left outer join",
    "text": "18.4 Left outer join\nYou can perform a left outer join by specifying all.x = TRUE:\n\n(ab.leftOuter <- merge(a, b, all.x = TRUE))\n\n  PID Hospital Age Sex  V1 Department\n1 101     UCSF  22   1  NA       <NA>\n2 102      HUP  34   1  NA       <NA>\n3 103 Stanford  41   0  NA       <NA>\n4 104 Stanford  19   1  NA       <NA>\n5 105     UCSF  53   0  NA       <NA>\n6 106      HUP  21   0 153  Neurology\n7 107      HUP  63   1  89  Radiology\n8 108 Stanford  22   0 112  Emergency\n9 109     UCSF  19   0 228 Cardiology\n\n\nNote that the resulting data frame contains all cases present in the left input dataset (i.e. the one defined first in the arguments) only."
  },
  {
    "objectID": "TableJoins.html#right-outer-join",
    "href": "TableJoins.html#right-outer-join",
    "title": "18  Table joins",
    "section": "18.5 Right outer join",
    "text": "18.5 Right outer join\nYou can perform a right outer join by specifying all.y = TRUE:\n\n(ab.rightOuter <- merge(a, b, all.y = TRUE))\n\n  PID Hospital Age Sex  V1 Department\n1 106      HUP  21   0 153  Neurology\n2 107      HUP  63   1  89  Radiology\n3 108 Stanford  22   0 112  Emergency\n4 109     UCSF  19   0 228 Cardiology\n5 110     <NA>  NA  NA  91    Surgery\n6 111     <NA>  NA  NA 190  Neurology\n7 112     <NA>  NA  NA 101 Psychiatry\n\n\nNote how the resulting data frame contains all cases present in the right input dataset (i.e. the one defined seecond in the arguments) only."
  },
  {
    "objectID": "TableJoins.html#specifying-columns",
    "href": "TableJoins.html#specifying-columns",
    "title": "18  Table joins",
    "section": "18.6 Specifying columns",
    "text": "18.6 Specifying columns\nAs mentioned above, if the ID columns in the two data.frames to be merged do not have the same name, you can specify them directly:\n\na <- data.frame(PID = c(101:109),\n                Hospital = c(\"UCSF\", \"HUP\", \"Stanford\",\n                             \"Stanford\", \"UCSF\", \"HUP\", \n                             \"HUP\", \"Stanford\", \"UCSF\"),\n                Age = c(22, 34, 41, 19, 53, 21, 63, 22, 19),\n                Sex = c(1, 1, 0, 1, 0, 0, 1, 0, 0))\n\nb <- data.frame(PatientID = c(106:112),\n                 V1 = c(153, 89, 112, 228,  91, 190, 101),\n                 Department = c(\"Neurology\", \"Radiology\",\n                                \"Emergency\", \"Cardiology\",\n                                \"Surgery\", \"Neurology\", \"Psychiatry\"))\n\n\nmerge(a, b, by.x = \"PID\", by.y = \"PatientID\")\n\n  PID Hospital Age Sex  V1 Department\n1 106      HUP  21   0 153  Neurology\n2 107      HUP  63   1  89  Radiology\n3 108 Stanford  22   0 112  Emergency\n4 109     UCSF  19   0 228 Cardiology"
  },
  {
    "objectID": "Reshaping.html",
    "href": "Reshaping.html",
    "title": "19  Reshaping",
    "section": "",
    "text": "Wide and Long data format example. Take a moment to notice how the wide table on the left with 3 cases (3 IDs) and 3 variables gets converted from a 3 x 4 table to a 9 x 3 long table on the right. The values (outlined in magenta) are present once in each table: on the wide table they form an ID x Variable matrix, while on the long they are stacked on a single column. The IDs have to be repeated on the long table, once for each variable and there is a new ‘Variable’ column to provide the information present in the wide table’s column names.\nA wide dataset contains only a single row per case (e.g. patient), while a long dataset can contain multiple rows per case (e.g. for multiple timepoints). We want to be able to reshape from one form to the other because different programs (e.g. statistical models, visualization) may expect data in one of the other format for different applications (e.g. longitudinal modeling or grouped visualizations)."
  },
  {
    "objectID": "Reshaping.html#wide-to-long",
    "href": "Reshaping.html#wide-to-long",
    "title": "19  Reshaping",
    "section": "19.1 Wide to Long",
    "text": "19.1 Wide to Long\nLet’s create an example data frame:\n\ndat_wide <- data.frame(ID = c(1, 2, 3),\n                       mango = c(1.1, 2.1, 3.1),\n                       banana = c(1.2, 2.2, 3.2),\n                       tangerine = c(1.3, 2.3, 3.3),\n                       Group = c(\"a\", \"b\", \"b\"))\ndat_wide\n\n  ID mango banana tangerine Group\n1  1   1.1    1.2       1.3     a\n2  2   2.1    2.2       2.3     b\n3  3   3.1    3.2       3.3     b\n\n\n\n19.1.1 base\nThe reshape() function is probably one of the more complicated builtin functions because its documentation is not entirely clear, especially if you’re not used to the jargon and specifically with regards to which arguments refer to the input vs. output data frame. Use the following figure as a guide to understand reshape()’s syntax. You can use it as a reference when building your own reshape() command by following steps 1 through 5:\n\n\n\n\n\nreshape() syntax for Wide to Long transformation.\n\n\n\n\n\ndat_wide2long <- reshape(# Data in wide format\n                         data = dat_wide,\n                         # The column name that defines case ID\n                         idvar = \"ID\",\n                         # The columns whose values we want to keep\n                         varying = list(2:4),\n                         # The name of the new column which will contain all \n                         # the values from the columns above\n                         v.names = \"Score\",\n                         # The values/names, of length = (N columns in \"varying\"), \n                         #that will be recycled to indicate which column from the \n                         #wide dataset each row corresponds to\n                         times = c(colnames(dat_wide)[2:4]),\n                         # The name of the new column created to hold the values \n                         # defined by \"times\"\n                         timevar = \"Fruit\",                  \n                         direction = \"long\")\ndat_wide2long\n\n            ID Group     Fruit Score\n1.mango      1     a     mango   1.1\n2.mango      2     b     mango   2.1\n3.mango      3     b     mango   3.1\n1.banana     1     a    banana   1.2\n2.banana     2     b    banana   2.2\n3.banana     3     b    banana   3.2\n1.tangerine  1     a tangerine   1.3\n2.tangerine  2     b tangerine   2.3\n3.tangerine  3     b tangerine   3.3\n\n\nYou can also define varying with a character vector:\nvarying = list(c(\"mango\", \"banana\",\"tangerine\")\nExplore the resulting data frame’s attributes:\n\nattributes(dat_wide2long)\n\n$row.names\n[1] \"1.mango\"     \"2.mango\"     \"3.mango\"     \"1.banana\"    \"2.banana\"   \n[6] \"3.banana\"    \"1.tangerine\" \"2.tangerine\" \"3.tangerine\"\n\n$names\n[1] \"ID\"    \"Group\" \"Fruit\" \"Score\"\n\n$class\n[1] \"data.frame\"\n\n$reshapeLong\n$reshapeLong$varying\n$reshapeLong$varying[[1]]\n[1] \"mango\"     \"banana\"    \"tangerine\"\n\n\n$reshapeLong$v.names\n[1] \"Score\"\n\n$reshapeLong$idvar\n[1] \"ID\"\n\n$reshapeLong$timevar\n[1] \"Fruit\"\n\n\nThese attributes are present if and only if a long data.frame was created from a wide data.frame as above. In this case, reshaping back to wide format is as easy as calling reshape() on the previously converted data.frame with no arguments:\n\ndat_wideagain <- reshape(dat_wide2long)\ndat_wideagain\n\n        ID Group mango banana tangerine\n1.mango  1     a   1.1    1.2       1.3\n2.mango  2     b   2.1    2.2       2.3\n3.mango  3     b   3.1    3.2       3.3\n\n\nNote that the reverse does not work, you need to specify the wide to long reshaping normally.\n\n\n19.1.2 tidyr\n\ndat_wide2long_tv <- pivot_longer(dat_wide,\n                           cols = 2:4,\n                           names_to = \"Fruit\",\n                           values_to = \"Score\")\ndat_wide2long_tv\n\n# A tibble: 9 × 4\n     ID Group Fruit     Score\n  <dbl> <chr> <chr>     <dbl>\n1     1 a     mango       1.1\n2     1 a     banana      1.2\n3     1 a     tangerine   1.3\n4     2 b     mango       2.1\n5     2 b     banana      2.2\n6     2 b     tangerine   2.3\n7     3 b     mango       3.1\n8     3 b     banana      3.2\n9     3 b     tangerine   3.3\n\n\n\n\n19.1.3 data.table\n\ndat_wide_dt <- as.data.table(dat_wide)\ndat_wide2long_dt <- melt(dat_wide_dt,\n                         id.vars = c(1, 5),\n                         measure.vars = 2:4,\n                         variable.name = \"Fruit\",\n                         value.name = \"Score\")\nsetorder(dat_wide2long_dt, \"ID\")\ndat_wide2long_dt\n\n   ID Group     Fruit Score\n1:  1     a     mango   1.1\n2:  1     a    banana   1.2\n3:  1     a tangerine   1.3\n4:  2     b     mango   2.1\n5:  2     b    banana   2.2\n6:  2     b tangerine   2.3\n7:  3     b     mango   3.1\n8:  3     b    banana   3.2\n9:  3     b tangerine   3.3"
  },
  {
    "objectID": "Reshaping.html#long-to-wide",
    "href": "Reshaping.html#long-to-wide",
    "title": "19  Reshaping",
    "section": "19.2 Long to Wide",
    "text": "19.2 Long to Wide\nLet’s recreate the same long dataset:\n\ndat_long <- data.frame(ID = c(1, 2, 3, 1, 2, 3, 1, 2, 3),\n                       Fruit = c(\"mango\", \"mango\", \"mango\", \n                                 \"banana\", \"banana\", \"banana\", \n                                 \"tangerine\", \"tangerine\", \"tangerine\"),\n                       Score = c(1.1, 2.1, 3.1, 1.2, 2.2, 3.2, 1.3, 2.3, 3.3),\n                       Group = c(\"a\", \"b\", \"b\", \"a\", \"b\", \"b\", \"a\", \"b\", \"b\"))\ndat_long\n\n  ID     Fruit Score Group\n1  1     mango   1.1     a\n2  2     mango   2.1     b\n3  3     mango   3.1     b\n4  1    banana   1.2     a\n5  2    banana   2.2     b\n6  3    banana   3.2     b\n7  1 tangerine   1.3     a\n8  2 tangerine   2.3     b\n9  3 tangerine   3.3     b\n\n\n\n19.2.1 base\nUsing base reshape() for long-to-wide transformation is simpler than wide-to-long:\n\n\n\n\n\nreshape() syntax for Long to Wide transformation.\n\n\n\n\n\ndat_long2wide <- reshape(dat_long,\n                         idvar = \"ID\",\n                         timevar = \"Fruit\",\n                         v.names = \"Score\",\n                         direction = \"wide\")\n# Optionally rename columns\ncolnames(dat_long2wide) <- gsub(\"Score.\", \"\", colnames(dat_long2wide))\ndat_long2wide\n\n  ID Group mango banana tangerine\n1  1     a   1.1    1.2       1.3\n2  2     b   2.1    2.2       2.3\n3  3     b   3.1    3.2       3.3\n\n\n\n\n19.2.2 tidyr\n\ndat_long2wide_tv <- pivot_wider(dat_long,\n                                id_cols = c(\"ID\", \"Group\"),\n                                names_from = \"Fruit\",\n                                values_from = \"Score\")\ndat_long2wide_tv\n\n# A tibble: 3 × 5\n     ID Group mango banana tangerine\n  <dbl> <chr> <dbl>  <dbl>     <dbl>\n1     1 a       1.1    1.2       1.3\n2     2 b       2.1    2.2       2.3\n3     3 b       3.1    3.2       3.3\n\n\n\n\n19.2.3 data.table\ndata.table’s long to wide procedure is defined with a convenient formula notation:\n\ndat_long_dt <- as.data.table(dat_long)\ndat_long2wide_dt <- dcast(dat_long_dt,\n                          ID + Group ~ Fruit,\n                          value.var = \"Score\")\ndat_long2wide_dt\n\n   ID Group banana mango tangerine\n1:  1     a    1.2   1.1       1.3\n2:  2     b    2.2   2.1       2.3\n3:  3     b    3.2   3.1       3.3"
  },
  {
    "objectID": "DataTrans.html#continuous-variables",
    "href": "DataTrans.html#continuous-variables",
    "title": "20  Data Transformations",
    "section": "\n20.1 Continuous variables",
    "text": "20.1 Continuous variables\n\n20.1.1 Standardization / Scaling & Centering with scale()\n\nScaling of a numeric vector is achieved by elementwise division with the standard deviation. A scaled vector therefore has standard deviation equal to 1.\nCentering of a numeric vector is achieved by elementwise subtraction of its mean. A centered vector therefore has mean equal to 0.\nStandardizing, a.k.a. converting to Z-scores, involves scaling and centering. Scaling and centering is performed in R with the scale() function.\nDepending on your modeling needs / the algorithms you plan to use, it is often important to scale and/or center your data. Note that many functions, but not all, may automatically scale and center data internally if it is required by the algorithm. Check the function documentation to see if you should manually scale or not.\nscale() can be applied to a single vector or a matrix/data.frame. In the case of a matrix or data.frame, scaling is applied on each column individually. By default, both arguments scale and center are set to TRUE.\nScale a vector:\n\nhead(iris$Sepal.Length)\n\n[1] 5.1 4.9 4.7 4.6 5.0 5.4\n\nPetal.Length_scaled <- scale(iris$Petal.Length)\nhead(Petal.Length_scaled)\n\n          [,1]\n[1,] -1.335752\n[2,] -1.335752\n[3,] -1.392399\n[4,] -1.279104\n[5,] -1.335752\n[6,] -1.165809\n\n\nScale multiple columns of a matrix/data.frame:\n\niris.scaled <- scale(iris[, -5])\nhead(iris.scaled)\n\n     Sepal.Length Sepal.Width Petal.Length Petal.Width\n[1,]   -0.8976739  1.01560199    -1.335752   -1.311052\n[2,]   -1.1392005 -0.13153881    -1.335752   -1.311052\n[3,]   -1.3807271  0.32731751    -1.392399   -1.311052\n[4,]   -1.5014904  0.09788935    -1.279104   -1.311052\n[5,]   -1.0184372  1.24503015    -1.335752   -1.311052\n[6,]   -0.5353840  1.93331463    -1.165809   -1.048667\n\n\nFirst, let’s verify that scale() did what we wanted:\n\ncolMeans(iris.scaled)\n\n Sepal.Length   Sepal.Width  Petal.Length   Petal.Width \n-1.457168e-15 -1.638319e-15 -1.292300e-15 -5.543714e-16 \n\n\n\napply(iris.scaled, 2, sd)\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n           1            1            1            1 \n\n\nWe got effectively 0 mean and standard deviation of 1 for each column.\nscale() outputs the scaled vector(s) along with the scaling and/or centering parameters saved as attributes in the output.\nNote that in both cases, whether a vector input or data.frame, the output is a matrix:\n\nclass(Petal.Length_scaled)\n\n[1] \"matrix\" \"array\" \n\nclass(iris.scaled)\n\n[1] \"matrix\" \"array\" \n\n\nGet the output attributes:\n\nattributes(Petal.Length_scaled)\n\n$dim\n[1] 150   1\n\n$`scaled:center`\n[1] 3.758\n\n$`scaled:scale`\n[1] 1.765298\n\n\ncenter is the mean:\n\nmean(iris$Petal.Length)\n\n[1] 3.758\n\n\nscale is the standard deviation:\n\nsd(iris$Petal.Length)\n\n[1] 1.765298\n\n\nFor a matrix/data.frame input, you get center and scale attributes per column:\n\nattributes(iris.scaled)\n\n$dim\n[1] 150   4\n\n$dimnames\n$dimnames[[1]]\nNULL\n\n$dimnames[[2]]\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\" \n\n\n$`scaled:center`\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    5.843333     3.057333     3.758000     1.199333 \n\n$`scaled:scale`\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n   0.8280661    0.4358663    1.7652982    0.7622377 \n\n\nLet’s save the scale and center attributes and then double check the calculations:\n\n.center <- attr(iris.scaled, \"scaled:center\")\n.center\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    5.843333     3.057333     3.758000     1.199333 \n\n.scale <- attr(iris.scaled, \"scaled:scale\")\n.scale\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n   0.8280661    0.4358663    1.7652982    0.7622377 \n\nSepal.Length_scaled <- (iris$Sepal.Length - .center[1]) / .scale[1]\nall(Sepal.Length_scaled == iris.scaled[, \"Sepal.Length\"])\n\n[1] TRUE\n\n\nNote: Due to limitation in numerical precision, checking sets of floats for equality after multiple operations is not recommended. One simple option is to plot:\n\nmplot3_fit(Sepal.Length_scaled, iris.scaled[, \"Sepal.Length\"])\n\n\n\n\n\n\n\nIf you are manually scaling and/or centering data for supervised learning, you must:\n\n\n\nPerform scaling and centering on your training data\n\n\nSave the centering and scaling parameters for each feature\n\n\nApply the training set-derived centering and scaling parameters to the test set prior to prediction/inference.\n\n\n\n\nA common mistake is to either scale training and testing data together in the beginning, or scale them independently.\n\n20.1.2 Normalization\nNormalization has different meanings in different contexts; in the context of a numeric variable it usually refers to converting to a 0-1 range.\nLet’s write a simple function to achieve this:\n\nnormalize <- function(x) {\n  .min <- min(x, na.rm = TRUE)\n  (x - .min) / max(x - .min, na.rm = TRUE)\n}\n\n\nx <- rnorm(20, 13, 1.4)\nx\n\n [1] 13.72844 13.88471 10.65830 14.28062 12.22112 13.26676 12.10861 12.23567\n [9] 12.61766 13.49497 13.40578 11.05761 13.18081 14.57949 13.02443 11.70124\n[17] 13.88403 13.00814 13.54833 13.25709\n\n\n\nx_normalized <- normalize(x)\nx_normalized\n\n [1] 0.7829621 0.8228134 0.0000000 0.9237808 0.3985572 0.6652228 0.3698632\n [8] 0.4022679 0.4996858 0.7234210 0.7006756 0.1018340 0.6433011 1.0000000\n[15] 0.6034219 0.2659759 0.8226409 0.5992683 0.7370295 0.6627551\n\nmin(x_normalized)\n\n[1] 0\n\nmax(x_normalized)\n\n[1] 1\n\n\nNote that it is easy to make the normalize() function more general, by adding lo and hi arguments to convert to any range:\n\ndr <- function(x, lo = 0, hi = 1) {\n    .min <- min(x, na.rm = TRUE)\n   (x - .min) / max(x - .min, na.rm = TRUE) * (hi - lo) + lo\n  }\n\n\ndr(x, -1, 1)\n\n [1]  0.5659241725  0.6456268181 -1.0000000000  0.8475615533 -0.2028855133\n [6]  0.3304456871 -0.2602735987 -0.1954642084 -0.0006284076  0.4468419576\n[11]  0.4013511580 -0.7963320211  0.2866021882  1.0000000000  0.2068438642\n[16] -0.4680481633  0.6452818201  0.1985365379  0.4740590019  0.3255101768\n\n\n\n20.1.3 Log-transform with log()\n\nFor the following example, x is an unknown feature in a new dataset we were just given.\nWe start by plotting its distribution:\n\nmplot3_x(x)\n\n\n\n\nWe can see it is skewed right. A log transform can help here:\n\nmplot3_x(log(x))\n\n\n\n\n\n20.1.4 Data binning with cut()\n\nA different approach for the above variable might be to bin it.\nLet’s look at a few different ways to bin continuous data.\n\n20.1.4.1 Evenly-spaced interval\ncut() allows us to bin a numeric variable into evenly-spaced intervals.\nThe breaks argument defines the number of intervals:\n\nx_cut4 <- cut(x, breaks = 4)\nhead(x_cut4)\n\n[1] (0.291,178] (0.291,178] (0.291,178] (0.291,178] (0.291,178] (0.291,178]\nLevels: (0.291,178] (178,355] (355,533] (533,711]\n\ntable(x_cut4)\n\nx_cut4\n(0.291,178]   (178,355]   (355,533]   (533,711] \n        977          19           3           1 \n\n\n\n\n\nInterval Notation\n\n\n[3, 9) represents the interval of real numbers between 3 and 9, including 3 and excluding 9.\n\n\n\nBecause the data is so skewed, equal intervals are not helpful in this case. The majority of the data points get grouped into a single bin.\nLet’s visualize the cuts:\n\nxcuts5 <- seq(min(x), max(x), length.out = 5)\nxcuts5\n\n[1]   1.0000 178.2453 355.4905 532.7358 709.9811\n\n\n\nmplot3_x(x, par.reset = FALSE)\n# plot(density(x)) # in base R\nabline(v = xcuts5, col = \"red\", lwd = 1.5)\n\n\n\n\n[Note: We used par.reset = FALSE to stop mplot3_x() from resetting its custom par() settings so that we can continue adding elements to the same plot, in this case with the abline() command.]\n\n20.1.4.2 Quantile cuts\nInstead of evenly-spaced intervals, we can get quantiles with quantile(). We ask for 5 quantiles using the length.out argument, which corresponds to 4 intervals:\n\nxquants5 <- quantile(x, seq(0, 1, length.out = 5))\nxquants5 <- quantile(x, seq(0, 1, length.out = 5))\nmplot3_x(x, par.reset = F)\n# plot(density(x)) # in base R\nabline(v = xquants5, col = \"green\", lwd = 1.5)\n\n\n\n\nThe breaks argument of cut() allows us to pass either an integer to define the number of evenly-spaced breaks, or a numeric vector defining the position of breaks.\nWe can therefore pass the quantile values as break points.\nSince the quantile values begin at the lowest value in the data, we need to define include.lowest = TRUE so that the first interval is inclusive of the lowest value:\n\nx_cutq4 <- cut(x, breaks = xquants5, include.lowest = TRUE)\ntable(x_cutq4)\n\nx_cutq4\n   [1,11.5] (11.5,23.2] (23.2,47.2]  (47.2,710] \n        250         250         250         250 \n\n\nWith quantile cuts, each bin contains roughly the same number of observations (+/- 1)."
  },
  {
    "objectID": "DataTrans.html#categorical-variables",
    "href": "DataTrans.html#categorical-variables",
    "title": "20  Data Transformations",
    "section": "\n20.2 Categorical variables",
    "text": "20.2 Categorical variables\nMany algorithms (or their implementations) do not directly support categorical variables. To use them, you must therefore convert all categorical variables to some type of numerical encoding.\n\n20.2.1 Integer encoding\nIf the categorical data is ordinal, you can simply convert it to integers.\nFor example, the following ordered factor:\n\nbrightness <- factor(c(\"bright\", \"brightest\", \"darkest\",\n                        \"bright\", \"dark\", \"dim\", \"dark\"),\n                      levels = c(\"darkest\", \"dark\", \"dim\", \"bright\", \"brightest\"),\n                      ordered = TRUE)\nbrightness\n\n[1] bright    brightest darkest   bright    dark      dim       dark     \nLevels: darkest < dark < dim < bright < brightest\n\n\n…can be directly coerced to an integer:\n\nas.integer(brightness)\n\n[1] 4 5 1 4 2 3 2\n\n\n\n20.2.2 One-hot encoding\nWhen categorical features are not ordinal, and your algorithm cannot handle them directly, you can one-hot encode them. In one-hot encoding, each categorical feature is converted to k binary features, where k = number of unique values in the input, such that only one feature has the value 1 per case. This is similar to creating dummy variables in statistics, with the difference that dummy variables create k - 1 new variables.\n\nset.seed(21)\nadmission_reasons <- c(\"plannedSurgery\", \"emergencySurgery\", \"medical\")\nadmission <- sample(admission_reasons, 12, T)\nadmission\n\n [1] \"medical\"          \"plannedSurgery\"   \"medical\"          \"plannedSurgery\"  \n [5] \"emergencySurgery\" \"medical\"          \"plannedSurgery\"   \"medical\"         \n [9] \"medical\"          \"emergencySurgery\" \"emergencySurgery\" \"emergencySurgery\"\n\n\nMultiple packages include functions that perform one-hot encoding. It’s a simple operation and we don’t necessarily need to install a large package with many dependencies.\nLet’s write a simple function to perform one-hot encoding. (Note: you may have heard that for loops can be slow in R, but that depends on the operations performed. Here, we loop over an integer matrix and it is plenty fast)\n\nonehot <- function(x, xname = NULL) {\n  if (is.null(xname)) xname <- deparse(substitute(x))\n  x <- factor(x)\n  .levels <- levels(x)      # Get factor levels\n  ncases <- NROW(x)         # Get number of cases\n  index <- as.integer(x)    # Convert levels to integer\n  oh <- matrix(0, ncases, length(.levels))   # Initialize zeros matrix\n  colnames(oh) <- paste(xname, .levels, sep = \"_\")  # Name columns by levels\n  for (i in seq(ncases)) oh[i, index[i]] <- 1  # Assign \"1\" to appropriate level\n  oh\n}\n\nLet’s apply our new function to the admission vector:\n\nonehot(admission)\n\n      admission_emergencySurgery admission_medical admission_plannedSurgery\n [1,]                          0                 1                        0\n [2,]                          0                 0                        1\n [3,]                          0                 1                        0\n [4,]                          0                 0                        1\n [5,]                          1                 0                        0\n [6,]                          0                 1                        0\n [7,]                          0                 0                        1\n [8,]                          0                 1                        0\n [9,]                          0                 1                        0\n[10,]                          1                 0                        0\n[11,]                          1                 0                        0\n[12,]                          1                 0                        0\n\n\nNote: deparse(substitute(x)) above is used to automatically get the name of the input object (in this case “admission”). This is similar to how many of R’s internal functions (e.g. plot()) get the names of input objects."
  },
  {
    "objectID": "StringOps.html#reminder-create-coerce-check",
    "href": "StringOps.html#reminder-create-coerce-check",
    "title": "21  String Operations",
    "section": "21.1 Reminder: create, coerce, check",
    "text": "21.1 Reminder: create, coerce, check\n\ncharacter(): Initialize empty character vector\nas.character(): Coerce any vector to a character vector\nis.character(): Check object is character\n\n\nx <- character(10)\nx\n\n [1] \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\n\n\n\nv <- c(10, 20, 22, 43)\nx <- as.character(v)\nx\n\n[1] \"10\" \"20\" \"22\" \"43\"\n\n\n\nx <- c(\"PID\", \"Age\", \"Sex\", \"Handedness\")\nis.character(x)\n\n[1] TRUE"
  },
  {
    "objectID": "StringOps.html#cat-concatenate-and-print",
    "href": "StringOps.html#cat-concatenate-and-print",
    "title": "21  String Operations",
    "section": "21.2 cat(): Concatenate and print",
    "text": "21.2 cat(): Concatenate and print\ncat() concatenates strings in order to print to screen (console) or to file.\nIt does not return any object. It is therefore useful to produce informative messages in your programs.\n\nsbp <- 130\ntemp <- 98.4\ncat(\"The blood pressure was\", sbp, \"and the temperature was\", temp, \"\\n\")\n\nThe blood pressure was 130 and the temperature was 98.4 \n\n\nUse the file argument to write to a text file. The append argument allows using multiple cat() calls to append to the same file."
  },
  {
    "objectID": "StringOps.html#paste-concatenate-character-vectors",
    "href": "StringOps.html#paste-concatenate-character-vectors",
    "title": "21  String Operations",
    "section": "21.3 paste(): Concatenate character vectors",
    "text": "21.3 paste(): Concatenate character vectors\npaste() is a commonly used command.\nIn its simplest form, it acts like as.character():\n\nv <- c(10, 20, 22, 43)\npaste(v)\n\n[1] \"10\" \"20\" \"22\" \"43\"\n\n\nBut its main job is to combine strings from multiple vectors elementwise:\n\nid = c(\"001\", \"010\", \"018\", \"020\", \"021\", \"051\")\ndept = c(\"Emergency\", \"Cardiology\", \"Neurology\",\n         \"Anesthesia\", \"Surgery\", \"Psychiatry\")\nid\n\n[1] \"001\" \"010\" \"018\" \"020\" \"021\" \"051\"\n\ndept\n\n[1] \"Emergency\"  \"Cardiology\" \"Neurology\"  \"Anesthesia\" \"Surgery\"   \n[6] \"Psychiatry\"\n\n\n\npaste(id, dept)\n\n[1] \"001 Emergency\"  \"010 Cardiology\" \"018 Neurology\"  \"020 Anesthesia\"\n[5] \"021 Surgery\"    \"051 Psychiatry\"\n\n\nThe sep argument defines the separator:\n\npaste(id, dept, sep = \"+++\")\n\n[1] \"001+++Emergency\"  \"010+++Cardiology\" \"018+++Neurology\"  \"020+++Anesthesia\"\n[5] \"021+++Surgery\"    \"051+++Psychiatry\"\n\n\npaste0() is an alias for the commonly used paste(..., sep = \"\"):\n\npaste0(id, dept)\n\n[1] \"001Emergency\"  \"010Cardiology\" \"018Neurology\"  \"020Anesthesia\"\n[5] \"021Surgery\"    \"051Psychiatry\"\n\n\nAs with other vectorized operations, value recycling can be very convenient:\n\npaste0(\"Feature_\", 1:10)\n\n [1] \"Feature_1\"  \"Feature_2\"  \"Feature_3\"  \"Feature_4\"  \"Feature_5\" \n [6] \"Feature_6\"  \"Feature_7\"  \"Feature_8\"  \"Feature_9\"  \"Feature_10\"\n\n\nThe argument collapse helps output a single character element after collapsing with some string:\n\npaste0(\"Feature_\", 1:10, collapse = \", \")\n\n[1] \"Feature_1, Feature_2, Feature_3, Feature_4, Feature_5, Feature_6, Feature_7, Feature_8, Feature_9, Feature_10\""
  },
  {
    "objectID": "StringOps.html#nchar-get-number-of-characters-in-element",
    "href": "StringOps.html#nchar-get-number-of-characters-in-element",
    "title": "21  String Operations",
    "section": "21.4 nchar(): Get number of characters in element",
    "text": "21.4 nchar(): Get number of characters in element\nnchar() counts the number of characters in each element of type character in a vector:\n\nx <- c(\"a\", \"bb\", \"ccc\")\nnchar(x)\n\n[1] 1 2 3"
  },
  {
    "objectID": "StringOps.html#substr-get-substring",
    "href": "StringOps.html#substr-get-substring",
    "title": "21  String Operations",
    "section": "21.5 substr(): Get substring",
    "text": "21.5 substr(): Get substring\nsubstr() allows you to get and set individual (literal) characters from a character (R base type) vector, by position:\n\n21.5.1 Extract\ne.g. Get characters 1-3:\n\nx <- c(\"001Emergency\", \"010Cardiology\", \"018Neurology\", \n       \"020Anesthesia\", \"021Surgery\", \"051Psychiatry\")\nsubstr(x, start = 1, stop = 3)\n\n[1] \"001\" \"010\" \"018\" \"020\" \"021\" \"051\"\n\n\nNeither start nor stop need to be valid character positions.\nFor example, if you want to get all characters from the fourth one to the last one, you can specify a very large stop\n\nsubstr(x, 4, 99)\n\n[1] \"Emergency\"  \"Cardiology\" \"Neurology\"  \"Anesthesia\" \"Surgery\"   \n[6] \"Psychiatry\"\n\n\nIf you start with too high an index, you end up with empty strings:\n\nsubstr(x, 20, 24)\n\n[1] \"\" \"\" \"\" \"\" \"\" \"\"\n\n\nNote: substring() is also available, with similar syntax to substr(): (first, last) instead of (start, stop). It is available for compatibility with S (check its source code to see how it’s an alias for substr())\n\n\n21.5.2 Replace\n\nx <- c(\"Jan_1987\")\nx\n\n[1] \"Jan_1987\"\n\n\nReplace the first three letters:\n\nsubstr(x, 1, 3) <- \"Feb\"\nx\n\n[1] \"Feb_1987\"\n\n\nNote that if the replacement is longer, it is “cropped” to the length of the substring being replaced:\n\nsubstr(x, 1, 3) <- \"April\"\nx\n\n[1] \"Apr_1987\""
  },
  {
    "objectID": "StringOps.html#strsplit-split-strings",
    "href": "StringOps.html#strsplit-split-strings",
    "title": "21  String Operations",
    "section": "21.6 strsplit(): Split strings",
    "text": "21.6 strsplit(): Split strings\nstrsplit() allows you to split a character vector elements based on any character or regular expression\n\nx <- \"This is one sentence\"\nstrsplit(x, \" \")\n\n[[1]]\n[1] \"This\"     \"is\"       \"one\"      \"sentence\"\n\n\n\nx <- \"14,910\"\nstrsplit(x, \",\")\n\n[[1]]\n[1] \"14\"  \"910\"\n\n\nAs with any functions, you can compose string operations in complex ways (though it may often be considerably easier to perform multiple separate operations instead):\n\nx <- c(\"1,950\", \"2,347\")\nx\n\n[1] \"1,950\" \"2,347\"\n\n\n\nlapply(strsplit(x, \",\"), function(i) \n  paste(i, c(\"thousand\", \"dollars\"), collapse = \" and \"))\n\n[[1]]\n[1] \"1 thousand and 950 dollars\"\n\n[[2]]\n[1] \"2 thousand and 347 dollars\""
  },
  {
    "objectID": "StringOps.html#string-formatting",
    "href": "StringOps.html#string-formatting",
    "title": "21  String Operations",
    "section": "21.7 String formatting",
    "text": "21.7 String formatting\n\n21.7.1 Change case with toupper() and tolower()\n\nfeatures <- c(\"id\", \"age\", \"sex\", \"sbp\", \"dbp\", \"hct\", \"urea\", \"creatinine\")\nfeatures\n\n[1] \"id\"         \"age\"        \"sex\"        \"sbp\"        \"dbp\"       \n[6] \"hct\"        \"urea\"       \"creatinine\"\n\n\n\nfeatures_upper <- toupper(features)\nfeatures_upper\n\n[1] \"ID\"         \"AGE\"        \"SEX\"        \"SBP\"        \"DBP\"       \n[6] \"HCT\"        \"UREA\"       \"CREATININE\"\n\n\n\nfeatures_lower <- tolower(features_upper)\nfeatures_lower\n\n[1] \"id\"         \"age\"        \"sex\"        \"sbp\"        \"dbp\"       \n[6] \"hct\"        \"urea\"       \"creatinine\"\n\n\n\n\n21.7.2 abbreviate()\nabbreviate() allows to reduce elements of a character vector to short, unique abbreviations of a minimumn length (defaults to 4)\n\nx <- c(\"Emergency\", \"Cardiology\", \"Surgery\", \"Anesthesia\", \"Neurology\", \"Psychiatry\", \"Clinical Psychology\")\nabbreviate(x)\n\n          Emergency          Cardiology             Surgery          Anesthesia \n             \"Emrg\"              \"Crdl\"              \"Srgr\"              \"Anst\" \n          Neurology          Psychiatry Clinical Psychology \n             \"Nrlg\"              \"Psyc\"              \"ClnP\" \n\nabbreviate(x, minlength = 4)\n\n          Emergency          Cardiology             Surgery          Anesthesia \n             \"Emrg\"              \"Crdl\"              \"Srgr\"              \"Anst\" \n          Neurology          Psychiatry Clinical Psychology \n             \"Nrlg\"              \"Psyc\"              \"ClnP\" \n\nabbreviate(x, minlength = 5)\n\n          Emergency          Cardiology             Surgery          Anesthesia \n            \"Emrgn\"             \"Crdlg\"             \"Srgry\"             \"Ansth\" \n          Neurology          Psychiatry Clinical Psychology \n            \"Nrlgy\"             \"Psych\"             \"ClncP\""
  },
  {
    "objectID": "StringOps.html#pattern-matching",
    "href": "StringOps.html#pattern-matching",
    "title": "21  String Operations",
    "section": "21.8 Pattern matching",
    "text": "21.8 Pattern matching\nA very common task in programming is to find +/- replace string patterns in a vector of strings.\ngrep() and grepl() help find strings that contain a given pattern.\nsub() and gsub() help find and replace strings.\n\n21.8.1 grep(): Get an integer index of elements that include a pattern\n\nx <- c(\"001Age\", \"002Sex\", \"010Temp\", \"014SBP\", \"018Hct\", \"022PFratio\", \"030GCS\", \"112SBP-DBP\")\ngrep(pattern = \"SBP\", x = x)\n\n[1] 4 8\n\n\ngrep()’s value arguments which defaults to FALSE, allows returning the matched string itself (the value of the element) instead of its integer index:\n\ngrep(\"SBP\", x, value = TRUE)\n\n[1] \"014SBP\"     \"112SBP-DBP\"\n\n\n\n\n21.8.2 grepl(): Get a logical index of elements that include a pattern\ngrepl() is similar to grep(), but returns a logical index instead:\n\ngrepl(\"SBP\", x)\n\n[1] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE\n\n\n\n\n21.8.3 sub(): Find replace first match of a pattern\n\nx <- c(\"The most important variable was PF ratio. Other significant variables are listed in the supplementary information.\")\nsub(pattern = \"variable\", replacement = \"feature\", x = x)\n\n[1] \"The most important feature was PF ratio. Other significant variables are listed in the supplementary information.\"\n\n\n“First match” refers to each element of a character vector:\n\nx <- c(\"var 1, var 2\", \"var 3, var 4\")\nsub(\"var\", \"feat\", x)\n\n[1] \"feat 1, var 2\" \"feat 3, var 4\"\n\n\n\n\n21.8.4 gsub(): Find and replace all matches of a pattern\n\nx <- c(\"The most important variable was PF ratio. Other significant variables are listed in the supplementary information.\")\ngsub(pattern = \"variable\", replacement = \"feature\", x = x)\n\n[1] \"The most important feature was PF ratio. Other significant features are listed in the supplementary information.\"\n\n\n“All matches” means all matches across all elements:\n\nx <- c(\"var 1, var 2\", \"var 3, var 4\")\ngsub(\"var\", \"feat\", x)\n\n[1] \"feat 1, feat 2\" \"feat 3, feat 4\""
  },
  {
    "objectID": "StringOps.html#regex",
    "href": "StringOps.html#regex",
    "title": "21  String Operations",
    "section": "21.9 Regular expressions",
    "text": "21.9 Regular expressions\nRegular expressions allow you to perform flexible pattern matching. For example, you can look for a pattern specifically at the beginning or the end of a word, or for a variable pattern with certain characteristics.\nRegular expressions are very powerful and heavily used. They exist in multiple programming languages - with many similarities and some differences in their syntax.\nThere are many rules in defining regular expressions and take a little getting used to. You can read the R manual by typing ?base::regex.\nSome of the most important rules are liste below:\n\n21.9.1 Match a pattern at the beginning of a line/string with ^/\\\\<:\nUse the caret sign ^ in the beginning of a pattern to only match strings that begin with this pattern.\npattern 012 matches both 2nd and 3rd elements:\n\nx <- c(\"001xyz993\", \"012qwe764\", \"029aqw012\")\nx\n\n[1] \"001xyz993\" \"012qwe764\" \"029aqw012\"\n\ngrep(\"012\", x)\n\n[1] 2 3\n\n\nBy adding ^ or \\\\<, only the 2nd element matches:\n\ngrep(\"^012\", x)\n\n[1] 2\n\ngrep(\"\\\\<012\", x)\n\n[1] 2\n\n\n\n\n21.9.2 Match a pattern at the end of a line/string with $/\\\\>\nThe dollar sign $ is used at the end of a pattern to only match strings which end with this pattern:\n\nx\n\n[1] \"001xyz993\" \"012qwe764\" \"029aqw012\"\n\ngrep(\"012$\", x)\n\n[1] 3\n\ngrep(\"012\\\\>\", x)\n\n[1] 3\n\n\n\nx <- c(\"1one\", \"2one\", \"3two\", \"3three\")\ngrep(\"one$\", x)\n\n[1] 1 2\n\ngrep(\"one\\\\>\", x)\n\n[1] 1 2\n\n\n\n\n21.9.3 .: Match any character\n\ngrep(\"e.X\", c(\"eX\", \"enX\", \"ennX\", \"ennnX\", \"ennnnX\"))\n\n[1] 2\n\n\n\n\n21.9.4 +: Match preceding character one or more times:\n\ngrep(\"en+X\", c(\"eX\", \"enX\", \"ennX\", \"ennnX\", \"ennnnX\"))\n\n[1] 2 3 4 5\n\n\n\n\n21.9.5 {n}: Match preceding character n times:\n\ngrep(\"en{2}X\", c(\"eX\", \"enX\", \"ennX\", \"ennnX\", \"ennnnX\"))\n\n[1] 3\n\n\n\n\n21.9.6 {n,}: Match preceding character n or more times:\n\ngrep(\"en{2,}X\", c(\"eX\", \"enX\", \"ennX\", \"ennnX\", \"ennnnX\"))\n\n[1] 3 4 5\n\n\n\n\n21.9.7 {n,m}: Match preceding character at least n times and no more than m times:\n\ngrep(\"en{2,3}X\", c(\"eX\", \"enX\", \"ennX\", \"ennnX\", \"ennnnX\"))\n\n[1] 3 4\n\n\n\n\n21.9.8 Character classes\nYou can define a set of characters to be matched using square brackets. Any number of the characters in the set will be matched.\nFor example match and replace $ and/or @ with an underscore:\n\nx <- c(\"Feat1$alpha\", \"Feat2$gamma@5\", \"Feat9@zeta2\")\ngsub(\"[$@]\", \"_\", x)\n\n[1] \"Feat1_alpha\"   \"Feat2_gamma_5\" \"Feat9_zeta2\"  \n\n\n\n21.9.8.1 Predefined character classes\nA number of character classes are predefined. Slightly confusingly, they are themselves surrounded by brackets and to use them as a character class, you need a seconds set of brackets around them. Some of the most common ones include:\n\n[:alnum:]: alphanumeric, i.e. all letters and numbers\n[:alpha:]: all letters\n[:digit:]: all numbers\n[:lower:]: all lowercase letters\n[:upper:]: all uppercase letters\n[:punct:]: all punctuation characters (! ” # $ % & ’ ( ) * + , - . / : ; < = > ? @ [  ] ^ _ ` { | } ~.)\n[:blank:]: all spaces and tabs\n[:space:]: all spaces, tabs, newline characters, and some more\n\nLet’s look at some examples using them.\nHere we use [:digit:] to remove all numbers:\n\nx <- c(\"001Emergency\", \"010Cardiology\", \"018Neurology\", \"020Anesthesia\", \n       \"021Surgery\", \"051Psychiatry\")\nx\n\n[1] \"001Emergency\"  \"010Cardiology\" \"018Neurology\"  \"020Anesthesia\"\n[5] \"021Surgery\"    \"051Psychiatry\"\n\ngsub(\"[[:digit:]]\", \"\", x)\n\n[1] \"Emergency\"  \"Cardiology\" \"Neurology\"  \"Anesthesia\" \"Surgery\"   \n[6] \"Psychiatry\"\n\n\nWe can use [:alpha:] to remove all letters instead:\n\ngsub(\"[[:alpha:]]\", \"\", x)\n\n[1] \"001\" \"010\" \"018\" \"020\" \"021\" \"051\"\n\n\nWe can use a caret ^ in the beginning of a character class to match any character not in the character set:\n\nx <- c(\"001$Emergency\", \"010@Cardiology\", \"018*Neurology\", \"020!Anesthesia\", \n       \"021!Surgery\", \"051*Psychiatry\")\ngsub(\"[^[:alnum:]]\", \"_\", x)\n\n[1] \"001_Emergency\"  \"010_Cardiology\" \"018_Neurology\"  \"020_Anesthesia\"\n[5] \"021_Surgery\"    \"051_Psychiatry\"\n\n\n\n\n\n21.9.9 Combining character classes\nUse | to match from multiple character classes:\n\nx <- c(\"123#$%alphaBeta\")\ngsub(\"[[:digit:]|[:punct:]]\", \"\", x)\n\n[1] \"alphaBeta\"\n\n\n\n\n\nFor more information on regular expressions, start by reading the built-in documentation: ?regex\n\n\n\n\n\n21.9.10 Escaping metacharacters\nMetacharacters are characters that have a special meaning within a regular expression. They include:\n. \\ | ( ) [ { ^ $ * + ?.\nFor example, we have seen above that the period matches any character and the square brackets are used to define character classes If you want to match one of these characters itself, you must “escape” it using a double backslash. Escaping a character simply means “this is not part of a regular expression, match it as is”.\nFor example, to match a period (.) and replace it with underscores:\n\nx <- c(\"systolic.blood.pressure\", \"diastolic.blood.pressure\")\nx\n\n[1] \"systolic.blood.pressure\"  \"diastolic.blood.pressure\"\n\ngsub(\"\\\\.\", \"_\", x)\n\n[1] \"systolic_blood_pressure\"  \"diastolic_blood_pressure\"\n\n\nIf we didn’t escape the period above, it would have matched every character:\n\ngsub(\".\", \"_\", x)\n\n[1] \"_______________________\"  \"________________________\"\n\n\nAnother example, include an escaped metacharacter within a regular expression. In the example below we want to remove everything up to and including the dollar sign:\n\nx <- c(\"df$ID\", \"df$Age\")\ngsub(\".*\\\\$\", \"\", x)\n\n[1] \"ID\"  \"Age\"\n\n\nOur regular expression .*\\\\$, decomposed:\n\n.: match any character\n.*: match any character any number of times\n.*\\\\$: match any character any number of times till you find a dollar sign\n\nIf we had not escaped the $, it wouldn’t have worked:\n\ngsub(\".*$\", \"\", x)\n\n[1] \"\" \"\""
  },
  {
    "objectID": "DateTime.html",
    "href": "DateTime.html",
    "title": "22  Date and Time",
    "section": "",
    "text": "R includes builtin support for working with date +/- time data. A number of external packages further extend this support.\nThere are three builtin classes:\nBackground info: Portable Operating System Interface (POSIX) is a set of standards for maintaining compatibility among operating systems."
  },
  {
    "objectID": "DateTime.html#date-objects",
    "href": "DateTime.html#date-objects",
    "title": "22  Date and Time",
    "section": "22.1 Date objects",
    "text": "22.1 Date objects\n\n22.1.1 Character to Date: as.Date()\nYou can create a Date object from a string:\n\nx <- as.Date(\"1981-02-12\")\nx\n\n[1] \"1981-02-12\"\n\nclass(x)\n\n[1] \"Date\"\n\n\nThe tryFormats argument defines which formats are recognized.\nThe default is tryFormats = c(\"%Y-%m-%d\", \"%Y/%m/%d\"), i.e. a date of the form “2020-11-16” or “2020/11/16”\n\n\n22.1.2 Get current date & time\nGet current data:\n\ntoday <- Sys.Date()\ntoday\n\n[1] \"2022-07-19\"\n\nclass(today)\n\n[1] \"Date\"\n\n\nGet current date and time:\n\nnow <- Sys.time()\nnow\n\n[1] \"2022-07-19 03:01:22 PDT\"\n\nclass(now)\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\nGet local timezone:\n\nSys.timezone()\n\n[1] \"America/Los_Angeles\"\n\n\n\n\n22.1.3 Math on Dates\nThe reason we care about Date objects in R is because we can apply useful mathematical operations on them.\nFor example, we can substract date objects to get time intervals:\n\nstart_date <- as.Date(\"2020-09-15\")\ntime_diff <- Sys.Date() - start_date\ntime_diff\n\nTime difference of 672 days\n\nclass(time_diff)\n\n[1] \"difftime\"\n\n\nNote: While you can use the subtraction operator -, it is advised you use the difftime() function to perform subtraction on dates instead, because it allows you to specify units:\n\ntimepoint1 <- as.Date(\"2020-01-07\")\ntimepoint2 <- as.Date(\"2020-02-03\")\ndifftime(timepoint2, timepoint1, units = \"weeks\")\n\nTime difference of 3.857143 weeks\n\ndifftime(timepoint2, timepoint1, units = \"days\")\n\nTime difference of 27 days\n\ndifftime(timepoint2, timepoint1, units = \"hours\")\n\nTime difference of 648 hours\n\ndifftime(timepoint2, timepoint1, units = \"mins\")\n\nTime difference of 38880 mins\n\ndifftime(timepoint2, timepoint1, units = \"secs\")\n\nTime difference of 2332800 secs\n\n\n\n\n\nWhy is there no option for “months” or “years” in units?\n\n\nThink about it.\n\n\nBecause, unlike seconds, minutes, hours, days, and weeks, months and years do not have fixed length, i.e. literally a month or a year are not “units” of time.\n\n\nYou can always get a difference in days and divide by 365 (or 365.242.\n\n\n\n\nDOB <- as.Date(\"1969-08-04\")\nAge <- Sys.Date() - DOB\nAge\n\nTime difference of 19342 days\n\ncat(\"Age today is\", round(Age/365), \"years\")\n\nAge today is 53 years\n\n\n\n\n22.1.4 mean/median Date\n\nx <- as.Date(c(5480, 5723, 5987, 6992), origin = \"1970-01-01\")\nx\n\n[1] \"1985-01-02\" \"1985-09-02\" \"1986-05-24\" \"1989-02-22\"\n\nmean(x)\n\n[1] \"1986-07-21\"\n\nmedian(x)\n\n[1] \"1986-01-12\"\n\n\nTo check the median, we can do a mathematical operation using mmultiplication subtraction and addition, and the result is still a Date(!):\n\nx[2] + .5 * (x[3] - x[2])\n\n[1] \"1986-01-12\"\n\n\n\n\n22.1.5 Sequence of dates\nYou can create a sequence of dates using seq().\nIf an integer is passed to by, the unit is assumed to be days:\n\nstart_date <- as.Date(\"2020-09-14\")\nend_date <- as.Date(\"2020-12-07\")\nseq(from = start_date, to = end_date, by = 7)\n\n [1] \"2020-09-14\" \"2020-09-21\" \"2020-09-28\" \"2020-10-05\" \"2020-10-12\"\n [6] \"2020-10-19\" \"2020-10-26\" \"2020-11-02\" \"2020-11-09\" \"2020-11-16\"\n[11] \"2020-11-23\" \"2020-11-30\" \"2020-12-07\"\n\n\nUnlike mathematical operations like difftime() which require strict units of time, seq() can work with months and years.\nby can be one of:\n“day”, “week”, “month”, “quarter”, “year”.\nThe above is therefore equivalent to:\n\nseq(from = start_date, to = end_date, by = \"week\")\n\n [1] \"2020-09-14\" \"2020-09-21\" \"2020-09-28\" \"2020-10-05\" \"2020-10-12\"\n [6] \"2020-10-19\" \"2020-10-26\" \"2020-11-02\" \"2020-11-09\" \"2020-11-16\"\n[11] \"2020-11-23\" \"2020-11-30\" \"2020-12-07\"\n\n\nAs with numeric sequences, you can also define the length.out argument:\n\nstart_date <- as.Date(\"2020-01-20\")\nseq(from = start_date, by = \"year\", length.out = 4)\n\n[1] \"2020-01-20\" \"2021-01-20\" \"2022-01-20\" \"2023-01-20\"\n\n\nAn integer can be provided as part of character input to by:\n\nstart_date <- as.Date(\"2020-01-20\")\nend_date <- as.Date(\"2021-01-20\")\nseq(start_date, end_date, by = \"2 months\")\n\n[1] \"2020-01-20\" \"2020-03-20\" \"2020-05-20\" \"2020-07-20\" \"2020-09-20\"\n[6] \"2020-11-20\" \"2021-01-20\""
  },
  {
    "objectID": "DateTime.html#date-time-objects",
    "href": "DateTime.html#date-time-objects",
    "title": "22  Date and Time",
    "section": "22.2 Date-Time objects",
    "text": "22.2 Date-Time objects\n\n22.2.1 Character to Date-Time: as.POSIXct(), as.POSIXlt(), strptime():\n(As always, it can be very informative to look at the source code. Many of these functions call eachother internally)\nRead strptime()’s documentation for conversion specifications. These define the order and format of characters to be read as year, month, day, hour, minute, and second information.\nFor example, the ISO 8601 international standard is defined as:\n\"%Y-%m-%d %H:%M:%S\"\n\n%Y: Year with century, (0-9999 accepted) e.g. 2020\n%m: Month, 01-12, e.g. 03\n%d: Day, 01-31, e.g. 04\n%H: Hours, 00-23, e.g. 13\n%M: Minutes, 00-59, e.g. 38\n%S: Seconds, 00-61 (!) allowing for up to two leap seconds, e.g. 54\n\n\ndt <- \"2020-03-04 13:38:54\"\ndt\n\n[1] \"2020-03-04 13:38:54\"\n\nclass(dt)\n\n[1] \"character\"\n\n\nUse attributres() to see the difference between the POSIXct and POSIXlt classes:\n\ndt_posixct <- as.POSIXct(dt)\ndt_posixct\n\n[1] \"2020-03-04 13:38:54 PST\"\n\nclass(dt_posixct)\n\n[1] \"POSIXct\" \"POSIXt\" \n\nstr(dt_posixct)\n\n POSIXct[1:1], format: \"2020-03-04 13:38:54\"\n\nattributes(dt_posixct)\n\n$class\n[1] \"POSIXct\" \"POSIXt\" \n\n$tzone\n[1] \"\"\n\n\n\ndt_posixlt <- as.POSIXlt(dt)\ndt_posixlt\n\n[1] \"2020-03-04 13:38:54 PST\"\n\nclass(dt_posixlt)\n\n[1] \"POSIXlt\" \"POSIXt\" \n\nstr(dt_posixlt)\n\n POSIXlt[1:1], format: \"2020-03-04 13:38:54\"\n\ndt_posixlt$year\n\n[1] 120\n\nattributes(dt_posixlt)\n\n$names\n [1] \"sec\"    \"min\"    \"hour\"   \"mday\"   \"mon\"    \"year\"   \"wday\"   \"yday\"  \n [9] \"isdst\"  \"zone\"   \"gmtoff\"\n\n$class\n[1] \"POSIXlt\" \"POSIXt\" \n\n\nYou can compose a really large number of combination formats to match your data.\n\ndt2 <- c(\"03.04.20 01:38.54 pm\")\ndt2_posix <- as.POSIXct(dt2, format = \"%m.%d.%y %I:%M.%S %p\")\ndt2_posix\n\n[1] \"2020-03-04 13:38:54 PST\""
  },
  {
    "objectID": "DateTime.html#format-dates",
    "href": "DateTime.html#format-dates",
    "title": "22  Date and Time",
    "section": "22.3 format() Dates",
    "text": "22.3 format() Dates\nformat() operates on Date and POSIX objects to convert between representations\nDefine Date in US format:\n\ndt_us <- as.Date(\"07-04-2020\", format = \"%m-%d-%Y\")\ndt_us\n\n[1] \"2020-07-04\"\n\n\nConvert to European format:\n\ndt_eu <- format(dt_us, \"%d.%m.%y\")\ndt_eu\n\n[1] \"04.07.20\""
  },
  {
    "objectID": "DateTime.html#extract-partial-date-information",
    "href": "DateTime.html#extract-partial-date-information",
    "title": "22  Date and Time",
    "section": "22.4 Extract partial date information",
    "text": "22.4 Extract partial date information\n\nweekdays(): Get name of day of the week\nmonths(): Get name of month\nquarters(): Get quarter\njulia(): Get number of days since a specific origin\n\n\nx <- as.Date(c(18266, 18299, 18359, 18465), origin = \"1970-01-01\")\nx\n\n[1] \"2020-01-05\" \"2020-02-07\" \"2020-04-07\" \"2020-07-22\"\n\n\n\nweekdays(x)\n\n[1] \"Sunday\"    \"Friday\"    \"Tuesday\"   \"Wednesday\"\n\nmonths(x)\n\n[1] \"January\"  \"February\" \"April\"    \"July\"    \n\nquarters(x)\n\n[1] \"Q1\" \"Q1\" \"Q2\" \"Q3\"\n\njulian(x)\n\n[1] 18266 18299 18359 18465\nattr(,\"origin\")\n[1] \"1970-01-01\"\n\njulian(x, origin = as.Date(\"2020-01-01\"))\n\n[1]   4  37  97 203\nattr(,\"origin\")\n[1] \"2020-01-01\""
  },
  {
    "objectID": "DateTime.html#handling-dates-with-lubridate",
    "href": "DateTime.html#handling-dates-with-lubridate",
    "title": "22  Date and Time",
    "section": "22.5 Handling dates with lubridate",
    "text": "22.5 Handling dates with lubridate\nInstead of defining Date and/or time formats using POSIX standard abbreviations, we can let the lubridate package do some guesswork for us, which works well most of the time.\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\ndt <- c(\"2020-03-04 13:38:54\")\ndt_posix <- as_datetime(dt)\ndt_posix\n\n[1] \"2020-03-04 13:38:54 UTC\"\n\n\nNote that timezone defaults to UTC (Coordinated Universal Time) and must be set manually. PST is defined with “America/Los_Angeles” or the (officially deprecated) “US/Pacific” (tz database)\n\ndt_posix <- as_datetime(dt, tz = \"America/Los_Angeles\")\ndt_posix\n\n[1] \"2020-03-04 13:38:54 PST\"\n\n\n\ndt2_posix <- as_datetime(dt2)\ndt2_posix\n\n[1] \"2003-04-20 13:38:54 UTC\"\n\n\ndt2 got misinterpreted as year-month-day.\nFor these cases, lubridate includes a number of convenient functions to narrow down the guessing. The functions are named using all permutations of y, m, and d. The letter order signifies the order the information appears in the character you are trying to import, i.e. ymd, dmy, mdy, ydm, myd\n\ndt2 <- c(\"03.04.20 01:38.54 pm\")\ndt2_posix <- mdy_hms(dt2, tz = \"America/Los_Angeles\")\ndt2_posix\n\n[1] \"2020-03-04 13:38:54 PST\""
  },
  {
    "objectID": "MissingData.html",
    "href": "MissingData.html",
    "title": "21  Handling Missing data",
    "section": "",
    "text": "Missing data is a very common issue in statistics and data science.\nData may be missing for a variety of reasons. We often characterize the type of missingness using the following three types (Mack, Su, and Westreich 2018):"
  },
  {
    "objectID": "MissingData.html#check-for-missing-data",
    "href": "MissingData.html#check-for-missing-data",
    "title": "21  Handling Missing data",
    "section": "\n21.1 Check for missing data",
    "text": "21.1 Check for missing data\nYou can use your favorite base R commands to check for missing data, count NA elements by row, by column, total, etc.\nLet’s load the PimaIndiansDiabetes2 dataset from the mlbench package and make a copy of it to variable dat. Remember to check the class of a new object you didn’t create yourself with class(), check its dimensions, if applicable, with dim(), and a get a summary of its structure including data types with str():\n\ndata(\"PimaIndiansDiabetes2\", package = \"mlbench\")\ndat <- PimaIndiansDiabetes2\nclass(dat)\n\n[1] \"data.frame\"\n\ndim(dat)\n\n[1] 768   9\n\nstr(dat)\n\n'data.frame':   768 obs. of  9 variables:\n $ pregnant: num  6 1 8 1 0 5 3 10 2 8 ...\n $ glucose : num  148 85 183 89 137 116 78 115 197 125 ...\n $ pressure: num  72 66 64 66 40 74 50 NA 70 96 ...\n $ triceps : num  35 29 NA 23 35 NA 32 NA 45 NA ...\n $ insulin : num  NA NA NA 94 168 NA 88 NA 543 NA ...\n $ mass    : num  33.6 26.6 23.3 28.1 43.1 25.6 31 35.3 30.5 NA ...\n $ pedigree: num  0.627 0.351 0.672 0.167 2.288 ...\n $ age     : num  50 31 32 21 33 30 26 29 53 54 ...\n $ diabetes: Factor w/ 2 levels \"neg\",\"pos\": 2 1 2 1 2 1 2 1 2 2 ...\n\n\nCheck if there are any missing values in the data.frame with anyNA():\n\nanyNA(dat)\n\n[1] TRUE\n\n\nThe above suggests there is one or more NA values in the dataset.\nWe can create a logical index of NA values using is.na(). Remember that the output of is.na() is a logical matrix with the same dimensions as the dataset:\n\nna_index <- is.na(dat)\ndim(na_index)\n\n[1] 768   9\n\nhead(na_index)\n\n  pregnant glucose pressure triceps insulin  mass pedigree   age diabetes\n1    FALSE   FALSE    FALSE   FALSE    TRUE FALSE    FALSE FALSE    FALSE\n2    FALSE   FALSE    FALSE   FALSE    TRUE FALSE    FALSE FALSE    FALSE\n3    FALSE   FALSE    FALSE    TRUE    TRUE FALSE    FALSE FALSE    FALSE\n4    FALSE   FALSE    FALSE   FALSE   FALSE FALSE    FALSE FALSE    FALSE\n5    FALSE   FALSE    FALSE   FALSE   FALSE FALSE    FALSE FALSE    FALSE\n6    FALSE   FALSE    FALSE    TRUE    TRUE FALSE    FALSE FALSE    FALSE\n\n\nOne way to count missing values is with sum(is.na()). Remember that a logical array is coerced to an integer array for mathematical operations, where TRUE becomes 1 and FALSE becomes 0. Therefore, calling sum() on a logical index counts the number of TRUE elements (and since we are applying it on the index of NA values, it counts the number of elements with missing values):\n\nsum(is.na(dat))\n\n[1] 652\n\n\nThere are 652 NA values in total in the data.frame.\nLet’s count the number of missing values per feature, i.e. per column, using sapply()(#sapply):\n\nsapply(dat, function(i) sum(is.na(i)))\n\npregnant  glucose pressure  triceps  insulin     mass pedigree      age \n       0        5       35      227      374       11        0        0 \ndiabetes \n       0 \n\n\nThe features insulin and triceps have the most NA values.\nLet’s count the number of missing values per case (i.e. row):\n\nsapply(1:nrow(dat), function(i) sum(is.na(dat[i, ])))\n\n  [1] 1 1 2 0 0 2 0 3 0 3 2 2 2 0 0 3 0 2 0 0 0 2 2 1 0 0 2 0 0 2 1 0 0 2 1 0 2\n [38] 1 1 0 0 2 1 0 2 1 2 1 1 4 0 0 0 0 0 1 0 0 2 0 4 2 2 0 2 1 1 2 0 0 0 0 2 0\n [75] 1 2 2 1 3 1 1 4 0 1 2 0 1 0 0 1 2 0 0 2 0 0 1 0 0 0 2 2 2 0 2 0 2 0 0 0 0\n[112] 0 0 2 0 2 2 2 1 0 0 1 0 2 2 0 0 0 0 2 0 2 0 1 0 0 0 0 2 0 2 1 0 2 0 2 1 0\n[149] 2 1 0 2 0 0 2 1 0 0 0 0 1 0 0 1 2 0 1 2 2 0 2 0 2 0 0 0 2 0 2 2 2 0 1 2 2\n[186] 1 0 0 0 0 2 0 2 3 1 0 2 0 0 0 1 2 1 0 0 1 0 2 0 1 1 1 1 0 0 0 0 0 1 2 0 2\n[223] 3 0 0 0 2 1 0 0 2 0 0 2 0 2 0 1 1 2 1 0 2 0 0 1 2 0 0 1 2 2 0 1 0 1 1 1 0\n[260] 0 0 3 1 1 2 0 3 1 2 3 1 0 2 0 2 0 1 0 2 0 2 0 0 2 2 0 0 0 0 0 0 0 0 0 2 0\n[297] 0 0 0 2 3 0 0 2 2 0 0 0 0 0 1 0 0 0 1 0 0 2 0 2 0 1 1 0 1 0 0 2 0 0 1 0 3\n[334] 2 0 0 3 2 0 2 0 0 2 2 2 0 0 3 0 2 2 2 1 0 2 2 0 2 0 0 0 2 1 2 0 0 2 1 0 0\n[371] 0 1 0 0 0 0 0 0 2 0 0 1 0 0 0 0 1 1 0 0 0 2 0 0 2 0 0 1 2 1 2 2 0 1 2 0 2\n[408] 2 2 0 1 0 0 0 0 0 1 1 2 0 0 0 0 1 0 0 4 0 0 0 3 0 0 2 1 3 1 2 1 2 1 0 0 2\n[445] 1 0 0 0 0 0 0 2 0 3 0 1 2 0 0 0 0 2 0 1 2 0 0 0 3 0 1 1 1 2 2 1 0 0 0 1 0\n[482] 1 0 0 3 0 0 0 1 2 0 1 1 0 4 2 2 0 0 0 0 1 2 0 1 2 0 0 0 2 1 0 2 2 0 0 0 2\n[519] 2 0 0 0 4 2 2 1 0 0 0 2 0 2 0 3 0 3 2 2 0 0 0 0 1 0 0 0 0 0 0 1 1 0 2 0 0\n[556] 0 1 2 1 2 2 0 0 0 2 0 0 0 0 0 2 2 0 0 0 0 0 2 2 1 1 1 1 2 0 1 2 2 0 3 1 0\n[593] 2 0 0 0 2 0 2 0 1 3 1 0 3 1 0 0 0 0 0 0 0 1 0 2 2 0 1 3 0 1 2 0 2 0 2 2 2\n[630] 1 2 0 2 0 2 2 2 0 0 0 0 2 2 3 0 0 0 0 0 1 0 0 0 2 0 0 0 0 2 0 2 1 0 0 1 0\n[667] 1 1 0 0 0 1 0 0 2 2 2 2 2 0 0 1 0 2 3 0 2 1 0 0 2 2 0 0 2 0 0 3 0 2 0 1 1\n[704] 3 0 1 4 0 2 0 0 0 1 0 2 0 0 1 0 1 1 0 0 0 2 1 0 1 2 2 0 2 0 0 2 1 0 1 0 2\n[741] 0 0 0 2 0 0 1 0 0 2 2 0 1 0 1 0 1 2 2 2 0 1 2 0 1 0 2 1\n\n\nIf we wanted to get the row with the most missing values, we can use which.max():\n\nwhich.max(sapply(1:nrow(dat), function(i) sum(is.na(dat[i, ]))))\n\n[1] 50\n\n\n\nsum(is.na(dat[50, ]))\n\n[1] 4\n\n\nRow 50 has 4 missing values.\n\n21.1.1 Visualize\nIt may be helpful to visualize missing data to get a quick impression of missingness. The rtemis package includes the function mplot3_missing():\n\nlibrary(rtemis)\n\n  .:rtemis 0.91.1 🌊 aarch64-apple-darwin20 (64-bit)\n  Defaults\n  |   Theme: whitegrid\n  |    Font: Fira Sans\n  | Palette: rtCol1\n  |    Plan: multicore\n  |   Cores: 8/10 available\n  Resources\n  | Documentation: https://rtemis.lambdamd.org\n  |       Learn R: https://class.lambdamd.org/pdsr\n  | rtemis themes: https://egenn.lambdamd.org/software/#rtemis_themes\n  |          Cite: `citation(\"rtemis\")`\n  Setup\n  | Enable progress reporting: `progressr::handlers(global = TRUE)`\n\nmplot3_missing(dat)\n\n\n\n\nMissing data is shown in magenta by default. The row below the image shows total NA values per column\n\n21.1.2 Summarize\nGet N of missing per column:\n\nsapply(dat, function(i) sum(is.na(i)))\n\npregnant  glucose pressure  triceps  insulin     mass pedigree      age \n       0        5       35      227      374       11        0        0 \ndiabetes \n       0 \n\n\nrtemis::checkData() includes information on missing data:\n\ncheckData(dat)\n dat: A data.frame with 768 rows and 9 features\n\n  Data types________________\n  * 8 continuous features\n  * 0 integer features\n  * 1 categorical feature, which is not ordered\n  * 0 character features\n  * 0 date features\n\n  Issues____________________\n  * 0 constant features\n  * 0 duplicated cases\n  * 5 features include 'NA' values; 652 'NA' values total\n     - Max percent missing in a feature is 48.70% (insulin)\n     - Max percent missing in a case is 44.44% (case #50)\n\n  Recommendations___________\n  * Consider imputing missing values or use complete cases only"
  },
  {
    "objectID": "MissingData.html#handle-missing-data",
    "href": "MissingData.html#handle-missing-data",
    "title": "21  Handling Missing data",
    "section": "\n21.2 Handle missing data",
    "text": "21.2 Handle missing data\nDifferent approaches can be used to handle missing data:\n\nDo nothing - if your algorithm(s) can handle missing data (decision trees!)\n\nExclude data: Use complete cases only\n\nFill in (make up) data: Replace or Impute\n\nReplace with median/mean\nPredict missing from present\n\nSingle imputation\nMultiple imputation\n\n\n\n\n\n\n21.2.1 Do nothing\nAlgorithms like decision trees and ensemble methods that use decision trees like random forest and gradient boosting can handle missing data, depending on the particular implementation. For example, rpart::rpart() which is used by rtemis::s_CART() has no trouble with missing data in the predictors:\n\ndat.cart <- s_CART(dat)\n[2022-07-19 03:01:24 s_CART] Hello, egenn \n\n[2022-07-19 03:01:24 dataPrepare] Imbalanced classes: using Inverse Probability Weighting \n\n.:Classification Input Summary\nTraining features: 768 x 8 \n Training outcome: 768 x 1 \n Testing features: Not available\n  Testing outcome: Not available\n\n[2022-07-19 03:01:24 s_CART] Training CART... \n\n.:CART Classification Training Summary\n                   Reference \n        Estimated  neg  pos  \n              neg  426   89\n              pos   74  179\n\n                   Overall  \n      Sensitivity  0.8520 \n      Specificity  0.6679 \nBalanced Accuracy  0.7600 \n              PPV  0.8272 \n              NPV  0.7075 \n               F1  0.8394 \n         Accuracy  0.7878 \n              AUC  0.7854 \n\n  Positive Class:  neg \n\n\n\n\n[2022-07-19 03:01:24 s_CART] Run completed in 0.01 minutes (Real: 0.47; User: 0.42; System: 0.03) \n\n\n\n21.2.2 Use complete cases only\nR’s builtin complete.cases() function returns, as the name suggests, a logical index of cases (i.e. rows) that have no missing values, i.e. are complete.\n\ndim(dat)\n\n[1] 768   9\n\nindex_cc <- complete.cases(dat)\nclass(index_cc)\n\n[1] \"logical\"\n\nlength(index_cc)\n\n[1] 768\n\nhead(index_cc)\n\n[1] FALSE FALSE FALSE  TRUE  TRUE FALSE\n\ndat_cc <- dat[index_cc, ]\ndim(dat_cc)\n\n[1] 392   9\n\n\nWe lost 376 cases in the above example. That’s quite a few, so, for this dataset, we probably want to look at options that do not exclude cases.\n\n21.2.3 Replace with a fixed value\nWe can manually replace missing values with the mean or median in the case of a continuous variable, or with the mode in the case of a categorical feature.\nFor example, to replace the first feature’s missing values with the mean:\n\npressure_mean <- mean(dat$pressure, na.rm = TRUE)\ndat_im <- dat\ndat_im$pressure[is.na(dat_im$pressure)] <- pressure_mean\n\nrtemis::preprocess() can replace missing values with mean (for numeric features) and the mode (for factors) for all columns:\n\ndat_pre <- preprocess(dat, impute = TRUE, impute.type = \"meanMode\")\n[2022-07-19 03:01:24 preprocess] Imputing missing values using mean and getMode... \n[2022-07-19 03:01:24 preprocess] Done \n\n\nVerify there are no missing data by rerunning checkData():\n\ncheckData(dat_pre)\n dat_pre: A data.frame with 768 rows and 9 features\n\n  Data types________________\n  * 8 continuous features\n  * 0 integer features\n  * 1 categorical feature, which is not ordered\n  * 0 character features\n  * 0 date features\n\n  Issues____________________\n  * 0 constant features\n  * 0 duplicated cases\n  * 0 features include 'NA' values\n\n  Recommendations___________\n  * Everything looks good\n\n\nYou may want to include a “missingness” column that indicates which cases were imputed to include in your model. You can create this simply by running:\n\npressure_missing = factor(as.integer(is.na(dat$pressure)))\n\npreprocess() includes the option missingness to add indicator columns after imputation:\n\ndat_pre <- preprocess(dat, impute = TRUE, impute.type = \"meanMode\",\n                      missingness = TRUE)\n[2022-07-19 03:01:24 preprocess] Created missingness indicator for glucose... \n[2022-07-19 03:01:24 preprocess] Created missingness indicator for pressure... \n[2022-07-19 03:01:24 preprocess] Created missingness indicator for triceps... \n[2022-07-19 03:01:24 preprocess] Created missingness indicator for insulin... \n[2022-07-19 03:01:24 preprocess] Created missingness indicator for mass... \n[2022-07-19 03:01:24 preprocess] Imputing missing values using mean and getMode... \n[2022-07-19 03:01:24 preprocess] Done \n\nhead(dat_pre)\n\n  pregnant glucose pressure  triceps  insulin mass pedigree age diabetes\n1        6     148       72 35.00000 155.5482 33.6    0.627  50      pos\n2        1      85       66 29.00000 155.5482 26.6    0.351  31      neg\n3        8     183       64 29.15342 155.5482 23.3    0.672  32      pos\n4        1      89       66 23.00000  94.0000 28.1    0.167  21      neg\n5        0     137       40 35.00000 168.0000 43.1    2.288  33      pos\n6        5     116       74 29.15342 155.5482 25.6    0.201  30      neg\n  glucose_missing pressure_missing triceps_missing insulin_missing mass_missing\n1               0                0               0               1            0\n2               0                0               0               1            0\n3               0                0               1               1            0\n4               0                0               0               0            0\n5               0                0               0               0            0\n6               0                0               1               1            0\n\n\n\n21.2.3.1 Add new level “missing”\nOne option to handle missing data in categorical variables, is to introduce a new level of “missing” to the factor, instead of replacing with the mode, for example. If we bin a continuous variable to convert to categorical, the same can then also be applied.\nSince no factors have missing values in the current dataset we create a copy and replace some data with NA:\n\ndat2 <- dat\ndat2$diabetes[sample(1:NROW(dat2), 35)] <- NA\nsum(is.na(dat2$diabetes))\n\n[1] 35\n\nlevels(dat2$diabetes)\n\n[1] \"neg\" \"pos\"\n\n\nReplace NA values with new level missing:\n\ndat_pre2 <- preprocess(dat2, factorNA2missing = TRUE)\n[2022-07-19 03:01:24 preprocess] Converting NA in factors to level \"missing\"... \n[2022-07-19 03:01:24 preprocess] Done \n\nanyNA(dat_pre2$diabetes)\n\n[1] FALSE\n\nlevels(dat_pre2$diabetes)\n\n[1] \"neg\"     \"pos\"     \"missing\"\n\n\n\n21.2.4 Last observation carried forward (LOCF)\nIn longitudinal / timeseries data, we may want to replace missing values with the last observed value. This is called last observation carried forward (LOCF). As always, whether this procedure is appropriate depend the reasons for missingness. The zoo and DescTool packages provide commands to perform LOCF.\nSome simulated data. We are missing blood pressure measurements on Saturdays and Sundays:\n\ndat <- data.frame(Day = rep(c(\"Mon\", \"Tues\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"), 3),\n                  SBP = sample(105:125, 21, TRUE))\ndat$SBP[dat$Day == \"Sat\" | dat$Day == \"Sun\"] <- NA\ndat\n\n    Day SBP\n1   Mon 107\n2  Tues 108\n3   Wed 117\n4   Thu 121\n5   Fri 123\n6   Sat  NA\n7   Sun  NA\n8   Mon 110\n9  Tues 111\n10  Wed 109\n11  Thu 124\n12  Fri 121\n13  Sat  NA\n14  Sun  NA\n15  Mon 119\n16 Tues 116\n17  Wed 121\n18  Thu 106\n19  Fri 107\n20  Sat  NA\n21  Sun  NA\n\n\nThe zoo package includes the na.locf().\n\ndat$SBPlocf <- zoo::na.locf(dat$SBP)\ndat\n\n    Day SBP SBPlocf\n1   Mon 107     107\n2  Tues 108     108\n3   Wed 117     117\n4   Thu 121     121\n5   Fri 123     123\n6   Sat  NA     123\n7   Sun  NA     123\n8   Mon 110     110\n9  Tues 111     111\n10  Wed 109     109\n11  Thu 124     124\n12  Fri 121     121\n13  Sat  NA     121\n14  Sun  NA     121\n15  Mon 119     119\n16 Tues 116     116\n17  Wed 121     121\n18  Thu 106     106\n19  Fri 107     107\n20  Sat  NA     107\n21  Sun  NA     107\n\n\nSimilar functionality is included in DescTools’ LOCF() function:\n\nDescTools::LOCF(dat$SBP)\n\n [1] 107 108 117 121 123 123 123 110 111 109 124 121 121 121 119 116 121 106 107\n[20] 107 107\n\n\n\n21.2.5 Replace missing values with estimated values\n\n21.2.5.1 Single imputation\nYou can use non-missing data to predict missing data in an iterative procedure (Buuren and Groothuis-Oudshoorn 2010; Stekhoven and Bühlmann 2012). The missRanger package uses the optimized (and parallel-capable) package ranger (Wright and Ziegler 2015) to iteratively train random forest models for imputation.\n\nlibrary(missRanger)\ndat <- iris\nset.seed(2020)\ndat[sample(1:150, 5), 1] <- dat[sample(1:150, 22), 4] <- dat[sample(1:150, 18), 4] <- NA\ndat_rfimp <- missRanger(dat, num.trees = 100)\n\n\nMissing value imputation by random forests\n\n  Variables to impute:      Sepal.Length, Petal.Width\n  Variables used to impute: Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, Species\niter 1: ..\niter 2: ..\niter 3: ..\niter 4: ..\n\nhead(dat_rfimp)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1     5.100000         3.5          1.4         0.2  setosa\n2     4.900000         3.0          1.4         0.2  setosa\n3     4.732533         3.2          1.3         0.2  setosa\n4     4.600000         3.1          1.5         0.2  setosa\n5     5.000000         3.6          1.4         0.2  setosa\n6     5.400000         3.9          1.7         0.4  setosa\n\ncheckData(dat_rfimp)\n dat_rfimp: A data.frame with 150 rows and 5 features\n\n  Data types________________\n  * 4 continuous features\n  * 0 integer features\n  * 1 categorical feature, which is not ordered\n     - 1 unordered categorical feature has more than 2 levels\n  * 0 character features\n  * 0 date features\n\n  Issues____________________\n  * 0 constant features\n  * 1 duplicated case\n  * 0 features include 'NA' values\n\n  Recommendations___________\n  * Remove the duplicated case\n  * Check the unordered categorical feature with more than 2 levels and consider\n    if ordering would make sense\n\n\nNote: The default method for preprocess(impute = TRUE) is to use missRanger.\n\n21.2.5.2 Multiple imputation\nMultiple imputation creates multiple estimates of the missing data. It is more statistically valid for small datasets, especially when the goal is to get accurate estimates of a summary statistics, but may not be practical for larger datasets. It is not usually considered an option for machine learning (where duplicating cases may add bias and complexity in resampling). The package mice is a popular choice for multiple imputation in R.\n\nlibrary(mice)\ndat_mice <- mice(dat)\n\n\n\n\n\nBuuren, S van, and Karin Groothuis-Oudshoorn. 2010. “Mice: Multivariate Imputation by Chained Equations in r.” Journal of Statistical Software, 1–68.\n\n\nMack, Christina, Zhaohui Su, and Daniel Westreich. 2018. “Managing Missing Data in Patient Registries: Addendum to Registries for Evaluating Patient Outcomes: A User’s Guide, [Internet].”\n\n\nStekhoven, Daniel J, and Peter Bühlmann. 2012. “MissForest—Non-Parametric Missing Value Imputation for Mixed-Type Data.” Bioinformatics 28 (1): 112–18.\n\n\nWright, Marvin N, and Andreas Ziegler. 2015. “Ranger: A Fast Implementation of Random Forests for High Dimensional Data in c++ and r.” arXiv Preprint arXiv:1508.04409."
  },
  {
    "objectID": "ClassesAndOOP.html",
    "href": "ClassesAndOOP.html",
    "title": "24  Classes and Object-Oriented Programming",
    "section": "",
    "text": "Object-Oriented Programming (OOP) is a programming paradigm built around objects with associated data, known as attributes, and functions, known as methods.\nThere are 4 main class systems in R:\nS3 and S4 methods are part of generic functions. RC and R6 methods are part of the object, but you can (and should) write generic functions for them as well.\nThis chapter will focus on the ubiquitous S3 system. For more advanced (and real OOP) applications, we recommend looking into the R6 system."
  },
  {
    "objectID": "ClassesAndOOP.html#s3",
    "href": "ClassesAndOOP.html#s3",
    "title": "24  Classes and Object-Oriented Programming",
    "section": "24.1 S3",
    "text": "24.1 S3\nMost R objects we have been using so far are S3 objects. Data frames are some of the most common S3 objects.\nGeneric functions are functions that act differently based on the class of the input object. We have already used many of them. For example, summary() works differently on a data.frame, on a factor, or a glm object, etc.\nGeneric functions in R are saved as functionname.classname() and called automatically, based on the class of the first argument. This allows the same function, e.g. print(), summary(), c(), to have a different effect on objects of different classes. For example, the print() function applied on a data frame, will actually call print.data.frame(), while applied on a factor, it will call print.factor().\nThis means that when you type print(iris) this calls print.data.frame(iris)\nNote how the R documentation lists usage information separately for each S3 method, e.g. ## S3 method for class 'factor'\n\n24.1.1 methods()\nTo get a list of all available methods defined for a specific class,\ni.e. “What different functions can I use on this object?”\n\nmethods(class = \"data.frame\")\n\n [1] [             [[            [[<-          [<-           $<-          \n [6] aggregate     anyDuplicated anyNA         as.data.frame as.list      \n[11] as.matrix     as.vector     by            cbind         coerce       \n[16] dim           dimnames      dimnames<-    droplevels    duplicated   \n[21] edit          format        formula       head          initialize   \n[26] is.na         Math          merge         na.exclude    na.omit      \n[31] Ops           plot          print         prompt        rbind        \n[36] row.names     row.names<-   rowsum        show          slotsFromS3  \n[41] split         split<-       stack         str           subset       \n[46] summary       Summary       t             tail          transform    \n[51] type.convert  unique        unstack       within        xtfrm        \nsee '?methods' for accessing help and source code\n\n\nConversely, to get a list of all available methods for a generic function (i.e. which classes have)\n(i.e. “What objects can I use this function on?”)\n\nmethods(generic.function = \"plot\")\n\n [1] plot.acf*           plot.data.frame*    plot.decomposed.ts*\n [4] plot.default        plot.dendrogram*    plot.density*      \n [7] plot.ecdf           plot.factor*        plot.formula*      \n[10] plot.function       plot.hclust*        plot.histogram*    \n[13] plot.HoltWinters*   plot.isoreg*        plot.lm*           \n[16] plot.medpolish*     plot.mlm*           plot.ppr*          \n[19] plot.prcomp*        plot.princomp*      plot.profile.nls*  \n[22] plot.raster*        plot.spec*          plot.stepfun       \n[25] plot.stl*           plot.table*         plot.ts            \n[28] plot.tskernel*      plot.TukeyHSD*     \nsee '?methods' for accessing help and source code\n\n\n\n\n24.1.2 Defining custom S3 classes\nIt very simple to assign an object to a new class.\nThere is no formal class definition, an object is directly assigned to a class by name. An object can belong to multiple classes:\n\nx <- 1:10\nclass(x) <- c(\"specialvector\", \"numeric\")\nclass(x)\n\n[1] \"specialvector\" \"numeric\"      \n\n\nThe hierarchy of classes goes left to right, meaning that generic methods are searched for classes in the order they appear in the output of class().\nIf we print x, since there is no print method for class specialvector or for numeric, the default print.default() command is automatically called:\n\nprint(x)\n\n [1]  1  2  3  4  5  6  7  8  9 10\nattr(,\"class\")\n[1] \"specialvector\" \"numeric\"      \n\nprint.default(x)\n\n [1]  1  2  3  4  5  6  7  8  9 10\nattr(,\"class\")\n[1] \"specialvector\" \"numeric\"      \n\n\nTo create a custom print() function for out new class specialvector, we define a function named print.[classname]:\n\nprint.specialvector <- function(x, ...) {\n  cat(\"This is a special vector of length\", length(x), \"\\n\")\n  cat(\"Its mean value is\", mean(x, na.rm = TRUE), \"and its median is\", median(x, na.rm = TRUE))\n  cat(\"\\nHere are the first few elements:\\n\", head(x), \"\\n\")\n}\n\nNow, when you print an object of class specialvector, the custom print() command is invoked:\n\nx\n\nThis is a special vector of length 10 \nIts mean value is 5.5 and its median is 5.5\nHere are the first few elements:\n 1 2 3 4 5 6 \n\n\nIf needed, you can call the default or another appropriate method directly:\n\nprint.default(x)\n\n [1]  1  2  3  4  5  6  7  8  9 10\nattr(,\"class\")\n[1] \"specialvector\" \"numeric\"      \n\n\nYou can change the vector back to a regular numeric vector, or a different class, just as easily:\n\nclass(x) <- \"numeric\"\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10"
  },
  {
    "objectID": "DataTable.html",
    "href": "DataTable.html",
    "title": "25  Efficient data analysis with data.table",
    "section": "",
    "text": "The data.table package provides a modern and highly optimized version of R’s data.frame structure. It is highly memory efficient and automatically parallelizes internal operations to achieve substantial speed improvements over data.frames. The data.table package weighs in at just a few kilobytes, has zero dependencies, and maintains compatibility with R versions going as far back as possible."
  },
  {
    "objectID": "DataTable.html#data.table-singificantly-extends-the-power-of-data.frame",
    "href": "DataTable.html#data.table-singificantly-extends-the-power-of-data.frame",
    "title": "25  Efficient data analysis with data.table",
    "section": "\n25.1 data.table singificantly extends the power of data.frame\n",
    "text": "25.1 data.table singificantly extends the power of data.frame\n\nSome of the ways in which a data.table differs from a data.frame:\n\nA lot more than indexing can be done within a data.table’s “frame” (dt[i, j, by]): filter, select & operate on columns, group-by operations\nAccess column names directly without quoting\nMany operations can be performed “in-place” (i.e. with no assignment)\nWorking on big data within a data.table can be orders of magnitude faster.\n\ndata.table operations remain as close as possible to data.frame operations, trying to extend rather than replace the latter’s functionality.\ndata.table includes thorough and helpful error messages that often point to a solution. This includes common mistakes new users may make when trying commands that would work on a data.frame but are different on a data.table."
  },
  {
    "objectID": "DataTable.html#dtinstallation",
    "href": "DataTable.html#dtinstallation",
    "title": "25  Efficient data analysis with data.table",
    "section": "\n25.2 Installation",
    "text": "25.2 Installation\nYou can install data.table from CRAN or GitHub. Check out the data.table wiki for more info.\nTo get the latest version on CRAN:\n\ninstall.packages(\"data.table\")\n\nTo get the latest development version:\n\n# install the \"remotes\" package if you don't already have it and then\nremotes::install_github(\"Rdatatable/data.table\")\n\ndata.table also includes a built-in command to update to the latest development version:\n\ndata.table::update.dev.pkg()\n\n\n25.2.1 Load the data.table package\n\nlibrary(data.table)"
  },
  {
    "objectID": "DataTable.html#create-a-data.table",
    "href": "DataTable.html#create-a-data.table",
    "title": "25  Efficient data analysis with data.table",
    "section": "\n25.3 Create a data.table\n",
    "text": "25.3 Create a data.table\n\n\n25.3.1 By assignment: data.table()\n\nLet’s create a data.frame and a data.table to explore side by side.\n\ndf <- data.frame(A = 1:5,\n                 B = c(1.2, 4.3, 9.7, 5.6, 8.1),\n                 C = c(\"a\", \"b\", \"b\", \"a\", \"a\"))\nclass(df)\n\n[1] \"data.frame\"\n\ndf\n\n  A   B C\n1 1 1.2 a\n2 2 4.3 b\n3 3 9.7 b\n4 4 5.6 a\n5 5 8.1 a\n\n\ndata.table() syntax is similar to data.frame() (differs in some arguments)\n\ndt <- data.table(A = 1:5,\n                 B = c(1.2, 4.3, 9.7, 5.6, 8.1),\n                 C = c(\"a\", \"b\", \"b\", \"a\", \"a\"))\ndt\n\n   A   B C\n1: 1 1.2 a\n2: 2 4.3 b\n3: 3 9.7 b\n4: 4 5.6 a\n5: 5 8.1 a\n\nclass(dt)\n\n[1] \"data.table\" \"data.frame\"\n\n\nNotice how a data.table object also inherits from data.frame. This means that if a method does not exist for data.table, the method for data.frame will be used - review classes and generic functions.\nAs part of improving efficieny, data.tables do away with row names. Instead of using rownames, you can and should add an extra column (e.g. “ID”) with the same information - this is advisable when working with data.frames as well.\nA rather convenient option is to have data.tables print each column’s class below the column name. You can pass the argument class = TRUE to print() or set the global option datatable.print.class using options()\n\noptions(datatable.print.class = TRUE)\ndt\n\n       A     B      C\n   <int> <num> <char>\n1:     1   1.2      a\n2:     2   4.3      b\n3:     3   9.7      b\n4:     4   5.6      a\n5:     5   8.1      a\n\n\nSame as with a data.frame, to automatically convert string to factors, you can use the stringsAsFactors argument (or factor() directly):\n\ndt <- data.table(A = 1:5,\n                 B = c(1.2, 4.3, 9.7, 5.6, 8.1),\n                 C = c(\"a\", \"b\", \"b\", \"a\", \"a\"),\n                 stringsAsFactors = TRUE)\ndt\n\n       A     B      C\n   <int> <num> <fctr>\n1:     1   1.2      a\n2:     2   4.3      b\n3:     3   9.7      b\n4:     4   5.6      a\n5:     5   8.1      a\n\n\n\n25.3.2 By coercion: as.data.table()\n\n\ndat <- data.frame(A = 1:5,\n                  B = c(1.2, 4.3, 9.7, 5.6, 8.1),\n                  C = c(\"a\", \"b\", \"b\", \"a\", \"a\"),\n                  stringsAsFactors = TRUE)\ndat\n\n  A   B C\n1 1 1.2 a\n2 2 4.3 b\n3 3 9.7 b\n4 4 5.6 a\n5 5 8.1 a\n\ndat2 <- as.data.table(dat)\ndat2\n\n       A     B      C\n   <int> <num> <fctr>\n1:     1   1.2      a\n2:     2   4.3      b\n3:     3   9.7      b\n4:     4   5.6      a\n5:     5   8.1      a\n\n\n\n25.3.3 By coercion in-place: setDT()\n\nsetDT converts a list or data.frame into a data.table in-place. Note: the original object itself is changed, you do not need to assign the output of setDT to a new name.\n\ndat <- data.frame(A = 1:5,\n                  B = c(1.2, 4.3, 9.7, 5.6, 8.1),\n                  C = c(\"a\", \"b\", \"b\", \"a\", \"a\"))\nclass(dat)\n\n[1] \"data.frame\"\n\nsetDT(dat)\nclass(dat)\n\n[1] \"data.table\" \"data.frame\"\n\n\nYou can similarly convert a data.table to a data.frame, in-place:\n\nsetDF(dat)\nclass(dat)\n\n[1] \"data.frame\"\n\n\n\n25.3.4 Read into a data.table from file with fread()\n\ndata.table includes the fread() function to read data from files, in a similar way as the base functions read.csv() and read.table(). It is orders of magnitude faster for very large data (e.g. thousands to millions of rows) and it can read directly from URLs and zipped files. The sep argument defines the separator (same as in read.csv() and read.table()), but when set to \"auto\" (the default) it does a great job of figuring it out by itself.\n\ndat <- fread(\"path/to/input.csv\")\ndat <- fread(\"https::/url/to/input.csv.gz\")\n\nFor its speed and convenience, fread() is recommended over read.csv()/read.table() even if you intend to work with a data.frame exclusively, in which case you can pass the argument data.table = FALSE.\n\n25.3.5 Write a data.table to a CSV: fwrite()\n\nSimilar to fread(), fwrite() can be a lot faster than write.csv()\n\nfwrite(dt, \"/path/to/text.csv\")\n\n\n25.3.6 Save data.table to an RDS:\nSame as any R object, you can save a data.table to disk using saveRDS(). Suppose you have read data in with fread() or coerced a dataset using as.data.table(), done some cleaning up, type conversions, data transformations, etc, this is the preferred way to save your work, so you can reload at any time.\n\nsaveRDS(dt, \"/path/to/data.rds\")"
  },
  {
    "objectID": "DataTable.html#display-data.table-structure-with-str",
    "href": "DataTable.html#display-data.table-structure-with-str",
    "title": "25  Efficient data analysis with data.table",
    "section": "\n25.4 Display data.table structure with str()\n",
    "text": "25.4 Display data.table structure with str()\n\nstr() works the same (and you should keep using it!)\n\nstr(df)\n\n'data.frame':   5 obs. of  3 variables:\n $ A: int  1 2 3 4 5\n $ B: num  1.2 4.3 9.7 5.6 8.1\n $ C: chr  \"a\" \"b\" \"b\" \"a\" ...\n\n\n\nstr(dt)\n\nClasses 'data.table' and 'data.frame':  5 obs. of  3 variables:\n $ A: int  1 2 3 4 5\n $ B: num  1.2 4.3 9.7 5.6 8.1\n $ C: Factor w/ 2 levels \"a\",\"b\": 1 2 2 1 1\n - attr(*, \".internal.selfref\")=<externalptr>"
  },
  {
    "objectID": "DataTable.html#combine-data.tables",
    "href": "DataTable.html#combine-data.tables",
    "title": "25  Efficient data analysis with data.table",
    "section": "\n25.5 Combine data.tables",
    "text": "25.5 Combine data.tables\ncbind() and rbind() work on data.tables the same as on data.frames:\n\ndt1 <- data.table(a = 1:5)\ndt2 <- data.table(b = 11:15)\ncbind(dt1, dt2)\n\n       a     b\n   <int> <int>\n1:     1    11\n2:     2    12\n3:     3    13\n4:     4    14\n5:     5    15\n\nrbind(dt1, dt1)\n\n        a\n    <int>\n 1:     1\n 2:     2\n 3:     3\n 4:     4\n 5:     5\n 6:     1\n 7:     2\n 8:     3\n 9:     4\n10:     5"
  },
  {
    "objectID": "DataTable.html#filter-rows",
    "href": "DataTable.html#filter-rows",
    "title": "25  Efficient data analysis with data.table",
    "section": "\n25.6 Filter rows",
    "text": "25.6 Filter rows\nThere are many similarities and some notable differences in how indexing works in a data.table vs. a data.frame.\nFiltering rows with an integer or logical index is largely the same in a data.frame and a data.table, but in a data.table you can omit the comma to select all columns:\n\ndf[c(1, 3, 5), ]\n\n  A   B C\n1 1 1.2 a\n3 3 9.7 b\n5 5 8.1 a\n\ndt[c(1, 3, 5), ]\n\n       A     B      C\n   <int> <num> <fctr>\n1:     1   1.2      a\n2:     3   9.7      b\n3:     5   8.1      a\n\ndt[c(1, 3, 5)]\n\n       A     B      C\n   <int> <num> <fctr>\n1:     1   1.2      a\n2:     3   9.7      b\n3:     5   8.1      a\n\n\nUsing a variable that holds a row index, whether integer or logical:\n\nrowid <- c(1, 3, 5)\ndf[rowid, ]\n\n  A   B C\n1 1 1.2 a\n3 3 9.7 b\n5 5 8.1 a\n\ndt[rowid, ]\n\n       A     B      C\n   <int> <num> <fctr>\n1:     1   1.2      a\n2:     3   9.7      b\n3:     5   8.1      a\n\ndt[rowid]\n\n       A     B      C\n   <int> <num> <fctr>\n1:     1   1.2      a\n2:     3   9.7      b\n3:     5   8.1      a\n\n\n\nrowbn <- c(T, F, T, F, T)\ndf[rowbn, ]\n\n  A   B C\n1 1 1.2 a\n3 3 9.7 b\n5 5 8.1 a\n\ndt[rowbn, ]\n\n       A     B      C\n   <int> <num> <fctr>\n1:     1   1.2      a\n2:     3   9.7      b\n3:     5   8.1      a\n\ndt[rowbn]\n\n       A     B      C\n   <int> <num> <fctr>\n1:     1   1.2      a\n2:     3   9.7      b\n3:     5   8.1      a\n\n\n\n25.6.1 Conditional filtering\nAs a reminder, there are a few ways to conditionally filter cases in a data.frame:\n\ndf[df$A > mean(df$A) & df$B > mean(df$B), ]\n\n  A   B C\n5 5 8.1 a\n\nsubset(df, A > mean(A) & B > mean(B))\n\n  A   B C\n5 5 8.1 a\n\nwith(df, df[A > mean(A) & B > mean(B), ])\n\n  A   B C\n5 5 8.1 a\n\n\ndata.table allows you to refer to column names directly and unquoted, which makes writing filter conditions easier/more compact:\n\ndt[A > mean(A) & B > mean(B)]\n\n       A     B      C\n   <int> <num> <fctr>\n1:     5   8.1      a\n\n\nThe data.table package also includes an S3 method for subset() that works the same way as with a data.frame:\n\nsubset(dt, A > mean(A) & B > mean(B))\n\n       A     B      C\n   <int> <num> <fctr>\n1:     5   8.1      a\n\n\nAs another example, exclude cases base on missingness in a specific column:\n\nadf <- as.data.frame(sapply(1:5, function(i) rnorm(10)))\nadf |> head()\n\n           V1          V2         V3          V4         V5\n1  0.07785112 -1.40989075  1.6050699  0.01631735 -0.1554661\n2  0.12488378 -1.51786634  0.8803421  0.32047494  1.6758771\n3 -0.43436603  0.04537685  0.6465605 -0.49056223  0.2339048\n4  2.06915688 -0.43561869 -0.3254917  0.96350877  0.5602833\n5 -0.09792629 -0.68803980  0.9869626 -0.04083866  0.3872971\n6  2.58905257 -2.66503830  0.1428826  0.78446498 -0.5310147\n\nadf[1, 3] <- adf[3, 4] <- adf[5, 3] <- adf[7, 3] <- NA\nadt <- as.data.table(adf)\n\n\nadf[!is.na(adf$V3), ]\n\n             V1          V2         V3         V4         V5\n2   0.124883777 -1.51786634  0.8803421  0.3204749  1.6758771\n3  -0.434366031  0.04537685  0.6465605         NA  0.2339048\n4   2.069156876 -0.43561869 -0.3254917  0.9635088  0.5602833\n6   2.589052572 -2.66503830  0.1428826  0.7844650 -0.5310147\n8   1.092553531 -0.38771044  0.7547444  0.1606748 -0.3455334\n9   0.288307212  0.56306195 -0.2049352  0.1328987  0.4723829\n10 -0.005963278 -2.41017845 -0.9503053 -0.1594507  0.7091141\n\nadt[!is.na(V3)]\n\n             V1          V2         V3         V4         V5\n          <num>       <num>      <num>      <num>      <num>\n1:  0.124883777 -1.51786634  0.8803421  0.3204749  1.6758771\n2: -0.434366031  0.04537685  0.6465605         NA  0.2339048\n3:  2.069156876 -0.43561869 -0.3254917  0.9635088  0.5602833\n4:  2.589052572 -2.66503830  0.1428826  0.7844650 -0.5310147\n5:  1.092553531 -0.38771044  0.7547444  0.1606748 -0.3455334\n6:  0.288307212  0.56306195 -0.2049352  0.1328987  0.4723829\n7: -0.005963278 -2.41017845 -0.9503053 -0.1594507  0.7091141"
  },
  {
    "objectID": "DataTable.html#select-columns",
    "href": "DataTable.html#select-columns",
    "title": "25  Efficient data analysis with data.table",
    "section": "\n25.7 Select columns",
    "text": "25.7 Select columns\n\n25.7.1 By position(s)\nSelecting a single column in data.table does not drop to a vector, similar to using drop = FALSE in a data.frame:\n\ndf[, 1]\n\n[1] 1 2 3 4 5\n\ndf[, 1, drop = FALSE]\n\n  A\n1 1\n2 2\n3 3\n4 4\n5 5\n\ndt[, 1]\n\n       A\n   <int>\n1:     1\n2:     2\n3:     3\n4:     4\n5:     5\n\n\nDouble bracket indexing of a single column works the same on a data.frame and a data.table, returning a vector:\n\ndf[[2]]\n\n[1] 1.2 4.3 9.7 5.6 8.1\n\ndt[[2]]\n\n[1] 1.2 4.3 9.7 5.6 8.1\n\n\nA vector of column positions returns a smaller data.table, similar to how it returns a smaller data.frame :\n\ndf[, c(1, 2)]\n\n  A   B\n1 1 1.2\n2 2 4.3\n3 3 9.7\n4 4 5.6\n5 5 8.1\n\ndt[, c(1, 2)]\n\n       A     B\n   <int> <num>\n1:     1   1.2\n2:     2   4.3\n3:     3   9.7\n4:     4   5.6\n5:     5   8.1\n\n\n\n25.7.2 By name(s)\nIn data.table, you access column names directly without quoting or using the $ notation:\n\ndf[, \"B\"]\n\n[1] 1.2 4.3 9.7 5.6 8.1\n\ndf$B\n\n[1] 1.2 4.3 9.7 5.6 8.1\n\ndt[, B]\n\n[1] 1.2 4.3 9.7 5.6 8.1\n\n\nBecause of the above, data.table requires a slightly different syntax to use a variable as a column index which can contain integer positions, logical index, or column names:\n\ncolid <- c(1, 2)\ncolbn <- c(F, T, T)\ncolnm <- c(\"A\", \"C\")\ndf[, colid]\n\n  A   B\n1 1 1.2\n2 2 4.3\n3 3 9.7\n4 4 5.6\n5 5 8.1\n\ndf[, colbn]\n\n    B C\n1 1.2 a\n2 4.3 b\n3 9.7 b\n4 5.6 a\n5 8.1 a\n\ndf[, colnm]\n\n  A C\n1 1 a\n2 2 b\n3 3 b\n4 4 a\n5 5 a\n\n\nTo use a variable holding a column index in a data.table, prefix it with two periods:\n\ndt[, ..colid]\n\n       A     B\n   <int> <num>\n1:     1   1.2\n2:     2   4.3\n3:     3   9.7\n4:     4   5.6\n5:     5   8.1\n\ndt[, ..colbn]\n\n       B      C\n   <num> <fctr>\n1:   1.2      a\n2:   4.3      b\n3:   9.7      b\n4:   5.6      a\n5:   8.1      a\n\ndt[, ..colnm]\n\n       A      C\n   <int> <fctr>\n1:     1      a\n2:     2      b\n3:     3      b\n4:     4      a\n5:     5      a\n\n\nIf you are familiar with the system shell:\nThink of working inside the data.table frame (i.e. within the “[…]”) like an environment. You have direct access to the variables within it. If you want to refer to variables outside the data.table, you prefix the variable name with .. similar to how you access the directory above your current working directory in the system shell:\nAlternatively, you can use the .SD special symbol together with the .SDcols argument:\n\ndt[, .SD, .SDcols = colid]\n\n       A     B\n   <int> <num>\n1:     1   1.2\n2:     2   4.3\n3:     3   9.7\n4:     4   5.6\n5:     5   8.1\n\n\nThink of .SD as a sub-data.table with columns defined by .SDcols (if SDcols is not defined, .SD refers to the entire data.table).\nThe two dots tell the data.table to not look for the variable within the data.table columns, but in the enclosing environment.\nSelecting a single column by name returns a vector:\n\ndt[, A]\n\n[1] 1 2 3 4 5\n\n\nSelecting one or more columns by name enclosed in list() or .() (which, in this case, is short for list()), always returns a data.table:\n\ndt[, .(A)]\n\n       A\n   <int>\n1:     1\n2:     2\n3:     3\n4:     4\n5:     5\n\ndt[, .(A, B)]\n\n       A     B\n   <int> <num>\n1:     1   1.2\n2:     2   4.3\n3:     3   9.7\n4:     4   5.6\n5:     5   8.1"
  },
  {
    "objectID": "DataTable.html#add-new-column-in-place",
    "href": "DataTable.html#add-new-column-in-place",
    "title": "25  Efficient data analysis with data.table",
    "section": "\n25.8 Add new column in-place\n",
    "text": "25.8 Add new column in-place\n\nUse := assignment to add a new column in the existing data.table. In-place assignment means you do not have to assign the result to a variable, the existing data.table will be modified.\n\ndt[, AplusB := A + B]\ndt\n\n       A     B      C AplusB\n   <int> <num> <fctr>  <num>\n1:     1   1.2      a    2.2\n2:     2   4.3      b    6.3\n3:     3   9.7      b   12.7\n4:     4   5.6      a    9.6\n5:     5   8.1      a   13.1\n\n\nNote how dt was modified even though we did not run dt <- dt[, AplusB := A + B]"
  },
  {
    "objectID": "DataTable.html#add-multiple-columns-in-place",
    "href": "DataTable.html#add-multiple-columns-in-place",
    "title": "25  Efficient data analysis with data.table",
    "section": "\n25.9 Add multiple columns in-place\n",
    "text": "25.9 Add multiple columns in-place\n\nYou can define multiple new column names using a character vector of new column names on the left of := and a list on the right.\n\ndt[, c(\"AtimesB\", \"AoverB\") := list(A*B, A/B)]\n\nWe can use lapply() since it always returns a list:\n\nvnames <- c(\"A\", \"B\")\ndt[, paste0(\"log\", vnames) := lapply(.SD, log), .SDcols = vnames]\ndt\n\n       A     B      C AplusB AtimesB    AoverB      logA      logB\n   <int> <num> <fctr>  <num>   <num>     <num>     <num>     <num>\n1:     1   1.2      a    2.2     1.2 0.8333333 0.0000000 0.1823216\n2:     2   4.3      b    6.3     8.6 0.4651163 0.6931472 1.4586150\n3:     3   9.7      b   12.7    29.1 0.3092784 1.0986123 2.2721259\n4:     4   5.6      a    9.6    22.4 0.7142857 1.3862944 1.7227666\n5:     5   8.1      a   13.1    40.5 0.6172840 1.6094379 2.0918641\n\n\nYou can also use := in a little more awkward syntax:\n\ndt[, `:=`(AminusB = A - B, AoverC = A / B)]\ndt\n\n       A     B      C AplusB AtimesB    AoverB      logA      logB AminusB\n   <int> <num> <fctr>  <num>   <num>     <num>     <num>     <num>   <num>\n1:     1   1.2      a    2.2     1.2 0.8333333 0.0000000 0.1823216    -0.2\n2:     2   4.3      b    6.3     8.6 0.4651163 0.6931472 1.4586150    -2.3\n3:     3   9.7      b   12.7    29.1 0.3092784 1.0986123 2.2721259    -6.7\n4:     4   5.6      a    9.6    22.4 0.7142857 1.3862944 1.7227666    -1.6\n5:     5   8.1      a   13.1    40.5 0.6172840 1.6094379 2.0918641    -3.1\n      AoverC\n       <num>\n1: 0.8333333\n2: 0.4651163\n3: 0.3092784\n4: 0.7142857\n5: 0.6172840"
  },
  {
    "objectID": "DataTable.html#convert-column-type",
    "href": "DataTable.html#convert-column-type",
    "title": "25  Efficient data analysis with data.table",
    "section": "\n25.10 Convert column type",
    "text": "25.10 Convert column type\n\n25.10.1 Assignment by reference with :=\n\nUse any base R coercion function (as.*) to convert a column in-place using the := notation\n\ndt[, A := as.numeric(A)]\ndt\n\n       A     B      C AplusB AtimesB    AoverB      logA      logB AminusB\n   <num> <num> <fctr>  <num>   <num>     <num>     <num>     <num>   <num>\n1:     1   1.2      a    2.2     1.2 0.8333333 0.0000000 0.1823216    -0.2\n2:     2   4.3      b    6.3     8.6 0.4651163 0.6931472 1.4586150    -2.3\n3:     3   9.7      b   12.7    29.1 0.3092784 1.0986123 2.2721259    -6.7\n4:     4   5.6      a    9.6    22.4 0.7142857 1.3862944 1.7227666    -1.6\n5:     5   8.1      a   13.1    40.5 0.6172840 1.6094379 2.0918641    -3.1\n      AoverC\n       <num>\n1: 0.8333333\n2: 0.4651163\n3: 0.3092784\n4: 0.7142857\n5: 0.6172840\n\n\n\n25.10.2 Delete columns in-place with :=\n\nTo delete a column, use := to set it to NULL:\n\ndt[, AoverB := NULL]\ndt\n\n       A     B      C AplusB AtimesB      logA      logB AminusB    AoverC\n   <num> <num> <fctr>  <num>   <num>     <num>     <num>   <num>     <num>\n1:     1   1.2      a    2.2     1.2 0.0000000 0.1823216    -0.2 0.8333333\n2:     2   4.3      b    6.3     8.6 0.6931472 1.4586150    -2.3 0.4651163\n3:     3   9.7      b   12.7    29.1 1.0986123 2.2721259    -6.7 0.3092784\n4:     4   5.6      a    9.6    22.4 1.3862944 1.7227666    -1.6 0.7142857\n5:     5   8.1      a   13.1    40.5 1.6094379 2.0918641    -3.1 0.6172840\n\n\nDelete multiple columns\n\ndt[, c(\"logA\", \"logB\") := NULL]\n\nOr:\n\ndt[, `:=`(AplusB = NULL, AminusB = NULL)]\ndt\n\n       A     B      C AtimesB    AoverC\n   <num> <num> <fctr>   <num>     <num>\n1:     1   1.2      a     1.2 0.8333333\n2:     2   4.3      b     8.6 0.4651163\n3:     3   9.7      b    29.1 0.3092784\n4:     4   5.6      a    22.4 0.7142857\n5:     5   8.1      a    40.5 0.6172840\n\n\n\n25.10.3 Fast loop-able assignment with set()\n\ndata.table’s set() is a looop-able version of the \"= operator. Use it in a for loop to operate on multiple columns.\nSyntax: set(dt, i, j, value)\n\n\ndt the data.table to operate on\n\ni optionally define which rows to operate on. i = NULL to operate on all\n\nj column names or index to be assigned value\n\n\nvalue values to be assigned to j by reference\n\nAs a simple example, transform the first two columns in-place by squaring:\n\nfor (i in 1:2) {\n  set(dt, i = NULL, j = i, value = dt[[i]]^2)\n}"
  },
  {
    "objectID": "DataTable.html#summarize",
    "href": "DataTable.html#summarize",
    "title": "25  Efficient data analysis with data.table",
    "section": "\n25.11 Summarize",
    "text": "25.11 Summarize\nYou can apply one or multiple summary functions on one of multiple columns. Surround the operations in list() or .() to output a new data.table holding the outputs of the operations (the input data.table remains unchanged).\n\nAsummary <- dt[, .(Amax = max(A), Amin = min(A), Asd = sd(A))]\nAsummary\n\n    Amax  Amin     Asd\n   <num> <num>   <num>\n1:    25     1 9.66954\n\n\nExample: Get sd of all numeric columns:\n\nnumid <- sapply(dt, is.numeric)\ndt_mean <- dt[, lapply(.SD, sd), .SDcols = numid]\ndt_mean\n\n         A        B  AtimesB    AoverC\n     <num>    <num>    <num>     <num>\n1: 9.66954 37.35521 15.74462 0.2060219\n\n\nIf your function returns more than one value, the output will have multiple rows:\n\ndt_range <- dt[, lapply(.SD, range), .SDcols = numid]\ndt_range\n\n       A     B AtimesB    AoverC\n   <num> <num>   <num>     <num>\n1:     1  1.44     1.2 0.3092784\n2:    25 94.09    40.5 0.8333333"
  },
  {
    "objectID": "DataTable.html#group-by-operations",
    "href": "DataTable.html#group-by-operations",
    "title": "25  Efficient data analysis with data.table",
    "section": "\n25.12 Group-by operations",
    "text": "25.12 Group-by operations\nUp to now, we have learned how to use the data.table frame dat[i, j] to filter cases in i or add/remove/transform columns in-place in j. dat[i, j, by] allows to perform operations separately on groups of cases.\n\ndt <- data.table(A = 1:5,\n                 B = c(1.2, 4.3, 9.7, 5.6, 8.1),\n                 C = rnorm(5),\n                 Group = c(\"a\", \"b\", \"b\", \"a\", \"a\"))\ndt\n\n       A     B           C  Group\n   <int> <num>       <num> <char>\n1:     1   1.2  0.52912070      a\n2:     2   4.3 -0.67660576      b\n3:     3   9.7 -0.31898369      b\n4:     4   5.6  1.79279641      a\n5:     5   8.1 -0.09152723      a\n\n\n\n25.12.1 Group-by summary\nAs we’ve seen, using .() or list() in j, returns a new data.table:\n\ndt[, .(meanAbyGroup = mean(A)), by = Group]\n\n    Group meanAbyGroup\n   <char>        <num>\n1:      a     3.333333\n2:      b     2.500000\n\ndt[, list(medianBbyGroup = median(B)), by = Group]\n\n    Group medianBbyGroup\n   <char>          <num>\n1:      a            5.6\n2:      b            7.0\n\n\n\n25.12.2 Group-by operation and assignment\nMaking an assignment with := in j, adds a column in-place. Since here we are grouping, the same value will be assigned to all cases of the group:\n\ndt[, meanAbyGroup := mean(A), by = Group]\ndt\n\n       A     B           C  Group meanAbyGroup\n   <int> <num>       <num> <char>        <num>\n1:     1   1.2  0.52912070      a     3.333333\n2:     2   4.3 -0.67660576      b     2.500000\n3:     3   9.7 -0.31898369      b     2.500000\n4:     4   5.6  1.79279641      a     3.333333\n5:     5   8.1 -0.09152723      a     3.333333\n\n\nFor more complex operations, you may need to refer to the slice of the data.table defined by by within j. There is a special notation for this: .SD (think sub-data.table):\n\ndt[, B_DiffFromGroupMin := B - min(B), by = Group]\ndt\n\n       A     B           C  Group meanAbyGroup B_DiffFromGroupMin\n   <int> <num>       <num> <char>        <num>              <num>\n1:     1   1.2  0.52912070      a     3.333333                0.0\n2:     2   4.3 -0.67660576      b     2.500000                0.0\n3:     3   9.7 -0.31898369      b     2.500000                5.4\n4:     4   5.6  1.79279641      a     3.333333                4.4\n5:     5   8.1 -0.09152723      a     3.333333                6.9\n\n\n\n\n\nBy now, it should be clearer that the data.table frame provides a very flexible way to perform a very wide range of operations with minimal new notation."
  },
  {
    "objectID": "DataTable.html#apply-functions-to-columns",
    "href": "DataTable.html#apply-functions-to-columns",
    "title": "25  Efficient data analysis with data.table",
    "section": "\n25.13 Apply functions to columns",
    "text": "25.13 Apply functions to columns\nAny function that returns a list can be used in j to return a new data.table - therefore lapply is perfect for getting summary on multiple columns:\n\ndt1 <- as.data.table(sapply(1:3, \\(i) rnorm(10)))\ndt1\n\n              V1          V2          V3\n           <num>       <num>       <num>\n 1:  0.668502204 -0.05969575  0.83717006\n 2:  0.597326456 -0.46056888 -0.38031235\n 3: -0.006375072 -1.91841874  0.01005586\n 4:  0.284818822 -0.41976522 -2.15343630\n 5:  1.261268116 -0.03468561 -1.40507718\n 6: -0.893029837 -1.73596789 -0.47709508\n 7:  0.757157660 -0.55460528 -0.73053883\n 8: -0.159937785  1.60336929  1.16789727\n 9:  2.587670877  0.36316138  2.27203881\n10:  0.468011889 -0.83516791 -1.77513127\n\nsetnames(dt1, names(dt1), c(\"Alpha\", \"Beta\", \"Gamma\"))\ndt1[, lapply(.SD, mean)]\n\n       Alpha       Beta      Gamma\n       <num>      <num>      <num>\n1: 0.5565413 -0.4052345 -0.2634429\n\n\nYou can specify which columns to operate on using the .SDcols argument:\n\ndt2 <- data.table(A = 1:5,\n                  B = c(1.2, 4.3, 9.7, 5.6, 8.1),\n                  C = rnorm(5),\n                  Group = c(\"a\", \"b\", \"b\", \"a\", \"a\"))\ndt2\n\n       A     B           C  Group\n   <int> <num>       <num> <char>\n1:     1   1.2 -1.06274979      a\n2:     2   4.3 -0.02536416      b\n3:     3   9.7  1.04159798      b\n4:     4   5.6 -0.29917428      a\n5:     5   8.1  0.66285486      a\n\ndt2[, lapply(.SD, mean), .SDcols = 1:2]\n\n       A     B\n   <num> <num>\n1:     3  5.78\n\n# same as\ndt2[, lapply(.SD, mean), .SDcols = c(\"A\", \"B\")]\n\n       A     B\n   <num> <num>\n1:     3  5.78\n\ncols <- c(\"A\", \"B\")\ndt2[, lapply(.SD, mean), .SDcols = cols]\n\n       A     B\n   <num> <num>\n1:     3  5.78\n\n\nYou can combine .SDcols and by:\n\ndt2[, lapply(.SD, median), .SDcols = c(\"B\", \"C\"), by = Group]\n\n    Group     B          C\n   <char> <num>      <num>\n1:      a   5.6 -0.2991743\n2:      b   7.0  0.5081169\n\n\nCreate multiple new columns from transformation of existing and store with custom prefix:\n\ndt1\n\n           Alpha        Beta       Gamma\n           <num>       <num>       <num>\n 1:  0.668502204 -0.05969575  0.83717006\n 2:  0.597326456 -0.46056888 -0.38031235\n 3: -0.006375072 -1.91841874  0.01005586\n 4:  0.284818822 -0.41976522 -2.15343630\n 5:  1.261268116 -0.03468561 -1.40507718\n 6: -0.893029837 -1.73596789 -0.47709508\n 7:  0.757157660 -0.55460528 -0.73053883\n 8: -0.159937785  1.60336929  1.16789727\n 9:  2.587670877  0.36316138  2.27203881\n10:  0.468011889 -0.83516791 -1.77513127\n\ndt1[, paste0(names(dt1), \"_abs\") := lapply(.SD, abs)]\ndt1\n\n           Alpha        Beta       Gamma   Alpha_abs   Beta_abs  Gamma_abs\n           <num>       <num>       <num>       <num>      <num>      <num>\n 1:  0.668502204 -0.05969575  0.83717006 0.668502204 0.05969575 0.83717006\n 2:  0.597326456 -0.46056888 -0.38031235 0.597326456 0.46056888 0.38031235\n 3: -0.006375072 -1.91841874  0.01005586 0.006375072 1.91841874 0.01005586\n 4:  0.284818822 -0.41976522 -2.15343630 0.284818822 0.41976522 2.15343630\n 5:  1.261268116 -0.03468561 -1.40507718 1.261268116 0.03468561 1.40507718\n 6: -0.893029837 -1.73596789 -0.47709508 0.893029837 1.73596789 0.47709508\n 7:  0.757157660 -0.55460528 -0.73053883 0.757157660 0.55460528 0.73053883\n 8: -0.159937785  1.60336929  1.16789727 0.159937785 1.60336929 1.16789727\n 9:  2.587670877  0.36316138  2.27203881 2.587670877 0.36316138 2.27203881\n10:  0.468011889 -0.83516791 -1.77513127 0.468011889 0.83516791 1.77513127\n\n\n\ndt2\n\n       A     B           C  Group\n   <int> <num>       <num> <char>\n1:     1   1.2 -1.06274979      a\n2:     2   4.3 -0.02536416      b\n3:     3   9.7  1.04159798      b\n4:     4   5.6 -0.29917428      a\n5:     5   8.1  0.66285486      a\n\ncols <- c(\"A\", \"C\")\ndt2[, paste0(cols, \"_groupMean\") := lapply(.SD, mean), .SDcols = cols, by = Group]\ndt2\n\n       A     B           C  Group A_groupMean C_groupMean\n   <int> <num>       <num> <char>       <num>       <num>\n1:     1   1.2 -1.06274979      a    3.333333  -0.2330231\n2:     2   4.3 -0.02536416      b    2.500000   0.5081169\n3:     3   9.7  1.04159798      b    2.500000   0.5081169\n4:     4   5.6 -0.29917428      a    3.333333  -0.2330231\n5:     5   8.1  0.66285486      a    3.333333  -0.2330231"
  },
  {
    "objectID": "DataTable.html#row-wise-operations",
    "href": "DataTable.html#row-wise-operations",
    "title": "25  Efficient data analysis with data.table",
    "section": "\n25.14 Row-wise operations",
    "text": "25.14 Row-wise operations\n\ndt <- data.table(a = 1:5, b = 11:15, c = 21:25, \n                 d = 31:35, e = 41:45)\ndt\n\n       a     b     c     d     e\n   <int> <int> <int> <int> <int>\n1:     1    11    21    31    41\n2:     2    12    22    32    42\n3:     3    13    23    33    43\n4:     4    14    24    34    44\n5:     5    15    25    35    45\n\n\nTo operate row-wise, we can use by = 1:nrow(dt). For example, to add a column, in-place, with row-wise sums of variables b through d:\n\ndt[, bcd.sum := sum(.SD[, b:d]), by = 1:nrow(dt)]\ndt\n\n       a     b     c     d     e bcd.sum\n   <int> <int> <int> <int> <int>   <int>\n1:     1    11    21    31    41      63\n2:     2    12    22    32    42      66\n3:     3    13    23    33    43      69\n4:     4    14    24    34    44      72\n5:     5    15    25    35    45      75"
  },
  {
    "objectID": "DataTable.html#melt",
    "href": "DataTable.html#melt",
    "title": "25  Efficient data analysis with data.table",
    "section": "\n25.15 Wide <=> Long",
    "text": "25.15 Wide <=> Long\n\n25.15.1 Wide to long: melt()\n\n\ndt_wide <- data.table(ID = 1:4, Timepoint_A = 11:14,\n                      Timepoint_B = 21:24, Timepoint_C = 51:54)\ndt_wide\n\n      ID Timepoint_A Timepoint_B Timepoint_C\n   <int>       <int>       <int>       <int>\n1:     1          11          21          51\n2:     2          12          22          52\n3:     3          13          23          53\n4:     4          14          24          54\n\ndt_long <- melt(dt_wide, id.vars = \"ID\",\n                measure.vars = 2:4, # defaults to all non-id columns\n                variable.name = \"Timepoint\",\n                value.name = c(\"Score\"))\ndt_long\n\n       ID   Timepoint Score\n    <int>      <fctr> <int>\n 1:     1 Timepoint_A    11\n 2:     2 Timepoint_A    12\n 3:     3 Timepoint_A    13\n 4:     4 Timepoint_A    14\n 5:     1 Timepoint_B    21\n 6:     2 Timepoint_B    22\n 7:     3 Timepoint_B    23\n 8:     4 Timepoint_B    24\n 9:     1 Timepoint_C    51\n10:     2 Timepoint_C    52\n11:     3 Timepoint_C    53\n12:     4 Timepoint_C    54\n\n\n\n25.15.2 Long to wide: dcast()\n\n\ndt_long\n\n       ID   Timepoint Score\n    <int>      <fctr> <int>\n 1:     1 Timepoint_A    11\n 2:     2 Timepoint_A    12\n 3:     3 Timepoint_A    13\n 4:     4 Timepoint_A    14\n 5:     1 Timepoint_B    21\n 6:     2 Timepoint_B    22\n 7:     3 Timepoint_B    23\n 8:     4 Timepoint_B    24\n 9:     1 Timepoint_C    51\n10:     2 Timepoint_C    52\n11:     3 Timepoint_C    53\n12:     4 Timepoint_C    54\n\ndcast(dt_long, ID ~ Timepoint,\n      value.var = \"Score\")\n\n      ID Timepoint_A Timepoint_B Timepoint_C\n   <int>       <int>       <int>       <int>\n1:     1          11          21          51\n2:     2          12          22          52\n3:     3          13          23          53\n4:     4          14          24          54\n\n\n\n25.15.2.1 dcast() + aggregate\nIf your ID ~ Timepoint combination does not define a unique row in your input dataset, you need to specify an aggregate function.\nFor example, suppose you have four subjects with IDs “A”, “B”, “C”, “D” who had a couple variables measured 3 times in the AM and 3 times in the PM.\n\ndt_long2 <- data.table(ID = rep(LETTERS[1:4], each = 6),\n                      Timepoint = rep(c(\"AM\", \"PM\"), length.out = 24, each = 3),\n                      Var1 = rnorm(24, 10),\n                      Var2 = rnorm(24, 20))\n\ndt_long2[sample(24, 4), Var1 := NA]\ndt_long2[sample(24, 4), Var2 := NA]\ndt_long2\n\n        ID Timepoint      Var1     Var2\n    <char>    <char>     <num>    <num>\n 1:      A        AM        NA 20.11550\n 2:      A        AM        NA 18.14154\n 3:      A        AM  9.640870       NA\n 4:      A        PM  9.097022 21.86922\n 5:      A        PM 12.304842 19.95239\n 6:      A        PM 11.583788 19.84827\n 7:      B        AM  9.721776 18.94182\n 8:      B        AM  9.757424 18.43157\n 9:      B        AM  8.288608       NA\n10:      B        PM 10.411570 20.01701\n11:      B        PM  9.431159 20.65191\n12:      B        PM 10.211825 18.24647\n13:      C        AM  9.771170 19.45073\n14:      C        AM  9.225321 19.68336\n15:      C        AM 10.017563 20.28643\n16:      C        PM  8.760350 20.98256\n17:      C        PM        NA       NA\n18:      C        PM        NA 18.06725\n19:      D        AM  9.320472 21.68731\n20:      D        AM 11.436406       NA\n21:      D        AM  8.600014 19.45014\n22:      D        PM 11.123851 19.81983\n23:      D        PM 11.224824 19.17619\n24:      D        PM 11.310426 20.52099\n        ID Timepoint      Var1     Var2\n\n\nIf you wanted to convert the above data.table to wide format and get mean AM and PM values using the fun.aggregate argument:\n\ndcast(dt_long2,\n      ID ~ Timepoint,\n      value.var = c(\"Var1\", \"Var2\"),\n      fun.aggregate = mean, na.rm = T)\n\n       ID  Var1_AM  Var1_PM  Var2_AM  Var2_PM\n   <char>    <num>    <num>    <num>    <num>\n1:      A 9.640870 10.99522 19.12852 20.55663\n2:      B 9.255936 10.01818 18.68670 19.63846\n3:      C 9.671351  8.76035 19.80684 19.52491\n4:      D 9.785631 11.21970 20.56872 19.83900\n\n\nYou can apply multiple aggregating functions by passing a list to fun.aggregate:\n\ndcast(dt_long2,\n      ID ~ Timepoint,\n      value.var = c(\"Var1\", \"Var2\"),\n      fun.aggregate = list(mean, max, min), na.rm = T)\n\n       ID Var1_mean_AM Var1_mean_PM Var2_mean_AM Var2_mean_PM Var1_max_AM\n   <char>        <num>        <num>        <num>        <num>       <num>\n1:      A     9.640870     10.99522     19.12852     20.55663    9.640870\n2:      B     9.255936     10.01818     18.68670     19.63846    9.757424\n3:      C     9.671351      8.76035     19.80684     19.52491   10.017563\n4:      D     9.785631     11.21970     20.56872     19.83900   11.436406\n   Var1_max_PM Var2_max_AM Var2_max_PM Var1_min_AM Var1_min_PM Var2_min_AM\n         <num>       <num>       <num>       <num>       <num>       <num>\n1:    12.30484    20.11550    21.86922    9.640870    9.097022    18.14154\n2:    10.41157    18.94182    20.65191    8.288608    9.431159    18.43157\n3:     8.76035    20.28643    20.98256    9.225321    8.760350    19.45073\n4:    11.31043    21.68731    20.52099    8.600014   11.123851    19.45014\n   Var2_min_PM\n         <num>\n1:    19.84827\n2:    18.24647\n3:    18.06725\n4:    19.17619\n\n\nNote how na.rm = T was successfully applied to all aggregating functions"
  },
  {
    "objectID": "DataTable.html#table-joins",
    "href": "DataTable.html#table-joins",
    "title": "25  Efficient data analysis with data.table",
    "section": "\n25.16 Table Joins",
    "text": "25.16 Table Joins\ndata.table allow you to perform table joins with the base merge() function using the same syntax as for data.frame objects or the “data.table way” using bracket notation:\n\na <- data.table(PID = c(1:9),\n                Hospital = c(\"UCSF\", \"HUP\", \"Stanford\", \n                             \"Stanford\", \"UCSF\", \"HUP\", \n                             \"HUP\", \"Stanford\", \"UCSF\"),\n                Age = c(22, 34, 41, 19, 53, 21, 63, 22, 19),\n                Sex = c(1, 1, 0, 1, 0, 0, 1, 0, 0),\n                key = \"PID\")\na\n\n     PID Hospital   Age   Sex\n   <int>   <char> <num> <num>\n1:     1     UCSF    22     1\n2:     2      HUP    34     1\n3:     3 Stanford    41     0\n4:     4 Stanford    19     1\n5:     5     UCSF    53     0\n6:     6      HUP    21     0\n7:     7      HUP    63     1\n8:     8 Stanford    22     0\n9:     9     UCSF    19     0\n\nb <- data.table(PID = c(6:12),\n                V1 = c(153, 89, 112, 228,  91, 190, 101),\n                Department = c(\"Neurology\", \"Radiology\", \"Emergency\",\n                               \"Cardiology\", \"Surgery\", \"Neurology\",\n                               \"Psychiatry\"),\n                key = \"PID\")\nb\n\n     PID    V1 Department\n   <int> <num>     <char>\n1:     6   153  Neurology\n2:     7    89  Radiology\n3:     8   112  Emergency\n4:     9   228 Cardiology\n5:    10    91    Surgery\n6:    11   190  Neurology\n7:    12   101 Psychiatry\n\n\nIn the above command we use the key argument to set PID as key. This can be performed after the data.table has been created using the setkey() command:\n\nsetkey(a, PID)\n\nMultiple keys can be set, in order, with the same setkey() command, separated by commas, e.g.:\n\nsetkey(a, PID, Hospital)\n\nKeys sort the data.table by the corresponding columns and can be used to perform left and right joins with bracket notation seen later.\n\n25.16.1 Inner\n\nmerge(a, b)\n\n     PID Hospital   Age   Sex    V1 Department\n   <int>   <char> <num> <num> <num>     <char>\n1:     6      HUP    21     0   153  Neurology\n2:     7      HUP    63     1    89  Radiology\n3:     8 Stanford    22     0   112  Emergency\n4:     9     UCSF    19     0   228 Cardiology\n\n\n\n25.16.2 Outer\n\nmerge(a, b, all = TRUE)\n\n      PID Hospital   Age   Sex    V1 Department\n    <int>   <char> <num> <num> <num>     <char>\n 1:     1     UCSF    22     1    NA       <NA>\n 2:     2      HUP    34     1    NA       <NA>\n 3:     3 Stanford    41     0    NA       <NA>\n 4:     4 Stanford    19     1    NA       <NA>\n 5:     5     UCSF    53     0    NA       <NA>\n 6:     6      HUP    21     0   153  Neurology\n 7:     7      HUP    63     1    89  Radiology\n 8:     8 Stanford    22     0   112  Emergency\n 9:     9     UCSF    19     0   228 Cardiology\n10:    10     <NA>    NA    NA    91    Surgery\n11:    11     <NA>    NA    NA   190  Neurology\n12:    12     <NA>    NA    NA   101 Psychiatry\n\n\n\n25.16.3 Left outer\nUsing merge():\n\nmerge(a, b, all.x = TRUE)\n\n     PID Hospital   Age   Sex    V1 Department\n   <int>   <char> <num> <num> <num>     <char>\n1:     1     UCSF    22     1    NA       <NA>\n2:     2      HUP    34     1    NA       <NA>\n3:     3 Stanford    41     0    NA       <NA>\n4:     4 Stanford    19     1    NA       <NA>\n5:     5     UCSF    53     0    NA       <NA>\n6:     6      HUP    21     0   153  Neurology\n7:     7      HUP    63     1    89  Radiology\n8:     8 Stanford    22     0   112  Emergency\n9:     9     UCSF    19     0   228 Cardiology\n\n\nUsing bracket notation:\n\nb[a, ]\n\n     PID    V1 Department Hospital   Age   Sex\n   <int> <num>     <char>   <char> <num> <num>\n1:     1    NA       <NA>     UCSF    22     1\n2:     2    NA       <NA>      HUP    34     1\n3:     3    NA       <NA> Stanford    41     0\n4:     4    NA       <NA> Stanford    19     1\n5:     5    NA       <NA>     UCSF    53     0\n6:     6   153  Neurology      HUP    21     0\n7:     7    89  Radiology      HUP    63     1\n8:     8   112  Emergency Stanford    22     0\n9:     9   228 Cardiology     UCSF    19     0\n\n\nIf keys were not set for a and b, you could specify the column to match on using the on argument:\n\nb[a, on = \"PID\"]\n\n     PID    V1 Department Hospital   Age   Sex\n   <int> <num>     <char>   <char> <num> <num>\n1:     1    NA       <NA>     UCSF    22     1\n2:     2    NA       <NA>      HUP    34     1\n3:     3    NA       <NA> Stanford    41     0\n4:     4    NA       <NA> Stanford    19     1\n5:     5    NA       <NA>     UCSF    53     0\n6:     6   153  Neurology      HUP    21     0\n7:     7    89  Radiology      HUP    63     1\n8:     8   112  Emergency Stanford    22     0\n9:     9   228 Cardiology     UCSF    19     0\n\n\n\n\n\nThe easy way to understand the bracket notation merges is to think that the data.table inside the bracket is used to index the data.table on the outside, therefore the resulting table will have rows dictated by the inside table’s key.\n\n\n\n\n25.16.4 Right outer\n\nmerge(a, b, all.y = TRUE)\n\n     PID Hospital   Age   Sex    V1 Department\n   <int>   <char> <num> <num> <num>     <char>\n1:     6      HUP    21     0   153  Neurology\n2:     7      HUP    63     1    89  Radiology\n3:     8 Stanford    22     0   112  Emergency\n4:     9     UCSF    19     0   228 Cardiology\n5:    10     <NA>    NA    NA    91    Surgery\n6:    11     <NA>    NA    NA   190  Neurology\n7:    12     <NA>    NA    NA   101 Psychiatry\n\n\nUsing bracket notation:\n\na[b, ]\n\n     PID Hospital   Age   Sex    V1 Department\n   <int>   <char> <num> <num> <num>     <char>\n1:     6      HUP    21     0   153  Neurology\n2:     7      HUP    63     1    89  Radiology\n3:     8 Stanford    22     0   112  Emergency\n4:     9     UCSF    19     0   228 Cardiology\n5:    10     <NA>    NA    NA    91    Surgery\n6:    11     <NA>    NA    NA   190  Neurology\n7:    12     <NA>    NA    NA   101 Psychiatry"
  },
  {
    "objectID": "DataTable.html#understanding-reference-semantics-in-data.table",
    "href": "DataTable.html#understanding-reference-semantics-in-data.table",
    "title": "25  Efficient data analysis with data.table",
    "section": "\n25.17 Understanding reference semantics in data.table\n",
    "text": "25.17 Understanding reference semantics in data.table\n\n\n25.17.1 Get object’s location in memory with address()\n\nWhen you add a new column to an existing data.frame, the data.frame is copied behind the scenes - you can tell becasue its memory address (where it’s physically stored in your computer) changes:\n\ndf1 <- data.frame(alpha = 1:5, beta = 11:15)\naddress(df1)\n\n[1] \"0x105c60808\"\n\ndf1$gamma <- df1$alpha + df1$beta\naddress(df1)\n\n[1] \"0x105484d28\"\n\n\nWhen you add a new column in a data.table in-place its address remains unchanged:\n\ndt1 <- data.table(alpha = 1:5, beta = 11:15)\naddress(dt1)\n\n[1] \"0x104fbae00\"\n\ndt1[, gamma := alpha + beta]\naddress(dt1)\n\n[1] \"0x104fbae00\"\n\n\n\n25.17.2 Reference semantics at work\nUp to now, you are likely used to working with regular R objects that behave like this:\n\ndf1 <- data.frame(a = rep(1, 5))\ndf1\n\n  a\n1 1\n2 1\n3 1\n4 1\n5 1\n\ndf2 <- df1\ndf2\n\n  a\n1 1\n2 1\n3 1\n4 1\n5 1\n\ndf2$a <- df2$a*2\ndf2\n\n  a\n1 2\n2 2\n3 2\n4 2\n5 2\n\ndf1\n\n  a\n1 1\n2 1\n3 1\n4 1\n5 1\n\naddress(df1)\n\n[1] \"0x106c097b0\"\n\naddress(df2)\n\n[1] \"0x1053db390\"\n\n\ndata.table uses “reference semantics” or “pass-by-reference”. Be very careful or you might be mightily confused:\n\ndt1 <- data.table(a = rep(1, 5))\ndt1\n\n       a\n   <num>\n1:     1\n2:     1\n3:     1\n4:     1\n5:     1\n\ndt2 <- dt1\ndt2\n\n       a\n   <num>\n1:     1\n2:     1\n3:     1\n4:     1\n5:     1\n\ndt2[, a := a * 2]\ndt2\n\n       a\n   <num>\n1:     2\n2:     2\n3:     2\n4:     2\n5:     2\n\ndt1\n\n       a\n   <num>\n1:     2\n2:     2\n3:     2\n4:     2\n5:     2\n\naddress(dt1)\n\n[1] \"0x10565ac00\"\n\naddress(dt2)\n\n[1] \"0x10565ac00\"\n\n\n\n\n\nIf you want to create a copy of a data.table, use copy():\n\n\n\n\ndt3 <- copy(dt1)\ndt3\n\n       a\n   <num>\n1:     2\n2:     2\n3:     2\n4:     2\n5:     2\n\naddress(dt3)\n\n[1] \"0x104fc3600\"\n\ndt3[, a := a * 2]\ndt3\n\n       a\n   <num>\n1:     4\n2:     4\n3:     4\n4:     4\n5:     4\n\ndt1\n\n       a\n   <num>\n1:     2\n2:     2\n3:     2\n4:     2\n5:     2"
  },
  {
    "objectID": "DataTable.html#dtresources",
    "href": "DataTable.html#dtresources",
    "title": "25  Efficient data analysis with data.table",
    "section": "\n25.18 Resources",
    "text": "25.18 Resources\ndata.table GitHub data.table docs Introduction to data.table vignette"
  },
  {
    "objectID": "dplyr.html",
    "href": "dplyr.html",
    "title": "26  Introduction to dplyr",
    "section": "",
    "text": "The dplyr package offers functionality for data manipulation and is part of what is known as the tidyverse.\ndplyr’s functions are named after verbs and depend heavily on the usage of the pipe operator to build pipelines. The package offers a large number of functions in total, often with multiple versions of the same “verb”. It has undergone many major changes since its introduction, so always make sure to consult the latest documentation. Some of the welcome recent changes aim to reduce the total number of functions exported by the package.\nCore operations include:\ndplyr operates on data.frames as well as the tidyverse’s data.frame replacement, known by the bizarre name of tibble. Here, we convert iris to a tibble as an easy way to limit the number of rows printed by default in the output (without changing other Rmarkdown options, custom hooks, etc.) and as an introduction to tiblles.\nNote that dplyr masks a number of builtin functions when loaded."
  },
  {
    "objectID": "dplyr.html#filter",
    "href": "dplyr.html#filter",
    "title": "26  Introduction to dplyr",
    "section": "26.1 Filter",
    "text": "26.1 Filter\n\niris |> filter(Species == \"setosa\")\n\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# … with 40 more rows"
  },
  {
    "objectID": "dplyr.html#select",
    "href": "dplyr.html#select",
    "title": "26  Introduction to dplyr",
    "section": "26.2 Select",
    "text": "26.2 Select\n\niris |> select(\"Sepal.Length\")\n\n# A tibble: 150 × 1\n   Sepal.Length\n          <dbl>\n 1          5.1\n 2          4.9\n 3          4.7\n 4          4.6\n 5          5  \n 6          5.4\n 7          4.6\n 8          5  \n 9          4.4\n10          4.9\n# … with 140 more rows"
  },
  {
    "objectID": "dplyr.html#mutate",
    "href": "dplyr.html#mutate",
    "title": "26  Introduction to dplyr",
    "section": "26.3 Mutate",
    "text": "26.3 Mutate\n\niris |> mutate(Sepal_minus_Petal_length = Sepal.Length - Petal.Length)\n\n# A tibble: 150 × 6\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species Sepal_minus_Petal_…\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>                 <dbl>\n 1          5.1         3.5          1.4         0.2 setosa                  3.7\n 2          4.9         3            1.4         0.2 setosa                  3.5\n 3          4.7         3.2          1.3         0.2 setosa                  3.4\n 4          4.6         3.1          1.5         0.2 setosa                  3.1\n 5          5           3.6          1.4         0.2 setosa                  3.6\n 6          5.4         3.9          1.7         0.4 setosa                  3.7\n 7          4.6         3.4          1.4         0.3 setosa                  3.2\n 8          5           3.4          1.5         0.2 setosa                  3.5\n 9          4.4         2.9          1.4         0.2 setosa                  3  \n10          4.9         3.1          1.5         0.1 setosa                  3.4\n# … with 140 more rows\n\n\n\n26.3.1 Grouped\n\niris |> \n  group_by(Species) |> \n  mutate(Sepal.Length_minus_mean = Sepal.Length - mean(Sepal.Length))\n\n# A tibble: 150 × 6\n# Groups:   Species [3]\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species Sepal.Length_minus…\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>                 <dbl>\n 1          5.1         3.5          1.4         0.2 setosa              0.0940 \n 2          4.9         3            1.4         0.2 setosa             -0.106  \n 3          4.7         3.2          1.3         0.2 setosa             -0.306  \n 4          4.6         3.1          1.5         0.2 setosa             -0.406  \n 5          5           3.6          1.4         0.2 setosa             -0.00600\n 6          5.4         3.9          1.7         0.4 setosa              0.394  \n 7          4.6         3.4          1.4         0.3 setosa             -0.406  \n 8          5           3.4          1.5         0.2 setosa             -0.00600\n 9          4.4         2.9          1.4         0.2 setosa             -0.606  \n10          4.9         3.1          1.5         0.1 setosa             -0.106  \n# … with 140 more rows"
  },
  {
    "objectID": "dplyr.html#summarize",
    "href": "dplyr.html#summarize",
    "title": "26  Introduction to dplyr",
    "section": "26.4 Summarize",
    "text": "26.4 Summarize\n\n26.4.1 Single variable\n\niris |> summarize(mean(Sepal.Length))\n\n# A tibble: 1 × 1\n  `mean(Sepal.Length)`\n                 <dbl>\n1                 5.84\n\n\n\n\n26.4.2 Multiple variables\n\niris |> summarize(across(c(Sepal.Length, Petal.Length), mean))\n\n# A tibble: 1 × 2\n  Sepal.Length Petal.Length\n         <dbl>        <dbl>\n1         5.84         3.76\n\n\n\n\n26.4.3 Grouped single var\n\niris |> \n  group_by(Species) |> \n  summarize(mean(Sepal.Length))\n\n# A tibble: 3 × 2\n  Species    `mean(Sepal.Length)`\n  <fct>                     <dbl>\n1 setosa                     5.01\n2 versicolor                 5.94\n3 virginica                  6.59\n\n\n\n\n26.4.4 Grouped multivar\n\niris |> \n  group_by(Species) |> \n  summarize(across(c(Sepal.Length, Petal.Length), mean))\n\n# A tibble: 3 × 3\n  Species    Sepal.Length Petal.Length\n  <fct>             <dbl>        <dbl>\n1 setosa             5.01         1.46\n2 versicolor         5.94         4.26\n3 virginica          6.59         5.55"
  },
  {
    "objectID": "dplyr.html#arrange",
    "href": "dplyr.html#arrange",
    "title": "26  Introduction to dplyr",
    "section": "26.5 Arrange",
    "text": "26.5 Arrange\n\niris |> arrange(Sepal.Length)\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          4.3         3            1.1         0.1 setosa \n 2          4.4         2.9          1.4         0.2 setosa \n 3          4.4         3            1.3         0.2 setosa \n 4          4.4         3.2          1.3         0.2 setosa \n 5          4.5         2.3          1.3         0.3 setosa \n 6          4.6         3.1          1.5         0.2 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          4.6         3.6          1           0.2 setosa \n 9          4.6         3.2          1.4         0.2 setosa \n10          4.7         3.2          1.3         0.2 setosa \n# … with 140 more rows\n\n\n\n26.5.1 Grouped\n\niris |> \n  group_by(Species) |> \n  arrange(Sepal.Length)\n\n# A tibble: 150 × 5\n# Groups:   Species [3]\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          4.3         3            1.1         0.1 setosa \n 2          4.4         2.9          1.4         0.2 setosa \n 3          4.4         3            1.3         0.2 setosa \n 4          4.4         3.2          1.3         0.2 setosa \n 5          4.5         2.3          1.3         0.3 setosa \n 6          4.6         3.1          1.5         0.2 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          4.6         3.6          1           0.2 setosa \n 9          4.6         3.2          1.4         0.2 setosa \n10          4.7         3.2          1.3         0.2 setosa \n# … with 140 more rows"
  },
  {
    "objectID": "dplyr.html#specifying-multiple-variables",
    "href": "dplyr.html#specifying-multiple-variables",
    "title": "26  Introduction to dplyr",
    "section": "26.6 Specifying multiple variables",
    "text": "26.6 Specifying multiple variables\ndplyr includes a number of ways to identify multiple variables. The latest version of dplyr suggests using across() within dplyr functions that allow specifying columns.\nThis replaces separate functions previously used for each of filter/select/mutate/summarize/arrange that had independent functions ending with *_all(), *_at(), *_each(), *_if().\nWe’ll use summarize() to demonstrate.\n\n26.6.1 By integer column index\n\niris |> summarize(across(1:4, mean))\n\n# A tibble: 1 × 4\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n         <dbl>       <dbl>        <dbl>       <dbl>\n1         5.84        3.06         3.76        1.20\n\n\n\n\n26.6.2 By character name range\n\niris |> summarize(across(Sepal.Length:Petal.Width, mean))\n\n# A tibble: 1 × 4\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n         <dbl>       <dbl>        <dbl>       <dbl>\n1         5.84        3.06         3.76        1.20\n\n\n\n\n26.6.3 Pattern-matching\n\niris |> summarize(across(starts_with(\"Sepal\"), mean))\n\n# A tibble: 1 × 2\n  Sepal.Length Sepal.Width\n         <dbl>       <dbl>\n1         5.84        3.06\n\n\n\niris |> summarize(across(ends_with(\"Length\"), mean))\n\n# A tibble: 1 × 2\n  Sepal.Length Petal.Length\n         <dbl>        <dbl>\n1         5.84         3.76\n\n\n\n\n26.6.4 Using predicate function wrapped in where()\n\niris |> summarize(across(where(is.numeric), mean))\n\n# A tibble: 1 × 4\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n         <dbl>       <dbl>        <dbl>       <dbl>\n1         5.84        3.06         3.76        1.20\n\n\nUsing table() in this way does not output level names:\n\niris |> summarize(across(where(is.factor), table))\n\n# A tibble: 3 × 1\n  Species\n  <table>\n1 50     \n2 50     \n3 50"
  },
  {
    "objectID": "dplyr.html#dplyrresources",
    "href": "dplyr.html#dplyrresources",
    "title": "26  Introduction to dplyr",
    "section": "26.7 Resources",
    "text": "26.7 Resources\n\ndplyr website\ndplyr cheatsheet"
  },
  {
    "objectID": "BaseGraphics.html",
    "href": "BaseGraphics.html",
    "title": "27  Base Graphics",
    "section": "",
    "text": "R has powerful graphical capabilities built in to the core language. This chapter is an introduction to what is known as base graphics which is provided by the graphics built-in package. Their defaults produce minimalist plots, but they can be customized extensively. In this chapter we shall begin with default plots and demonstrate some of the more common/useful ways to customize them.\nR documentation for each of the above commands provides extensive coverage of graphical parameters. ?par gives the main documentation file for a long list of graphical parameters. These can be set either with the par() command before using any plotting command.\nLet’s create some synthetic data:"
  },
  {
    "objectID": "BaseGraphics.html#scatter-plot",
    "href": "BaseGraphics.html#scatter-plot",
    "title": "27  Base Graphics",
    "section": "27.1 Scatter plot",
    "text": "27.1 Scatter plot\nInput: 2 numeric vectors\nA 2D scatterplot displays points using two numeric vectors as X and Y coordinates.\n\nplot(x, y)\n\n\n\n\n\n27.1.1 col: point color\nSee Colors in R to learn about the different ways to define colors in R.\nSome common ways include:\n\nBy name using one of 657 names given by colors(), e.g. “red”, “magenta”, “blue”, “navy”, “cyan”\nBy RGB code\n\n\nplot(x, y, col = \"red\")\n\n\n\n\n\n\n27.1.2 bty: box type\nThere are 7 bty options: “o” “l”, “7”, “c”, “u”, or “]” and “none”. They produce a box that resembles the corresponding symbol. “none” draws no box but allows the axes to show:\n\nplot(x, y, bty = \"l\")\n\n\n\n\n\nplot(x, y, bty = \"none\")\n\n\n\n\n\n\n27.1.3 pch: point character\nThe default point character is a circle as seen above. This helps visualize overlapping points (especially for devices that do not support transparency).\nThere are 25 point characters, designated by integers 1 through 25.\nHere’s a preview of all 25 pch options. pch types 21 through 25 can be filled by a color specified by bg.\n\nplot(1:25, rep(1, 25), pch = 1:25, bg = \"blue\")\n\n\n\n\nLet’s use a solid disc:\n\nplot(x, y, bty = \"n\", pch = 16)\n\n\n\n\nWe cannot tell how many points are overlapping in the middle and therefore it’s a good idea to make the points a little transparent.\nThere are different ways to add transparency (see Colors). The easiest way is probably to use adjustcolor(). In the context of colors, alpha refers to transparency: a = 1 is opaque and a = 0 is completely transparent (therefore use a value greater than 0).\n\nplot(x, y,\n     bty = \"n\", pch = 16,\n     col = adjustcolor(\"skyblue\", alpha.f = .5))\n\n\n\n\n\n\n27.1.4 grid\nWe can add a grid behind the plot area using the panel.first argument, which accepts a graphical expression (a function that draws something), which will be evaluated before plotting the points on the graph (therefore appears behind the points).\n\nplot(x, y,\n     bty = \"n\", pch = 16,\n     col = adjustcolor(\"skyblue\", alpha.f = .5),\n     panel.first = grid(lty = 1, col = 'gray90'))\n\n\n\n\n\n\n27.1.5 main, xlab, ylab: Title and axes labels\n\nplot(x, y,\n     bty = \"n\", pch = 16,\n     col = adjustcolor(\"skyblue\", alpha.f = .5),\n     panel.first = grid(lty = 1, col = 'gray90'),\n     main = \"y vs. x\",\n     xlab = \"Variable x (xunits)\",\n     ylab = \"Variable y (yunits)\")\n\n\n\n\nNote that depending on where you intend to display the plot, you may leave the title blank and instead place it in the figure caption along with an explanation of the data (e.g. in a journal article)"
  },
  {
    "objectID": "BaseGraphics.html#histogram",
    "href": "BaseGraphics.html#histogram",
    "title": "27  Base Graphics",
    "section": "27.2 Histogram",
    "text": "27.2 Histogram\nInput: numeric vector\nA histogram displays an approximation of the distribution of a numeric vector. First the data is binned and then the number of elements that falls in each bin is counted. The histogram plot draws bars for each bin whose heights corresponds to the count of elements in the corresponding interval.\n\nhist(x)\n\n\n\n\n\n27.2.1 col: bar color\n\nhist(x, col = \"slategrey\")\n\n\n\n\n\n\n27.2.2 border: border color\nSetting border color to the same as the background gives a clean look:\n\nhist(x, col = \"slategrey\", border = \"white\")\n\n\n\n\n\n\n27.2.3 breaks: number or value of breakpoints\nThe breaks argument can be used to define the breakpoints to use for the binning of the values of the input to hist(). See the documentation in ?hist for the full range of options. An easy way to control the number of bins is to pass an integer to the breaks argument. Depending on the length of x and its distribution, it may or may not be possible to use the exact number requested, but the closest possible number will be automatically chosen.\n\nhist(x, col = \"slategrey\", border = \"white\",\n     breaks = 8)"
  },
  {
    "objectID": "BaseGraphics.html#density-plot",
    "href": "BaseGraphics.html#density-plot",
    "title": "27  Base Graphics",
    "section": "27.3 Density plot",
    "text": "27.3 Density plot\nInput: numeric vector\nA density plot is a different way to display an approximation of the distribution of a numeric vector. The density() function estimates the density of x and can be passed to plot() directly:\n\nplot(density(x))\n\n\n\n\nYou can use main = NA or main = \"\" to suppress printing a title.\n\nplot(density(x), col = \"blue\",\n     bty = \"n\",\n     main = NA)"
  },
  {
    "objectID": "BaseGraphics.html#barplot",
    "href": "BaseGraphics.html#barplot",
    "title": "27  Base Graphics",
    "section": "27.4 Barplot",
    "text": "27.4 Barplot\nInput: vector or matrix\nLet’s look at the VADeaths built-in dataset which describes death rater per 1000 population per year broken down by age range and population group.\n\n27.4.1 Single vector\nWe can plot a single column or row. Note how R automatically gets the corresponding dimension names. For this example we use the builtin VADeaths dataset, which is a matrix.\n\nbarplot(VADeaths[, 1])\n\n\n\n\n\nbarplot(VADeaths[1, ])\n\n\n\n\n\n27.4.1.1 col and border: bar fill and border color\nAs in most plotting functions, color is controlled by the col argument. border can be set to any color separately, or to NA to omit, which gives a clean look:\n\nbarplot(VADeaths[, 1],\n        col = \"aquamarine3\", border = NA)\n\n\n\n\n\n\n\n27.4.2 Matrix\nWe can draw barplots of multiple columns at the same time by passing a matrix input. The grouping on the x-axis is based on the columns. By default, data from different rows is stacked. The argument legend.text can be used to add a legend with the row labels:\n\nbarplot(VADeaths, legend.text = TRUE)\n\n\n\n\nAlternatively, we can draw groups of bars beside each other with the argument beside = TRUE:\n\nbarplot(VADeaths, beside = TRUE,\n        legend.text = TRUE, args.legend = list(x = \"topright\"))\n\n\n\n\nTo use custom colors, we pass a vector of length equal to the number of bars within each group. These will get recycled across groups, giving a consistent color coding.\nHere, we use the adjustcolor() function again to produce 5 shades of navy.\n\ncol <- sapply(seq(.2, .8, length.out = 5), function(i) adjustcolor(\"navy\", i))\nbarplot(VADeaths,\n        col = col,\n        border = NA,\n        beside = TRUE,\n        legend.text = TRUE, args.legend = list(x = \"topright\"))\n\n\n\n\n\n\n27.4.3 Formula interface"
  },
  {
    "objectID": "BaseGraphics.html#boxplot",
    "href": "BaseGraphics.html#boxplot",
    "title": "27  Base Graphics",
    "section": "27.5 Boxplot",
    "text": "27.5 Boxplot\nInput: One or more vectors of any length\nA boxplot is another way to visualize the distribution of one or more vectors. Each vector does not need to be of the same length. For example if you are plotting lab results of a patient and control group, they do not have to contain the same number of individuals.\nThere are two ways to use the boxplot() function. Either pass two separate vectors of data (whet)\nboxplot() makes it easy to plot your data from different objects. It can accept:\n\nindividual vectors\ncolumns of a matrix, columns/elements of a data.frame, elements of a list\nformula interface of the form variable ~ factor\n\n\n27.5.1 Single vector\n\na <- rnorm(500, mean = 12, sd = 2)\nboxplot(a)\n\n\n\n\n\n\n27.5.2 Anatomy of a boxplot\nA boxplot shows:\n\nthe median\nfirst and third quartiles\noutliers (defined as x < Q1 - 1.5 * IQR | x > Q3 + 1.5 * IQR)\nrange after excluding outliers\n\n\n\n\n\n\nBoxplot anatomy\n\n\n\n\nSome synthetic data:\n\nalpha <- rnorm(10)\nbeta <- rnorm(100)\ngamma <- rnorm(200, 1, 2)\ndl <- list(alpha = alpha, beta = beta, gamma = gamma)\n\n\n\n27.5.3 Multiple vectors\n\nboxplot(alpha, beta, gamma)\n\n\n\n\n\n\n27.5.4 List\n\nboxplot(dl)\n\n\n\n\n\n\n27.5.5 Matrix\nPassing a matrix to boxplot() draws one boxplot per column:\n\nmat <- sapply(seq(5), function(i) rnorm(20))\nboxplot(mat)\n\n\n\n\n\n\n27.5.6 Formula interface\nThe formula interface can be used to group any vector by a factor of the same length.\nLet’s use the built-in sleep dataset which shows the effect of two different drugs in increasing hours of sleep compared to a control group.\n\nboxplot(extra ~ group, sleep)\n\n\n\n\nThe col and border arguments work as expected. Here we define two custom colors using their hexadecimal RGB code and use the solid version for the border and a 50% transparent version for the fill. Note that we do not need two separate colors to produce an unambiguous plot since they are clearly labeled in the y-axis. It is often considered desirable/preferred to use the minimum number of different colors that is necessary. (Color coding like the following could be useful if for example data from the two groups were used on a different plot, like a scatterplot, in a multi-panel figure).\n\nborder <- c(\"#18A3AC\", \"#F48024\")\ncol <- c(adjustcolor(\"#18A3AC\", .5), adjustcolor(\"#F48024\", .5))\nboxplot(extra ~ group, sleep,\n        col = col, border = border)\n\n\n\n\n\n\n27.5.7 names: group labels\nThe x-axis group names can be defined with the names argument:\n\nboxplot(extra ~ group, sleep,\n        col = col, border = border,\n        names = c(\"Drug A\", \"Drug B\"))"
  },
  {
    "objectID": "BaseGraphics.html#heatmap",
    "href": "BaseGraphics.html#heatmap",
    "title": "27  Base Graphics",
    "section": "27.6 Heatmap",
    "text": "27.6 Heatmap\nInput: matrix\nA heatmap is a 2D matrix-like plot with x- and y-axis labels and a value in each cell. It can be used to display many different types of data. A common usage in data science is to plot the correlation matrix of a set of numerical features. In many cases, the rows and/or columns of a heatmap can be reordered based on hierarchical clustering.\n\nx <- sapply(1:20, function(i) rnorm(20))\nx_cor <- cor(x)\n\nBy default, the heatmap() function draws marginal dendrograms and rearranges rows and columns. We can prevent that by setting Rowv and Colv to NA:\n\nheatmap(x_cor, Rowv = NA, Colv = NA)\n\n\n\n\nTo allow clustering and row and column reordering, use the defaults:\n\nheatmap(x_cor)"
  },
  {
    "objectID": "BaseGraphics.html#mosaicplot",
    "href": "BaseGraphics.html#mosaicplot",
    "title": "27  Base Graphics",
    "section": "27.7 Mosaic plot",
    "text": "27.7 Mosaic plot\nMosaic plots are used to visualize contingency tables. They can be informative to look at during data exploration. They are less likely to be included in a research article where the table itself is more likely to be included.\nSynthetic data:\n\nset.seed(2021)\nCohort <- factor(sample(c(\"Control\", \"Case\"), 500, TRUE),\n                 levels = c(\"Control\", \"Case\"))\nSex <- factor(\n  sapply(seq(Cohort), \\(i) sample(c(\"Male\", \"Female\"), 1,\n                                  prob = if (Cohort[i] == \"Control\") c(1, 1) else c(2, 1))))\n\nUse mosaicplot() on the output of table():\n\nmosaicplot(table(Cohort), main = \"Cases vs. Controls\")\n\n\n\n\nWe can plot the breakdown of sexes, this time also adding color:\n\nmosaicplot(table(Sex), main = \"Males vs. Females\",\n           col = c(\"orchid\", \"skyblue\"))\n\n\n\n\nCross-tabulating is usually most informatively. We us the same color for the sexes, which will be recycled. We also remove the border for a cleaner look:\n\nmosaicplot(table(Cohort, Sex),\n           color = c(\"orchid\", \"skyblue\"),\n           border = NA,\n           main = \"Cohort x Sex\")"
  },
  {
    "objectID": "BaseGraphics.html#graphical-parameters",
    "href": "BaseGraphics.html#graphical-parameters",
    "title": "27  Base Graphics",
    "section": "27.8 Graphical parameters",
    "text": "27.8 Graphical parameters\nThe par() function allows setting or querying graphical parameters of the base graphics system. Have a look at its documentation (?par).\nSome graphical parameters can only be set with a call to par() prior to using a base plotting function. However, many parameters can also be passed using the ... construct of each base plotting function.\nSome common base graphical parameters:\n\npch: Point character\ncol: Color\ncex: Character expansion, i.e. relative size\nbty: Box type\nxlab: x-axis label\nylab: y-axis label\nmain: Main title\nmar: Plot margins\n\nYou can see what the current value of these parameters is by calling par() or directly accessing a specific parameter:\n\npar()$mar\n\n[1] 5.1 4.1 4.1 2.1\n\n\nmar sets the plot margins. It is a vector of length 4 and each number corresponds to the bottom-left-top-right margin, in that order. Use it to reduce empty white space between plots or add space if labels are getting cropped, for example.\nAlways make sure that your plotting characters, axis labels and titles are legible. You must avoid, at all costs, ever using a huge graph with tiny letters spread over an entire slide in a presentation.\n\ncex: Character expansion for the plotting characters\ncex.axis: cex for axis annotation\ncex.lab: cex for x and y labels\ncex.main: cex for main title\n\nNote: All of these can be set either with a call to par() prior to plotting or passed as arguments in a plotting command, like plot().\nThere is one important distinction: cex set with par() (which defaults to 1), sets the baseline and all other cex parameters multiply it. However, cex set within plot() still multiplies cex set with par(), but only affects the plotting character size.\n\n27.8.1 Save and reload graphical parameters\nYou can make a copy of all current graphical parameters:\n\npar_default <- par()\n\nThere are a few parameters that you cannot control, those are read-only. You can optionally exclude those since you cannot edit them anyway:\n\npar_default <- par(no.readonly = T)\n\nIf you make changes to par() to produce plots and you want to recover the parameters you saved above, you can use reload them by passing them to par():\n\npar(par_default)\n\nAlternatively, you can always restart the graphics device using dev.off() and then making a new plot.\nNote: here “device” does not refere to a physical device but software graphics interfaces that show a plot to screen or save to file.\n\ndev.off() # shuts down graphics device\n\nnull device \n          1 \n\nplot(rnorm(10))"
  },
  {
    "objectID": "BaseGraphics.html#multipanel-plots",
    "href": "BaseGraphics.html#multipanel-plots",
    "title": "27  Base Graphics",
    "section": "27.9 Multipanel plots",
    "text": "27.9 Multipanel plots\nThere are different ways to create multipanel plots, but probably the most straightforward is to use either the mfrow or the mfcol argument of par().\n\nset.seed(2020)\nx <- rnorm(500)\ny <- x^3 + rnorm(500) * 2\nz <- x^2 + rnorm(500)\n\nBoth mfrow and mfcol accept an integer vector of length 2 indicating number of rows and number of columns, respectively. With mfrow, the plots are drawn row-wise and with mfcol they are drawn column-wise. Remember to reset mfrow or mfcol back to c(1, 1)\nFor example, let’s plot a 2-by-3 panel of plots, drawn row-wise:\n\npar(mfrow = c(2, 3), mar = c(4, 4, 1, 1))\nhist(x, col = \"#052049bb\", border = \"white\", main = \"\")\nhist(y, col = \"#052049bb\", border = \"white\", main = \"\")\nhist(z, col = \"#052049bb\", border = \"white\", main = \"\")\nplot(x, y, col = \"#05204955\", pch = 16, bty = \"n\")\nplot(x, z, col = \"#05204955\", pch = 16, bty = \"n\")\nplot(y, z, col = \"#05204955\", pch = 16, bty = \"n\")\n\n\n\npar(mfrow = c(1, 1))"
  },
  {
    "objectID": "BaseGraphics.html#saveplots",
    "href": "BaseGraphics.html#saveplots",
    "title": "27  Base Graphics",
    "section": "27.10 Saving plots to file",
    "text": "27.10 Saving plots to file\nYou can save base graphics to disk using a number of different file formats. To do this, you have to:\n\nOpen a graphics device - e.g. pdf(\"path/to/xy_scatter.pdf\")\nWrite to it - e.g. plot(x, y)\nClose graphics device - dev.off()\n\nThe following commands are used to open graphical devices that will save to a file of the corresponding type:\n\nbmp(filename = \"path/to/file\", width = [in pixels], height = [in pixels])\njpeg(filename = \"path/to/file\", width = [in pixels], height = [in pixels])\npng(filename = \"path/to/file\", width = [in pixels], height = [in pixels])\ntiff(filename = \"path/to/file\", width = [in pixels], height = [in pixels])\nsvg(filename = \"path/to/file\", width = [in inches], height = [in inches]\npdf(file = \"path/to/file\", width = [in inches], height = [in inches])\n\nNotice that when writing to a vector graphics format (svg and pdf), you defined width and height in inches, not pixels. Also, you specify file instead of filename in Notice the difference when writing to PDF: you define a file instead of a filename, and width and height are in inches, not pixels.\nIt is recommended to save plots in PDF format because it handles vector graphics therefore plots will scale, and it is easy to export to other graphics formats later on if needed.\n\npdf(\"~/Desktop/plot.pdf\", width = 5, height = 5)\nplot(iris$Sepal.Length, iris$Petal.Length,\n     pch = 16,\n     col = \"#18A3AC66\",\n     cex = 1.8,\n     bty = \"n\", # also try \"l\"\n     xlab = \"Sepal Length\", ylab = \"Petal Length\")\ndev.off()"
  },
  {
    "objectID": "3xGraphics.html",
    "href": "3xGraphics.html",
    "title": "28  3x Graphics",
    "section": "",
    "text": "Visualization is central to statistics and data science. It is used to check data, explore data, and communicate results.\nR has powerful graphical capabilities built in to the core language. It contains two largely separate graphics systems: ‘base’ graphics in the graphics package, inherited from the S language, and ‘grid’ graphics in the grid package: a “rewrite of the graphics layout capabilities”. There is limited support for interaction between the two. In practice, for a given application, choose one or the other. There are no high level functions for the grid graphics system built into the base R distribution, but a few very popular packages have been built on top of it. Both graphics systems can produce beautiful, layered, high quality graphics. It is possible to build functions using either system to produce most, if not all, types of plots."
  },
  {
    "objectID": "3xGraphics.html#base-graphics",
    "href": "3xGraphics.html#base-graphics",
    "title": "28  3x Graphics",
    "section": "28.1 Base graphics",
    "text": "28.1 Base graphics\nCommon R plotting functions like plot, barplot, boxplot, heatmap, etc. are built ontop of base graphics (Murrell 2018). Their default arguments provide a minimalist output, but can be tweaked extensively. An advantage of base graphics is they are very fast and relatively easy to extend.\nThe par function allows setting or querying graphical parameters of the base graphics system. Have a look at its documentation (?par).\nSome graphical parameters can only be set with a call to par prior to using a base plotting function. However, many parameters can also be passed using the ... construct of each base plotting function.\nSome common base graphical parameters:\n\npch: Point character\ncol: Color\ncex: Character expansion, i.e. relative size\nbty: Box type\nxlab: x-axis label\nylab: y-axis label\nmain: Main title\n\nAlways make sure that your plotting characters, axis labels and titles are legible. You must avoid, at all costs, ever using a huge graph with tiny letters spread over a whole slide in a presentation.\n\ncex: Character expansion for the plotting characters\ncex.axis: cex for axis annotation\ncex.lab: cex for x and y labels\ncex.main: cex for main title\n\nNote: All of these can be set either with a call to par() prior to plotting or passed as arguments in a plotting command, like plot().\nHowever, there is one important distinction: cex set with par() (which defaults to 1), sets the baseline and all other cex parameters multiply it. However, cex set within plot() stil multiplies cex set with par(), but only affectts the plotting character size."
  },
  {
    "objectID": "3xGraphics.html#grid-graphics",
    "href": "3xGraphics.html#grid-graphics",
    "title": "28  3x Graphics",
    "section": "28.2 Grid graphics",
    "text": "28.2 Grid graphics\nThe two most popular packages built on top of the grid package are:\n\nlattice (Sarkar 2008)\nggplot2 (Wickham 2011)\n\n\n28.2.1 ggplot2\nggplot2, created by Hadley Wickham (Wickham 2011), follows the Grammar of Graphics approach of Leland Wilkinson (Wilkinson 2012) and has a very different syntax than base functions.\nThe general idea is to start by defining the data and then add and/or modify graphical elements in a stepwise manner, which allows one to build complex and layered visualizations. A simplified interface to ggplot graphics is provided in the qplot() function of ggplot2 (but you should avoid it and use learn to use the ggplot() command which is fun and much more flexible and useful to know)"
  },
  {
    "objectID": "3xGraphics.html#rd-party-apis",
    "href": "3xGraphics.html#rd-party-apis",
    "title": "28  3x Graphics",
    "section": "28.3 3rd party APIs",
    "text": "28.3 3rd party APIs\nThere are also third party libraries with R APIs that provide even more modern graphic capabilities to the R user:\n\nplotly (Sievert et al. 2017)\nrbokeh\n\nBoth build interactive plots, which can be viewed in a web browser or exported to bitmap graphics, and both also follow the grammar of graphics paradigm, and therefore follow similar syntax to ggplot2.\nThe rtemis package (Gennatas 2017) provides visualization functions built on top of base graphics (for speed and extendability) and plotly (for interactivity):\n\nmplot3 static graphics (base)\ndplot3 interactive graphics (plotly)\n\nLet’s go over the most common plot types using base graphics, rtemis (`mplot3, dplot3), ggplot2, and plotly.\nThis is meant to get you started but is barely scratching the surface. There is extensive functionality included in each plotting library and you should consult the respective documentation for details."
  },
  {
    "objectID": "3xGraphics.html#box-plot",
    "href": "3xGraphics.html#box-plot",
    "title": "28  3x Graphics",
    "section": "28.4 Box plot",
    "text": "28.4 Box plot\nLet’s create some synthetic data.\n\nset.seed(2019)\nx <- as.data.frame(matrix(rnorm(200*4), 200))\ncolnames(x) <- c(\"mango\", \"banana\", \"tangerine\", \"sugar\")\n\n\n28.4.1 base\n\nboxplot(x)\n\n\n\n\n\nboxplot(x, col = \"steelblue4\")\n\n\n\n\n\n\n28.4.2 mplot3\n\nmplot3_box(x)\n\n\n\n\n\n\n28.4.3 dplot3\n\ndplot3_box(x)\n\n\n\n\n\n\n\n28.4.4 ggplot2\nAgain, ggplot requires an explicit categorical x-axis. In this case, this means we need to convert our dataset from wide to long.\nHere we use tidyr’s pivot_longer() function, since it is part of the same cult known as the tidyverse. You can instead use the builtin reshape() function or any wide-to-long operation of your choosing, e.g. data.table’s melt().\n\nlibrary(tidyr)\nx.long <- pivot_longer(x, 1:4, \n                       names_to = \"Fruit\", \n                       values_to = \"Feature\")\nx.long\n# A tibble: 800 × 2\n   Fruit     Feature\n   <chr>       <dbl>\n 1 mango      0.739 \n 2 banana     0.721 \n 3 tangerine -1.26  \n 4 sugar     -0.0187\n 5 mango     -0.515 \n 6 banana    -0.395 \n 7 tangerine  0.258 \n 8 sugar     -0.0301\n 9 mango     -1.64  \n10 banana     0.983 \n# … with 790 more rows\n\np <- ggplot(x.long, aes(Fruit, Feature)) + geom_boxplot()\np\n\n\n\n\nAdd some color:\n\n(p <- ggplot(x.long, aes(Fruit, Feature)) +\n   geom_boxplot(fill = c(\"#44A6AC66\", \"#F4A36266\", \"#3574A766\", \"#C23A7066\"),\n                colour = c(\"#44A6ACFF\", \"#F4A362FF\", \"#3574A7FF\", \"#C23A70FF\")))\n\n\n\n\n\n\n28.4.5 plotly\nIn plotly, we can use a loop to add each column’s boxplot one at a time. In the following example, we turn off the legend, since the names also appear below each boxplot:\n\nplt <- plot_ly(type = \"box\")\nfor (i in seq_along(x)) {\n  plt <- add_trace(plt, y = x[, i], name = colnames(x)[i])\n}\nplt |> layout(showlegend = F)"
  },
  {
    "objectID": "3xGraphics.html#histogram",
    "href": "3xGraphics.html#histogram",
    "title": "28  3x Graphics",
    "section": "28.5 Histogram",
    "text": "28.5 Histogram\n\nset.seed(2020)\na <- rnorm(500)\n\n\n28.5.1 base\n\nhist(a)\n\n\n\nhist(a, col = \"#18A3AC66\")\n\n\n\nhist(a, col = \"#18A3AC99\", border = \"white\", main = \"\", breaks = 30)\n\n\n\n\n\n\n28.5.2 mplot3\n\nmplot3_x(a, \"histogram\")\n\n\n\nmplot3_x(a, \"histogram\", hist.breaks = 30)\n\n\n\n\n\n\n28.5.3 dplot3\n\ndplot3_x(a, \"hist\")\n\n\n\n\ndplot3_x(a, \"hist\", hist.n.bins = 40)\n\n\n\n\n\n\n\n28.5.4 ggplot2\n\n(p <- ggplot(mapping = aes(a)) + geom_histogram())\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n(p <- ggplot(mapping = aes(a)) + geom_histogram(binwidth = .2))\n\n\n\n(p <- ggplot(mapping = aes(a)) +\n    geom_histogram(binwidth = .2, fill = \"#18A3AC99\"))\n\n\n\n\n\n\n28.5.5 plotly\n\nplt <- plot_ly(x = a, type = \"histogram\") |> \n  layout(bargap = .1)\nplt\n\n\n\n\n\n\n28.5.5.1 Grouped\n\nmplot3_x(iris$Petal.Length, \"hist\", group = iris$Species, hist.breaks = 10)\n\n\n\n\n\ndplot3_x(iris$Sepal.Length, \"hist\", group = iris$Species)\n\n\n\n\n\nor “ridge”-mode:\n\ndplot3_x(iris$Sepal.Length, \"hist\", group = iris$Species,\n         mode = \"ridge\")\n\n\n\n\n\n\nggplot(iris, aes(x = Sepal.Length, fill = Species)) + \n  geom_histogram(binwidth = .1)"
  },
  {
    "objectID": "3xGraphics.html#density-plot",
    "href": "3xGraphics.html#density-plot",
    "title": "28  3x Graphics",
    "section": "28.6 Density plot",
    "text": "28.6 Density plot\nThere is no builtin density plot, but you can get x and y coordinates from the density function and add a polygon:\n\n28.6.1 base\n\n.density <- density(iris$Sepal.Length)\nclass(.density)\n\n[1] \"density\"\n\nplot(.density$x, .density$y,\n     type = \"l\", yaxs = \"i\")\n\n\n\nplot(.density$x, .density$y,\n     type = 'l', yaxs = \"i\",\n     bty = \"n\",\n     xlab = \"\",  ylab = \"Density\",\n     col = \"#18A3AC66\",\n     main = \"Sepal Length Density\")\npolygon(c(.density$x, rev(.density$x)), c(.density$y, rep(0, length(.density$y))),\n        col = \"#18A3AC66\", border = NA)\n\n\n\n\n\n\n28.6.2 mplot3\n\nmplot3_x(iris$Sepal.Length, 'density')\n\n\n\n\n\n\n28.6.3 dplot3\n\ndplot3_x(iris$Sepal.Length)\n\n\n\n\n\n\n\n28.6.4 ggplot2\n\nggplot(iris, aes(x = Sepal.Length)) + geom_density()\n\n\n\n\nAdd color:\n\nggplot(iris, aes(x = Sepal.Length)) + geom_density(color = \"#18A3AC66\", fill = \"#18A3AC66\")\n\n\n\n\n\n28.6.4.1 Grouped\n\nmplot3_x(iris$Sepal.Length, group = iris$Species)\n\n\n\n\n\ndplot3_x(iris$Sepal.Length, group = iris$Species)\n\n\n\n\n\n\n(ggplot(iris, aes(Sepal.Length, color = Species, fill = Species)) + \n  geom_density(alpha = .5) +\n  scale_color_manual(values = c(\"#44A6AC\", \"#F4A362\", \"#3574A7\")) +\n  scale_fill_manual(values = c(\"#44A6AC\", \"#F4A362\", \"#3574A7\")) +\n  labs(x = \"Sepal Length\", y = \"Density\"))"
  },
  {
    "objectID": "3xGraphics.html#barplot",
    "href": "3xGraphics.html#barplot",
    "title": "28  3x Graphics",
    "section": "28.7 Barplot",
    "text": "28.7 Barplot\n\nschools <- data.frame(UCSF = 4, Stanford = 7, Penn = 12)\n\n\n28.7.1 base\n\nbarplot(as.matrix(schools))\n\n\n\nbarplot(as.matrix(schools), col = \"dodgerblue3\")\n\n\n\n\n\n\n28.7.2 mplot3\n\nmplot3_bar(schools)\n\n\n\n\n\n\n28.7.3 dplot3\n\ndplot3_bar(schools)\n\n\n\n\n\n\n\n28.7.4 ggplot2\nggplot requires an explicit column in the data that define the categorical x-axis:\n\nschools.df <- data.frame(University = colnames(schools),\n                         N_schools = as.numeric(schools[1, ]))\nggplot(schools.df, aes(University, N_schools)) +\n  geom_bar(stat = \"identity\", color = \"#18A3AC\", fill = \"#18A3AC\")\n\n\n\n\n\n\n28.7.5 plotly\n\nplt <- plot_ly(x = names(schools),\n               y = unlist(schools),\n               name = \"Schools\",\n               type = \"bar\")\nplt\n\n\n\n\n\nNote that for the above to work, y needs to be a vector, and since x was a data.frame of one line, we use unlist() to convert to a vector."
  },
  {
    "objectID": "3xGraphics.html#scatterplot",
    "href": "3xGraphics.html#scatterplot",
    "title": "28  3x Graphics",
    "section": "28.8 Scatterplot",
    "text": "28.8 Scatterplot\n\n28.8.1 base\nA default base graphics plot is rather minimalist:\n\nplot(iris$Sepal.Length, iris$Petal.Length)\n\n\n\n\nBy tweaking a few parameters, we get a perhaps prettier result:\n\nplot(iris$Sepal.Length, iris$Petal.Length,\n     pch = 16,\n     col = \"#18A3AC66\",\n     cex = 1.4,\n     bty = \"n\",\n     xlab = \"Sepal Length\", ylab = \"Petal Length\")\n\n\n\n\n\n\n28.8.2 mplot3\n\nmplot3_xy(iris$Sepal.Length, iris$Petal.Length)\n\n\n\n\ndplot3 provides similar functionality to mplot3, built on top of plotly. Notice how you can interact with the plot using the mouse:\n\n\n28.8.3 dplot3\n\ndplot3_xy(iris$Sepal.Length, iris$Petal.Length)\n\n\n\n\n\n\n\n28.8.4 ggplot2\nNote: The name of the package is ggplot2, the name of the function is ggplot.\n\nggplot(iris, aes(Sepal.Length, Petal.Length)) + geom_point()\n\n\n\n\n\n\n28.8.5 plotly\n\np <- plot_ly(iris, x = ~Sepal.Length, y = ~Petal.Length) %>% \n  add_trace(type = \"scatter\", mode = \"markers\")\np\n\n\n\n\n\n\n\n28.8.6 Grouped\nIn mplot3 and dplot3, add a group argument:\n\nmplot3_xy(iris$Sepal.Length, iris$Petal.Length,\n          group = iris$Species)\n\n\n\n\n\ndplot3_xy(iris$Sepal.Length, iris$Petal.Length,\n          group = iris$Species)\n\n\n\n\n\nIn ggplot2, specify color within aes.\nggplot() plots can be assigned to an object. Print the object to view it.\n\np <- ggplot(iris, aes(Sepal.Length, Petal.Length, color = Species)) +\n  geom_point()\np\n\n\n\n\nIn plotly define the color argument:\n\np <- plot_ly(iris, x = ~Sepal.Length, y = ~Petal.Length, color = ~Species) %>% \n  add_trace(type = \"scatter\", mode = \"markers\")\np"
  },
  {
    "objectID": "3xGraphics.html#scatterplot-with-fit",
    "href": "3xGraphics.html#scatterplot-with-fit",
    "title": "28  3x Graphics",
    "section": "28.9 Scatterplot with fit",
    "text": "28.9 Scatterplot with fit\n\n28.9.1 mplot3\nIn mplot3_xy(), define the algorithm to use to fit a curve, with fit. se.fit allows plotting the standard error bar (if it can be provided by the algorithm in fit)\n\nmplot3_xy(iris$Sepal.Length, iris$Petal.Length,\n          fit = \"gam\", se.fit = T)\n\n\n\n\nPassing a group argument, automatically fits separate models:\n\nmplot3_xy(iris$Sepal.Length, iris$Petal.Length,\n          fit = \"gam\", se.fit = T,\n          group = iris$Species)\n\n\n\n\n\n\n28.9.2 dplot3\nSame syntax as mplot3_xy() above:\n\ndplot3_xy(iris$Sepal.Length, iris$Petal.Length,\n          fit = \"gam\", se.fit = T)\n\n\n\n\n\n\ndplot3_xy(iris$Sepal.Length, iris$Petal.Length,\n          fit = \"gam\", se.fit = T,\n          group = iris$Species)\n\n\n\n\n\n\n\n28.9.3 ggplot2\nIn ggplot(), add a geom_smooth:\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Length)) +\n  geom_point() +\n  geom_smooth(method = 'gam')\n\n`geom_smooth()` using formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nTo group, again, use color:\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Length, color = Species)) +\n  geom_point() +\n  geom_smooth(method = 'gam')\n\n`geom_smooth()` using formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n28.9.4 plotly\nIn plot_ly(), add_lines():\n\nlibrary(mgcv)\nmod.gam <- gam(Petal.Length ~ s(Sepal.Length), data = iris)\nplot_ly(iris, x = ~Sepal.Length) %>%\n  add_trace(y = ~Petal.Length, type = \"scatter\", mode = \"markers\") %>% \n  add_lines(y = mod.gam$fitted.values)\n\n\n\n\n\nTo get fit by group, you add all elements one after the other - one way would be this:\n\niris.bySpecies <- split(iris, iris$Species)\ngam.fitted <- lapply(iris.bySpecies, function(i) {\n  gam(Petal.Length ~ s(Sepal.Length), data = i)$fitted\n})\nindex <- lapply(iris.bySpecies, function(i) order(i$Sepal.Length))\ncol <- c(\"#44A6AC\", \"#F4A362\", \"#3574A7\")\n.names <- names(iris.bySpecies)\np <- plot_ly()\nfor (i in seq_along(iris.bySpecies)) {\n  p <- add_trace(p, x = ~Sepal.Length, y = ~Petal.Length, \n                 type = \"scatter\", mode = \"markers\",\n                 data = iris.bySpecies[[i]],\n                 name = .names[i],\n                 color = col[i])\n}\nfor (i in seq_along(iris.bySpecies)) {\n  p <- add_lines(p, x = iris.bySpecies[[i]]$Sepal.Length[index[[i]]],\n                 y = gam.fitted[[i]][index[[i]]], \n                 # type = \"scatter\", mode = \"markers\",\n                 data = iris.bySpecies[[i]],\n                 name = paste(.names[i], \"GAM fit\"),\n                 color = col[i])\n}\np\n\n\n\n\n\nIt’s a lot of work, and that’s why dplot3 exists."
  },
  {
    "objectID": "3xGraphics.html#heatmap",
    "href": "3xGraphics.html#heatmap",
    "title": "28  3x Graphics",
    "section": "28.10 Heatmap",
    "text": "28.10 Heatmap\nLet’s create some synthetic correlation data:\n\nset.seed(2020)\nx <- matrix(rnorm(400), 20)\nx.cor <- cor(x)\n\n\n28.10.1 base\nR has a great builtin heatmap function, which supports hierarchical clustering and plots the dendrogram in the margins by default:\n\nheatmap(x.cor)\n\n\n\n\nIt may be a little surprising that clustering is on by default. To disable row and column dendrograms, set Rowv and Colv to NA:\n\nheatmap(x.cor, Rowv = NA, Colv = NA)\n\n\n\n\n\n\n28.10.2 mplot3\nmplot3 adds a colorbar to the side of the heatmap. Notice there are 10 circles above and 10 circles below zero to represent 10% increments.\n\nmplot3_heatmap(x.cor)\n\n\n\n\n\n\n28.10.3 dplot3\n\ndplot3_heatmap(x.cor)\n\n\n\n\n\n\n\n28.10.4 ggplot2\nggplot does not have a builtin heatmap function per se, but you can use geom_tile to build one. It also needs a data frame input in long form once again:\n\nx.cor.dat <- as.data.frame(x.cor)\ncolnames(x.cor.dat) <- rownames(x.cor.dat) <- paste0(\"V\", seq(20))\ncolnames(x.cor) <- rownames(x.cor) <- paste0(\"V\", seq(20))\nx.cor.long <- data.frame(NodeA = rownames(x.cor)[row(x.cor)],\n                         NodeB = colnames(x.cor)[col(x.cor)],\n                         Weight = c(x.cor))\n(p <- ggplot(x.cor.long, aes(NodeA, NodeB, fill = Weight)) +\n    geom_tile() + coord_equal())\n\n\n\n\n\n\n\n\nGennatas, Efstathios Dimitrios. 2017. “Towards Precision Psychiatry: Gray Matter Development and Cognition in Adolescence.”\n\n\nMurrell, Paul. 2018. R Graphics. CRC Press.\n\n\nSarkar, Deepayan. 2008. Lattice: Multivariate Data Visualization with r. New York: Springer. http://lmdvr.r-forge.r-project.org.\n\n\nSievert, Carson, Chris Parmer, Toby Hocking, Scott Chamberlain, Karthik Ram, Marianne Corvellec, and Pedro Despouy. 2017. “Plotly: Create Interactive Web Graphics via ‘Plotly. Js’.” R Package Version 4 (1): 110.\n\n\nWickham, Hadley. 2011. “Ggplot2.” Wiley Interdisciplinary Reviews: Computational Statistics 3 (2): 180–85.\n\n\nWilkinson, Leland. 2012. “The Grammar of Graphics.” In Handbook of Computational Statistics, 375–414. Springer."
  },
  {
    "objectID": "Colors.html",
    "href": "Colors.html",
    "title": "29  Colors in R",
    "section": "",
    "text": "Colors in R can be defined in many different ways:"
  },
  {
    "objectID": "Colors.html#color-names",
    "href": "Colors.html#color-names",
    "title": "29  Colors in R",
    "section": "29.1 Color names",
    "text": "29.1 Color names\nThere is a long list of color names R understands, and can be listed using colors().\nThey can be passed directly as characters.\nShades of gray are provided as gray0/grey0 (white) to gray100/grey100 (black).\nAbsurdly wide PDFs with all built-in R colors, excluding the grays/greys, are available sorted alphabeticaly and sorted by increasing Red and decreasing Green and Blue values"
  },
  {
    "objectID": "Colors.html#hexadecimal-codes",
    "href": "Colors.html#hexadecimal-codes",
    "title": "29  Colors in R",
    "section": "29.2 Hexadecimal codes",
    "text": "29.2 Hexadecimal codes\nHexadecimal color codes are characters starting with the pound sign, followed by 4 pairs of hex codes representing Red, Green, Blue, and Alpha values. Since RGB values go from 0 to 255, hex goes from 00 to FF. You can convert decimal to hex using as.hexmode:\n\nas.hexmode(0)\n\n[1] \"0\"\n\nas.hexmode(127)\n\n[1] \"7f\"\n\nas.hexmode(255)\n\n[1] \"ff\"\n\n\nThe last two values for the alpha setting are optional: if not included, defaults to max (opaque)"
  },
  {
    "objectID": "Colors.html#rgb",
    "href": "Colors.html#rgb",
    "title": "29  Colors in R",
    "section": "29.3 RGB",
    "text": "29.3 RGB\n\nrgb(0, 0, 1)\n\n[1] \"#0000FF\"\n\n\nNote the default maxColorValue = 1, set to 255 to use the usual RGB range of 0 to 255:\n\nrgb(0, 0, 255, maxColorValue = 255)\n\n[1] \"#0000FF\""
  },
  {
    "objectID": "Colors.html#hsv",
    "href": "Colors.html#hsv",
    "title": "29  Colors in R",
    "section": "29.4 HSV",
    "text": "29.4 HSV\nColor can also be parameterized using the hue, saturation, and value system (HSV). Each range from 0 to 1.\nSimplistically: Hue controls the color. Saturation 1 is max color and 0 is white. Value 1 is max color and 0 is black.\n\nhsv(1, 1, 1)\n\n[1] \"#FF0000\"\n\n\nIn the following plot, the values around the polar plot represent hue. Moving inwards to the center, saturation changes from 1 to 0.\n\nmplot_hsv()\n\n\n\nmplot_hsv(v = .5)"
  },
  {
    "objectID": "Profiling.html",
    "href": "Profiling.html",
    "title": "30  Benchmarking & Profiling",
    "section": "",
    "text": "Benchmarking is the process of timing the execution of code for the purpose of comparison. For example, you can compare the execution time of a program in two different systems, e.g. a laptop and a high performance server. Another common case is to compare the performance of two different programs that produce the same output on the same computer.\nProfiling refers to timing the different steps of a program to identify bottlenecks and potential targets for optimization."
  },
  {
    "objectID": "Profiling.html#system.time-time-the-execution-of-an-expression",
    "href": "Profiling.html#system.time-time-the-execution-of-an-expression",
    "title": "30  Benchmarking & Profiling",
    "section": "30.1 system.time(): Time the execution of an expression",
    "text": "30.1 system.time(): Time the execution of an expression\nThe base package’s system.time() function allows you to measure the execution time of an R expression.\n\nsystem.time(rnorm(9999999))\n\n   user  system elapsed \n  0.285   0.007   0.294 \n\n\n“elapsed” time is real time in seconds.\n“user” and “system” are time used by the CPU on different types of tasks (see ?proc.time).\nAs always, you can pass any R expression within curly brackets:\n\nx <- rnorm(9999)\nsystem.time({\n    for (i in 2:9999) {\n      x[i]\n      x[i] <- x[i]^3\n    }\n})\n\n   user  system elapsed \n  0.002   0.000   0.002 \n\n\nYou can use replicate() to get a measure of time over multiple executions and average it:\n\nset.seed(2020)\nx <- matrix(rnorm(500000), 5000)\ny <- 12 + x[, 3] + x[, 5]^2 + x[, 7]^3 + rnorm(5000)\nfit.glm <- function(x, y) glm.fit(x, y)\n    \nfit.glm_time10 <- replicate(10, system.time(fit.glm(x, y))[[1]])\n\n\nboxplot(fit.glm_time10)"
  },
  {
    "objectID": "Profiling.html#compare-execution-times-with-microbenchmarkmicrobenchmark",
    "href": "Profiling.html#compare-execution-times-with-microbenchmarkmicrobenchmark",
    "title": "30  Benchmarking & Profiling",
    "section": "30.2 Compare execution times with microbenchmark::microbenchmark()",
    "text": "30.2 Compare execution times with microbenchmark::microbenchmark()\nThe microbenchmark package’s microbenchmark() function allows you to time the execution of multiple expressions with sub-millisecond accuracy. It will execute each command a number of times as defined by the times argument (default = 100), and output statistics of execution time per expression in nanoseconds. Using plot() on the output produces a boxplot comparing the time distributions.\n\n# install.packages(\"microbenchmark\")\nlibrary(microbenchmark)\n\n\n30.2.1 Example: loop over matrix vs. data.frame\nLet’s create xmat, a 500 by 5 matrix and, xdf a data.frame with the same data.\n\nset.seed(2021)\nxmat <- matrix(rnorm(500*5), 5)\nxdf <- as.data.frame(xmat)\n\nIf you wanted to square either of them, you would just use ^2. Here, we create a function specifically to demonstrate the difference in working on a numeric matrix vs. a data.frame by using a nested loop that replaces each element one at a time.\n\nsilly_square <- function(x) {\n  for (i in seq_len(NROW(x))) {\n    for (j in seq_len(NCOL(x))) {\n      x[i, j] <- x[i, j]^2\n    }\n  }\n}\n\n\nmat_df_sq <- microbenchmark(silly_square_mat = silly_square(xmat),\n               silly_square_df = silly_square(xdf),\n               mat_squared = xmat^2,\n               df_squared = xdf^2)\n\nWarning in microbenchmark(silly_square_mat = silly_square(xmat), silly_square_df\n= silly_square(xdf), : less accurate nanosecond times to avoid potential integer\noverflows\n\nclass(mat_df_sq)\n\n[1] \"microbenchmark\" \"data.frame\"    \n\n\nPrint microbenchmark’s output:\n\nmat_df_sq\n\nUnit: nanoseconds\n             expr      min       lq        mean   median         uq      max\n silly_square_mat   101844   104796   124962.67   108404   111335.5  1755046\n  silly_square_df 38292319 40130616 41460636.23 41052336 41980802.0 63218720\n      mat_squared      943     1271     2449.75     2378     3054.5     7626\n       df_squared  9058417  9379878 10179430.39  9775978 10987221.0 11855519\n neval\n   100\n   100\n   100\n   100\n\n\nNotice how a) either operation is much faster on a matrix vs. a data.frame and b) vectorized squaring with ^2 is much faster than the nested loop as expected.\nThere is a plot() method for microbenchmark objects:\n\nplot(mat_df_sq)\n\n\n\n\n\n\n30.2.2 Example: Group means\nLet’s perform a simple mean-by-group operation and compare three different approaches. As an example, we use the flights dataset from the nycflights13 package which includes data on 336,776 flights that departed from NY area airports in 2013. The data comes as a tibble, and we create data.frame and data.table versions.th\n\nlibrary(data.table)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:data.table':\n\n    between, first, last\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(nycflights13)\nclass(flights)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\ndim(flights)\n\n[1] 336776     19\n\nflights_df <- as.data.frame(flights)\nflights_dt <- as.data.table(flights)\n\nCompare performance of the same operation using different functions:\n\nbase R aggregate() with formula input\nbase R aggregate() with list input\nbase R tapply()\n\n\nflights_aggregate_formula <- function() {\n  aggregate(arr_delay ~ carrier, \n            data = flights_df,\n            mean, na.rm = TRUE)\n}\n  \nflights_aggregate <- function() {\n  aggregate(flights_df$arr_delay, \n            by = list(flights_df$carrier), \n            mean, na.rm = TRUE)\n}\n\nflights_tapply <- function() {\n  tapply(flights_df$arr_delay, \n         flights_df$carrier, \n         mean, na.rm = TRUE)\n}\n\ngroupmean_3x <- microbenchmark(\n  aggregate_formula = flights_aggregate_formula(),\n  aggregate = flights_aggregate(),\n  tapply = flights_tapply()\n  )\n\n\ngroupmean_3x\n\nUnit: milliseconds\n              expr       min        lq     mean    median       uq      max\n aggregate_formula 44.319729 49.144998 53.00802 51.265765 53.80057 91.07724\n         aggregate 39.153852 42.042979 44.97512 43.651798 45.52107 85.39845\n            tapply  8.379006  9.045953 12.09955  9.703532 11.09044 51.18690\n neval\n   100\n   100\n   100\n\n\n\nplot(groupmean_3x)"
  },
  {
    "objectID": "Profiling.html#profile-a-function-with-profvis",
    "href": "Profiling.html#profile-a-function-with-profvis",
    "title": "30  Benchmarking & Profiling",
    "section": "30.3 Profile a function with profvis()",
    "text": "30.3 Profile a function with profvis()\nThe profvis package’s profvis() function provides an interactive output to visualize the time spent in different calls within a program.\n\nlibrary(profvis)\nprofvis({\n  hf <- read.csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00519/heart_failure_clinical_records_dataset.csv\")\n  str(hf)\n  lt5 <- which(sapply(hf, \\(i) length(unique(i))) < 5)\n  for (i in lt5) hf[, i] <- factor(hf[, i])\n  index_numeric <- which(sapply(hf, is.numeric))\n  par(mfrow = c(1, length(index_numeric)))\n  for (i in index_numeric) boxplot(hf[, i])\n  par(mfrow = c(1, 1))\n})\n\n'data.frame':   299 obs. of  13 variables:\n $ age                     : num  75 55 65 50 65 90 75 60 65 80 ...\n $ anaemia                 : int  0 0 0 1 1 1 1 1 0 1 ...\n $ creatinine_phosphokinase: int  582 7861 146 111 160 47 246 315 157 123 ...\n $ diabetes                : int  0 0 0 0 1 0 0 1 0 0 ...\n $ ejection_fraction       : int  20 38 20 20 20 40 15 60 65 35 ...\n $ high_blood_pressure     : int  1 0 0 0 0 1 0 0 0 1 ...\n $ platelets               : num  265000 263358 162000 210000 327000 ...\n $ serum_creatinine        : num  1.9 1.1 1.3 1.9 2.7 2.1 1.2 1.1 1.5 9.4 ...\n $ serum_sodium            : int  130 136 129 137 116 132 137 131 138 133 ...\n $ sex                     : int  1 1 1 1 0 1 1 1 0 1 ...\n $ smoking                 : int  0 0 1 0 0 1 0 1 0 1 ...\n $ time                    : int  4 6 7 7 8 8 10 10 10 10 ...\n $ DEATH_EVENT             : int  1 1 1 1 1 1 1 1 1 1 ..."
  },
  {
    "objectID": "Optimization.html",
    "href": "Optimization.html",
    "title": "31  Optimization",
    "section": "",
    "text": "R provides a general purpose optimization tool, optim(). You can use it to estimate parameters that minimize any defined function.\nSupervised and unsupervised learning involves defining a loss function to minimize or an objective function to minimize or maximize.\nTo learn how optim() works, let’s write a simple function that returns linear coefficients by minimizing squared error."
  },
  {
    "objectID": "Optimization.html#data",
    "href": "Optimization.html#data",
    "title": "31  Optimization",
    "section": "31.1 Data",
    "text": "31.1 Data\n\nset.seed(2020)\nx <- sapply(seq(10), function(i) rnorm(500))\ny <- 12 + 1.5 * x[, 3] + 3.2 * x[, 7] + .5 * x[, 9] + rnorm(500)"
  },
  {
    "objectID": "Optimization.html#glm-glm-s_glm",
    "href": "Optimization.html#glm-glm-s_glm",
    "title": "31  Optimization",
    "section": "31.2 GLM (glm, s_GLM)",
    "text": "31.2 GLM (glm, s_GLM)\n\nyx.glm <- glm(y ~ x)\nsummary(yx.glm)\n\n\nCall:\nglm(formula = y ~ x)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-2.38739  -0.67391   0.00312   0.65531   3.08524  \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 11.979070   0.043252 276.962   <2e-16 ***\nx1           0.061798   0.040916   1.510   0.1316    \nx2          -0.003873   0.043271  -0.090   0.9287    \nx3           1.488113   0.042476  35.034   <2e-16 ***\nx4           0.031115   0.044015   0.707   0.4800    \nx5           0.034217   0.043664   0.784   0.4336    \nx6           0.034716   0.042189   0.823   0.4110    \nx7           3.183398   0.040605  78.399   <2e-16 ***\nx8          -0.034252   0.043141  -0.794   0.4276    \nx9           0.541219   0.046550  11.627   <2e-16 ***\nx10          0.087120   0.044000   1.980   0.0483 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.9207315)\n\n    Null deviance: 7339.42  on 499  degrees of freedom\nResidual deviance:  450.24  on 489  degrees of freedom\nAIC: 1390.5\n\nNumber of Fisher Scoring iterations: 2\n\n\nOr, using rtemis:\n\nmod.glm <- s_GLM(x, y)\n[2022-07-19 03:02:32 s_GLM] Hello, egenn \n\n.:Regression Input Summary\nTraining features: 500 x 10 \n Training outcome: 500 x 1 \n Testing features: Not available\n  Testing outcome: Not available\n\n[2022-07-19 03:02:32 s_GLM] Training GLM... \n\n.:GLM Regression Training Summary\n    MSE = 0.90 (93.87%)\n   RMSE = 0.95 (75.23%)\n    MAE = 0.77 (74.88%)\n      r = 0.97 (p = 5.3e-304)\n   R sq = 0.94\n\n\n\n\n[2022-07-19 03:02:32 s_GLM] Run completed in 0.01 minutes (Real: 0.67; User: 0.42; System: 0.03) \n\n\n\nsummary(mod.glm$mod)\n\n\nCall:\nglm(formula = .formula, family = family, data = df.train, weights = .weights, \n    na.action = na.action)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-2.38739  -0.67391   0.00312   0.65531   3.08524  \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 11.979070   0.043252 276.962   <2e-16 ***\nV1           0.061798   0.040916   1.510   0.1316    \nV2          -0.003873   0.043271  -0.090   0.9287    \nV3           1.488113   0.042476  35.034   <2e-16 ***\nV4           0.031115   0.044015   0.707   0.4800    \nV5           0.034217   0.043664   0.784   0.4336    \nV6           0.034716   0.042189   0.823   0.4110    \nV7           3.183398   0.040605  78.399   <2e-16 ***\nV8          -0.034252   0.043141  -0.794   0.4276    \nV9           0.541219   0.046550  11.627   <2e-16 ***\nV10          0.087120   0.044000   1.980   0.0483 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.9207315)\n\n    Null deviance: 7339.42  on 499  degrees of freedom\nResidual deviance:  450.24  on 489  degrees of freedom\nAIC: 1390.5\n\nNumber of Fisher Scoring iterations: 2"
  },
  {
    "objectID": "Optimization.html#optim",
    "href": "Optimization.html#optim",
    "title": "31  Optimization",
    "section": "31.3 optim",
    "text": "31.3 optim\nBasic usage of optim to find values of parameters that minimize a function:\n\nDefine a list of initial parameter values\nDefine a loss function whose first argument is the above list of initial parameter values\nPass parameter list and objective function to optim\n\nIn the following example, we wrap these three steps in a function called linearcoeffs, which will output the linear coefficients that minimize squared error, given a matrix/data.frame of features x and an outcome y. We also specify the optimization method to be used (See ?base::optim for details):\n\nlinearcoeffs <- function(x, y, method = \"BFGS\") {\n  \n  # 1. List of initial parameter values\n  params <- as.list(c(mean(y), rep(0, NCOL(x))))\n  names(params) <- c(\"Intercept\", paste0(\"Coefficient\", seq(NCOL(x))))\n  \n  # 2. Loss function: first argument is parameter list\n  loss <- function(params, x, y) {\n    estimated <- c(params[[1]] + x %*% unlist(params[-1]))\n    mean((y - estimated)^2)\n  }\n  \n  # 3. optim!\n  coeffs <- optim(params, loss, x = x, y = y, method = method)\n  \n  # The values that minimize the loss function are stored in $par\n  coeffs$par\n}\n\n\ncoeffs.optim <- linearcoeffs(x, y)\nestimated.optim <- cbind(1, x) %*% coeffs.optim\nmplot3_fit(y, estimated.optim)\n\n\n\ncoeffs.glm <- mod.glm$mod$coefficients\nestimated.glm <- cbind(1, x) %*% coeffs.glm\nmplot3_fit(y, estimated.glm)\n\n\n\n\n\nmplot3_fit(coeffs.glm, coeffs.optim)"
  },
  {
    "objectID": "HypothesisTesting.html",
    "href": "HypothesisTesting.html",
    "title": "32  Common statistical tests",
    "section": "",
    "text": "R includes a large number of functions to perform statistical hypothesis testing in the built-in stats package. This chapter includes a brief overview of the syntax for some common tests along with code to produce relevant plots of your data."
  },
  {
    "objectID": "HypothesisTesting.html#correlation-test",
    "href": "HypothesisTesting.html#correlation-test",
    "title": "32  Common statistical tests",
    "section": "32.1 Correlation test",
    "text": "32.1 Correlation test\n\nset.seed(2020)\nx <- rnorm(100)\ny1 <- .1 * x + rnorm(100)\ny2 <- .3 * x + rnorm(100)\ny3 <- x + rnorm(100)/5\ny4 <- x^2 + rnorm(100)\n\nScatterplot with linear fit:\n\nplot(x, y1,\n     col = \"#00000077\",\n     pch = 16, bty = \"none\")\nabline(lm(y1 ~ x), col = \"red\", lwd = 2)\n\n\n\n\n\nplot(x, y2,\n     col = \"#00000077\",\n     pch = 16, bty = \"none\")\nabline(lm(y3 ~ x), col = \"red\", lwd = 2)\n\n\n\n\n\nplot(x, y3,\n     col = \"#00000077\",\n     pch = 16, bty = \"none\")\nabline(lm(y3 ~ x), col = \"red\", lwd = 2)\n\n\n\n\nScatterplot with a LOESS fit\n\nscatter.smooth(x, y4,\n               col = \"#00000077\",\n               pch = 16, bty = \"none\",\n               lpars = list(col = \"red\", lwd = 2))\n\n\n\n\n\ncor.test(x, y1)\n\n\n    Pearson's product-moment correlation\n\ndata:  x and y1\nt = 0.66018, df = 98, p-value = 0.5107\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.1315978  0.2595659\nsample estimates:\n       cor \n0.06654026 \n\ncor.test(x, y2)\n\n\n    Pearson's product-moment correlation\n\ndata:  x and y2\nt = 3.3854, df = 98, p-value = 0.001024\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1357938 0.4889247\nsample estimates:\n      cor \n0.3235813 \n\ncor.test(x, y3)\n\n\n    Pearson's product-moment correlation\n\ndata:  x and y3\nt = 53.689, df = 98, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9754180 0.9888352\nsample estimates:\n      cor \n0.9834225 \n\ncor.test(x, y4)\n\n\n    Pearson's product-moment correlation\n\ndata:  x and y4\nt = 0.66339, df = 98, p-value = 0.5086\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.1312793  0.2598681\nsample estimates:\n       cor \n0.06686289"
  },
  {
    "objectID": "HypothesisTesting.html#students-t-test",
    "href": "HypothesisTesting.html#students-t-test",
    "title": "32  Common statistical tests",
    "section": "32.2 Student’s t-test",
    "text": "32.2 Student’s t-test\n\nset.seed(2021)\nx0 <- rnorm(500)\nx1 <- rnorm(500, mean = .7)\n\nFor all tests of differences in means, a boxplot is a good way to visualize. It accepts individual vectors, list or data.frame of vectors, or a formula to split a vector into groups by a factor.\n\nboxplot(x0, x1,\n        col = \"#05204999\", border = \"#052049\",\n        boxwex = .3, names = c(\"x0\", \"x1\"))\n\n\n\n\n\n32.2.1 One sample t-test\n\nt.test(x0)\n\n\n    One Sample t-test\n\ndata:  x0\nt = 0.093509, df = 499, p-value = 0.9255\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.08596896  0.09456106\nsample estimates:\n  mean of x \n0.004296046 \n\n\n\nt.test(x1)\n\n\n    One Sample t-test\n\ndata:  x1\nt = 15.935, df = 499, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.6322846 0.8101311\nsample estimates:\nmean of x \n0.7212079 \n\n\n\nboxplot(extra ~ group, sleep,\n        col = \"#05204999\", border = \"#052049\",\n        boxwex = .3)\n\n\n\n\n\n\n32.2.2 Two-sample T-test\nBoth t.test() and wilcox.test() (below) either accept input as two vectors, t.test(x, y) or a formula of the form t.test(x ~ group). The paired argument allows us to define a paired test. Since the sleep dataset includes measurements on the same cases in two conditions, we set paired = TRUE.\n\nt.test(extra ~ group, sleep, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  extra by group\nt = -4.0621, df = 9, p-value = 0.002833\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -2.4598858 -0.7001142\nsample estimates:\nmean difference \n          -1.58"
  },
  {
    "objectID": "HypothesisTesting.html#wilcoxon-test",
    "href": "HypothesisTesting.html#wilcoxon-test",
    "title": "32  Common statistical tests",
    "section": "32.3 Wilcoxon test",
    "text": "32.3 Wilcoxon test\nData from R Documentation:\n\n## Hollander & Wolfe (1973), 29f.\n## Hamilton depression scale factor measurements in 9 patients with\n##  mixed anxiety and depression, taken at the first (x) and second\n##  (y) visit after initiation of a therapy (administration of a\n##  tranquilizer).\nx <- c(1.83,  0.50,  1.62,  2.48, 1.68, 1.88, 1.55, 3.06, 1.30)\ny <- c(0.878, 0.647, 0.598, 2.05, 1.06, 1.29, 1.06, 3.14, 1.29)\ndepression <- data.frame(first = x, second = y, change = y - x)\n\n\n32.3.1 One-sample Wilcoxon\n\nwilcox.test(depression$change)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  depression$change\nV = 5, p-value = 0.03906\nalternative hypothesis: true location is not equal to 0\n\n\n\n\n32.3.2 Two-sample Wilcoxon rank sum test (unpaired)\na.k.a Mann-Whitney U test a.k.a. Mann–Whitney–Wilcoxon (MWW) a.k.a. Wilcoxon–Mann–Whitney test\n\nx1 <- rnorm(500, 3, 1.5)\nx2 <- rnorm(500, 5, 2)\nwilcox.test(x1, x2)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  x1 and x2\nW = 47594, p-value < 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n\n32.3.3 Two-sample Wilcoxon signed-rank test (paired)\n\nwilcox.test(x, y, paired = TRUE)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  x and y\nV = 40, p-value = 0.03906\nalternative hypothesis: true location shift is not equal to 0\n\n\n\nwilcox.test(x, y, paired = TRUE, alternative = \"greater\")\n\n\n    Wilcoxon signed rank exact test\n\ndata:  x and y\nV = 40, p-value = 0.01953\nalternative hypothesis: true location shift is greater than 0"
  },
  {
    "objectID": "HypothesisTesting.html#analysis-of-variance",
    "href": "HypothesisTesting.html#analysis-of-variance",
    "title": "32  Common statistical tests",
    "section": "32.4 Analysis of Variance",
    "text": "32.4 Analysis of Variance\n\nset.seed(20)\nBP_drug <- data.frame(Group = factor(rep(c(\"Placebo\", \"Drug_A\", \"Drug_B\"), each = 20),\n                                     levels = c(\"Placebo\", \"Drug_A\", \"Drug_B\")),\n                  SBP = c(rnorm(20, 140, 2.2), rnorm(20, 132, 2.1), rnorm(20, 138, 2)))\n\n\nboxplot(SBP ~ Group, BP_drug,\n        col = \"#05204999\", border = \"#052049\",\n        boxwex = .3)\n\n\n\n\n\nSBP_aov <- aov(SBP ~ Group, BP_drug)\nSBP_aov\n\nCall:\n   aov(formula = SBP ~ Group, data = BP_drug)\n\nTerms:\n                   Group Residuals\nSum of Squares  728.2841  264.4843\nDeg. of Freedom        2        57\n\nResidual standard error: 2.154084\nEstimated effects may be unbalanced\n\n\n\nsummary(SBP_aov)\n\n            Df Sum Sq Mean Sq F value Pr(>F)    \nGroup        2  728.3   364.1   78.48 <2e-16 ***\nResiduals   57  264.5     4.6                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe analysis of variance p-value is highly significant, but doesn’t tell us which levels of the Group factor are significantly different from each other. The boxplot already gives us a pretty good idea, but we can follow up with a pairwise t-test\n\n32.4.1 Pot-hoc pairwise t-tests\n\npairwise.t.test(BP_drug$SBP, BP_drug$Group,\n                p.adj = \"holm\")\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  BP_drug$SBP and BP_drug$Group \n\n       Placebo Drug_A \nDrug_A 2.9e-16 -      \nDrug_B 0.065   1.7e-13\n\nP value adjustment method: holm \n\n\nThe pairwise tests suggest that the difference between Placebo and Drug_A and between Drug_a and Drug_b are highly significant, while difference between Placebo and Drub_B is not (p = 0.065)."
  },
  {
    "objectID": "HypothesisTesting.html#kruskal-wallis-test",
    "href": "HypothesisTesting.html#kruskal-wallis-test",
    "title": "32  Common statistical tests",
    "section": "32.5 Kruskal-Wallis test",
    "text": "32.5 Kruskal-Wallis test\nKruskal-Wallis rank sum test of the null that the location parameters of the distribution of x are the same in each group (sample). The alternative is that they differ in at least one. It is a generalization of the Wilcoxon test to multiple independent samples.\nFrom the R Documentation:\n\n## Hollander & Wolfe (1973), 116.\n## Mucociliary efficiency from the rate of removal of dust in normal\n##  subjects, subjects with obstructive airway disease, and subjects\n##  with asbestosis.\nx <- c(2.9, 3.0, 2.5, 2.6, 3.2) # normal subjects\ny <- c(3.8, 2.7, 4.0, 2.4)      # with obstructive airway disease\nz <- c(2.8, 3.4, 3.7, 2.2, 2.0) # with asbestosis\nkruskal.test(list(x, y, z))\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  list(x, y, z)\nKruskal-Wallis chi-squared = 0.77143, df = 2, p-value = 0.68\n\n\n\nboxplot(Ozone ~ Month, data = airquality,\n        col = \"#05204999\", border = \"#052049\",\n        boxwex = .3)\n\n\n\n\n\nkruskal.test(Ozone ~ Month, data = airquality)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  Ozone by Month\nKruskal-Wallis chi-squared = 29.267, df = 4, p-value = 6.901e-06"
  },
  {
    "objectID": "HypothesisTesting.html#chi-squared-test",
    "href": "HypothesisTesting.html#chi-squared-test",
    "title": "32  Common statistical tests",
    "section": "32.6 Chi-squared Test",
    "text": "32.6 Chi-squared Test\nPearson’s chi-squared test for count data\nSome synthetic data:\n\nset.seed(2021)\nset.seed(2021)\nCohort <- factor(sample(c(\"Control\", \"Case\"), 500, TRUE),\n                 levels = c(\"Control\", \"Case\"))\nSex <- factor(\n  sapply(seq(Cohort), \\(i) sample(c(\"Male\", \"Female\"), 1,\n                                  prob = if (Cohort[i] == \"Control\") c(1, 1) else c(2, 1))))\ndat <- data.frame(Cohort, Sex)\nhead(dat)\n\n   Cohort    Sex\n1 Control   Male\n2    Case   Male\n3    Case   Male\n4    Case Female\n5 Control Female\n6    Case   Male\n\n\nYou can lot count data using a mosaic plot, with either a table or formula input:\n\nmosaicplot(table(Cohort, Sex),\n           color = c(\"orchid\", \"skyblue\"),\n           border = NA,\n           main = \"Cohort x Sex\")\n\n\n\n\n\nmosaicplot(Cohort ~ Sex, dat,\n           color = c(\"orchid\", \"skyblue\"),\n           border = NA,\n           main = \"Cohort x Sex\")\n\n\n\n\nchisq.test() accepts either two factors, or a table:\n\ncohort_sex_chisq <- chisq.test(dat$Cohort, dat$Sex)\ncohort_sex_chisq\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  dat$Cohort and dat$Sex\nX-squared = 18.015, df = 1, p-value = 2.192e-05\n\n\n\ncohort_sex_chisq <- chisq.test(table(dat$Cohort, dat$Sex))\ncohort_sex_chisq\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table(dat$Cohort, dat$Sex)\nX-squared = 18.015, df = 1, p-value = 2.192e-05"
  },
  {
    "objectID": "HypothesisTesting.html#fishers-exact-test",
    "href": "HypothesisTesting.html#fishers-exact-test",
    "title": "32  Common statistical tests",
    "section": "32.7 Fisher’s exact test",
    "text": "32.7 Fisher’s exact test\nFisher’s exact test for count data\nWorking on the same data as above, fisher.test() also accepts either two factors or a table as input:\n\ncohort_sex_fisher <- fisher.test(dat$Cohort, dat$Sex)\ncohort_sex_fisher\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  dat$Cohort and dat$Sex\np-value = 1.512e-05\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 1.516528 3.227691\nsample estimates:\nodds ratio \n  2.207866 \n\n\n\ncohort_sex_fisher <- fisher.test(table(dat$Cohort, dat$Sex))\ncohort_sex_fisher\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  table(dat$Cohort, dat$Sex)\np-value = 1.512e-05\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 1.516528 3.227691\nsample estimates:\nodds ratio \n  2.207866"
  },
  {
    "objectID": "HypothesisTesting.html#f-test-to-compare-two-variances",
    "href": "HypothesisTesting.html#f-test-to-compare-two-variances",
    "title": "32  Common statistical tests",
    "section": "32.8 F Test to compare two variances",
    "text": "32.8 F Test to compare two variances\n\nx1 <- rnorm(500, sd = 1)\nx2 <- rnorm(400, sd = 1.5)\nvar.test(x1, x2)\n\n\n    F test to compare two variances\n\ndata:  x1 and x2\nF = 0.43354, num df = 499, denom df = 399, p-value < 2.2e-16\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.3594600 0.5218539\nsample estimates:\nratio of variances \n         0.4335363 \n\n\n\nboxplot(x1, x2,\n        col = \"#05204999\", border = \"#052049\",\n        boxwex = .3)\n\n\n\n\nFrom R Documentation:\n\nx <- rnorm(50, mean = 0, sd = 2)\ny <- rnorm(30, mean = 1, sd = 1)\nvar.test(x, y)                  # Do x and y have the same variance?\n\n\n    F test to compare two variances\n\ndata:  x and y\nF = 5.4776, num df = 49, denom df = 29, p-value = 5.817e-06\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n  2.752059 10.305597\nsample estimates:\nratio of variances \n          5.477573 \n\nvar.test(lm(x ~ 1), lm(y ~ 1))  # same\n\n\n    F test to compare two variances\n\ndata:  lm(x ~ 1) and lm(y ~ 1)\nF = 5.4776, num df = 49, denom df = 29, p-value = 5.817e-06\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n  2.752059 10.305597\nsample estimates:\nratio of variances \n          5.477573"
  },
  {
    "objectID": "HypothesisTesting.html#bartlett-test-of-homogeneity-of-variances",
    "href": "HypothesisTesting.html#bartlett-test-of-homogeneity-of-variances",
    "title": "32  Common statistical tests",
    "section": "32.9 Bartlett test of homogeneity of variances",
    "text": "32.9 Bartlett test of homogeneity of variances\nPerforms Bartlett’s test of the null that the variances in each of the groups (samples) are the same.\nFrom the R Documentation:\n\nplot(count ~ spray, data = InsectSprays)\n\n\n\n\n\nbartlett.test(InsectSprays$count, InsectSprays$spray)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  InsectSprays$count and InsectSprays$spray\nBartlett's K-squared = 25.96, df = 5, p-value = 9.085e-05\n\n\n\nbartlett.test(count ~ spray, data = InsectSprays)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  count by spray\nBartlett's K-squared = 25.96, df = 5, p-value = 9.085e-05"
  },
  {
    "objectID": "HypothesisTesting.html#fligner-killeen-test-of-homogeneity-of-variances",
    "href": "HypothesisTesting.html#fligner-killeen-test-of-homogeneity-of-variances",
    "title": "32  Common statistical tests",
    "section": "32.10 Fligner-Killeen test of homogeneity of variances",
    "text": "32.10 Fligner-Killeen test of homogeneity of variances\nPerforms a Fligner-Killeen (median) test of the null that the variances in each of the groups (samples) are the same.\n\nboxplot(count ~ spray, data = InsectSprays)\n\n\n\n# works the same if you do plot(count ~ spray, data = InsectSprays)\nfligner.test(InsectSprays$count, InsectSprays$spray)\n\n\n    Fligner-Killeen test of homogeneity of variances\n\ndata:  InsectSprays$count and InsectSprays$spray\nFligner-Killeen:med chi-squared = 14.483, df = 5, p-value = 0.01282\n\nfligner.test(count ~ spray, data = InsectSprays)\n\n\n    Fligner-Killeen test of homogeneity of variances\n\ndata:  count by spray\nFligner-Killeen:med chi-squared = 14.483, df = 5, p-value = 0.01282"
  },
  {
    "objectID": "HypothesisTesting.html#ansari-bradley-test",
    "href": "HypothesisTesting.html#ansari-bradley-test",
    "title": "32  Common statistical tests",
    "section": "32.11 Ansari-Bradley test",
    "text": "32.11 Ansari-Bradley test\nPerforms the Ansari-Bradley two-sample test for a difference in scale parameters.\n\nramsay <- c(111, 107, 100, 99, 102, 106, 109, 108, 104, 99,\n            101, 96, 97, 102, 107, 113, 116, 113, 110, 98)\njung.parekh <- c(107, 108, 106, 98, 105, 103, 110, 105, 104,\n            100, 96, 108, 103, 104, 114, 114, 113, 108, 106, 99)\nansari.test(ramsay, jung.parekh)\n\nWarning in ansari.test.default(ramsay, jung.parekh): cannot compute exact p-\nvalue with ties\n\n\n\n    Ansari-Bradley test\n\ndata:  ramsay and jung.parekh\nAB = 185.5, p-value = 0.1815\nalternative hypothesis: true ratio of scales is not equal to 1\n\n\n\nx <- rnorm(40, sd = 1.5)\ny <- rnorm(40, sd = 2.5)\nansari.test(x, y)\n\n\n    Ansari-Bradley test\n\ndata:  x and y\nAB = 963, p-value = 0.005644\nalternative hypothesis: true ratio of scales is not equal to 1"
  },
  {
    "objectID": "HypothesisTesting.html#mood-two-sample-test-of-scale",
    "href": "HypothesisTesting.html#mood-two-sample-test-of-scale",
    "title": "32  Common statistical tests",
    "section": "32.12 Mood two-sample test of scale",
    "text": "32.12 Mood two-sample test of scale\n\nmood.test(x, y)\n\n\n    Mood two-sample test of scale\n\ndata:  x and y\nZ = -2.7363, p-value = 0.006213\nalternative hypothesis: two.sided"
  },
  {
    "objectID": "HypothesisTesting.html#kolmogorov-smirnoff-test",
    "href": "HypothesisTesting.html#kolmogorov-smirnoff-test",
    "title": "32  Common statistical tests",
    "section": "32.13 Kolmogorov-Smirnoff test",
    "text": "32.13 Kolmogorov-Smirnoff test\nPerform a one- or two-sample Kolmogorov-Smirnov test Null: x and y were drawn from the same continuous distribution.\n\nx1 <- rnorm(200, 0, 1)\nx2 <- rnorm(200, -.5, 1.5)\nks.test(x1, x2)\n\n\n    Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  x1 and x2\nD = 0.35, p-value = 4.579e-11\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "HypothesisTesting.html#shapiro-wilk-test-of-normality",
    "href": "HypothesisTesting.html#shapiro-wilk-test-of-normality",
    "title": "32  Common statistical tests",
    "section": "32.14 Shapiro-Wilk test of normality",
    "text": "32.14 Shapiro-Wilk test of normality\n\nset.seed(2021)\nx <- rnorm(2000)\ny1 <- .7 * x\ny2 <- x + x^3\n\n\n32.14.1 Q-Q Plot\n\nqqplot(rnorm(300), y1, pch = 16, col = \"#00000077\")\nqqline(y1, col = \"red\", lwd = 2)\n\n\n\n\n\nqqplot(rnorm(300), y2, pch = 16, col = \"#00000077\")\nqqline(y2, col = \"red\", lwd = 2)"
  },
  {
    "objectID": "HypothesisTesting.html#shapiro-wilk-test",
    "href": "HypothesisTesting.html#shapiro-wilk-test",
    "title": "32  Common statistical tests",
    "section": "32.15 Shapiro-Wilk test",
    "text": "32.15 Shapiro-Wilk test\n\nshapiro.test(y1)\n\n\n    Shapiro-Wilk normality test\n\ndata:  y1\nW = 0.99952, p-value = 0.9218\n\nshapiro.test(y2)\n\n\n    Shapiro-Wilk normality test\n\ndata:  y2\nW = 0.72936, p-value < 2.2e-16"
  },
  {
    "objectID": "HypothesisTesting.html#statsresources",
    "href": "HypothesisTesting.html#statsresources",
    "title": "32  Common statistical tests",
    "section": "32.16 Resources",
    "text": "32.16 Resources\n\nRegression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models"
  },
  {
    "objectID": "GLM.html",
    "href": "GLM.html",
    "title": "33  Introduction to the GLM",
    "section": "",
    "text": ".:rtemis 0.91.1 🌊 aarch64-apple-darwin20 (64-bit)\n  Defaults\n  |   Theme: whitegrid\n  |    Font: Fira Sans\n  | Palette: rtCol1\n  |    Plan: multicore\n  |   Cores: 8/10 available\n  Resources\n  | Documentation: https://rtemis.lambdamd.org\n  |       Learn R: https://class.lambdamd.org/pdsr\n  | rtemis themes: https://egenn.lambdamd.org/software/#rtemis_themes\n  |          Cite: `citation(\"rtemis\")`\n  Setup\n  | Enable progress reporting: `progressr::handlers(global = TRUE)`"
  },
  {
    "objectID": "GLM.html#generalized-linear-model-glm",
    "href": "GLM.html#generalized-linear-model-glm",
    "title": "33  Introduction to the GLM",
    "section": "33.1 Generalized Linear Model (GLM)",
    "text": "33.1 Generalized Linear Model (GLM)\nThe Generalized Linear Model is one of the most popular and important models in statistics.\nIt fits a model of the form:\n\\[y = β_0 + β_1 x_1 + β_2 x_2 + ... β_n x_n + ε\\]\nwhere:\n\n\\(y\\) is the dependent variable, i.e. outcome of interest\n\\(x_1\\) to \\(x_n\\) are the independent variables, a.k.a. covariates, a.k.a. predictors\n\\(β_0\\) is the intercept\n\\(β_1\\) to \\(β_n\\) are the coefficients\n\\(ε\\) is the error\n\nIn matrix notation:\n\\[y = Χβ + ε\\]\nLet’s look at an example using the GLM for regression. We’ll use the diabetes dataset from the mlbench package as an example.\n\ndata(PimaIndiansDiabetes2, package = 'mlbench')\nstr(PimaIndiansDiabetes2)\n\n'data.frame':   768 obs. of  9 variables:\n $ pregnant: num  6 1 8 1 0 5 3 10 2 8 ...\n $ glucose : num  148 85 183 89 137 116 78 115 197 125 ...\n $ pressure: num  72 66 64 66 40 74 50 NA 70 96 ...\n $ triceps : num  35 29 NA 23 35 NA 32 NA 45 NA ...\n $ insulin : num  NA NA NA 94 168 NA 88 NA 543 NA ...\n $ mass    : num  33.6 26.6 23.3 28.1 43.1 25.6 31 35.3 30.5 NA ...\n $ pedigree: num  0.627 0.351 0.672 0.167 2.288 ...\n $ age     : num  50 31 32 21 33 30 26 29 53 54 ...\n $ diabetes: Factor w/ 2 levels \"neg\",\"pos\": 2 1 2 1 2 1 2 1 2 2 ...\n\n\nWe fit a model predicting glucose level from all other covariates:\n\nmod <- glm(glucose ~ ., family = \"gaussian\", data = PimaIndiansDiabetes2)\nmod\n\n\nCall:  glm(formula = glucose ~ ., family = \"gaussian\", data = PimaIndiansDiabetes2)\n\nCoefficients:\n(Intercept)     pregnant     pressure      triceps      insulin         mass  \n    75.5742      -0.2168       0.1826       0.0277       0.1174      -0.0896  \n   pedigree          age  diabetespos  \n     0.1955       0.3700      21.6502  \n\nDegrees of Freedom: 391 Total (i.e. Null);  383 Residual\n  (376 observations deleted due to missingness)\nNull Deviance:      372400 \nResidual Deviance: 192000   AIC: 3561\n\nclass(mod)\n\n[1] \"glm\" \"lm\" \n\n\nThe glm() function accepts a formula that defines the model.\nThe formula hp ~ . means “regress hp on all other variables”. The family argument defines we are performing regression and the data argument points to the data frame where the covariates used in the formula are found.\nFor a gaussian output, we can also use the lm() function. There are minor differences in the output created, but the model is the same:\n\nmod_lm <- lm(glucose ~ ., data = PimaIndiansDiabetes2)\nmod_lm\n\n\nCall:\nlm(formula = glucose ~ ., data = PimaIndiansDiabetes2)\n\nCoefficients:\n(Intercept)     pregnant     pressure      triceps      insulin         mass  \n    75.5742      -0.2168       0.1826       0.0277       0.1174      -0.0896  \n   pedigree          age  diabetespos  \n     0.1955       0.3700      21.6502  \n\nclass(mod_lm)\n\n[1] \"lm\"\n\n\n\n33.1.1 Summary\nGet summary of the model using summary():\n\nsummary(mod)\n\n\nCall:\nglm(formula = glucose ~ ., family = \"gaussian\", data = PimaIndiansDiabetes2)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-73.274  -14.043   -2.433   13.004   78.112  \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 75.57419    8.08972   9.342  < 2e-16 ***\npregnant    -0.21685    0.48754  -0.445   0.6567    \npressure     0.18262    0.10014   1.824   0.0690 .  \ntriceps      0.02770    0.14665   0.189   0.8503    \ninsulin      0.11740    0.01027  11.433  < 2e-16 ***\nmass        -0.08960    0.22836  -0.392   0.6950    \npedigree     0.19555    3.40567   0.057   0.9542    \nage          0.36997    0.16183   2.286   0.0228 *  \ndiabetespos 21.65020    2.75623   7.855 4.07e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 501.4034)\n\n    Null deviance: 372384  on 391  degrees of freedom\nResidual deviance: 192037  on 383  degrees of freedom\n  (376 observations deleted due to missingness)\nAIC: 3560.6\n\nNumber of Fisher Scoring iterations: 2\n\n\nNote how R prints stars next to covariates whose p-values falls within certain limits, described right below the table of estimates.\nAlso notice that categorical variables of n levels get n-1 separate coefficients; the first level is considered the baseline. Therefore, make sure to set up factors appropriately before modeling to ensure the correct level is used as baseline.\n\n\n33.1.2 Coefficients\n\ncoefficients(mod)\n\n(Intercept)    pregnant    pressure     triceps     insulin        mass \n75.57418905 -0.21684536  0.18261621  0.02769953  0.11739871 -0.08959822 \n   pedigree         age diabetespos \n 0.19554841  0.36997121 21.65020282 \n\n# or\nmod$coefficients\n\n(Intercept)    pregnant    pressure     triceps     insulin        mass \n75.57418905 -0.21684536  0.18261621  0.02769953  0.11739871 -0.08959822 \n   pedigree         age diabetespos \n 0.19554841  0.36997121 21.65020282 \n\n\n\n\n33.1.3 Fitted values\n\nfitted(mod) |> head()\n\n       4        5        7        9       14       15 \n104.3669 134.0163 123.8123 191.4744 227.1301 147.0313 \n\n# or\nmod$fitted.values |> head()\n\n       4        5        7        9       14       15 \n104.3669 134.0163 123.8123 191.4744 227.1301 147.0313 \n\n\n\n\n33.1.4 Residuals\n\nresiduals(mod) |> head()\n\n         4          5          7          9         14         15 \n-15.366923   2.983712 -45.812340   5.525562 -38.130138  18.968718 \n\n# or\nmod$residuals |> head()\n\n         4          5          7          9         14         15 \n-15.366923   2.983712 -45.812340   5.525562 -38.130138  18.968718 \n\n\n\n\n33.1.5 p-values\nTo extract the p-values of the intercept and each coefficient, we use coef() on summary(). The final (4th) column lists the p-values:\n\ncoef(summary(mod))\n\n               Estimate Std. Error     t value     Pr(>|t|)\n(Intercept) 75.57418905 8.08972421  9.34199821 7.916237e-19\npregnant    -0.21684536 0.48753958 -0.44477489 6.567337e-01\npressure     0.18261621 0.10014453  1.82352656 6.900293e-02\ntriceps      0.02769953 0.14664887  0.18888336 8.502843e-01\ninsulin      0.11739871 0.01026851 11.43288871 3.047384e-26\nmass        -0.08959822 0.22835576 -0.39236241 6.950087e-01\npedigree     0.19554841 3.40566786  0.05741852 9.542418e-01\nage          0.36997121 0.16183342  2.28612371 2.279239e-02\ndiabetespos 21.65020282 2.75623039  7.85500474 4.071061e-14\n\n\n\n\n33.1.6 Plot linear fit\nYou use lines() to add a line on top of a scatterplot drawn with plot().\nlines() accepts x and y vectors of coordinates:\n\nset.seed(2020)\nx <- rnorm(500)\ny <- .73 * x + .5 * rnorm(500)\nxy.fit <- lm(y~x)$fitted\nplot(x, y, pch = 16, col = \"#18A3AC99\")\nlines(x, xy.fit, col = \"#178CCB\", lwd = 2)\n\n\n\n\nIn rtemis, you can use argument fit to use any supported algorithm (see modSelect()) to estimate the fit:\n\nmplot3_xy(x, y, fit = \"glm\")\n\n\n\n\n\n\n33.1.7 Logistic Regression\nFor logistic regression, i.e. classification, you can use glm() with family = binomial\nUsing the same dataset, let’s predict diabetes status:\n\ndiabetes_mod <- glm(diabetes ~ ., PimaIndiansDiabetes2, \n                    family = \"binomial\")\ndiabetes_mod\n\n\nCall:  glm(formula = diabetes ~ ., family = \"binomial\", data = PimaIndiansDiabetes2)\n\nCoefficients:\n(Intercept)     pregnant      glucose     pressure      triceps      insulin  \n -1.004e+01    8.216e-02    3.827e-02   -1.420e-03    1.122e-02   -8.253e-04  \n       mass     pedigree          age  \n  7.054e-02    1.141e+00    3.395e-02  \n\nDegrees of Freedom: 391 Total (i.e. Null);  383 Residual\n  (376 observations deleted due to missingness)\nNull Deviance:      498.1 \nResidual Deviance: 344  AIC: 362\n\n\n\nsummary(diabetes_mod)\n\n\nCall:\nglm(formula = diabetes ~ ., family = \"binomial\", data = PimaIndiansDiabetes2)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.7823  -0.6603  -0.3642   0.6409   2.5612  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.004e+01  1.218e+00  -8.246  < 2e-16 ***\npregnant     8.216e-02  5.543e-02   1.482  0.13825    \nglucose      3.827e-02  5.768e-03   6.635 3.24e-11 ***\npressure    -1.420e-03  1.183e-02  -0.120  0.90446    \ntriceps      1.122e-02  1.708e-02   0.657  0.51128    \ninsulin     -8.253e-04  1.306e-03  -0.632  0.52757    \nmass         7.054e-02  2.734e-02   2.580  0.00989 ** \npedigree     1.141e+00  4.274e-01   2.669  0.00760 ** \nage          3.395e-02  1.838e-02   1.847  0.06474 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 498.10  on 391  degrees of freedom\nResidual deviance: 344.02  on 383  degrees of freedom\n  (376 observations deleted due to missingness)\nAIC: 362.02\n\nNumber of Fisher Scoring iterations: 5"
  },
  {
    "objectID": "GLM.html#mass-univariate-analysis",
    "href": "GLM.html#mass-univariate-analysis",
    "title": "33  Introduction to the GLM",
    "section": "33.2 Mass-univariate analysis",
    "text": "33.2 Mass-univariate analysis\nThere are many cases where we have a large number of predictors and, along with any other number of tests or models, we may want to regress our outcome of interest on each covariate, one at a time.\nLet’s create some synthetic data with 1000 cases and 100 covariates\nThe outcome is generated using just 4 of those 100 covariates and has added noise.\n\nset.seed(2020)\nn_col <- 100\nn_row <- 1000\nx <- as.data.frame(lapply(seq(n_col), function(i) rnorm(n_row)),\n                   col.names = paste0(\"Feature_\", seq(n_col)))\ndim(x)\n\n[1] 1000  100\n\ny <- .7 + x[, 10] + .3 * x[, 20] + 1.3 * x[, 30] + x[, 50] + rnorm(500)\n\nLet’s fit a linear model regressing y on each column of x using lm:\n\nmod.xy.massuni <- lapply(seq(x), function(i) lm(y ~ x[, i]))\nlength(mod.xy.massuni)\n\n[1] 100\n\nnames(mod.xy.massuni) <- paste0(\"mod\", seq(x))\n\nTo extract p-values for each model, we must find where exactly to look.\nLet’s look into the first model:\n\n(ms1 <- summary(mod.xy.massuni$mod1))\n\n\nCall:\nlm(formula = y ~ x[, i])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.5402 -1.4881 -0.0618  1.4968  5.8152 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.61800    0.06878   8.985   <2e-16 ***\nx[, i]       0.08346    0.06634   1.258    0.209    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.174 on 998 degrees of freedom\nMultiple R-squared:  0.001584,  Adjusted R-squared:  0.0005831 \nF-statistic: 1.583 on 1 and 998 DF,  p-value: 0.2086\n\nms1$coefficients\n\n              Estimate Std. Error  t value     Pr(>|t|)\n(Intercept) 0.61800326 0.06878142 8.985032 1.266204e-18\nx[, i]      0.08346393 0.06634074 1.258110 2.086464e-01\n\n\nThe p-values for each feature is stored in row 1, column 4 fo the coefficients matrix. Let’s extract all of them:\n\nmod.xy.massuni.pvals <- sapply(mod.xy.massuni, function(i) summary(i)$coefficients[2, 4])\n\nLet’s see which variable are significant at the 0.05:\n\nwhich(mod.xy.massuni.pvals < .05)\n\n mod5 mod10 mod12 mod20 mod28 mod30 mod42 mod50 mod61 mod65 mod72 mod82 mod85 \n    5    10    12    20    28    30    42    50    61    65    72    82    85 \nmod91 mod94 mod99 \n   91    94    99 \n\n\n…and which are significant at the 0.01 level:\n\nwhich(mod.xy.massuni.pvals < .01)\n\nmod10 mod20 mod28 mod30 mod50 \n   10    20    28    30    50"
  },
  {
    "objectID": "GLM.html#correction-for-multiple-comparisons",
    "href": "GLM.html#correction-for-multiple-comparisons",
    "title": "33  Introduction to the GLM",
    "section": "33.3 Correction for multiple comparisons",
    "text": "33.3 Correction for multiple comparisons\nWe’ve performed a large number of tests and before reporting the results, we need to control for multiple comparisons.\nTo do that, we use R’s p.adjust() function. It adjusts a vector of p-values to account for multiple comparisons using one of multiple methods. The default, and recommended, is the Holm method. It ensures that FWER < α, i.e. controls the family-wise error rate, a.k.a. the probability of making one or more false discoveries (Type I errors)\n\nmod.xy.massuni.pvals.holm_adjusted <- p.adjust(mod.xy.massuni.pvals)\n\nNow, let’s see which features’ p-values survive the magical .05 threshold:\n\nwhich(mod.xy.massuni.pvals.holm_adjusted < .05)\n\nmod10 mod20 mod30 mod50 \n   10    20    30    50 \n\n\nThese are indeed the correct features (not surprisingly, still reassuringly)."
  },
  {
    "objectID": "GLM.html#glmresources",
    "href": "GLM.html#glmresources",
    "title": "33  Introduction to the GLM",
    "section": "33.4 Resources",
    "text": "33.4 Resources\n\nRegression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models"
  },
  {
    "objectID": "Resampling.html",
    "href": "Resampling.html",
    "title": "34  Resampling",
    "section": "",
    "text": "Resampling refers to a collection of techniques for selecting cases from a sample. It is central to many machine learning algorithms and pipelines. The two core uses of resampling in predictive modeling / machinbe learning are model selection (a.k.a. tuning) and model assessment. By convention, we use the terms training and validation set when refering to model selection, and training and testing set when refering to model assessment. The terminology is unfortunately not intuitive and has led to much confusion. Some people reverse the terms, but we use the terms training, validation, and testing as they are used in the Elements of Statistical Learning (p. 222, Second edition, 12th printing)."
  },
  {
    "objectID": "Resampling.html#model-selection-and-assessment",
    "href": "Resampling.html#model-selection-and-assessment",
    "title": "34  Resampling",
    "section": "\n34.1 Model Selection and Assessment",
    "text": "34.1 Model Selection and Assessment\n\n\nModel Selection aka Hyperparameter tuning\n\n\nResamples of the training set are drawn creating multiple training and validation sets. For each resample, a combination of hyperparameters is used to train a model. The mean validation-set error across resamples is calculated. The combination of hyperparameters with the minimum loss on average across validation-set resamples is selected to train the full training sample.\n\nModel assessment\n\nResamples of the full sample are drawn, resulting into multiple training - testing sets. A model is trained on each training set and its performance assessed on the corresponding test set. Model performance is averaged across all test sets.\nNested resampling or nested crossvalidation is the procedure where 1. and 2. are nested so that hyperparameter tuning (resampling of the training set) is performed within each of multiple training resamples and performance is tested in each corresponding test set. [elevate] performs automatic nested resampling and is one of the core supervised learning functions in rtemis.\n\n\n\n\n10-fold crossvalidation"
  },
  {
    "objectID": "Resampling.html#the-resample-function",
    "href": "Resampling.html#the-resample-function",
    "title": "34  Resampling",
    "section": "\n34.2 The resample function",
    "text": "34.2 The resample function\nThe resample() function is responsible for all resampling in rtemis.\n\nx <- rnorm(500)\nres <- resample(x)\n.:Resampling Parameters\n    n.resamples: 10 \n      resampler: strat.sub \n   stratify.var: y \n        train.p: 0.75 \n   strat.n.bins: 4 \n[2022-07-19 03:02:39 resample] Created 10 stratified subsamples \n\nclass(res)\n\n[1] \"resample\" \"list\"    \n\n\nIt outputs a list which is an S3 object of class resample, with print and plot methods.\n\nres\n.:rtemis resample object \n              N: 10 \n           type: strat.sub \n        train.p: 0.75 \n   strat.n.bins: 4 \n\nplot(res)\n\n\n\n\nThe teal-colored lines represent the training cases selected for each resample, the white are testing cases (held out).\nresample() supports 5 types of resampling:\n\nk-fold crossvalidation (Stratified)\n\nYou split the cases into k sets (folds). Each set is used once as the validation or testing set. This means each cases is left out exactly once and there is no overlap between different validation/test sets. In rtemis, the folds are also stratified by default on the outcome unless otherwise chosen. Stratification tries to maintain the full sample’s distribution in both training and left-out sets. This is crucial for non-normally distributed continuous outcomes or imbalanced datasets. 10 is a common value for k, called 10-fold. Note that the size of the training and left-out sets depends on the sample size.\n\nres.10fold <- resample(x, 10, \"kfold\")\n.:Resampling Parameters\n    n.resamples: 10 \n      resampler: kfold \n   stratify.var: y \n   strat.n.bins: 4 \n[2022-07-19 03:02:39 resample] Created 10 independent folds \n\n\n\nStratified subsampling\n\nDraw n.resamples stratified samples from the data given a certain probability (train.p) that each case belongs to the training set. Since you are randomly sampling from the full sample each time, there will be overlap in the test set cases, but you control the training-to-testing ratio and number of resamples independently, unlike in k-fold resampling.\n\nres.25ss <- resample(x, 25, \"strat.sub\")\n.:Resampling Parameters\n    n.resamples: 25 \n      resampler: strat.sub \n   stratify.var: y \n        train.p: 0.75 \n   strat.n.bins: 4 \n[2022-07-19 03:02:39 resample] Created 25 stratified subsamples \n\n\n\nBootstrap\n\nThe bootstrap: random sampling with replacement. Since cases are replicated, you should use bootstrap as the outer resampler if you will also have inner resampling for tuning, since the same case may end up in both training and validation sets.\n\nres.100boot <- resample(x, 100, \"bootstrap\")\n.:Resampling Parameters\n   n.resamples: 100 \n     resampler: bootstrap \n[2022-07-19 03:02:39 resample] Created 100 bootstrap resamples \n\n\n\nStratified Bootstrap\n\nThis is stratified subsampling with random replication of cases to match the length of the original sample. Same as the bootstrap, do not use if you will be further resampling each resample.\n\nres.100sboot <- resample(x, 100, \"strat.boot\")\n.:Resampling Parameters\n     n.resamples: 100 \n       resampler: strat.boot \n    stratify.var: y \n         train.p: 0.75 \n    strat.n.bins: 4 \n   target.length: 500 \n[2022-07-19 03:02:39 resample] Created 100 stratified bootstraps \n\n\n\nLeave-One-Out-Crossvalidation (LOOCV)\n\nThis is k-fold crossvalidation where \\(k = N\\), where \\(N\\) is number of data points/cases in the whole sample. It has been included for experimentation and completenes, but it is not recommended either for model selection or assessment over the other resampling methods.\n\nres.loocv <- resample(x, resampler = \"loocv\")\n.:Resampling Parameters\n   n.resamples: 500 \n     resampler: loocv \n[2022-07-19 03:02:39 resample] Created 500 independent folds (LOOCV)"
  },
  {
    "objectID": "Resampling.html#example-stratified-vs-random-sampling-in-a-binomial-distribution",
    "href": "Resampling.html#example-stratified-vs-random-sampling-in-a-binomial-distribution",
    "title": "34  Resampling",
    "section": "\n34.3 Example: Stratified vs random sampling in a binomial distribution",
    "text": "34.3 Example: Stratified vs random sampling in a binomial distribution\nImagine y is the outcome of interest where events occur with a probability of .1 - a common scenario in many fields.\n\nset.seed(2020)\nx <- rbinom(100, 1, .1)\nmplot3_x(x)\n\n\n\nfreq <- table(x)\nprob <- freq[2] / sum(freq)\nres.nonstrat <- lapply(seq(10), function(i) sample(seq(x), .75*length(x)))\nres.strat <- resample(x)\n.:Resampling Parameters\n    n.resamples: 10 \n      resampler: strat.sub \n   stratify.var: y \n        train.p: 0.75 \n   strat.n.bins: 4 \n[2022-07-19 03:02:40 strat.sub] Using max n bins possible = 2 \n[2022-07-19 03:02:40 resample] Created 10 stratified subsamples \n\nprob.nonstrat <- sapply(seq(10), function(i) {\n  freq <- table(x[res.nonstrat[[i]]])\n  freq[2]/sum(freq)\n})\nprob.strat <- sapply(seq(10), function(i) {\n  freq <- table(x[res.strat[[i]]])\n  freq[2]/sum(freq)\n})\nprob.nonstrat\n\n         1          1          1          1          1          1          1 \n0.09333333 0.08000000 0.08000000 0.06666667 0.06666667 0.10666667 0.10666667 \n         1          1          1 \n0.10666667 0.09333333 0.08000000 \n\nsd(prob.nonstrat)\n\n[1] 0.0156505\n\nprob.strat\n\n         1          1          1          1          1          1          1 \n0.08108108 0.08108108 0.08108108 0.08108108 0.08108108 0.08108108 0.08108108 \n         1          1          1 \n0.08108108 0.08108108 0.08108108 \n\nsd(prob.strat)\n\n[1] 0\n\n\nAs expected, the random sampling resulted in different event probability in each resample, whereas stratified subsampling maintained a constant probability across resamples."
  },
  {
    "objectID": "Supervised.html",
    "href": "Supervised.html",
    "title": "35  Supervised Learning",
    "section": "",
    "text": "This is a very brief introduction to machine learning using the rtemis package. rtemis includes functions for:"
  },
  {
    "objectID": "Supervised.html#installation",
    "href": "Supervised.html#installation",
    "title": "35  Supervised Learning",
    "section": "35.1 Installation",
    "text": "35.1 Installation\ninstall the remotes package, if you don’t have it:\n\ninstall.packages(\"remotes\")\n\nInstall rtemis:\n\nremotes::install_github(\"egenn/rtemis\")\n\nrtemis uses a large number of packages under the hood. Since you would not need to use all of them, they are not installed by default. Each time an rtemis function is called, a dependency check is run and a message is printed if any packages need to be installed.\nFor this short tutorial, start by installing ranger, if it is not already installed:\n\ninstall.packages(\"ranger\")\n\nLoad rtemis:\n\nlibrary(rtemis)\n\n  .:rtemis 0.91.1 🌊 aarch64-apple-darwin20 (64-bit)\n  Defaults\n  |   Theme: whitegrid\n  |    Font: Fira Sans\n  | Palette: rtCol1\n  |    Plan: multicore\n  |   Cores: 8/10 available\n  Resources\n  | Documentation: https://rtemis.lambdamd.org\n  |       Learn R: https://class.lambdamd.org/pdsr\n  | rtemis themes: https://egenn.lambdamd.org/software/#rtemis_themes\n  |          Cite: `citation(\"rtemis\")`\n  Setup\n  | Enable progress reporting: `progressr::handlers(global = TRUE)`"
  },
  {
    "objectID": "Supervised.html#data-input-for-supervised-learning",
    "href": "Supervised.html#data-input-for-supervised-learning",
    "title": "35  Supervised Learning",
    "section": "35.2 Data Input for Supervised Learning",
    "text": "35.2 Data Input for Supervised Learning\nAll rtemis supervised learning functions begin with s. (“supervised”).\nThey accept the same first four arguments:\nx, y, x.test, y.test\nbut are flexible and allow you to also provide combined (x, y) and (x.test, y.test) data frames, as explained below.\n\n35.2.1 Scenario 1 (x.train, y.train, x.test, y.test)\nIn the most straightforward case, provide each featureset and outcome individually:\n\nx: Training set features\ny: Training set outcome\nx.test: Testing set features (Optional)\ny.test: Testing set outcome (Optional)\n\n\nx <- rnormmat(200, 10, seed = 2019)\nw <- rnorm(10)\ny <- x %*% w + rnorm(200)\nres <- resample(y, seed = 2020)\n.:Resampling Parameters\n    n.resamples: 10 \n      resampler: strat.sub \n   stratify.var: y \n        train.p: 0.75 \n   strat.n.bins: 4 \n[2022-07-19 03:02:41 resample] Created 10 stratified subsamples \n\nx.train <- x[res$Subsample_1, ]\nx.test <- x[-res$Subsample_1, ]\ny.train <- y[res$Subsample_1]\ny.test <- y[-res$Subsample_1]\n\n\nmod_glm <- s_GLM(x.train, y.train, x.test, y.test)\n[2022-07-19 03:02:41 s_GLM] Hello, egenn \n\n.:Regression Input Summary\nTraining features: 147 x 10 \n Training outcome: 147 x 1 \n Testing features: 53 x 10 \n  Testing outcome: 53 x 1 \n\n[2022-07-19 03:02:41 s_GLM] Training GLM... \n\n.:GLM Regression Training Summary\n    MSE = 0.84 (91.88%)\n   RMSE = 0.92 (71.51%)\n    MAE = 0.75 (69.80%)\n      r = 0.96 (p = 5.9e-81)\n   R sq = 0.92\n\n.:GLM Regression Testing Summary\n    MSE = 1.22 (89.03%)\n   RMSE = 1.10 (66.88%)\n    MAE = 0.90 (66.66%)\n      r = 0.94 (p = 2.5e-26)\n   R sq = 0.89\n\n\n\n\n[2022-07-19 03:02:43 s_GLM] Run completed in 0.03 minutes (Real: 1.64; User: 0.40; System: 0.02) \n\n\n\n\n35.2.2 Scenario 2: (x.train, x.test)\nYou can provide training and testing sets as a single data.frame each, where the last column is the outcome. Now x is the full training data and y the full testing data:\n\nx: data.frame(x.train, y.train)\ny: data.frame(x.test, y.test)\n\n\nx <- rnormmat(200, 10, seed = 2019)\nw <- rnorm(10)\ny <- x %*% w + rnorm(200)\ndat <- data.frame(x, y)\nres <- resample(dat, seed = 2020)\n[2022-07-19 03:02:43 resample] Input contains more than one columns; will stratify on last \n.:Resampling Parameters\n    n.resamples: 10 \n      resampler: strat.sub \n   stratify.var: y \n        train.p: 0.75 \n   strat.n.bins: 4 \n[2022-07-19 03:02:43 resample] Created 10 stratified subsamples \n\ndat_train <- dat[res$Subsample_1, ]\ndat_test <- dat[-res$Subsample_1, ]\n\n\nmod_glm <- s_GLM(dat_train, dat_test)\n[2022-07-19 03:02:43 s_GLM] Hello, egenn \n\n.:Regression Input Summary\nTraining features: 147 x 10 \n Training outcome: 147 x 1 \n Testing features: 53 x 10 \n  Testing outcome: 53 x 1 \n\n[2022-07-19 03:02:43 s_GLM] Training GLM... \n\n.:GLM Regression Training Summary\n    MSE = 0.84 (91.88%)\n   RMSE = 0.92 (71.51%)\n    MAE = 0.75 (69.80%)\n      r = 0.96 (p = 5.9e-81)\n   R sq = 0.92\n\n.:GLM Regression Testing Summary\n    MSE = 1.22 (89.03%)\n   RMSE = 1.10 (66.88%)\n    MAE = 0.90 (66.66%)\n      r = 0.94 (p = 2.5e-26)\n   R sq = 0.89\n\n\n\n\n[2022-07-19 03:02:43 s_GLM] Run completed in 4.8e-04 minutes (Real: 0.03; User: 0.02; System: 0.01) \n\n\nThe dataPrepare() function will check data dimensions and determine whether data was input as separate feature and outcome sets or combined and ensure the correct number of cases and features was provided.\nIn either scenario, Regression will be performed if the outcome is numeric and Classification if the outcome is a factor."
  },
  {
    "objectID": "Supervised.html#regression",
    "href": "Supervised.html#regression",
    "title": "35  Supervised Learning",
    "section": "35.3 Regression",
    "text": "35.3 Regression\n\n35.3.1 Check Data with checkData()\n\nx <- rnormmat(500, 50, seed = 2019)\nw <- rnorm(50)\ny <- x %*% w + rnorm(500)\ndat <- data.frame(x, y)\nres <- resample(dat)\n[2022-07-19 03:02:43 resample] Input contains more than one columns; will stratify on last \n.:Resampling Parameters\n    n.resamples: 10 \n      resampler: strat.sub \n   stratify.var: y \n        train.p: 0.75 \n   strat.n.bins: 4 \n[2022-07-19 03:02:43 resample] Created 10 stratified subsamples \n\ndat_train <- dat[res$Subsample_1, ]\ndat_test <- dat[-res$Subsample_1, ]\n\n\ncheckData(x)\n x: A matrix with 500 rows and 50 features\n\n  Data types________________\n  * 50 continuous features\n  * 0 integer features\n  * 0 categorical features\n  * 0 character features\n  * 0 date features\n\n  Issues____________________\n  * 0 constant features\n  * 0 duplicated cases\n  * 0 features include 'NA' values\n\n  Recommendations___________\n  * Everything looks good\n\n\n\n\n35.3.2 Single Model\n\nmod <- s_GLM(dat_train, dat_test)\n[2022-07-19 03:02:43 s_GLM] Hello, egenn \n\n.:Regression Input Summary\nTraining features: 374 x 50 \n Training outcome: 374 x 1 \n Testing features: 126 x 50 \n  Testing outcome: 126 x 1 \n\n[2022-07-19 03:02:43 s_GLM] Training GLM... \n\n.:GLM Regression Training Summary\n    MSE = 1.02 (97.81%)\n   RMSE = 1.01 (85.18%)\n    MAE = 0.81 (84.62%)\n      r = 0.99 (p = 1.3e-310)\n   R sq = 0.98\n\n.:GLM Regression Testing Summary\n    MSE = 0.98 (97.85%)\n   RMSE = 0.99 (85.35%)\n    MAE = 0.76 (85.57%)\n      r = 0.99 (p = 2.7e-105)\n   R sq = 0.98\n\n\n\n\n[2022-07-19 03:02:43 s_GLM] Run completed in 7.3e-04 minutes (Real: 0.04; User: 0.03; System: 0.01) \n\n\n\n\n35.3.3 Crossvalidated Model\n\nmod <- elevate(dat, mod = \"glm\")\n[2022-07-19 03:02:43 elevate] Hello, egenn \n\n.:Regression Input Summary\nTraining features: 500 x 50 \n Training outcome: 500 x 1 \n[2022-07-19 03:02:43 elevate] Using future framework \n[2022-07-19 03:02:43 resLearn] Outer resampling plan set to sequential \n[2022-07-19 03:02:43 resLearn] Training Generalized Linear Model on 10 stratified subsamples... \n\n.:elevate GLM\nN repeats = 1 \nN resamples = 10 \nResampler = strat.sub \nMean MSE of 10 resamples in each repeat = 1.22\nMean MSE reduction in each repeat = 97.50%\n\n\n\n\n\n[2022-07-19 03:02:43 elevate] Run completed in 0.01 minutes (Real: 0.41; User: 0.33; System: 0.02) \n\n\nUse the describe() function to get a summary in (plain) English:\n\nmod$describe()\n\nRegression was performed using Generalized Linear Model. Model generalizability was assessed using 10 stratified subsamples. The mean R-squared across all testing set resamples was 0.97.\n\n\n\nmod$plot()"
  },
  {
    "objectID": "Supervised.html#classification",
    "href": "Supervised.html#classification",
    "title": "35  Supervised Learning",
    "section": "35.4 Classification",
    "text": "35.4 Classification\n\n35.4.1 Check Data\n\ndata(Sonar, package = 'mlbench')\ncheckData(Sonar)\n Sonar: A data.frame with 208 rows and 61 features\n\n  Data types________________\n  * 60 continuous features\n  * 0 integer features\n  * 1 categorical feature, which is not ordered\n  * 0 character features\n  * 0 date features\n\n  Issues____________________\n  * 0 constant features\n  * 0 duplicated cases\n  * 0 features include 'NA' values\n\n  Recommendations___________\n  * Everything looks good\n\nres <- resample(Sonar)\n[2022-07-19 03:02:44 resample] Input contains more than one columns; will stratify on last \n.:Resampling Parameters\n    n.resamples: 10 \n      resampler: strat.sub \n   stratify.var: y \n        train.p: 0.75 \n   strat.n.bins: 4 \n[2022-07-19 03:02:44 strat.sub] Using max n bins possible = 2 \n[2022-07-19 03:02:44 resample] Created 10 stratified subsamples \n\nsonar_train <- Sonar[res$Subsample_1, ]\nsonar_test <- Sonar[-res$Subsample_1, ]\n\n\n\n35.4.2 Single model\n\nmod <- s_RANGER(sonar_train, sonar_test)\n[2022-07-19 03:02:44 s_RANGER] Hello, egenn \n\n[2022-07-19 03:02:44 dataPrepare] Imbalanced classes: using Inverse Probability Weighting \n\n.:Classification Input Summary\nTraining features: 155 x 60 \n Training outcome: 155 x 1 \n Testing features: 53 x 60 \n  Testing outcome: 53 x 1 \n\n.:Parameters\n   n.trees: 1000 \n      mtry: NULL \n\n[2022-07-19 03:02:44 s_RANGER] Training Random Forest (ranger) Classification with 1000 trees... \n\n.:RANGER Classification Training Summary\n                   Reference \n        Estimated  M   R   \n                M  83   0\n                R   0  72\n\n                   Overall  \n      Sensitivity  1      \n      Specificity  1      \nBalanced Accuracy  1      \n              PPV  1      \n              NPV  1      \n               F1  1      \n         Accuracy  1      \n              AUC  1      \n\n  Positive Class:  M \n\n.:RANGER Classification Testing Summary\n                   Reference \n        Estimated  M   R   \n                M  25  11\n                R   3  14\n\n                   Overall  \n      Sensitivity  0.8929 \n      Specificity  0.5600 \nBalanced Accuracy  0.7264 \n              PPV  0.6944 \n              NPV  0.8235 \n               F1  0.7812 \n         Accuracy  0.7358 \n              AUC  0.8643 \n\n  Positive Class:  M \n\n\n\n\n[2022-07-19 03:02:44 s_RANGER] Run completed in 2e-03 minutes (Real: 0.12; User: 0.19; System: 0.05) \n\n\n\n\n35.4.3 Crossvalidated Model\n\nmod <- elevate(Sonar)\n[2022-07-19 03:02:44 elevate] Hello, egenn \n\n.:Classification Input Summary\nTraining features: 208 x 60 \n Training outcome: 208 x 1 \n[2022-07-19 03:02:44 elevate] Using future framework \n[2022-07-19 03:02:44 resLearn] Outer resampling plan set to sequential \n[2022-07-19 03:02:44 resLearn] Training Random Forest (ranger) on 10 stratified subsamples... \n\n.:elevate RANGER\nN repeats = 1 \nN resamples = 10 \nResampler = strat.sub \nMean Balanced Accuracy of 10 test sets in each repeat = 0.83\n\n\n\n\n[2022-07-19 03:02:46 elevate] Run completed in 0.02 minutes (Real: 1.10; User: 2.02; System: 0.41) \n\n\n\nmod$describe()\n\nClassification was performed using Random Forest (ranger). Model generalizability was assessed using 10 stratified subsamples. The mean Balanced Accuracy across all testing set resamples was 0.83.\n\n\n\nmod$plot()\n\n\n\n\n\nmod$plotROC()\n\n\n\n\n\nmod$plotPR()\n\n\n\n\n\n\n35.4.4 Evaluation of a binary classifier\n\n\n\n\n\nEvaluation metrics for a binary classifier"
  },
  {
    "objectID": "Supervised.html#understanding-overfitting",
    "href": "Supervised.html#understanding-overfitting",
    "title": "35  Supervised Learning",
    "section": "35.5 Understanding Overfitting",
    "text": "35.5 Understanding Overfitting\nOverfitting occurs when a model fits noise in the outcome. To make this clear, consider the following example:\nAssume a random variable x:\n\nset.seed(2020)\nx <- sort(rnorm(500))\n\nand a data-generating function fn():\n\nfn <- function(x) 12 + x^5\n\nThe true y is therefore equal to fn(x):\n\ny_true <- fn(x)\n\nHowever, assume y is recorded with some noise, in this case gaussian:\n\ny_noise <- fn(x) + rnorm(500, sd = sd(y_true))\n\nWe plot:\n\nmplot3_xy(x, list(y_noise = y_noise, y_true = y_true))\n\n\n\n\nWe want to find a model that best approximates y_true, but we only know y_noise.\nA maximally overfitted model would model noise perfectly:\n\nmplot3_xy(x, list(Overfitted_model = y_noise, Ideal_model = y_true), type = \"l\")\n\n\n\n\nAn example of an SVM set up to overfit heavily:\n\nmplot3_xy(x, y_noise, fit = \"svm\",\n          fit.params = list(kernel = \"radial\", cost = 100, gamma = 100))\n\n\n\n\nAn example of a good approximation of fn using a GAM with penalized splines:\n\nmplot3_xy(x, y_noise, fit = \"gam\")"
  },
  {
    "objectID": "Supervised.html#rtemis-documentation",
    "href": "Supervised.html#rtemis-documentation",
    "title": "35  Supervised Learning",
    "section": "35.6 rtemis Documentation",
    "text": "35.6 rtemis Documentation\nFor more information on using rtemis, see the rtemis online documentation and vignettes"
  },
  {
    "objectID": "Unsupervised.html",
    "href": "Unsupervised.html",
    "title": "36  Unsupervised Learning",
    "section": "",
    "text": "Unsupervised learning aims to learn relationships within a dataset without focusing at a particular outcome. You will often hear of unsupervised learning being performed on unlabeled data. To be clear, it means it does not use the labels to guide learning - whether labels are available or not. You might, for example, perform unsupervised learning ahead of supervised learning as we shall see later. Unsupervised learning includes a number of approaches, most of which can be divided into two categories:\nIn rtemis, clustering algorithms begin with c_ and decomposition/dimensionality reduction algorithms begin with d_"
  },
  {
    "objectID": "Unsupervised.html#decomposition",
    "href": "Unsupervised.html#decomposition",
    "title": "36  Unsupervised Learning",
    "section": "36.1 Decomposition / Dimensionality Reduction",
    "text": "36.1 Decomposition / Dimensionality Reduction\nUse decomSelect() to get a listing of available decomposition algorithms:\n\ndecomSelect()\n\n.:decomSelect\nrtemis supports the following decomposition algorithms:\n\n    Name                                   Description\n     CUR                      CUR Matrix Approximation\n   H2OAE                               H2O Autoencoder\n H2OGLRM                H2O Generalized Low-Rank Model\n     ICA                Independent Component Analysis\n  ISOMAP                                        ISOMAP\n    KPCA           Kernel Principal Component Analysis\n     LLE                      Locally Linear Embedding\n     MDS                      Multidimensional Scaling\n     NMF             Non-negative Matrix Factorization\n     PCA                  Principal Component Analysis\n    SPCA           Sparse Principal Component Analysis\n     SVD                  Singular Value Decomposition\n    TSNE   t-distributed Stochastic Neighbor Embedding\n    UMAP Uniform Manifold Approximation and Projection\n\n\nWe can further divide decomposition algorithms into linear (e.g. PCA, ICA, NMF) and nonlinear dimensionality reduction, (also called manifold learning, like LLE and tSNE).\n\n36.1.1 Principal Component Analysic (PCA)\n\nx <- iris[, 1:4]\niris_PCA <- d_PCA(x)\n[2022-07-19 03:02:48 d_PCA] Hello, egenn \n[2022-07-19 03:02:48 d_PCA] ||| Input has dimensions 150 rows by 4 columns, \n[2022-07-19 03:02:48 d_PCA]     interpreted as 150 cases with 4 features. \n[2022-07-19 03:02:48 d_PCA] Performing Principal Component Analysis... \n[2022-07-19 03:02:48 d_PCA] Run completed in 3e-04 minutes (Real: 0.02; User: 4e-03; System: 0.00) \n\nmplot3_xy(iris_PCA$projections.train[, 1], \n          iris_PCA$projections.train[, 2], \n          group = iris$Species,\n          xlab = \"1st PCA component\", \n          ylab = \"2nd PCA component\", \n          main = \"PCA on iris\")\n\n\n\n\n\n\n36.1.2 Independent Component Analysis (ICA)\n\niris_ICA <- d_ICA(x, k = 2)\n[2022-07-19 03:02:48 d_ICA] Hello, egenn \n[2022-07-19 03:02:48 d_ICA] ||| Input has dimensions 150 rows by 4 columns, \n[2022-07-19 03:02:48 d_ICA]     interpreted as 150 cases with 4 features. \n[2022-07-19 03:02:48 d_ICA] Running Independent Component Analysis... \n[2022-07-19 03:02:48 d_ICA] Run completed in 3e-04 minutes (Real: 0.02; User: 3e-03; System: 1e-03) \n\nmplot3_xy(iris_ICA$projections.train[, 1], \n          iris_ICA$projections.train[, 2], \n          group = iris$Species,\n          xlab = \"1st ICA component\", \n          ylab = \"2nd ICA component\", \n          main = \"ICA on iris\")\n\n\n\n\n\n\n36.1.3 Non-negative Matrix Factorization (NMF)\n\niris_NMF <- d_NMF(x, k = 2)\n[2022-07-19 03:02:48 d_NMF] Hello, egenn \n[2022-07-19 03:02:49 d_NMF] ||| Input has dimensions 150 rows by 4 columns, \n[2022-07-19 03:02:49 d_NMF]     interpreted as 150 cases with 4 features. \n[2022-07-19 03:02:49 d_NMF] Running Non-negative Matrix Factorization... \n[2022-07-19 03:02:50 d_NMF] Run completed in 0.02 minutes (Real: 1.32; User: 0.98; System: 0.04) \n\nmplot3_xy(iris_NMF$projections.train[, 1], \n          iris_NMF$projections.train[, 2], \n          group = iris$Species,\n          xlab = \"1st NMF component\", \n          ylab = \"2nd NMF component\", \n          main = \"NMF on iris\")"
  },
  {
    "objectID": "Unsupervised.html#clustering",
    "href": "Unsupervised.html#clustering",
    "title": "36  Unsupervised Learning",
    "section": "36.2 Clustering",
    "text": "36.2 Clustering\nUse clustSelect() to get a listing of available clustering algorithms:\n\nclustSelect()\n\n.:clustSelect\nrtemis supports the following clustering algorithms:\n\n      Name                                                 Description\n    CMEANS                                    Fuzzy C-means Clustering\n    DBSCAN Density-based spatial clustering of applications with noise\n       EMC                         Expectation Maximization Clustering\n    HARDCL                                   Hard Competitive Learning\n    HOPACH     Hierarchical Ordered Partitioning And Collapsing Hybrid\n H2OKMEANS                                      H2O K-Means Clustering\n    KMEANS                                          K-Means Clustering\n MEANSHIFT                                       Mean Shift Clustering\n      NGAS                                       Neural Gas Clustering\n       PAM                                 Partitioning Around Medoids\n      PAMK               Partitioning Around Medoids with k estimation\n      SPEC                                         Spectral Clustering\n\n\nLet’s cluster iris and we shall also use an NMF decomposition as we saw above to project to 2 dimensions.\nWe’ll use two of the most popular clustering algorithms, K-means and PAM, aka K-medoids.\n\nx <- iris[, 1:4]\niris_NMF <- d_NMF(x, k = 2)\n[2022-07-19 03:02:50 d_NMF] Hello, egenn \n[2022-07-19 03:02:50 d_NMF] ||| Input has dimensions 150 rows by 4 columns, \n[2022-07-19 03:02:50 d_NMF]     interpreted as 150 cases with 4 features. \n[2022-07-19 03:02:50 d_NMF] Running Non-negative Matrix Factorization... \n[2022-07-19 03:02:50 d_NMF] Run completed in 4.3e-03 minutes (Real: 0.26; User: 0.25; System: 0.01) \n\n\n\n36.2.1 K-Means\n\niris.KMEANS <- c_KMEANS(x, k = 3)\n[2022-07-19 03:02:50 c_KMEANS] Hello, egenn \n[2022-07-19 03:02:50 c_KMEANS] Performing K-means Clustering with k = 3... \n[2022-07-19 03:02:50 c_KMEANS] Run completed in 1.7e-03 minutes (Real: 0.10; User: 0.09; System: 0.01) \n\nmplot3_xy(iris_NMF$projections.train[, 1], iris_NMF$projections.train[, 2],\n          group = iris.KMEANS$clusters.train,\n          xlab = \"1st NMF component\", \n          ylab = \"2nd NMF component\", \n          main = \"KMEANS on iris\")\n\n\n\n\n\n\n36.2.2 Partitioning Around Medoids with k estimation (PAMK)\n\niris_PAMK <- c_PAMK(x, krange = 3:10)\n[2022-07-19 03:02:50 c_PAMK] Hello, egenn \n[2022-07-19 03:02:51 c_PAMK] Partitioning Around Medoids... \n[2022-07-19 03:02:51 c_PAMK] Estimated optimal number of clusters: 3 \n[2022-07-19 03:02:51 c_PAMK] Run completed in 0.01 minutes (Real: 0.74; User: 0.41; System: 0.02) \n\nmplot3_xy(iris_NMF$projections.train[, 1], iris_NMF$projections.train[, 2],\n          group = iris_PAMK$clusters.train,\n          xlab = \"1st NMF component\", \n          ylab = \"2nd NMF component\", \n          main = \"PAMK on iris\")"
  },
  {
    "objectID": "GitHubIntro.html",
    "href": "GitHubIntro.html",
    "title": "37  Git & GitHub: the basics",
    "section": "",
    "text": "git is famously powerful and notoriously complex. This is a very brief introduction to a very small subset of git’s functionality. Multiple online resources can help you delve into git in considerably more depth.\nFirst, some important definitions:"
  },
  {
    "objectID": "GitHubIntro.html#installing-git",
    "href": "GitHubIntro.html#installing-git",
    "title": "37  Git & GitHub: the basics",
    "section": "37.1 Installing git",
    "text": "37.1 Installing git\nCheck if you system already includes an installation of git. If not you can download it from the official git website"
  },
  {
    "objectID": "GitHubIntro.html#basic-git-usage",
    "href": "GitHubIntro.html#basic-git-usage",
    "title": "37  Git & GitHub: the basics",
    "section": "37.2 Basic git usage",
    "text": "37.2 Basic git usage\nIn the system terminal, all git commanda begin with git and are followed by a command name:\n\n37.2.1 Cloning (“Downloading”)\nDownload a repository to your computer for the first time. Replace “user” with the username and “repo” with the repository name.\n\ngit clone https://github.com/user/repo.git\n\nThis will clone the remote repository to a folder name ‘repo’. You can optionally provide a different folder name after the URL.\nTo update a previously cloned repository:\n\ngit pull\n\n\n\n37.2.2 Pushing (“Uploading”)\nGet info on local changes to repository:\n\ngit status\n\nWorking locally, stage new or modified files:\n\ngit add /path/to/file\n\nStill working locally, commit changes with an informative message:\n\ngit commit -m Fixed this or added that\n\n(Note that the previous steps did not require an internet connection - this one does) Push one or multiple commits to remote repository:\n\ngit push\n\n\n\n37.2.3 Collaborating\nThe main way of contributing to a project is by a) making a new “branch” of the repository, b) making your edits, and c) either merging to master yourself or requesting your edits be merged by the owner/s of the repository. This allows multiple people to work on the codebase without getting in each other’s way.\n\n\n37.2.4 Branching and merging\nScenario: you are working on your own project, hosted on its own repository. You want to develop a new feature, which may take some time to code and test before you make it part of your official project code.\n* Create a new branch, e.g. devel * Work in your new branch until all testing is successful * Merge back to master branch\nAlways from your system terminal, from within a directory in your repository: Create a new branch:\n\ngit branch devel\n\nSwitch to your new branch:\n\ngit checkout devel\n\nWork on your code, using git add/commit/push as per usual.\nWhen you are done testing and are happy to merge back to master:\n\ngit checkout master\ngit merge devel\ngit push\n\nAll the commits performed while you were working in the devel branch will be included in that last git push from master.\n\n\n37.2.5 Pull request\nScenario: You are contributing to a repository along with other collaborators. You want to suggest a new feature is added to the code:\n\nCreate a new branch, e.g. mynewfeature\nWork in new branch until you are ready happy to share and testing is complete\nGo on to the repository website, select your branch and perform a “Pull request” asking that the changes in your mynewfeature branch are merged into master\nThe repository owner/s will review the request and can merge"
  },
  {
    "objectID": "GitHubIntro.html#gists",
    "href": "GitHubIntro.html#gists",
    "title": "37  Git & GitHub: the basics",
    "section": "37.3 Gists",
    "text": "37.3 Gists\nGitHub also offers a very convenient pastebin-like service called Gist, which lets you quickly and easily share code snippets.\nTo share some R code using a gist:\n\nVist the gist site.\nWrite in/copy-paste some code\nAdd a name including a .R suffix at the top left of the entry box\nCopy-paste the URL to share with others"
  },
  {
    "objectID": "GitHubIntro.html#gitresources",
    "href": "GitHubIntro.html#gitresources",
    "title": "37  Git & GitHub: the basics",
    "section": "37.4 Git Resources",
    "text": "37.4 Git Resources\nGit and GitHub are very powerful and flexible, with a great deal of functionality. Some resources to learn (a great deal) more:\n\nGit cheat sheet\nGitHub guides # Pro Git Book by Scott Chacon and Ben Straub"
  },
  {
    "objectID": "GitHubIntro.html#git-and-github-for-open-and-reproducible-science",
    "href": "GitHubIntro.html#git-and-github-for-open-and-reproducible-science",
    "title": "37  Git & GitHub: the basics",
    "section": "37.5 Git and GitHub for open and reproducible science",
    "text": "37.5 Git and GitHub for open and reproducible science\nIt is recommended to create a new GitHub repository for each new research project. It may be worthwhile creating a new repository when it’s time to publish a paper, to include all final working code that should accompany the publication (and e.g. exclude all trial-and-error, testing, etc. code). As Always, make sure to follow journal requirements for reporting data deposition (includes code) and accessibility."
  },
  {
    "objectID": "GitHubIntro.html#applications-with-builtin-git-support",
    "href": "GitHubIntro.html#applications-with-builtin-git-support",
    "title": "37  Git & GitHub: the basics",
    "section": "37.6 Applications with builtin git support",
    "text": "37.6 Applications with builtin git support\nMany applications support git, and allow you to pull / add / commit / push and more directly from the app using their GUI.\nA couple of interest for the R user:\n\nRStudio Out trusty IDE has a Git panel enabled when a project is in a directory that’s part of a git repository\nThe Atom editor GitHub’s own feature-packed text editor is naturally built around git and GitHub support. It offers its own package manager with access to a large and growing ecosystem of packages. Packages are available that transform Atom to a very capable and customizable IDE."
  },
  {
    "objectID": "TerminalIntro.html",
    "href": "TerminalIntro.html",
    "title": "38  Introduction to the system shell",
    "section": "",
    "text": "This is a very brief introduction to some of the most commonly used shell commands.\nA shell is a command line interface allowing access to an operating system’s services. Multiple different shells exist. The most popular is probably bash, which is the default in most Linux installations. In MacOS, the default shell switched form bash to zsh in 2019 with the release of Catalina. In Windows, various shells are available through the Windows Subsystem for Linux.\nThe commands listed here will work similarly in all/most shells."
  },
  {
    "objectID": "TerminalIntro.html#common-shell-commands",
    "href": "TerminalIntro.html#common-shell-commands",
    "title": "38  Introduction to the system shell",
    "section": "38.1 Common shell commands",
    "text": "38.1 Common shell commands\nThe first thing to look for in a new environment is the help system. In the shell, this is accessed with man:\n\nman: Print the manual pages\n\n\nman man\n\n\npwd: Print working directory (the directory you are currently in)\n\n\npwd\n\n\ncd: Set working directory to /path/to/dir\n\n\ncd /path/to/dir\n\n\nmv: Move file from /current/dir/ to /new/dir\n\n\nmv /current/dir/file /new/dir\n\n\nmv: Rename file to newfilename\n\n\nmv /current/dir/file /current/dir/newfilename\n\n\ncp: Make a copy of file from currentPath into altPath\n\n\ncp /currentPath/file /altPath/file\n\n\nmkdir: Create a new directory named ‘newdir’\n\n\nmkdir /path/to/newdir\n\n\nrmdir: Remove (i.e. delete) uselessFile\n\n\n\n\n\nrm: Remove (i.e. delete) uselessFile\n\n\nrm /path/to/uselessFile\n\n\ncat: Print contents of file to the console\n\n\ncat /path/to/file\n\n\nuname: Get system information\n\n\nuname -a\n\n\nwhoami: When you forget the basics\n\n\nwhoami"
  },
  {
    "objectID": "TerminalIntro.html#running-system-commands-within-r",
    "href": "TerminalIntro.html#running-system-commands-within-r",
    "title": "38  Introduction to the system shell",
    "section": "38.2 Running system commands within R",
    "text": "38.2 Running system commands within R\nYou can execute any system command within R using the system() command:\n\nsystem(\"uname -a\")"
  },
  {
    "objectID": "Resources.html",
    "href": "Resources.html",
    "title": "39  Resources",
    "section": "",
    "text": "The R Manuals include a number of resources, including:\n\nIntroduction to R\nCRAN task views offer curated lists of packages by topic"
  },
  {
    "objectID": "Resources.html#r-markdown",
    "href": "Resources.html#r-markdown",
    "title": "39  Resources",
    "section": "39.2 R markdown",
    "text": "39.2 R markdown\n\nR Markdown: The Definitive Guide by Yihui Xie, J. J. Allaire, Garrett Grolemund\nbookdown: Authoring Books and Technical Documents with R Markdown: how to make websites like this one you are on right now"
  },
  {
    "objectID": "Resources.html#documentation",
    "href": "Resources.html#documentation",
    "title": "39  Resources",
    "section": "39.3 Documentation",
    "text": "39.3 Documentation\n\nDocumentation with roxygen2"
  },
  {
    "objectID": "Resources.html#r-for-data-science",
    "href": "Resources.html#r-for-data-science",
    "title": "39  Resources",
    "section": "39.4 R for data science",
    "text": "39.4 R for data science\n\nR Programming for Data Science by Roger D. Peng, based mostly on base R, and also covers the basics of dplyr.\nR for Data Science by Hadley Wickham & Garrett Grolemund, based on the tidyverse\nData wrangling, exploration, and analysis with R"
  },
  {
    "objectID": "Resources.html#graphics",
    "href": "Resources.html#graphics",
    "title": "39  Resources",
    "section": "39.5 Graphics",
    "text": "39.5 Graphics\n\n39.5.1 ggplot2\n\nggplot2\n\n\n\n39.5.2 Plotly\n\nPlotly R API\nInteractive web-based data visualization with R, plotly, and shiny"
  },
  {
    "objectID": "Resources.html#advanced-r",
    "href": "Resources.html#advanced-r",
    "title": "39  Resources",
    "section": "39.6 Advanced R",
    "text": "39.6 Advanced R\n\nEfficient R Programming by Colin Gillespie & Robin Lovelace\nHigh performance functions with Rcpp"
  },
  {
    "objectID": "Resources.html#git-and-github",
    "href": "Resources.html#git-and-github",
    "title": "39  Resources",
    "section": "39.7 Git and GitHub",
    "text": "39.7 Git and GitHub\n\nGitHub guides\nPro Git Book by Scott Chacon and Ben Straub"
  },
  {
    "objectID": "Resources.html#machine-learning",
    "href": "Resources.html#machine-learning",
    "title": "39  Resources",
    "section": "39.8 Machine Learning",
    "text": "39.8 Machine Learning\n\nAn Introduction to Statistical Learning offers an accessible view of core learning algorithms, without being math-heavy.\nElements of Statistical Learning offers a deeper and more extensive view on learning algorithms.\nMachine Learning with rtemis"
  },
  {
    "objectID": "Resources.html#getting-help",
    "href": "Resources.html#getting-help",
    "title": "39  Resources",
    "section": "39.9 Getting help",
    "text": "39.9 Getting help\nStack Overflow is a massively popular Q&A site for programmers, part of the wider Stack Exchange network. Many R-related web searches will bring up posts in Stack Overflow. You can view all questions tagged with “r”.\nWhen posting a question in any setting, it is strongly recommended to provide a minimal reproducible example (MRE). Stack Overflow provides guidelines on how to create an MRE."
  },
  {
    "objectID": "CrashCourse.html",
    "href": "CrashCourse.html",
    "title": "Crash Course",
    "section": "",
    "text": "This is the online material for the UCSF DCR ‘Intro to R’ selective."
  },
  {
    "objectID": "CrashCourse.html#introduction-format",
    "href": "CrashCourse.html#introduction-format",
    "title": "Crash Course",
    "section": "Introduction & Format",
    "text": "Introduction & Format\nThis is a brief introduction to the R programming language for health data science. It covers basic commands to allow you to read in data, perform common manipulations, plot data and run common tests.  R is a programming language developed specifically for statistical computing and graphics.  It is often mis-characterized as a “statistical package”, similar to SPSS, for example, but as a full programming language it has far more extensive functionality.  For a more thorough coverage of the topic, see the main part of this book.  Links to book chapters will be provided throughout these notes for those interested in reading up further into particular topics.  R can be used to perform most, if not all, operations in statistics, data science, machine learning. This is not a crash course in statistics, data science, or machine learning, but an introduction to the language itself.  If you have any questions, write them down and make sure to ask them during each section’s Q&A."
  },
  {
    "objectID": "CrashCourse.html#introduction-to-programming",
    "href": "CrashCourse.html#introduction-to-programming",
    "title": "Crash Course",
    "section": "Introduction to Programming",
    "text": "Introduction to Programming\nEveryone can code  Everyone can learn how to program. Whether it takes you a minute or a little longer to learn how to write the code neessary to perform a certain, you can master it. Do not worry about comparing yourself to others.  As with more or less everything, a) motivation is key and b) you get good with practice.  You are here because, presumably, you have important questions to answet using data.  Knowing even a little code can give you the power to work with your own data without depending fully on someone else. At the same time, it makes collaborating with other scientistics, clinicians, statisticians, etc. much more effective & efficient.  You don’t learn to code by reading books or slides, you learn to code by doing. Ideally, work on data you are interested in, trying to asnwet questions you care about.  Learning to code can be exciting and frustrating. It’s similar to learning to play an instrument - like the guitar. At first, it may seem unnatural and annoying, but you can get better at it rather quickly and it’s very rewarding and satisfying.  It is important to be able to read & write code.  Coding requires logic & creativity  Programming is based on logic and learning to code helps structure your thoughts, your experiments, your reasoning.  Programming languages are human constructs. They developed to answer important needs. They develop with time, as needs evolve. Many design choices are explained historically, a few may be more arbitrary.  Everything is a series of simple, manageable steps  Remember this: the shortest and simplest piece of code up to the longest and most complex is made of a sequence of relatively simple steps.  Always be clear about what you want to achieve first, then break it down step-by-step in code. Each step is relatively easy to figure out and check that it is working as desired. A common mistake is to write multiple steps in one go, and then have a hard time figuring out where an error occurs or why.  To get from A to B using code there are virtually always multiple different paths. That can be confusing or perhaps frustrating, but it is also exciting. Programming to the uninitiated may seem a a rigid exercise but it is highly creative. Remember that there objective and subjective differences to consider when designing a code to take you from A to B. Suppose you have two approaches that have the same input and produce the same output. An objective difference would be how fast each completes the task and how many lines of code or number of function calls it requires. A subjective difference would be the programming style / syntax used / whether the code is “elegant” - a pretty broad topic.  Errors happen and they are not all the same  Errors in code happen all the time, it is part of the process. But, not all errors are the same, far from it. One crucial difference is coding errors that:  - stop execution of code and produce an error message. This is the best case scenario because it can’t go unnoticed. - do not stop execution of code but produce a warning. These warnings are too often ignored. They may be serios or trivial, but must be investigated. - do not stop execution and produce no warnings. This is the worst kind of error since it is silent. These are very common and the only way to recognize them are to check the output.  Details matter (a lot)  A lot of beginner and non-beginner mistakes occur because a variable or function name is misspelled.  ALWAYS READ ERROR AND WARNING MESSAGES.\nThe all-caps is because this is a) essential and b) far too often ignored.  Always check yourself  Remember: the most important thing is to ensure you produce correct results at each step. Don’t place writing smart or elegant code above writing correct code. Spend time reviewing your code. Ideally, if possible, have one or more other people review your code.  Document everything  Make a habit from the very beginning to always use comments in your code to explai what you are trying to achieve and why. You will often need to revisit your code after some time has passed. Life will be very hard if it’s not clear what is happening and why.  Programming is largely a team sport. A lot of code is written collaboratively or is used by people other than the author. Again, comprehensive documentation is super important.  Help is at your fingertips  Whether you are just starting out or you are a seasoned programmer, you have many sources of information to help you troubleshoot or learn new skills.  - Use the built-in documentation! Builtin help files, written by the code author, are almost always the best place to start. Their quality will vary, but they are often sufficient to learn how to use a function properly. - Programming is largely an online activity. All documentation and source code (for open source projects) is available online. Most errors or difficulties you encounter have been encountered many times before by others. A very large number of Q&A sites, blogs, forums are a web search away. Copy-pasting an error message into a search engine will often result in multiple hits."
  },
  {
    "objectID": "CrashCourse.html#the-r-language",
    "href": "CrashCourse.html#the-r-language",
    "title": "Crash Course",
    "section": "The R language",
    "text": "The R language\n\nThe S statistical programming language was developed in 1976 at Bell Labs by John Chambers and others “to turn ideas into software, quickly and faithfully”.\nR is an open source implementation of S developed by Ross Ihaka and Robert Gentleman at the University of Auckland, New Zealand; initial version released in 1995.\nSupported by the R Foundation for Statistical Computing, developed by the R Core Team and the contributions of many others.\nOfficial part of the Free Software Foundation’s GNU project available under GNU GPL v2.\nLatest version 4.1.3 released 2022-03-10 (as of writing)"
  },
  {
    "objectID": "CrashCourse.html#free-open-source-software",
    "href": "CrashCourse.html#free-open-source-software",
    "title": "Crash Course",
    "section": "Free Open Source Software",
    "text": "Free Open Source Software\nFree Open Source Software (FOSS) is software that is “free” and “open source” - what does that really mean?\nWhat is Free Software?\n\n“Free software is software that gives you, the user, the freedom to share, study and modify it. We call this free software because the user is free.”\n\n— Free Software Foundation\n\n\nWhat is Open Source Software\n\n“Open source software is software with source code that anyone can inspect, modify, and enhance.”\n\n— opensource.com\n\n\nWhy is FOSS important?  There are many advantage to FOSS, in general. Some of those, that are highly relevant in biomedical research and clinical applications include the promotion of inclusion, transparency, and trustworthiness."
  },
  {
    "objectID": "CrashCourse.html#rstudio-integrated-development-environment",
    "href": "CrashCourse.html#rstudio-integrated-development-environment",
    "title": "Crash Course",
    "section": "RStudio Integrated Development Environment",
    "text": "RStudio Integrated Development Environment\nRStudio offers a popular, feature-full Integrated Development Environment (IDE) for R.  More advanced users can use Visual Studio Code with the R Extension for a similar R IDE experience together with all the extra functionality and convenience of VS Code."
  },
  {
    "objectID": "CrashCourse.html#the-r-core-language-package-ecosystem",
    "href": "CrashCourse.html#the-r-core-language-package-ecosystem",
    "title": "Crash Course",
    "section": "The R core language & package ecosystem",
    "text": "The R core language & package ecosystem\nR boasts extensive quantitative and statistical functionality in the base system.\nThis functionality is extended through a vast ecosystem of external packages.\n\nCRAN: The Comprehensive R Archive Network (https://cran.r-project.org/): 19001 packages\nBioconductor: Bioinformatics-related packages and more (https://www.bioconductor.org/): 2083+ packages\nGitHub: The largest source code host (>200M repositories; https://github.com/): Likely hosts most of the above and quite a few more. Also hosts a copy of the entire CRAN."
  },
  {
    "objectID": "CrashCourse.html#reading-in-data",
    "href": "CrashCourse.html#reading-in-data",
    "title": "Crash Course",
    "section": "Reading in Data",
    "text": "Reading in Data\nWe shall use a heart failure dataset as an example. It is freely available at the UCI repository: “https://archive.ics.uci.edu/ml/machine-learning-databases/00519/heart_failure_clinical_records_dataset.csv”\n\nCSV\n\ndat <- read.csv(\"~/icloud/Data/UCI/heart_failure_clinical_records_dataset.csv\")\n\nThe head() function prints the first few lines of an object:\n\nhead(dat)\n\n  age anaemia creatinine_phosphokinase diabetes ejection_fraction\n1  75       0                      582        0                20\n2  55       0                     7861        0                38\n3  65       0                      146        0                20\n4  50       1                      111        0                20\n5  65       1                      160        1                20\n6  90       1                       47        0                40\n  high_blood_pressure platelets serum_creatinine serum_sodium sex smoking time\n1                   1    265000              1.9          130   1       0    4\n2                   0    263358              1.1          136   1       0    6\n3                   0    162000              1.3          129   1       1    7\n4                   0    210000              1.9          137   1       0    7\n5                   0    327000              2.7          116   0       0    8\n6                   1    204000              2.1          132   1       1    8\n  DEATH_EVENT\n1           1\n2           1\n3           1\n4           1\n5           1\n6           1\n\n\nThe read.csv() function read the contents of the CSV file into an R object known as a data.frame. This is essentially a table like a spreadsheet, where each row represents a case (e.g. a subject, patient, etc.) and each columnn represents a variable (e.g. Patient ID, Age, Sex, Dx, etc.)\n\n\nXLSX\n\ndat_too <- openxlsx::read.xlsx(\"~/icloud/Data/UCI/heart_failure_clinical_records_dataset.xlsx\")\n\n\nhead(dat_too)\n\n  age anaemia creatinine_phosphokinase diabetes ejection_fraction\n1  75       0                      582        0                20\n2  55       0                     7861        0                38\n3  65       0                      146        0                20\n4  50       1                      111        0                20\n5  65       1                      160        1                20\n6  90       1                       47        0                40\n  high_blood_pressure platelets serum_creatinine serum_sodium sex smoking time\n1                   1    265000              1.9          130   1       0    4\n2                   0    263358              1.1          136   1       0    6\n3                   0    162000              1.3          129   1       1    7\n4                   0    210000              1.9          137   1       0    7\n5                   0    327000              2.7          116   0       0    8\n6                   1    204000              2.1          132   1       1    8\n  DEATH_EVENT\n1           1\n2           1\n3           1\n4           1\n5           1\n6           1"
  },
  {
    "objectID": "CrashCourse.html#inspect-summarize-data",
    "href": "CrashCourse.html#inspect-summarize-data",
    "title": "Crash Course",
    "section": "Inspect & summarize data",
    "text": "Inspect & summarize data\nGet data dimensions:\n\ndim(dat)\n\n[1] 299  13\n\n\nLook at the data structure, including data types:\n\nstr(dat)\n\n'data.frame':   299 obs. of  13 variables:\n $ age                     : num  75 55 65 50 65 90 75 60 65 80 ...\n $ anaemia                 : int  0 0 0 1 1 1 1 1 0 1 ...\n $ creatinine_phosphokinase: int  582 7861 146 111 160 47 246 315 157 123 ...\n $ diabetes                : int  0 0 0 0 1 0 0 1 0 0 ...\n $ ejection_fraction       : int  20 38 20 20 20 40 15 60 65 35 ...\n $ high_blood_pressure     : int  1 0 0 0 0 1 0 0 0 1 ...\n $ platelets               : num  265000 263358 162000 210000 327000 ...\n $ serum_creatinine        : num  1.9 1.1 1.3 1.9 2.7 2.1 1.2 1.1 1.5 9.4 ...\n $ serum_sodium            : int  130 136 129 137 116 132 137 131 138 133 ...\n $ sex                     : int  1 1 1 1 0 1 1 1 0 1 ...\n $ smoking                 : int  0 0 1 0 0 1 0 1 0 1 ...\n $ time                    : int  4 6 7 7 8 8 10 10 10 10 ...\n $ DEATH_EVENT             : int  1 1 1 1 1 1 1 1 1 1 ...\n\n\nGet summary of dataset:\n\nsummary(dat)\n\n      age           anaemia       creatinine_phosphokinase    diabetes     \n Min.   :40.00   Min.   :0.0000   Min.   :  23.0           Min.   :0.0000  \n 1st Qu.:51.00   1st Qu.:0.0000   1st Qu.: 116.5           1st Qu.:0.0000  \n Median :60.00   Median :0.0000   Median : 250.0           Median :0.0000  \n Mean   :60.83   Mean   :0.4314   Mean   : 581.8           Mean   :0.4181  \n 3rd Qu.:70.00   3rd Qu.:1.0000   3rd Qu.: 582.0           3rd Qu.:1.0000  \n Max.   :95.00   Max.   :1.0000   Max.   :7861.0           Max.   :1.0000  \n ejection_fraction high_blood_pressure   platelets      serum_creatinine\n Min.   :14.00     Min.   :0.0000      Min.   : 25100   Min.   :0.500   \n 1st Qu.:30.00     1st Qu.:0.0000      1st Qu.:212500   1st Qu.:0.900   \n Median :38.00     Median :0.0000      Median :262000   Median :1.100   \n Mean   :38.08     Mean   :0.3512      Mean   :263358   Mean   :1.394   \n 3rd Qu.:45.00     3rd Qu.:1.0000      3rd Qu.:303500   3rd Qu.:1.400   \n Max.   :80.00     Max.   :1.0000      Max.   :850000   Max.   :9.400   \n  serum_sodium        sex            smoking            time      \n Min.   :113.0   Min.   :0.0000   Min.   :0.0000   Min.   :  4.0  \n 1st Qu.:134.0   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.: 73.0  \n Median :137.0   Median :1.0000   Median :0.0000   Median :115.0  \n Mean   :136.6   Mean   :0.6488   Mean   :0.3211   Mean   :130.3  \n 3rd Qu.:140.0   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:203.0  \n Max.   :148.0   Max.   :1.0000   Max.   :1.0000   Max.   :285.0  \n  DEATH_EVENT    \n Min.   :0.0000  \n 1st Qu.:0.0000  \n Median :0.0000  \n Mean   :0.3211  \n 3rd Qu.:1.0000  \n Max.   :1.0000"
  },
  {
    "objectID": "CrashCourse.html#data-types",
    "href": "CrashCourse.html#data-types",
    "title": "Crash Course",
    "section": "Data types",
    "text": "Data types\nA vector in R is a collection of items of the same type (e.g. numbers or characters) of any length, including 1 (i.e. there is no distinction between a scalar and a vector).\nData types in R are essentially different types of vectors.\nR includes a number of builtin data types. Some of the most common are:\n\nnumeric (e.g. 1.2, 5.9, 11.4)\ncharacter (e.g. “SF”, “SD”)\nlogical (e.g. “TRUE”, “FALSE”)\n\nTo create a new vector you can use the assignment operator <- or =.\n\na <- 4\n\nYou can print the contents of an object just by typing its name in the console:\n\na\n\n[1] 4\n\n\nis the same as:\n\nprint(a)\n\n[1] 4\n\n\nA comment beging with #. Anything placed after this will not be executed. Use comments to document every step in your code.\nUse c() to combine multiple values:\n\nb <- c(3, 5, 7)\n\n\nb\n\n[1] 3 5 7\n\n\nTo create a character vector, use single or double quotes around each element:\n\ndept <- c(\"ED\", \"Neuro\", \"Peds\")\n\n\ndept\n\n[1] \"ED\"    \"Neuro\" \"Peds\""
  },
  {
    "objectID": "CrashCourse.html#data-structures",
    "href": "CrashCourse.html#data-structures",
    "title": "Crash Course",
    "section": "Data Structures",
    "text": "Data Structures\nR includes multiple different data structures. Think of a data structure as a container that holds one or more vectors of data.\nThe data.frame is one of the most common data structures for statistics, because it can hold vectors of different kinds, e.g. numeric, categorical, and character.\n\n\n\nRead more about data structures\n\n\n\n\nFactors\nFactors in R are used to store categorical variables and therefore have many important uses in statistics / data science / machine learning.  Let’s convert binary categorical variables in our dataset to factors:\n\ndat$anaemia <- factor(dat$anaemia)\ndat$diabetes <- factor(dat$diabetes)\ndat$high_blood_pressure <- factor(dat$high_blood_pressure)\ndat$sex <- factor(dat$sex)\ndat$smoking <- factor(dat$smoking)\ndat$DEATH_EVENT <- factor(dat$DEATH_EVENT)\n\n\n\n\nRead more about factors"
  },
  {
    "objectID": "CrashCourse.html#working-with-data.frames",
    "href": "CrashCourse.html#working-with-data.frames",
    "title": "Crash Course",
    "section": "Working with data.frames",
    "text": "Working with data.frames\nOne way to select a column of a data.frame by name, is to use the $ notation. Note, we use head() to avoid printing the entire variable.\n\nhead(dat$age)\n\n[1] 75 55 65 50 65 90"
  },
  {
    "objectID": "CrashCourse.html#functions-in-r",
    "href": "CrashCourse.html#functions-in-r",
    "title": "Crash Course",
    "section": "Functions in R",
    "text": "Functions in R\nR includes a very large number of functions in the base language, which allow you to do a whole lot of data cleaning & manipulation, plotting, and modeling.  A function is called by typing its name, followed by a parenthesis with or without arguments.  For example, to get the mean of the b vector from above:\n\nmean(b)\n\n[1] 5\n\n\n\n\n\nLearn how to write your own functions"
  },
  {
    "objectID": "CrashCourse.html#summarize-data",
    "href": "CrashCourse.html#summarize-data",
    "title": "Crash Course",
    "section": "Summarize data",
    "text": "Summarize data\nA lot of statistical functionality is built in to the language. You can easily get summary statistics of variables using functions like mean(), median(), range(), max(), min().\n\nContinuous variables\n\nmean(dat$age)\n\n[1] 60.83389\n\nmedian(dat$age)\n\n[1] 60\n\nmin(dat$age)\n\n[1] 40\n\nmax(dat$age)\n\n[1] 95\n\nsummary(dat$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  40.00   51.00   60.00   60.83   70.00   95.00 \n\n\n\n\nCategorical variables\nThe table() function gives you the counts for each level/unique value of a variable:\n\ntable(dat$sex)\n\n\n  0   1 \n105 194 \n\ntable(dat$smoking)\n\n\n  0   1 \n203  96"
  },
  {
    "objectID": "CrashCourse.html#plots",
    "href": "CrashCourse.html#plots",
    "title": "Crash Course",
    "section": "Plots",
    "text": "Plots\nR has powerful and extensive support for graphics built in to the core language.\nHere, we look at how to produce some common and important plot types:\n\nHistogram\nDraw a histogram using hist(x)\n\nhist(dat$age, col = \"lightseagreen\")\n\n\n\n\n\n\nBoxplot\nDraw a boxplot using boxplot(x)\n\nboxplot(dat$ejection_fraction, col = \"lightseagreen\")\n\n\n\n\nYou can use a simple formula notation to draw boxplots grouped by a categorical variable using ~ symbol:continuous variable ~ grouping variable\n\nboxplot(dat$serum_sodium ~ dat$smoking, col = \"lightseagreen\")\n\n\n\n\n\n\nScatter plot\nDraw a scatter plot using plot(x, y)\n\nplot(dat$age, dat$serum_sodium, col = \"lightseagreen\")\n\n\n\n\n\nplot(dat$age, dat$serum_sodium, col = \"lightseagreen\")"
  },
  {
    "objectID": "CrashCourse.html#hypothesis-testing",
    "href": "CrashCourse.html#hypothesis-testing",
    "title": "Crash Course",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nT-test\nAre the means of two groups significantly different? We use a simple formula notation as with the boxplot above to split values by group:\n\nt.test(dat$serum_sodium ~ dat$sex)\n\n\n    Welch Two Sample t-test\n\ndata:  dat$serum_sodium by dat$sex\nt = 0.45176, df = 184.61, p-value = 0.652\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.8565791  1.3653665\nsample estimates:\nmean in group 0 mean in group 1 \n       136.7905        136.5361 \n\n\n\n\nChi-squared test\nTest for association between two categorical variables:\n\nchisq.test(dat$smoking, dat$DEATH_EVENT)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  dat$smoking and dat$DEATH_EVENT\nX-squared = 0.0073315, df = 1, p-value = 0.9318\n\n\n\nsmoking_sex <- chisq.test(dat$smoking, dat$sex)\nsmoking_sex\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  dat$smoking and dat$sex\nX-squared = 57.463, df = 1, p-value = 3.444e-14\n\n\nYou can print the observed frequencies:\n\nsmoking_sex$observed\n\n           dat$sex\ndat$smoking   0   1\n          0 101 102\n          1   4  92\n\n\nand the expected frequencies:\n\nsmoking_sex$expected\n\n           dat$sex\ndat$smoking        0         1\n          0 71.28763 131.71237\n          1 33.71237  62.28763"
  },
  {
    "objectID": "CrashCourse.html#saving-data",
    "href": "CrashCourse.html#saving-data",
    "title": "Crash Course",
    "section": "Saving data",
    "text": "Saving data\n\nCSV\nYou can write R objects to CSV file using `write.csv()’. These can be read directly into any program or language that can handle data.\n\nwrite.csv(dat, \"~/Data/dat.csv\")\n\n\n\nRDS\nYou can also directly save any R object as an “RDS” file. These can be read into R. The advantage is that they are compressed and therefore may take a lot less space, and will maintain any type conversion you have performed.\n\nsaveRDS(dat, \"~/Data/dat.rds\")"
  },
  {
    "objectID": "CrashCourse.html#builtin-documentation",
    "href": "CrashCourse.html#builtin-documentation",
    "title": "Crash Course",
    "section": "Builtin Documentation",
    "text": "Builtin Documentation\nAfter you’ve successfully installed R and RStudio, one of the first things to know is how to access and search the builtin documentation.\n\nGet help on a specific item\nIf you know the name of what you’re looking for (an R function most commonly, but possibly also the name of a dataset, or a package itself), just type ? followed by the name of said function, dataset, etc. in the R prompt:\n\n?sample\n\nIn RStudio, the above example will bring up the documentation for the sample function in the dedicated “Help” window, commonly situated at the bottom right (but can be moved by the user freely). If you are running R directly at the system shell, the same information is printed directly at the console.\nTry running the above example on your system.\n\n\nSearch the docs\nIf you do not know the name of what you are looking for, you can use double question marks, ??, followed by your query (this is short for the help.search command that provides a number of arguments you can look up using ?help.search):\n\n??bootstrap"
  },
  {
    "objectID": "References.html",
    "href": "References.html",
    "title": "References",
    "section": "",
    "text": "Bengtsson, Henrik. 2019. matrixStats: Functions That Apply to Rows\nand Columns of Matrices (and to Vectors). https://CRAN.R-project.org/package=matrixStats.\n\n\nBuuren, S van, and Karin Groothuis-Oudshoorn. 2010. “Mice:\nMultivariate Imputation by Chained Equations in r.” Journal\nof Statistical Software, 1–68.\n\n\nChambers, John M. 1998. Programming with Data: A Guide to the s\nLanguage. Springer Science & Business Media.\n\n\nGennatas, Efstathios Dimitrios. 2017. “Towards Precision\nPsychiatry: Gray Matter Development and Cognition in\nAdolescence.”\n\n\nMack, Christina, Zhaohui Su, and Daniel Westreich. 2018. “Managing\nMissing Data in Patient Registries: Addendum to Registries for\nEvaluating Patient Outcomes: A User’s Guide, [Internet].”\n\n\nMurrell, Paul. 2018. R Graphics. CRC Press.\n\n\nSarkar, Deepayan. 2008. Lattice: Multivariate Data Visualization\nwith r. New York: Springer. http://lmdvr.r-forge.r-project.org.\n\n\nSievert, Carson, Chris Parmer, Toby Hocking, Scott Chamberlain, Karthik\nRam, Marianne Corvellec, and Pedro Despouy. 2017. “Plotly: Create\nInteractive Web Graphics via ‘Plotly. Js’.” R\nPackage Version 4 (1): 110.\n\n\nStekhoven, Daniel J, and Peter Bühlmann. 2012.\n“MissForest—Non-Parametric Missing Value Imputation for Mixed-Type\nData.” Bioinformatics 28 (1): 112–18.\n\n\nWickham, Hadley. 2011. “Ggplot2.” Wiley\nInterdisciplinary Reviews: Computational Statistics 3 (2): 180–85.\n\n\nWilkinson, Leland. 2012. “The Grammar of Graphics.” In\nHandbook of Computational Statistics, 375–414. Springer.\n\n\nWright, Marvin N, and Andreas Ziegler. 2015. “Ranger: A Fast\nImplementation of Random Forests for High Dimensional Data in c++ and\nr.” arXiv Preprint arXiv:1508.04409."
  },
  {
    "objectID": "Resources.html#writing-r-documentation",
    "href": "Resources.html#writing-r-documentation",
    "title": "39  Resources",
    "section": "39.3 Writing R Documentation",
    "text": "39.3 Writing R Documentation\n\nDocumentation with roxygen2"
  },
  {
    "objectID": "Resources.html#datasets",
    "href": "Resources.html#datasets",
    "title": "39  Resources",
    "section": "39.10 Datasets",
    "text": "39.10 Datasets\nA number of online repositories offer free access to datasets suitable for data science / statistics / machine learning. Some of them are: \n\nNIH Data Repositories\nPLOS Recommended Repositories\nOpenML\nUCI Machine Learning Repository\nPenn ML Benchmarks (Essentially a subset of OpenML)\nWikipedia’s List of datasets for machine-learning research\nUCSF Library Resources"
  },
  {
    "objectID": "DataTypes.html#create-vectors-with-c",
    "href": "DataTypes.html#create-vectors-with-c",
    "title": "6  Data Types & Vectors",
    "section": "\n6.3 Create vectors with c()\n",
    "text": "6.3 Create vectors with c()\n\nUse c() to combine multiple values into a vector:\n\nx <- c(-12, 3.5, 104)\nx\n\n[1] -12.0   3.5 104.0"
  },
  {
    "objectID": "DataTypes.html#get-the-type-of-a-vector-using-typeof",
    "href": "DataTypes.html#get-the-type-of-a-vector-using-typeof",
    "title": "6  Data Types & Vectors",
    "section": "\n6.4 Get the type of a vector using typeof()\n",
    "text": "6.4 Get the type of a vector using typeof()\n\n\ntypeof(x)\n\n[1] \"double\""
  },
  {
    "objectID": "DataTypes.html#common-vector-types",
    "href": "DataTypes.html#common-vector-types",
    "title": "6  Data Types & Vectors",
    "section": "\n6.5 Common vector types",
    "text": "6.5 Common vector types\nLet’s create some example vectors of the most common data types:\n\n6.5.1 Integer\nNumeric vector default to double\n\nv <- c(12, 14, 23)\nv\n\n[1] 12 14 23\n\ntypeof(v)\n\n[1] \"double\"\n\n\nTo create an integer vector, you can follow numbers by an “L”\n\nvi <- c(12L, 14L, 23L)\nvi\n\n[1] 12 14 23\n\ntypeof(vi)\n\n[1] \"integer\"\n\n\nAlternatively you can coerce a double to integer using as.integer():\n\nvi <- as.integer(c(12, 14, 23))\nvi\n\n[1] 12 14 23\n\ntypeof(vi)\n\n[1] \"integer\"\n\n\n\n6.5.2 Double\n\nvd <- c(1.3, 2.8, 3.6)\nvd\n\n[1] 1.3 2.8 3.6\n\ntypeof(vd)\n\n[1] \"double\"\n\n\n\n6.5.3 Character\nA character vector consists of one or more elements, each of which consists of one or more actual characters, i.e. it is not a vector of single characters. (The length of a character vector is the number of individual elements, and is not related to the number of characters in each element)\n\nvc <- c(\"a\", \"d\", \"s\")\nvc\n\n[1] \"a\" \"d\" \"s\"\n\ntypeof(vc)\n\n[1] \"character\"\n\n\n\n6.5.4 Logical\nIf you are writing code, (it’s good practice to) use TRUE and FALSE.\nOn the console, you can abbreviate to T and F - the result is the same.\n\nvl <- c(TRUE, FALSE, FALSE)\nvl\n\n[1]  TRUE FALSE FALSE\n\ntypeof(vl)\n\n[1] \"logical\""
  },
  {
    "objectID": "DataTypes.html#generate-sequence-with",
    "href": "DataTypes.html#generate-sequence-with",
    "title": "6  Data Types & Vectors",
    "section": "6.4 Generate sequence with :",
    "text": "6.4 Generate sequence with :\n\nx <- 14:17\nx\n\n[1] 14 15 16 17"
  },
  {
    "objectID": "DataTypes.html#logical-1",
    "href": "DataTypes.html#logical-1",
    "title": "6  Data Types & Vectors",
    "section": "6.8 Logical",
    "text": "6.8 Logical\nIf you are writing code, (it’s good practice to) use TRUE and FALSE.\nOn the console, you can abbreviate to T and F, they are the same.\n\na <- c(TRUE, FALSE)\na <- c(T, F)\nx <- 4\nb <- x > 10\nb\n\n[1] FALSE\n\nstr(b)\n\n logi FALSE\n\ntypeof(b)\n\n[1] \"logical\""
  },
  {
    "objectID": "DataTypes.html#integer-1",
    "href": "DataTypes.html#integer-1",
    "title": "6  Data Types & Vectors",
    "section": "6.9 Integer",
    "text": "6.9 Integer\nCreate a range of integers using colon notation start:end:\n\n(x <- 11:15)\n\n[1] 11 12 13 14 15\n\ntypeof(x)\n\n[1] \"integer\"\n\nstr(x)\n\n int [1:5] 11 12 13 14 15\n\n\nNote that assigning an integer defaults to type double:\n\nx <- 1\ntypeof(x)\n\n[1] \"double\"\n\nstr(x)\n\n num 1\n\n\nYou can force it to be stored as integer by adding an L suffix:\n\nx <- 1L\ntypeof(x)\n\n[1] \"integer\"\n\nstr(x)\n\n int 1\n\n\n\nx <- c(1L, 3L, 5L)\nstr(x)\n\n int [1:3] 1 3 5"
  },
  {
    "objectID": "DataTypes.html#character-1",
    "href": "DataTypes.html#character-1",
    "title": "6  Data Types & Vectors",
    "section": "6.11 Character",
    "text": "6.11 Character\nA character vector consists of one or more elements, each of which consists of one or more actual characters, i.e. it is not a vector of single characters. (The length of a character vector is the number of individual elements, and is not related to the number of characters in each element)\n\nx <- \"word\"\ntypeof(x)\n\n[1] \"character\"\n\nlength(x)\n\n[1] 1\n\n\n\n(x <- c(\"a\", \"b\", \"gamma\", \"delta\"))\n\n[1] \"a\"     \"b\"     \"gamma\" \"delta\"\n\ntypeof(x)\n\n[1] \"character\"\n\nlength(x)\n\n[1] 4"
  },
  {
    "objectID": "DataTypes.html#double-1",
    "href": "DataTypes.html#double-1",
    "title": "6  Data Types & Vectors",
    "section": "6.10 Double",
    "text": "6.10 Double\n\nx <- c(1.2, 3.4, 10.987632419834556)\nx\n\n[1]  1.20000  3.40000 10.98763\n\ntypeof(x)\n\n[1] \"double\"\n\nstr(x)\n\n num [1:3] 1.2 3.4 11"
  },
  {
    "objectID": "DataStructures.html#initialize---coerce---test-data-structures",
    "href": "DataStructures.html#initialize---coerce---test-data-structures",
    "title": "7  Data Structures",
    "section": "\n7.10 Initialize - coerce - test data structures",
    "text": "7.10 Initialize - coerce - test data structures\nThe following table lists the functions to initialize, coerce (=convert), and test the core data structures, which are shown in more detail in the following paragraphs:\n\n\n\n\n\n\n\nInitialize\nCoerce\nTest\n\n\n\nmatrix(NA, nrow = x, ncol = y)\nas.matrix(x)\nis.matrix(x)\n\n\narray(NA, dim = c(x, y, z))\nas.array(x)\nis.array(x)\n\n\nvector(mode = \"list\", length = x)\nas.list(x)\nis.list(x)\n\n\ndata.frame(matrix(NA, x, y))\nas.data.frame(x)\nis.data.frame(x)"
  },
  {
    "objectID": "DataTypes.html#initialize---coerce---test-data-types",
    "href": "DataTypes.html#initialize---coerce---test-data-types",
    "title": "6  Data Types & Vectors",
    "section": "6.7 Initialize - coerce - test data types",
    "text": "6.7 Initialize - coerce - test data types\nThe following summary table lists the functions to initialize, coerce (=convert), and test the core data types, which are shown in more detail in the following paragraphs:\n\n\n\nInitialize\nCoerce\nTest\n\n\n\n\nlogical(n)\nas.logical(x)\nis.logical(x)\n\n\ninteger(n)\nas.integer(x)\nis.integer(x)\n\n\ndouble(n)\nas.double(x)\nis.double(x)\n\n\nnumeric(n)\nas.numeric(x)\nis.numeric(x)\n\n\ncharacter(n)\nas.character(x)\nis.character(x)\n\n\n\nNote: numeric and double functions on lines 3 and 4 above are equivalent. (Try printing numeric and double in the console)"
  },
  {
    "objectID": "DataTypes.html#initialize---coerce---test-vectors",
    "href": "DataTypes.html#initialize---coerce---test-vectors",
    "title": "6  Data Types & Vectors",
    "section": "\n6.12 Initialize - coerce - test vectors",
    "text": "6.12 Initialize - coerce - test vectors\nThe following summary table lists the functions to initialize, coerce (=convert), and test the main different vector types: \n\n\nInitialize\nCoerce\nTest\n\n\n\nlogical(n)\nas.logical(x)\nis.logical(x)\n\n\ninteger(n)\nas.integer(x)\nis.integer(x)\n\n\ndouble(n)\nas.double(x)\nis.double(x)\n\n\nnumeric(n)\nas.numeric(x)\nis.numeric(x)\n\n\ncharacter(n)\nas.character(x)\nis.character(x)\n\n\n\nNote: numeric and double functions on lines 3 and 4 above are equivalent. (Try printing numeric and double in the console and compare)"
  },
  {
    "objectID": "DataTypes.html#na-missing-value",
    "href": "DataTypes.html#na-missing-value",
    "title": "6  Data Types & Vectors",
    "section": "6.10 NA: Missing value",
    "text": "6.10 NA: Missing value\nMissing values in any data type - logical, integer, double, or character - are coded using NA.\nTo check for the presence of NA values, use is.na():\n\n(x <- c(1.2, 5.3, 4.8, NA, 9.6))\n\n[1] 1.2 5.3 4.8  NA 9.6\n\nis.na(x)\n\n[1] FALSE FALSE FALSE  TRUE FALSE\n\n\n\n(x <- c(\"mango\", \"banana\", NA, \"sugar\", \"ackee\"))\n\n[1] \"mango\"  \"banana\" NA       \"sugar\"  \"ackee\" \n\nis.na(x)\n\n[1] FALSE FALSE  TRUE FALSE FALSE\n\n\n\n(x <- c(T, T, F, T, F, F, NA))\n\n[1]  TRUE  TRUE FALSE  TRUE FALSE FALSE    NA\n\nis.na(x)\n\n[1] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n\n\nis.na() works similarly on matrices:\n\nx <- matrix(1:20, 5)\nx[4, 3] <- NA\nis.na(x)\n\n      [,1]  [,2]  [,3]  [,4]\n[1,] FALSE FALSE FALSE FALSE\n[2,] FALSE FALSE FALSE FALSE\n[3,] FALSE FALSE FALSE FALSE\n[4,] FALSE FALSE  TRUE FALSE\n[5,] FALSE FALSE FALSE FALSE\n\n\n\n\n\nNote that is.na() returns a response for each element (i.e. is vectorized) in contrast to is.numeric(), is.logical(), etc. The latter are checking the type of an object, while the former is checking individual elements.\n\n\n\nanyNA() is a very useful function to check if there an any NA values in an object:\n\nanyNA(x)\n\n[1] TRUE\n\n\n\n\n\nAny operations on an NA results in NA\n\n\n\n\nx <- c(1.2, 5.3, 4.8, NA, 9.6)\nx*2\n\n[1]  2.4 10.6  9.6   NA 19.2\n\n\nMultiple functions that accept as input an object with multiple values (a vector, a matrix, a data.frame, etc.) will return NA if any element is NA:\n\nmean(x)\n\n[1] NA\n\nmedian(x)\n\n[1] NA\n\nsd(x)\n\n[1] NA\n\nmin(x)\n\n[1] NA\n\nmax(x)\n\n[1] NA\n\nrange(x)\n\n[1] NA NA\n\n\nFirst, make sure NA values represent legitimate missing data and not some error. Then, decide how you want to handle it.\nIn all of the above commands you can pass na.rm = TRUE to ignore NA values:\n\nmean(x, na.rm = TRUE)\n\n[1] 5.225\n\nmedian(x, na.rm = TRUE)\n\n[1] 5.05\n\nsd(x, na.rm = TRUE)\n\n[1] 3.441293\n\nmin(x, na.rm = TRUE)\n\n[1] 1.2\n\nmax(x, na.rm = TRUE)\n\n[1] 9.6\n\nrange(x, na.rm = TRUE)\n\n[1] 1.2 9.6\n\n\nMore generally, you can use na.exclude() to exclude NA values from R objects. This can be very useful for function that do not include a na.rm or similar argument to handle NA values.\n\nx <- c(1, 2, NA, 4)\nna.exclude(x)\n\n[1] 1 2 4\nattr(,\"na.action\")\n[1] 3\nattr(,\"class\")\n[1] \"exclude\"\n\n\nOn a data.frame, na.exclude() excludes rows with any NAs:\n\ndf <- data.frame(a = c(1, 2, NA, 4),\n                 b = c(11, NA, 13, 14))\nna.exclude(df)\n\n  a  b\n1 1 11\n4 4 14\n\n\nThe chapter on Handling Missing Data describes some approaches to handling missing data in the context of statistics or machine learning."
  },
  {
    "objectID": "DataStructures.html#overview",
    "href": "DataStructures.html#overview",
    "title": "7  Data Structures",
    "section": "7.1 Overview",
    "text": "7.1 Overview\nThere are 5 main data structures in R: \n\n\n\n\n\n\n\n\n\nData Structure\nDimensionality\nContents\nNotes\n\n\n\n\nVector\n1-D\nhomogeneous\nthe “base” object\n\n\nMatrix\n2-D\nhomogeneous\na vector with 2 dimensions\n\n\nArray\nN-D\nhomogeneous\na vector with N dimensions\n\n\nList\n1-D; can be nested\nheterogeneous\na collection of any R objects, each of any length\n\n\nData frame\n2-D\nheterogeneous\na special kind of list: a collection of (column) vectors of any type, all of the same length\n\n\n\nVectors are homogeneous data structures which means all of their elements have to be of the same type (see Chapter 6), e.g. integer, double, character, logical.\nMatrices and arrays are vectors with more dimensions, and as such, are also homogeneous.\nLists are the most flexible. Their elements can be any R objects, including lists and therefore can be nested.\nData frames are a special kind of list. Their elements are one of more vectors, which can be of any type and form columns. Therefore a data.frame is a two-dimensional data structure where rows typically correspond to cases (e.g. individuals) and their columns represent variables. As such, data.frames are the most common data structure for statistical analysis. \n\n\n\n\n\nR Data Structure summary - Best to read through this chapter first and then refer back to this figure\n\n\n\n\n\n\n\nCheck object class with class() Check object class and contents’ types with str()"
  },
  {
    "objectID": "DataTypes.html#na",
    "href": "DataTypes.html#na",
    "title": "6  Data Types & Vectors",
    "section": "\n6.9 NA: Missing value",
    "text": "6.9 NA: Missing value\nMissing values in any data type - logical, integer, double, or character - are coded using NA.\nTo check for the presence of NA values, use is.na():\n\nx <- c(1.2, 5.3, 4.8, NA, 9.6)\nx\n\n[1] 1.2 5.3 4.8  NA 9.6\n\nis.na(x)\n\n[1] FALSE FALSE FALSE  TRUE FALSE\n\n\n\nx <- c(\"mango\", \"banana\", NA, \"sugar\", \"ackee\")\nx\n\n[1] \"mango\"  \"banana\" NA       \"sugar\"  \"ackee\" \n\nis.na(x)\n\n[1] FALSE FALSE  TRUE FALSE FALSE\n\n\n\nx <- c(T, T, F, T, F, F, NA)\nx\n\n[1]  TRUE  TRUE FALSE  TRUE FALSE FALSE    NA\n\nis.na(x)\n\n[1] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n\n\nis.na() works similarly on matrices:\n\nx <- matrix(1:20, 5)\nx[4, 3] <- NA\nis.na(x)\n\n      [,1]  [,2]  [,3]  [,4]\n[1,] FALSE FALSE FALSE FALSE\n[2,] FALSE FALSE FALSE FALSE\n[3,] FALSE FALSE FALSE FALSE\n[4,] FALSE FALSE  TRUE FALSE\n[5,] FALSE FALSE FALSE FALSE\n\n\n\n\n\nNote that is.na() returns a response for each element (i.e. is vectorized) in contrast to is.numeric(), is.logical(), etc. The latter are checking the type of an object, while the former is checking individual elements.\n\n\n\nanyNA() is a very useful function to check if there an any NA values in an object:\n\nanyNA(x)\n\n[1] TRUE\n\n\n\n\n\nAny operations on an NA results in NA\n\n\n\n\nx <- c(1.2, 5.3, 4.8, NA, 9.6)\nx*2\n\n[1]  2.4 10.6  9.6   NA 19.2\n\n\nMultiple functions that accept as input an object with multiple values (a vector, a matrix, a data.frame, etc.) will return NA if any element is NA:\n\nmean(x)\n\n[1] NA\n\nmedian(x)\n\n[1] NA\n\nsd(x)\n\n[1] NA\n\nmin(x)\n\n[1] NA\n\nmax(x)\n\n[1] NA\n\nrange(x)\n\n[1] NA NA\n\n\nFirst, make sure NA values represent legitimate missing data and not some error. Then, decide how you want to handle it.\nIn all of the above commands you can pass na.rm = TRUE to ignore NA values:\n\nmean(x, na.rm = TRUE)\n\n[1] 5.225\n\nmedian(x, na.rm = TRUE)\n\n[1] 5.05\n\nsd(x, na.rm = TRUE)\n\n[1] 3.441293\n\nmin(x, na.rm = TRUE)\n\n[1] 1.2\n\nmax(x, na.rm = TRUE)\n\n[1] 9.6\n\nrange(x, na.rm = TRUE)\n\n[1] 1.2 9.6\n\n\nMore generally, you can use na.exclude() to exclude NA values from R objects. This can be very useful for function that do not include a na.rm or similar argument to handle NA values.\n\nx <- c(1, 2, NA, 4)\nna.exclude(x)\n\n[1] 1 2 4\nattr(,\"na.action\")\n[1] 3\nattr(,\"class\")\n[1] \"exclude\"\n\n\nOn a data.frame, na.exclude() excludes rows with any NAs:\n\ndf <- data.frame(a = c(1, 2, NA, 4),\n                 b = c(11, NA, 13, 14))\nna.exclude(df)\n\n  a  b\n1 1 11\n4 4 14\n\n\nThe chapter on Handling Missing Data describes some approaches to handling missing data in the context of statistics or machine learning."
  },
  {
    "objectID": "IDEs.html#jupyter",
    "href": "IDEs.html#jupyter",
    "title": "\n3  IDEs\n",
    "section": "\n3.3 Jupyter / Jupyter Lab",
    "text": "3.3 Jupyter / Jupyter Lab\nJupyter is a popular notebook interface, which supports multiple programming languages, including R.\nJupyterLab is the “next-generation web-based user interface for Project Jupyter”.\nThere are different ways to install jupyter and jupyter-lab.\nOne way is:\n\nInstall miniforge\n\nUse conda to install jupyterlab:\n\n\nconda install jupyterlab\n\n\nInstall the R kernel:\n\n\nconda install r-irkernel   \n\n\nInstall the IRkernel R packages:\n\n\ninstall.packages('IRkernel')\nIRkernel::installspec()\n\n\nStart jupyter-lab:\n\n\njupyter-lab"
  },
  {
    "objectID": "DataTypes.html#named-vectors",
    "href": "DataTypes.html#named-vectors",
    "title": "6  Data Types & Vectors",
    "section": "6.12 Named vectors",
    "text": "6.12 Named vectors\nVector elements can be named.  Create a vector with named elements\n\nSBP = c(before = 179, after = 118)\nSBP\n\nbefore  after \n   179    118 \n\n\nUse names() to get a vector’s elements’ names:\n\nnames(SBP)\n\n[1] \"before\" \"after\" \n\n\nYou can add names to an existing, unnamed, vector:\n\nN <- c(112, 120)\nnames(N)\n\nNULL\n\nnames(N) <- c(\"Cases\", \"Controls\")\nN\n\n   Cases Controls \n     112      120"
  },
  {
    "objectID": "DataStructures.html#naming-object-elements",
    "href": "DataStructures.html#naming-object-elements",
    "title": "7  Data Structures",
    "section": "\n7.9 Naming object elements",
    "text": "7.9 Naming object elements\nAll objects’ elements can be named.\n\n7.9.1 Vectors\nYou can create a vector with named elements:\n\nSBP = c(before = 179, after = 118)\nSBP\n\nbefore  after \n   179    118 \n\n\nUse names() to get a vector’s elements’ names:\n\nnames(SBP)\n\n[1] \"before\" \"after\" \n\n\nYou can add names to an existing, unnamed, vector:\n\nN <- c(112, 120)\nnames(N)\n\nNULL\n\nnames(N) <- c(\"Cases\", \"Controls\")\nN\n\n   Cases Controls \n     112      120 \n\n\nMatrices and data frames can have column names (colnames) and row names (rownames):\n\nxm <- matrix(1:15, 5)\nxdf <- as.data.frame(xm)\ncolnames(xm)\n\nNULL\n\ncolnames(xdf)\n\n[1] \"V1\" \"V2\" \"V3\"\n\nrownames(xm)\n\nNULL\n\ncolnames(xm) <- colnames(xdf) <- paste0(\"Feature\", seq(3))\nrownames(xm) <- rownames(xdf) <- paste0(\"Case\", seq(5))\nxm\n\n      Feature1 Feature2 Feature3\nCase1        1        6       11\nCase2        2        7       12\nCase3        3        8       13\nCase4        4        9       14\nCase5        5       10       15\n\nxdf\n\n      Feature1 Feature2 Feature3\nCase1        1        6       11\nCase2        2        7       12\nCase3        3        8       13\nCase4        4        9       14\nCase5        5       10       15\n\n\nLists are vectors so they have names. These can be defined when a list is created using the name-value pairs or added/changed at any time.\n\nx <- list(HospitalName = \"CaliforniaGeneral\",\n          ParticipatingDepartments = c(\"Neurology\", \"Psychiatry\", \"Neurosurgery\"),\n          PatientIDs = 1001:1018)\nnames(x)\n\n[1] \"HospitalName\"             \"ParticipatingDepartments\"\n[3] \"PatientIDs\"              \n\n\nAdd/Change names:\n\nnames(x) <- c(\"Hospital\", \"Departments\", \"PIDs\")\nx\n\n$Hospital\n[1] \"CaliforniaGeneral\"\n\n$Departments\n[1] \"Neurology\"    \"Psychiatry\"   \"Neurosurgery\"\n\n$PIDs\n [1] 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015\n[16] 1016 1017 1018\n\n\nRemember that data a frame is a special type of list. Therefore in data frames colnames and names are equivalent:\n\ncolnames(iris)\n\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"     \n\nnames(iris)\n\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"     \n\n\n\n\n\nNote: As we saw, matrices have colnames() and rownames() Using names() on a matrix will assign names to individual elements, as if it was a long vector."
  },
  {
    "objectID": "DataTypes.html#initvectors",
    "href": "DataTypes.html#initvectors",
    "title": "6  Data Types & Vectors",
    "section": "\n6.6 Initialize vectors",
    "text": "6.6 Initialize vectors\nInitializing a vector or other data structure is the process by which you create an object of a certain size with some initial values, e.g. all zeros or all NAs, in order to replace with other values later.\nThis is usually computationaly more efficient than starting with a small object and appending to it multiple times.  You can create / initialize vectors of specific type with the vector command and specifying a mode or directly by calling the relevant function:\n\n(xl <- vector(mode = \"logical\", length = 10))\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n(xd <- vector(mode = \"double\", length = 10))\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\n(xn <- vector(mode = \"numeric\", length = 10)) # same as \"double\"\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\n(xi <- vector(mode = \"integer\", length = 10))\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\n(xc <- vector(mode = \"character\", length = 10))\n\n [1] \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\n\n\nThese are aliases of the vector command above (print their source code to see for yourself)\n\n(xl <- logical(10))\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n(xd <- double(10))\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\n(xn <- numeric(10)) # same as double\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\n(xi <- integer(10))\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\n(xc <- character(10))\n\n [1] \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\""
  },
  {
    "objectID": "ControlFlow.html#forloops",
    "href": "ControlFlow.html#forloops",
    "title": "12  Control flow",
    "section": "\n12.5 for loops",
    "text": "12.5 for loops\n\n\n\nUse for loops to repeat execution of a block of code a certain number of times.\n\n\n\nThe for loop syntax is for (var in vector) expression.\nThe expression is usually surrounded by curly brackets and can include any number of lines, any amount of code:\n\nfor (i in 1:3) {\n  print(\"I love coffee\")\n}\n\n[1] \"I love coffee\"\n[1] \"I love coffee\"\n[1] \"I love coffee\"\n\n\nThe loop executes for length(vector) times.\nAt iteration i, var = vector[i].\nYou will often use the value of var inside the loop (but you don’t have to):\n\nfor (i in seq(10)) {\n  cat(i^2, \"\\n\")\n}\n\n1 \n4 \n9 \n16 \n25 \n36 \n49 \n64 \n81 \n100 \n\n\nletters is a built-in constant that includes all 26 lowercase letters of the Roman alphabet; LETTERS similarly includes all 26 uppercase letters.\n\nfor (letter in letters[1:5]) {\n  cat(letter, \"is a letter!\\n\")\n}\n\na is a letter!\nb is a letter!\nc is a letter!\nd is a letter!\ne is a letter!\n\n\n\n12.5.1 Working on data within a for loop\nA common scenario involves working on a data object, whether a vector, matrix, list, data.frame, and performing an operation on each elements, one at a time. While a lot of these operations are often performed using loop functions instead, for loops can certainly be used.\nYou can start by initializing an object of the appropriate class and dimensions to hold the output. Then, each iteration of the for loop will assign its output to the corresponding element/s of this object.\nIn the following example we transform the mtcars built-in dataset’s features to z-scores. The built-in command scale() will do this for quickly and conveniently, this is for demonstration purposes:\nFirst, initialize the output to be the desired class and dimensions:\n\nclass(mtcars)\n\n[1] \"data.frame\"\n\ndim(mtcars)\n\n[1] 32 11\n\nmtcars_z <- data.frame(matrix(0, 32, 11))\ncolnames(mtcars_z) <- colnames(mtcars)\n\nor, it is much simpler to just make a copy of mtcars to be overwritten by the for loop later:\n\nmtcars_z <- mtcars\n\nStandardization involves subtracting the mean and dividing by the standard deviation.\nHere is the for loop - we iterate through each column and assign the transformed data:\n\nfor (i in 1:ncol(mtcars)) {\n  mtcars_z[, i] <- (mtcars[, i] - mean(mtcars[, i])) / sd(mtcars[, i])\n}\n\nLet’s compare to the output of the scale() command by print the first 3 rows and columns of each:\n\nmtcars_z2 <- as.data.frame(scale(mtcars))\nmtcars_z[1:3, 1:3]\n\n                    mpg        cyl       disp\nMazda RX4     0.1508848 -0.1049878 -0.5706198\nMazda RX4 Wag 0.1508848 -0.1049878 -0.5706198\nDatsun 710    0.4495434 -1.2248578 -0.9901821\n\nmtcars_z2[1:3, 1:3]\n\n                    mpg        cyl       disp\nMazda RX4     0.1508848 -0.1049878 -0.5706198\nMazda RX4 Wag 0.1508848 -0.1049878 -0.5706198\nDatsun 710    0.4495434 -1.2248578 -0.9901821\n\n\nNote that we wrapped scale() around as.data.frame() because it outputs a matrix.\nWe can check that all elements are the same with all():\n\nall(mtcars_z == mtcars_z2)\n\n[1] FALSE\n\n\n\n12.5.2 Nested for loops\n\na <- matrix(1:9, 3)\nfor (i in seq(3)) {\n  for (j in seq(3)) {\n    cat(\"  a[\", i, \",\", j, \"] is \", a[i, j], \"\\n\", sep = \"\")\n  }\n}\n\n  a[1,1] is 1\n  a[1,2] is 4\n  a[1,3] is 7\n  a[2,1] is 2\n  a[2,2] is 5\n  a[2,3] is 8\n  a[3,1] is 3\n  a[3,2] is 6\n  a[3,3] is 9\n\n\n\n12.5.3 Printing within a for loop\nIn the R console objects get printed just by typing their name:\n\na <- 4\na\n\n[1] 4\n\n# same as\nprint(a)\n\n[1] 4\n\n\nThis “automatic printing” does not happen within a for loop, so you simply use print() (or cat() as preferred):\nThe following loop does not print out anything:\n\na <- 0\nfor (i in 1:4) {\n  a <- a + i^2\n  a\n}\n\nbut this does:\n\na <- 0\nfor (i in 1:4) {\n  a <- a + i^2\n  print(a)\n}\n\n[1] 1\n[1] 5\n[1] 14\n[1] 30"
  },
  {
    "objectID": "DataStructures.html#generating-sequences",
    "href": "DataStructures.html#generating-sequences",
    "title": "7  Data Structures",
    "section": "\n7.8 Generating sequences",
    "text": "7.8 Generating sequences\nOther than assigning individual elements explicitly with c(), there are multiple ways to create numeric sequences.\nColon notation allows generating a simple integer sequence:\n\nx <- 1:5\nx\n\n[1] 1 2 3 4 5\n\ntypeof(x)\n\n[1] \"integer\"\n\n\nseq(from, to, by)\n\nseq(1, 10, .5)\n\n [1]  1.0  1.5  2.0  2.5  3.0  3.5  4.0  4.5  5.0  5.5  6.0  6.5  7.0  7.5  8.0\n[16]  8.5  9.0  9.5 10.0\n\n\nseq(n) is equivalent to 1:n\n\nseq(12)\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12\n\n# same output as\n1:12\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12\n\n\nseq_len(n) is an optimized version of seq(n):\n\nseq_len(12)\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12\n\n\nseq(object) generates a sequence of length equal to length(object)\n\nseq(iris)\n\n[1] 1 2 3 4 5\n\n\nseq_along(object) is the optimized version of seq(object):\n\nseq_along(iris)\n\n[1] 1 2 3 4 5\n\n\nseq(from, to, length.out = n):\n\nseq(-5, 12, length.out = 11)\n\n [1] -5.0 -3.3 -1.6  0.1  1.8  3.5  5.2  6.9  8.6 10.3 12.0"
  },
  {
    "objectID": "DataStructures.html#combining-different-types-with-c",
    "href": "DataStructures.html#combining-different-types-with-c",
    "title": "7  Data Structures",
    "section": "\n7.6 Combining different types with c()\n",
    "text": "7.6 Combining different types with c()\n\nIt’s best to use c() to either combine elements of the same type into a vector, or to combine lists. Otherwise you must inspect the outcome to be certain it was as intended.\nAs we’ve seen, if all arguments passed to c() are of a single type, you get a vector of that type:\n\nx <- c(12.9, 94.67, 23.74, 46.901)\nx\n\n[1] 12.900 94.670 23.740 46.901\n\nclass(x)\n\n[1] \"numeric\"\n\n\nIf arguments passed to c() are a mix of numeric and character, they all get coerced to character.\n\n(x <- c(23.54, \"mango\", \"banana\", 75))\n\n[1] \"23.54\"  \"mango\"  \"banana\" \"75\"    \n\nclass(x)\n\n[1] \"character\"\n\n\nIf you pass more types of objects (which cannot be coerced to character) you get a list, since it is the only structure that can support all of them together:\n\n(x <- c(42, mean, \"potatoes\"))\n\n[[1]]\n[1] 42\n\n[[2]]\nfunction (x, ...) \nUseMethod(\"mean\")\n<bytecode: 0x1208302f8>\n<environment: namespace:base>\n\n[[3]]\n[1] \"potatoes\"\n\nclass(x)\n\n[1] \"list\""
  },
  {
    "objectID": "Indexing.html#resources",
    "href": "Indexing.html#resources",
    "title": "\n8  Indexing\n",
    "section": "\n8.7 Resources",
    "text": "8.7 Resources\n“Indexing vectors” in An Introduction to R"
  },
  {
    "objectID": "IDEs.html#rstudio-posit",
    "href": "IDEs.html#rstudio-posit",
    "title": "\n3  IDEs\n",
    "section": "\n3.1 RStudio / Posit",
    "text": "3.1 RStudio / Posit\nRStudio (soon to be Posit) is a very popular Integrated Development Environment IDE dedicated to R. This is the recommended environment for beginners. Make sure to keep your installation up-to-date.\nIt is recommended to set up a new RStudio project for each data project:\nRStudio projects allows you to organize your work. Each project keeps track of your working directory, workspace, history, and source documents.\nTo create a new RStudio Project click on File > New Project… from the main menu or the “Create a project” icon (second from top-left usually) in the RStudio toolbar."
  },
  {
    "objectID": "DataTypes.html",
    "href": "DataTypes.html",
    "title": "6  Data Types & Vectors",
    "section": "",
    "text": "The simplest and most fundamental object in R is the vector: a one-dimensional collection of elements of the same data type, e.g. numbers, characters, etc. (known as an “atomic” vector).  For example, a numeric vector may consist of elements 12, 14, 20, and a character vector may consist of elements \"x\", \"y\", \"z\".  Vectors can exist as stand-alone objects, or they can exist within other data structures, e.g. data.frames, lists, etc.  This chapter covers different atomic vectors, and the next covers data structures (Chapter 7).  R includes a number of builtin data types. These are defined by the R core team - users cannot define their own data types.  Users can, however, define their own classes (Chapter 24).  The main/most common data types in R are:\n\n\nNumeric, including integer and double\n\nCharacter\n\nLogical (i.e. TRUE or FALSE, a.k.a. Boolean)\n\nOther data types include environments and closures i.e. functions (Chapter 15)."
  },
  {
    "objectID": "Factors.html#factorsintro",
    "href": "Factors.html#factorsintro",
    "title": "9  Factors",
    "section": "\n9.1 Factors (introductory)",
    "text": "9.1 Factors (introductory)\n\n\n\nFactors in R are a special type of vector.\n\n\n\n\nEach element can take one value from a set known as the factor’s levels\n\n\n\n\nA factor’s levels are stored in a particular order, which affects how that factor is treated by some functions, e.g. in hypothesis testing, model fitting, visualization, etc.\n\n\n\n\nYou can specify whether or not the order of the levels defines a quantitative relationship such that level1 < level2, etc., in which case the factor is known as ordered\n\n\n\n\n\nYou can create a factor by passing a numeric or character vector to factor() or to as.factor().\nThe difference between the two is that as.factor() does not accept any arguments while factor() does.\nLet’s start with a character vector that includes three unique values - “a”, “b”, and “c”:\n\nx <- c(\"a\", \"c\", \"b\", \"b\", \"a\", \"a\", \"b\", \"c\")\nx\n\n[1] \"a\" \"c\" \"b\" \"b\" \"a\" \"a\" \"b\" \"c\"\n\n\nAssume that “a”, “b”, and “c” define three different groups and we want to convert this character vector to a factor.as.factor() and factor() without any arguments produce the same output:\n\n9.1.1 Create a factor\n\nxf <- factor(x)\nxf\n\n[1] a c b b a a b c\nLevels: a b c\n\nclass(xf)\n\n[1] \"factor\"\n\nxftoo <- as.factor(x)\nxftoo\n\n[1] a c b b a a b c\nLevels: a b c\n\nclass(xftoo)\n\n[1] \"factor\"\n\n\nNotice how a factor is printed in the R console:\n\nThe elements are printed without double quotes around them, differentiating them from character vectors.\nThe factor levels are printed below the vector values.\n\ntable() is a very useful function for factors - it gives the counts for each level:\n\ntable(xf)\n\nxf\na b c \n3 3 2 \n\n\nLet’s look at a different example. We define a factor to identify cases and controls:\n\ng <- factor(c(\"case\", \"control\", \"control\", \"case\", \"control\"))\ng\n\n[1] case    control control case    control\nLevels: case control\n\n\nBy default, the levels are ordered alphabeticaly.\n\n9.1.2 Set the order of factor levels\nYou can define the order of the factor levels with the level argument of the factor() function.\nFor example, the first factor level is used as the baseline in some statistical operations, e.g. glm(), in which case the “control” level should be first.\nMany plotting functions order categorical axes’ labels using the level order.\n\ng <- factor(c(\"case\", \"control\", \"control\", \"case\", \"control\"),\n            levels = c(\"control\", \"case\"))\ng\n\n[1] case    control control case    control\nLevels: control case\n\n\nThe levels argument can include values not present in the input data vector. This may be used for example when some known categories are not present in your sample, but may be added in the future, or you want specifically show they are absent in a table or a plot, etc.\n\ng <- factor(c(\"Type I\", \"Type III\", \"Type III\", \"Type I\"),\n            levels = c(\"Type I\", \"Type II\", \"Type III\"))\ng\n\n[1] Type I   Type III Type III Type I  \nLevels: Type I Type II Type III\n\n\nOn the other hand, if the levels argument is specified and does not include one or more of the values present in the input data vector, the corresponding elements become NA:\n\ng <- factor(c(\"case\", \"control\", \"undefined\", \"control\", \n              \"case\", \"control\", \"undefined\"),\n            levels = c(\"control\", \"case\"))\ng\n\n[1] case    control <NA>    control case    control <NA>   \nLevels: control case\n\n\n\n9.1.3 Define level labels\nYou can define level names or labels other than the values in the input vector using the labels argument:\nAssume you started with the following character vector:\n\nx <- c(\"female\", \"female\", \"male\", \"female\", \"male\")\n\nYou can attach different labels to each level rather than default to “female” and “male” by passing a character vector to the labels arguments:\n\nxf <- factor(x, labels = c(\"F\", \"M\"))\nxf\n\n[1] F F M F M\nLevels: F M\n\n\nThe order of names in the labels argument must match the order of levels. In the above example, the levels default to c(\"female\", \"male\") because they are sorted alphabeticaly if not specified. Otherwise, we can specify both the levels and labels arguments to define both order of levels and provide new labels:\n\nxf <- factor(x, \n             levels = c(\"male\", \"female\"),\n             labels = c(\"M\", \"F\"))\nxf\n\n[1] F F M F M\nLevels: M F\n\n\n\n9.1.4 Change level labels of existing factor\nWe can change the labels of a factor object\n\nusing the labels argument of the factor() command in the same way we create a factor from a character or other vector\nusing the levels() command\n\nStart with a factor of two groups and change labels using the factor() command:\n\nxf <- factor(c(\"GroupA\", \"GroupB\", \"GroupB\", \"GroupA\"))\nxf <- factor(xf, labels = c(\"A\", \"B\"))\nxf\n\n[1] A B B A\nLevels: A B\n\n\nStart with the same factor and change labels using the levels() command.\nThis is similar to using colnames() on a data.frame and is much faster than using factor() as above:\n\nxf <- factor(c(\"GroupA\", \"GroupB\", \"GroupB\", \"GroupA\"))\nlevels(xf) <- c(\"A\", \"B\")\nxf\n\n[1] A B B A\nLevels: A B\n\n\n\n\n\nThe levels() command changes the names of the levels. It cannot be used to change the order of levels, which must be done using factor()"
  },
  {
    "objectID": "InputOutput.html",
    "href": "InputOutput.html",
    "title": "10  Data Input/Output",
    "section": "",
    "text": "List built-in datasets with data() and no arguments:\n\ndata()\n\nThese built-in datasets are normally readily available in the R console (because the datasets package is automatically loaded)\nYou can check if this is the case using search()\n\nsearch()\n\n [1] \".GlobalEnv\"        \"tools:quarto\"      \"package:stats\"    \n [4] \"package:graphics\"  \"package:grDevices\" \"package:utils\"    \n [7] \"package:datasets\"  \"package:methods\"   \"Autoloads\"        \n[10] \"package:base\"     \n\n\n\nList a dataset included with some R package:\n\ndata(package = \"glmnet\")\ndata(package = \"MASS\")\ndata(package = \"mlbench\")\n\nLoad a dataset from some R package:\n\ndata(Sonar, package = \"mlbench\")\n\nNote: quotes around “Sonar” in the data() command above are optional."
  },
  {
    "objectID": "DataIO.html",
    "href": "DataIO.html",
    "title": "10  Data Input/Output",
    "section": "",
    "text": "Common Data Input/Output commands in R\nTabular data typically consists of rows and columns, where the rows correspond to different cases (e.g. patients) and the columns correspond to different variables a.k.a. covariates a.k.a. features. Such data can be stored in multiple different file formats.  This includes plain text files, often in a delimited format, and binary files.  Common delimited format files includes comma- and tab-separated values (CSV, TSV).  Binary file formats can be either open (e.g. R’s RDS format) or proprietary (e.g. Microsoft’s XLS).  R includes built-in support for reading and writing multiple file formats, including delimited format files and its own binary RDS and RData files.  Third party packages add support for working with virtually any file type."
  },
  {
    "objectID": "DataIO.html#system-commands",
    "href": "DataIO.html#system-commands",
    "title": "10  Data Input/Output",
    "section": "\n10.7 System commands",
    "text": "10.7 System commands\nGet working directory with getwd()\n\ngetwd()\n\nSet the working directory with setwd()\n\nsetwd(\"/Data/\")\n\nYou can set a different working directory with setwd()\nList files in current directory:\n\ndir()\n\nYou can send operating system commands using system():\n\nsystem(\"uname -a\")\n\nNote: See issue here"
  },
  {
    "objectID": "DataIO.html#data-io",
    "href": "DataIO.html#data-io",
    "title": "10  Data Input/Output",
    "section": "\n10.3 Data I/O",
    "text": "10.3 Data I/O\n\n\n\n\nCommon Data Input/Output commands in R\n\n\n\n\n\n10.3.1 Read local CSV\nread.table() is the core function that reads data from formatted text files in R, where cases correspond to lines and variables to columns. Its many arguments allow to read different formats. read.csv() is an alias for read.table() that defaults to commas as separators and dots for decimal points. (Run read.csv in the console to print its source read the documentation with ?read.table).  Some important arguments for read.table() listed here with their default values for read.csv():\n\n\nsep = \",\": Character that separate entries. Default is a comma; use “ for tab-separated files (default setting in read.delim())\n\ndec = \".\": Character for the decimal point. Default is a dot; in some cases where a comma is used as the decimal point, the entry separator sep may be a semicolon (default setting in read.csv2())\n\nna.strings = \"NA\": Character vector of strings to be coded as “NA”\n\ncolClasses = NA: Either a character vector defining each column’s type (e.g. c(“character”, “numeric”, “numeric”) recycled as necessary or a named vector defining specific columns’ types (e.g. c(ICD9 = “character”, Sex = “factor”, SBP = “numeric”, DOB = “Date”)). Unspecified columns automatically determined. Note: Set a column to “NULL” (with quotes) to exclude column.\n\n\nmen <-  read.csv(\"../Data/pone.0204161.s001.csv\")\n\n\n10.3.2 Read data from the web\nread.csv() can directly read an online file. In the second example below, we also define that missing data is coded with ? using the na.strings argument:\n\nparkinsons <- read.csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data\")\n\nsleep <- read.csv(\"https://www.openml.org/data/get_csv/53273/sleep.arff\",\n                  na.strings = \"?\")\n\nThe above files are read from two very popular online data repositories. Confusingly, neither file ends in .csv, but they both work with read.csv(). You can always look at the plain text file to determine if it can work with read.table()/read.csv() and what settings to use.\n\n10.3.3 Read zipped data from the web\n\n10.3.3.1 using gzcon() and csv.read()\n\nread.table() /read.csv() also accepts a “connection” as input.\nHere we define a connection to a zipped file by nesting gzcon() and url():\n\ncon <- gzcon(url(\"https://github.com/EpistasisLab/pmlb/raw/master/datasets/breast_cancer_wisconsin/breast_cancer_wisconsin.tsv.gz\"),\n             text = TRUE)\n\nWe read the connection and specify the file is tab-separated, or call read.delim():\n\nbcw <- read.csv(con, header = TRUE, sep = \"\\t\")\n\n#same as\nbcw <- read.delim(con, header = TRUE)\n\n\n10.3.3.2 using data.table’s fread()\n\nYou can also use data.table’s fread(), which will directly handle zipped files:\n\nlibrary(data.table)\nbcw2 <- fread(\"https://github.com/EpistasisLab/penn-ml-benchmarks/raw/master/datasets/classification/breast-cancer-wisconsin/breast-cancer-wisconsin.tsv.gz\")\n\nIf you want to stick to using data frames, set the argument data.table to FALSE:\n\nbcw2 <- fread(\"https://github.com/EpistasisLab/penn-ml-benchmarks/raw/master/datasets/classification/breast-cancer-wisconsin/breast-cancer-wisconsin.tsv.gz\",\n              data.table = FALSE)\n\n\n10.3.4 Write to CSV\nUse the write.csv() function to write an R object (usually data frame or matrix) to a CSV file. Setting row.names = FALSE is usually a good idea. (Instead of storing data in rownames, it’s usually best to create a new column.)\n\nwrite.csv(iris, \"../Data/iris.csv\", row.names = FALSE)\n\nNote that in this case we did not need to save row names (which are just integers 1 to 150 and would add a useless extra column in the output)\n\n10.3.5 Read .xslx using openxlsx::read.xlsx()\n\nAs an example, we can read the csv we saved earlier into Excel and then save it as a .xlsx file.\n\niris.path <- normalizePath(\"../Data/iris.xlsx\")\niris2 <- openxlsx::read.xlsx(iris.path)\n\nNote: openxlsx::read.xlsx() does not work with a relative path like \"./Data/iris.xlsc\". Therefore we used the normalizePath() function to give us the full path of the file without having to type it out.\nCheck that the data is still identical:\n\nall(iris == iris2)\n\n\n10.3.6 Write an R object to RDS\nYou can write any R object directly to file so that you can recover it at any time, share it, etc. Remember that since a list can contain any number of objects of any type, you can save any collection of objects as an RDS file. For multiple objects, see also the save.image() command below.\n\nsaveRDS(iris, \"iris.rds\")\n\nTo load an object saved in an rds file, assign it to an object using readRDS():\n\niris_fromFile <- readRDS(\"iris.rds\")\nall(iris == iris_fromFile)\n\n\n10.3.7 Write multiple R objects to RData file using save()\n\n\nmat1 <- sapply(seq_len(10), function(i) rnorm(500))\nmat2 <- sapply(seq_len(10), function(i) rnorm(500))\nsave(mat1, mat2, file = \"./mat.RData\")\n\nNote: we will learn how to use sapply() later under “Loop functions”\nTo load the variables in the .RData file you saved, use the load() command:\n\nload(\"./Rmd/mat.RData\")\n\nNote that load() adds the objects to your workspace using with their original names. You do not assign them to a new object, unlike with the readRDS() call above.\n\n10.3.8 Write your entire workspace to a RData image using save.image()\n\nYou can save your entire workspace to a RData file using the save.image() function.\n\nsave.image(\"workspace_10_05_2020.RData\")\n\nSame as above, to re-load the workspace saved in the .RData file, use the load() command:\n\nload(\"workspace_10_05_2020.RData\")"
  },
  {
    "objectID": "Packages.html#r-datasets",
    "href": "Packages.html#r-datasets",
    "title": "\n4  R packages\n",
    "section": "\n4.7 R datasets",
    "text": "4.7 R datasets\n\n4.7.1 Datasets included with R\nList built-in datasets with data() and no arguments:\n\ndata()\n\nThese built-in datasets are normally readily available in the R console because the datasets package is automatically loaded at startup. You can list all loaded packages with search()\n\nsearch()\n\n [1] \".GlobalEnv\"        \"tools:quarto\"      \"package:stats\"    \n [4] \"package:graphics\"  \"package:grDevices\" \"package:utils\"    \n [7] \"package:datasets\"  \"package:methods\"   \"Autoloads\"        \n[10] \"package:base\"     \n\n\n\n4.7.2 Datasets included with other packages\nMany R packages come with included datasets. If available, we can list them using the package argument of data():\n\ndata(package = \"glmnet\")\ndata(package = \"MASS\")\ndata(package = \"mlbench\")\n\nTo load such a dataset:\n\ndata(Sonar, package = \"mlbench\")\n\nNote: quotes around “Sonar” in the data() command above are optional."
  },
  {
    "objectID": "DataIO.html#data-input",
    "href": "DataIO.html#data-input",
    "title": "10  Data Input/Output",
    "section": "\n10.1 Data Input",
    "text": "10.1 Data Input\n\n10.1.1 Read local CSV\nread.table() is the core function that reads data from formatted text files in R, where cases correspond to lines and variables to columns. Its many arguments allow to read different formats. read.csv() is an alias for read.table() that defaults to commas as separators and dots for decimal points. (Run read.csv in the console to print its source read the documentation with ?read.table).  Some important arguments for read.table() listed here with their default values for read.csv():\n\n\nsep = \",\": Character that separate entries. Default is a comma; use “ for tab-separated files (default setting in read.delim())\n\ndec = \".\": Character for the decimal point. Default is a dot; in some cases where a comma is used as the decimal point, the entry separator sep may be a semicolon (default setting in read.csv2())\n\nna.strings = \"NA\": Character vector of strings to be coded as “NA”\n\ncolClasses = NA: Either a character vector defining each column’s type (e.g. c(“character”, “numeric”, “numeric”) recycled as necessary or a named vector defining specific columns’ types (e.g. c(ICD9 = “character”, Sex = “factor”, SBP = “numeric”, DOB = “Date”)). Unspecified columns automatically determined. Note: Set a column to “NULL” (with quotes) to exclude column.\n\n\nmen <-  read.csv(\"../Data/pone.0204161.s001.csv\")\n\n\n10.1.2 Read data from the web\nread.csv() can directly read an online file. In the second example below, we also define that missing data is coded with ? using the na.strings argument:\n\nparkinsons <- read.csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data\")\n\nsleep <- read.csv(\"https://www.openml.org/data/get_csv/53273/sleep.arff\",\n                  na.strings = \"?\")\n\nThe above files are read from two very popular online data repositories. Confusingly, neither file ends in .csv, but they both work with read.csv(). You can always look at the plain text file to determine if it can work with read.table()/read.csv() and what settings to use.\n\n10.1.3 Read zipped data from the web\n\n10.1.3.1 using gzcon() and csv.read()\n\nread.table() /read.csv() also accepts a “connection” as input.\nHere we define a connection to a zipped file by nesting gzcon() and url():\n\ncon <- gzcon(url(\"https://github.com/EpistasisLab/pmlb/raw/master/datasets/breast_cancer_wisconsin/breast_cancer_wisconsin.tsv.gz\"),\n             text = TRUE)\n\nWe read the connection and specify the file is tab-separated, or call read.delim():\n\nbcw <- read.csv(con, header = TRUE, sep = \"\\t\")\n\n#same as\nbcw <- read.delim(con, header = TRUE)\n\n\n10.1.3.2 using data.table’s fread()\n\nYou can also use data.table’s fread(), which will directly handle zipped files:\n\nlibrary(data.table)\nbcw2 <- fread(\"https://github.com/EpistasisLab/penn-ml-benchmarks/raw/master/datasets/classification/breast-cancer-wisconsin/breast-cancer-wisconsin.tsv.gz\")\n\nIf you want to stick to using data frames, set the argument data.table to FALSE:\n\nbcw2 <- fread(\"https://github.com/EpistasisLab/penn-ml-benchmarks/raw/master/datasets/classification/breast-cancer-wisconsin/breast-cancer-wisconsin.tsv.gz\",\n              data.table = FALSE)"
  },
  {
    "objectID": "DataIO.html#read-.xslx-using-openxlsxread.xlsx",
    "href": "DataIO.html#read-.xslx-using-openxlsxread.xlsx",
    "title": "10  Data Input/Output",
    "section": "\n10.1 Read .xslx using openxlsx::read.xlsx()\n",
    "text": "10.1 Read .xslx using openxlsx::read.xlsx()\n\nAs an example, we can read the csv we saved earlier into Excel and then save it as a .xlsx file.\n\niris.path <- normalizePath(\"../Data/iris.xlsx\")\niris2 <- openxlsx::read.xlsx(iris.path)\n\nNote: openxlsx::read.xlsx() does not work with a relative path like \"./Data/iris.xlsc\". Therefore we used the normalizePath() function to give us the full path of the file without having to type it out.\nCheck that the data is still identical:\n\nall(iris == iris2)"
  },
  {
    "objectID": "DataIO.html#output",
    "href": "DataIO.html#output",
    "title": "10  Data Input/Output",
    "section": "\n10.2 Output",
    "text": "10.2 Output\n\n10.2.1 Write to CSV\nUse the write.csv() function to write an R object (usually data frame or matrix) to a CSV file. Setting row.names = FALSE is usually a good idea. (Instead of storing data in rownames, it’s usually best to create a new column.)\n\nwrite.csv(iris, \"../Data/iris.csv\", row.names = FALSE)\n\nNote that in this case we did not need to save row names (which are just integers 1 to 150 and would add a useless extra column in the output)\n\n10.2.2 Write an R object to RDS\nYou can write any R object directly to file so that you can recover it at any time, share it, etc. Remember that since a list can contain any number of objects of any type, you can save any collection of objects as an RDS file. For multiple objects, see also the save.image() command below.\n\nsaveRDS(iris, \"iris.rds\")\n\nTo load an object saved in an rds file, assign it to an object using readRDS():\n\niris_fromFile <- readRDS(\"iris.rds\")\nall(iris == iris_fromFile)\n\n\n10.2.3 Write multiple R objects to RData file using save()\n\n\nmat1 <- sapply(seq_len(10), function(i) rnorm(500))\nmat2 <- sapply(seq_len(10), function(i) rnorm(500))\nsave(mat1, mat2, file = \"./mat.RData\")\n\nNote: we will learn how to use sapply() later under “Loop functions”\nTo load the variables in the .RData file you saved, use the load() command:\n\nload(\"./Rmd/mat.RData\")\n\nNote that load() adds the objects to your workspace using with their original names. You do not assign them to a new object, unlike with the readRDS() call above.\n\n10.2.4 Write your entire workspace to a RData image using save.image()\n\nYou can save your entire workspace to a RData file using the save.image() function.\n\nsave.image(\"workspace_10_05_2020.RData\")\n\nSame as above, to re-load the workspace saved in the .RData file, use the load() command:\n\nload(\"workspace_10_05_2020.RData\")"
  },
  {
    "objectID": "DataIO.html#resources",
    "href": "DataIO.html#resources",
    "title": "10  Data Input/Output",
    "section": "\n10.9 Resources",
    "text": "10.9 Resources\n\n\nR Data Import/Export by the R Core Team."
  },
  {
    "objectID": "DataIO.html#csv",
    "href": "DataIO.html#csv",
    "title": "10  Data Input/Output",
    "section": "\n10.1 CSV",
    "text": "10.1 CSV\n\n10.1.1 Read local CSV\nread.table() is the core function that reads data from formatted text files in R, where cases correspond to lines and variables to columns. Its many arguments allow to read different formats. read.csv() is an alias for read.table() that defaults to commas as separators and dots for decimal points. (Run read.csv in the console to print its source read the documentation with ?read.table).  Some important arguments for read.table() listed here with their default values for read.csv():\n\n\nsep = \",\": Character that separate entries. Default is a comma; use “ for tab-separated files (default setting in read.delim())\n\ndec = \".\": Character for the decimal point. Default is a dot; in some cases where a comma is used as the decimal point, the entry separator sep may be a semicolon (default setting in read.csv2())\n\nna.strings = \"NA\": Character vector of strings to be coded as “NA”\n\ncolClasses = NA: Either a character vector defining each column’s type (e.g. c(“character”, “numeric”, “numeric”) recycled as necessary or a named vector defining specific columns’ types (e.g. c(ICD9 = “character”, Sex = “factor”, SBP = “numeric”, DOB = “Date”)). Unspecified columns are automatically determined. Note: Set a column to “NULL” (with quotes) to exclude that column.\n\n\nmen <-  read.csv(\"../Data/pone.0204161.s001.csv\")\n\n\n10.1.2 Read CSV from the web\nread.csv() can directly read an online file. In the second example below, we also define that missing data is coded with ? using the na.strings argument:\n\nparkinsons <- read.csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data\")\n\nsleep <- read.csv(\"https://www.openml.org/data/get_csv/53273/sleep.arff\",\n                  na.strings = \"?\")\n\nThe above files are read from two very popular online data repositories. Confusingly, neither file ends in .csv, but they both work with read.csv(). You can always look at the plain text file to determine if it can work with read.table()/read.csv() and what settings to use.\n\n10.1.3 Read zipped data from the web\n\n10.1.3.1 using gzcon() and csv.read()\n\nread.table() /read.csv() also accepts a “connection” as input.\nHere we define a connection to a zipped file by nesting gzcon() and url():\n\ncon <- gzcon(url(\"https://github.com/EpistasisLab/pmlb/raw/master/datasets/breast_cancer_wisconsin/breast_cancer_wisconsin.tsv.gz\"),\n             text = TRUE)\n\nWe read the connection and specify the file is tab-separated, or call read.delim():\n\nbcw <- read.csv(con, header = TRUE, sep = \"\\t\")\n\n#same as\nbcw <- read.delim(con, header = TRUE)\n\n\n10.1.3.2 using data.table’s fread()\n\nYou can also use data.table’s fread(), which will directly handle zipped files:\n\nlibrary(data.table)\nbcw <- fread(\"https://github.com/EpistasisLab/penn-ml-benchmarks/raw/master/datasets/classification/breast-cancer-wisconsin/breast-cancer-wisconsin.tsv.gz\")\n\nIf you want fread() to return a data.frame instead of a data.table, you can set the data.table argument FALSE:\n\nbcw <- fread(\"https://github.com/EpistasisLab/penn-ml-benchmarks/raw/master/datasets/classification/breast-cancer-wisconsin/breast-cancer-wisconsin.tsv.gz\",\n              data.table = FALSE)\n\n\n10.1.4 Write to CSV\nUse the write.csv() function to write an R object (usually data frame or matrix) to a CSV file. Setting row.names = FALSE is usually a good idea. (Instead of storing data in rownames, it’s usually best to create a new column.)\n\nwrite.csv(iris, \"../Data/iris.csv\", row.names = FALSE)\n\nNote that in this case we did not need to save row names (which are just integers 1 to 150 and would add a useless extra column in the output)"
  },
  {
    "objectID": "DataIO.html#excel-.xlsx-files",
    "href": "DataIO.html#excel-.xlsx-files",
    "title": "10  Data Input/Output",
    "section": "\n10.2 Excel .XLSX files",
    "text": "10.2 Excel .XLSX files\nTwo popular packages to read Excel files are openxlsx and readxl."
  },
  {
    "objectID": "DataIO.html#read-.xslx",
    "href": "DataIO.html#read-.xslx",
    "title": "10  Data Input/Output",
    "section": "\n10.3 Read .xslx",
    "text": "10.3 Read .xslx\n\n10.3.1 openxlsx::read.xlsx()\n\n\nbcw <- openxlsx::read.xlsx(\"../Data/bcw.xlsx\")\n\n\n10.3.2 readxl::read_xlsx()\n\n\nbcw <- readxl::read_xlsx(\"../Data/bcw.xlsx\")"
  },
  {
    "objectID": "DataIO.html#write-.xlsx",
    "href": "DataIO.html#write-.xlsx",
    "title": "10  Data Input/Output",
    "section": "\n10.4 Write .xlsx",
    "text": "10.4 Write .xlsx\n\n10.4.1 openxlsx::write.xlsx()\n\n\nopenxlsx::write.xlsx(bcw, \"../Data/bcw.xlsx\")\n\nNote: The readxl package does not include a function to write .XLSX files"
  },
  {
    "objectID": "DataIO.html#rds",
    "href": "DataIO.html#rds",
    "title": "10  Data Input/Output",
    "section": "\n10.5 RDS",
    "text": "10.5 RDS\n\n10.5.1 Write single R object to an RDS file\nYou can write any R object directly to file so that you can recover it at any time, share it, etc. Remember that since a list can contain any number of objects of any type, you can save essentially any collection of objects in an RDS file. For multiple objects, see also the save.image() command below.\n\nsaveRDS(bcw, \"bcw.rds\")\n\n\n10.5.2 Read single object from an RDS file\nTo load an object saved in an rds file, assign it to an object using readRDS():\n\niris_fromFile <- readRDS(\"iris.rds\")\nall(iris == iris_fromFile)"
  },
  {
    "objectID": "DataIO.html#rdata",
    "href": "DataIO.html#rdata",
    "title": "10  Data Input/Output",
    "section": "\n10.6 RData",
    "text": "10.6 RData\n\n10.6.1 Write multiple R objects to an RData file\nYou can use the save() function to save multiple R objects to a single .RData file:\n\nmat1 <- sapply(seq_len(10), function(i) rnorm(500))\nmat2 <- sapply(seq_len(10), function(i) rnorm(500))\nsave(mat1, mat2, file = \"./mat.RData\")\n\nNote: we will learn how to use sapply() later under Loop functions\nTo load the variables in the RData file you saved, use the load() command:\n\nload(\"./Rmd/mat.RData\")\n\nNote that load() adds the objects to your workspace using their original names. You do not assign them to a new object, unlike with the readRDS() call above.\n\n10.6.2 Write your entire workspace to an RData file\nYou can save your entire workspace to a RData file using the save.image() function:\n\nsave.image(\"workspace_2022-08-10.RData\")\n\nSame as above, to re-load the workspace saved in the .RData file, use the load() command:\n\nload(\"workspace_2022-08-10.RData\")"
  },
  {
    "objectID": "DataIO.html#read-other-common-third-party-formats",
    "href": "DataIO.html#read-other-common-third-party-formats",
    "title": "10  Data Input/Output",
    "section": "\n10.8 Read other common third-party formats",
    "text": "10.8 Read other common third-party formats\n\n\n\n\n\n\n\nFile type\nFile Extension\npackage::function\n\n\nJSON\n.json\n\njsonlite::read_json()  | | Stata data         |.dta|haven::read_dta()| | SPSS data          |.por,.sav,.zsav|haven::read_spss()| | SAS transport file |.xpt|haven::read_xpt()| | Matlab data        |.mat|  | Apache Arrow       |.arrow|arrow::read_arrow()| | Apache Arrow       |.feather|arrow::read_feather()| | Apache Arrow       |.parquet|arrow::read_parquet`()"
  },
  {
    "objectID": "DataIO.html#read-other-common-tabular-data-third-party-formats",
    "href": "DataIO.html#read-other-common-tabular-data-third-party-formats",
    "title": "10  Data Input/Output",
    "section": "\n10.8 Read other common tabular data third-party formats",
    "text": "10.8 Read other common tabular data third-party formats\n\n\n\n\n\n\n\nFile type\nFile Extension\npackage::function\n\n\n\nJSON\n.json\njsonlite::read_json()\n\n\nStata data\n.dta\nhaven::read_dta()\n\n\nSPSS data\n\n.por, .sav, .zsav\n\nhaven::read_spss()\n\n\nSAS transport file\n.xpt\nhaven::read_xpt()\n\n\nMatlab data\n.mat\nrmatio::read.mat()\n\n\nApache Arrow\n.arrow\narrow::read_arrow()\n\n\nApache Arrow\n.feather\narrow::read_feather()\n\n\nApache Arrow\n.parquet\narrow::read_parquet()"
  },
  {
    "objectID": "BasicOps.html#common-statistical-operations",
    "href": "BasicOps.html#common-statistical-operations",
    "title": "5  Basic operations",
    "section": "\n5.3 Common statistical operations",
    "text": "5.3 Common statistical operations\nFirst, let’s use the rnorm() function to draw 200 numbers at random from a normal distribution:\n\nx <- rnorm(200)\n\n\n5.3.1 Descriptive statistics\nmean, median, standard deviation, minimum, maximum, and range:\n\nmean(x)\n\n[1] 0.02334843\n\nmedian(x)\n\n[1] -0.008372719\n\nsd(x)\n\n[1] 1.030247\n\nmin(x)\n\n[1] -2.558759\n\nmax(x)\n\n[1] 2.590201\n\nrange(x)\n\n[1] -2.558759  2.590201\n\n\n\nsummary(x)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-2.558759 -0.769125 -0.008373  0.023348  0.748230  2.590201 \n\n\n\n5.3.2 Sampling\nR’s sample() allows you to samples elements of an R object with or without replacement:\nBy default, the replace argument is set to FALSE, i.e. sampling is performed without replacement and you can request a sample size up to the length of the object:\n\nx <- 21:30\nsample(x, size = 5)\n\n[1] 30 22 26 23 24\n\n\nsize is the second argument and therefore can be ommited if it is the second value you pass to sample():\n\nsample(x, 10)\n\n [1] 29 27 30 24 28 25 21 23 22 26\n\n\nSetting replace = TRUE performs sampling with replacement and you can set a sample size up to\n\nsample(x, 100, replace = TRUE)\n\n  [1] 23 26 29 29 21 26 26 23 25 26 24 26 26 23 26 24 25 29 28 21 22 30 30 28 22\n [26] 28 24 30 25 26 25 26 27 22 29 26 22 28 29 23 23 21 29 21 24 28 29 21 28 21\n [51] 29 24 28 30 26 21 21 25 29 23 25 30 22 29 27 22 29 26 30 24 22 21 29 30 29\n [76] 24 28 22 24 29 27 27 28 27 21 30 25 26 28 28 25 27 25 22 21 22 27 25 21 30\n\n\nreplace is the third argument and could therefore be omitted in the example above"
  }
]