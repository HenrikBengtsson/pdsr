# Supervised Learning {#supervised}

```{r echo = FALSE}
knitr::opts_chunk$set(fig.width = 5, fig.height = 5,
                      comment = NA, cache = TRUE)
options(rt.theme = "lightgrid")
options(rt.fit.theme = "lightgrid")
```

```{r, comment="", results="asis", echo=FALSE}
old.hooks <- fansi::set_knit_hooks(knitr::knit_hooks)
options(crayon.enabled = TRUE)
```

This is a very brief introduction to machine learning using the [**rtemis**](https://rtemis.lambdamd.org) package. **rtemis** includes functions for:

- Visualization: static & interactive plots
- Data preprocessing
- Unsupervised learning: clustering & dimensionality reduction
- Supervised learning: regression & classification

## Installation

install the **remotes** package, if you don't have it: 

```{r eval=FALSE}
install.packages("remotes")
```

Install **rtemis**:

```{r eval=FALSE}
remotes::install_github("egenn/rtemis")
```

**rtemis** uses a large number of packages under the hood. Since you would not need to use all of them, they are not installed by default. Each time an **rtemis** function is called, a dependency check is run and a message is printed if any packages need to be installed. 

For this short tutorial, start by installing **ranger**, if it is not already installed on your system:

```{r eval=FALSE}
install.packages("ranger")
```

Load **rtemis**:

```{r lib}
library(rtemis)
```

## Data Input for Supervised Learning

All __rtemis__ supervised learning functions begin with `s.` ("supervised").  
They accept the same first four arguments:  
`x`, `y`, `x.test`, `y.test`  
but are flexible and allow you to also provide combined (x, y) and (x.test, y.test) data frames, as explained below.

### Scenario 1 (`x.train, y.train, x.test, y.test`)

In the most straightforward case, provide each featureset and outcome individually:  

* `x`: Training set features
* `y`: Training set outcome
* `x.test`: Testing set features (Optional)
* `y.test`: Testing set outcome (Optional)

```{r}
x <- rnormmat(200, 10, seed = 2019)
w <- rnorm(10)
y <- x %*% w + rnorm(200)
res <- resample(y, seed = 2020)
x.train <- x[res$Subsample_1, ]
x.test <- x[-res$Subsample_1, ]
y.train <- y[res$Subsample_1]
y.test <- y[-res$Subsample_1]
```

```{r}
mod.glm <- s.GLM(x.train, y.train, x.test, y.test)
```

### Scenario 2: (`x.train, x.test`)

You can provide training and testing sets as a single data.frame each, where the last column is the outcome. Now `x` is the full training data and `y` the full testing data:  

* `x`: data.frame(x.train, y.train)
* `y`: data.frame(x.test, y.test)

```{r}
x <- rnormmat(200, 10, seed = 2019)
w <- rnorm(10)
y <- x %*% w + rnorm(200)
dat <- data.frame(x, y)
res <- resample(dat, seed = 2020)
dat.train <- dat[res$Subsample_1, ]
dat.test <- dat[-res$Subsample_1, ]
```

```{r}
mod.glm <- s.GLM(dat.train, dat.test)
```

The `dataPrepare()` function will check data dimensions and determine whether data was input as separate feature and outcome sets or combined and ensure the correct number of cases and features was provided.  

In either scenario, Regression will be performed if the outcome is numeric and Classification if the outcome is a factor.

## Regression

### Check Data with `checkData()`

```{r synth reg data}
x <- rnormmat(500, 50, seed = 2019)
w <- rnorm(50)
y <- x %*% w + rnorm(500)
dat <- data.frame(x, y)
res <- resample(dat)
dat.train <- dat[res$Subsample_1, ]
dat.test <- dat[-res$Subsample_1, ]
```

```{r}
checkData(x)
```

### Single Model

```{r}
mod <- s.GLM(dat.train, dat.test)
```

### Crossvalidated Model

```{r}
mod <- elevate(dat, mod = "glm")
```

Use the `describe()` function to get a summary in (plain) English:

```{r}
mod$describe()
```

```{r, fig.width = 5, fig.height = 5.5}
mod$plot()
```

## Classification

### Check Data

```{r}
data(Sonar, package = 'mlbench')
checkData(Sonar)
res <- resample(Sonar)
sonar.train <- Sonar[res$Subsample_1, ]
sonar.test <- Sonar[-res$Subsample_1, ]
```

### Single model

```{r}
mod <- s.RANGER(sonar.train, sonar.test)
```

### Crossvalidated Model

```{r}
mod <- elevate(Sonar)
```

```{r}
mod$describe()
```

```{r, fig.width = 5, fig.height = 5.5}
mod$plot()
```

```{r}
mod$plotROC()
```

```{r}
mod$plotPR()
```

### Evaluation of a binary classifier

```{r Classification, echo = FALSE, out.width = "100%", fig.align = 'center', fig.cap = "Evaluation metrics for a binary classifier"}
knitr::include_graphics("./Classification.png")
```

## Understanding Overfitting

Overfitting occurs when a model fits noise in the outcome. To make this clear, consider the following example:

Assume a random variable **x**:

```{r}
set.seed(2020)
x <- sort(rnorm(500))
```

and a data-generating function `fn()`:

```{r}
fn <- function(x) 12 + x^5
```

The true y is therefore equal to `fn(x)`:

```{r}
y_true <- fn(x)
```

However, assume y is recorded with some noise, in this case gaussian:

```{r}
y_noise <- fn(x) + rnorm(500, sd = sd(y_true))
```

We plot:

```{r}
mplot3.xy(x, list(y_noise = y_noise, y_true = y_true))
```

We want to find a model that best approximates `y_true`, but we only know `y_noise`.  

A maximally overfitted model would model noise perfectly:

```{r}
mplot3.xy(x, list(Overfitted_model = y_noise, Ideal_model = y_true), type = "l")
```

An example of an SVM set up to overfit heavily:

```{r}
mplot3.xy(x, y_noise, fit = "svm",
          fit.params = list(kernel = "radial", cost = 100, gamma = 100))
```

An example of a good approximation of `fn` using a GAM with penalized splines:

```{r}
mplot3.xy(x, y_noise, fit = "gam")
```

## **rtemis** Documentation

For more information on using **rtemis**, see the [rtemis online documentation and vignettes](https://rtemis.lambdamd.prg)
