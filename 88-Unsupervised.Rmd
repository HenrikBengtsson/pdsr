# Unsupervised Learning {#unsupervised}

```{r echo = FALSE}
knitr::opts_chunk$set(fig.width = 5, fig.height = 5,
                      comment = NA, cache = TRUE) 
options(rt.theme = "whitegrid")
options(rt.fit.theme = "whitegrid")
```

```{r, comment="", results="asis", echo=FALSE}
old.hooks <- fansi::set_knit_hooks(knitr::knit_hooks)
options(crayon.enabled = TRUE)
```

```{r echo = FALSE}
library(rtemis)
```

Unsupervised learning aims to learn relationships within a dataset without focusing at a particular outcome. You will often hear of unsupervised learning being performed on unlabeled data. To be clear, it means it does not use the labels to guide learning - whether labels are available or not. You might, for example, perform unsupervised learning ahead of supervised learning as we shall see later. Unsupervised learning includes a number of approaches, most of which can be divided into two categories:

* __Clustering__: Cases are grouped together based on some derived measure of similarity / distance metric.
* __Dimensionality Reduction / Matrix decomposition__: Variables are combined / projected into a lower dimensional space.

In **rtemis**, clustering algorithms begin with `u.` and decomposition/dimensionality reduction algorithms begin with `d.` (We use `u.` because `c.` is reserved for the builtin R function)

## Decomposition / Dimensionality Reduction {#decomposition}

Use `decomSelect()` to get a listing of available decomposition algorithms:
```{r}
decomSelect()
```

We can further divide decomposition algorithms into linear (e.g. PCA, ICA, NMF) and nonlinear dimensionality reduction, (also called manifold learning, like LLE and tSNE).

### Principal Component Analysic (PCA)

```{r}
x <- iris[, 1:4]
iris.PCA <- d.PCA(x)
mplot3.xy(iris.PCA$projections.train[, 1], iris.PCA$projections.train[, 2], group = iris$Species,
          xlab = "1st PCA component", ylab = "2nd PCA component", main = "PCA on iris")
```

### Independent Component Analysis (ICA)

```{r}
iris.ICA <- d.ICA(x, k = 2)
mplot3.xy(iris.ICA$projections.train[, 1], iris.ICA$projections.train[, 2], group = iris$Species,
          xlab = "1st ICA component", ylab = "2nd ICA component", main = "ICA on iris")
```

### Non-negative Matrix Factorization (NMF)

```{r}
iris.NMF <- d.NMF(x, k = 2)
mplot3.xy(iris.NMF$projections.train[, 1], iris.NMF$projections.train[, 2], group = iris$Species,
          xlab = "1st NMF component", ylab = "2nd NMF component", main = "NMF on iris")
```

## Clustering

Use `clustSelect()` to get a listing of available clustering algorithms:
```{r}
clustSelect()
```

Let's cluster iris and we shall also use an NMF decomposition as we saw above to project to 2 dimensions.  
We'll use two of the most popular clustering algorithms, K-means and PAM, aka K-medoids.

```{r}
x <- iris[, 1:4]
iris.NMF <- d.NMF(x, k = 2)
```

### K-Means

```{r}
iris.KMEANS <- u.KMEANS(x, k = 3)
mplot3.xy(iris.NMF$projections.train[, 1], iris.NMF$projections.train[, 2],
          group = iris.KMEANS$clusters.train,
          xlab = "1st NMF component", ylab = "2nd NMF component", main = "KMEANS on iris")
```

### Partitioning Around Medoids with k estimation (PAMK)

```{r}
iris.pamk <- u.PAMK(x, krange = 3:10)
mplot3.xy(iris.NMF$projections.train[, 1], iris.NMF$projections.train[, 2],
          group = iris.pamk$clusters.train,
          xlab = "1st NMF component", ylab = "2nd NMF component", main = "PAM on iris")
```
