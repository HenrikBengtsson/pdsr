[
  {
    "objectID": "48_StringOps.html#reminder-create-coerce-check",
    "href": "48_StringOps.html#reminder-create-coerce-check",
    "title": "20  String Operations",
    "section": "20.1 Reminder: create, coerce, check",
    "text": "20.1 Reminder: create, coerce, check\n\ncharacter(): Initialize empty character vector\nas.character(): Coerce any vector to a character vector\nis.character(): Check object is character\n\n\nx <- character(10)\nx\n\n [1] \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\n\n\n\nv <- c(10, 20, 22, 43)\nx <- as.character(v)\nx\n\n[1] \"10\" \"20\" \"22\" \"43\"\n\n\n\nx <- c(\"PID\", \"Age\", \"Sex\", \"Handedness\")\nis.character(x)\n\n[1] TRUE"
  },
  {
    "objectID": "48_StringOps.html#cat-concatenate-and-print",
    "href": "48_StringOps.html#cat-concatenate-and-print",
    "title": "20  String Operations",
    "section": "20.2 cat(): Concatenate and print",
    "text": "20.2 cat(): Concatenate and print\ncat() concatenates strings in order to print to screen (console) or to file.\nIt does not return any object. It is therefore useful to produce informative messages in your programs.\n\nsbp <- 130\ntemp <- 98.4\ncat(\"The blood pressure was\", sbp, \"and the temperature was\", temp, \"\\n\")\n\nThe blood pressure was 130 and the temperature was 98.4 \n\n\nUse the file argument to write to a text file. The append argument allows using multiple cat() calls to append to the same file."
  },
  {
    "objectID": "48_StringOps.html#paste-concatenate-character-vectors",
    "href": "48_StringOps.html#paste-concatenate-character-vectors",
    "title": "20  String Operations",
    "section": "20.3 paste(): Concatenate character vectors",
    "text": "20.3 paste(): Concatenate character vectors\npaste() is a commonly used command.\nIn its simplest form, it acts like as.character():\n\nv <- c(10, 20, 22, 43)\npaste(v)\n\n[1] \"10\" \"20\" \"22\" \"43\"\n\n\nBut its main job is to combine strings from multiple vectors elementwise:\n\nid = c(\"001\", \"010\", \"018\", \"020\", \"021\", \"051\")\ndept = c(\"Emergency\", \"Cardiology\", \"Neurology\",\n         \"Anesthesia\", \"Surgery\", \"Psychiatry\")\nid\n\n[1] \"001\" \"010\" \"018\" \"020\" \"021\" \"051\"\n\ndept\n\n[1] \"Emergency\"  \"Cardiology\" \"Neurology\"  \"Anesthesia\" \"Surgery\"   \n[6] \"Psychiatry\"\n\n\n\npaste(id, dept)\n\n[1] \"001 Emergency\"  \"010 Cardiology\" \"018 Neurology\"  \"020 Anesthesia\"\n[5] \"021 Surgery\"    \"051 Psychiatry\"\n\n\nThe sep argument defines the separator:\n\npaste(id, dept, sep = \"+++\")\n\n[1] \"001+++Emergency\"  \"010+++Cardiology\" \"018+++Neurology\"  \"020+++Anesthesia\"\n[5] \"021+++Surgery\"    \"051+++Psychiatry\"\n\n\npaste0() is an alias for the commonly used paste(..., sep = \"\"):\n\npaste0(id, dept)\n\n[1] \"001Emergency\"  \"010Cardiology\" \"018Neurology\"  \"020Anesthesia\"\n[5] \"021Surgery\"    \"051Psychiatry\"\n\n\nAs with other vectorized operations, value recycling can be very convenient:\n\npaste0(\"Feature_\", 1:10)\n\n [1] \"Feature_1\"  \"Feature_2\"  \"Feature_3\"  \"Feature_4\"  \"Feature_5\" \n [6] \"Feature_6\"  \"Feature_7\"  \"Feature_8\"  \"Feature_9\"  \"Feature_10\"\n\n\nThe argument collapse helps output a single character element after collapsing with some string:\n\npaste0(\"Feature_\", 1:10, collapse = \", \")\n\n[1] \"Feature_1, Feature_2, Feature_3, Feature_4, Feature_5, Feature_6, Feature_7, Feature_8, Feature_9, Feature_10\""
  },
  {
    "objectID": "48_StringOps.html#nchar-get-number-of-characters-in-element",
    "href": "48_StringOps.html#nchar-get-number-of-characters-in-element",
    "title": "20  String Operations",
    "section": "20.4 nchar(): Get number of characters in element",
    "text": "20.4 nchar(): Get number of characters in element\nnchar() counts the number of characters in each element of type character in a vector:\n\nx <- c(\"a\", \"bb\", \"ccc\")\nnchar(x)\n\n[1] 1 2 3"
  },
  {
    "objectID": "48_StringOps.html#substr-get-substring",
    "href": "48_StringOps.html#substr-get-substring",
    "title": "20  String Operations",
    "section": "20.5 substr(): Get substring",
    "text": "20.5 substr(): Get substring\nsubstr() allows you to get and set individual (literal) characters from a character (R base type) vector, by position:\n\n20.5.1 Extract\ne.g. Get characters 1-3:\n\nx <- c(\"001Emergency\", \"010Cardiology\", \"018Neurology\", \n       \"020Anesthesia\", \"021Surgery\", \"051Psychiatry\")\nsubstr(x, start = 1, stop = 3)\n\n[1] \"001\" \"010\" \"018\" \"020\" \"021\" \"051\"\n\n\nNeither start nor stop need to be valid character positions.\nFor example, if you want to get all characters from the fourth one to the last one, you can specify a very large stop\n\nsubstr(x, 4, 99)\n\n[1] \"Emergency\"  \"Cardiology\" \"Neurology\"  \"Anesthesia\" \"Surgery\"   \n[6] \"Psychiatry\"\n\n\nIf you start with too high an index, you end up with empty strings:\n\nsubstr(x, 20, 24)\n\n[1] \"\" \"\" \"\" \"\" \"\" \"\"\n\n\nNote: substring() is also available, with similar syntax to substr(): (first, last) instead of (start, stop). It is available for compatibility with S (check its source code to see how it’s an alias for substr())\n\n\n20.5.2 Replace\n\nx <- c(\"Jan_1987\")\nx\n\n[1] \"Jan_1987\"\n\n\nReplace the first three letters:\n\nsubstr(x, 1, 3) <- \"Feb\"\nx\n\n[1] \"Feb_1987\"\n\n\nNote that if the replacement is longer, it is “cropped” to the length of the substring being replaced:\n\nsubstr(x, 1, 3) <- \"April\"\nx\n\n[1] \"Apr_1987\""
  },
  {
    "objectID": "48_StringOps.html#strsplit-split-strings",
    "href": "48_StringOps.html#strsplit-split-strings",
    "title": "20  String Operations",
    "section": "20.6 strsplit(): Split strings",
    "text": "20.6 strsplit(): Split strings\nstrsplit() allows you to split a character vector elements based on any character or regular expression\n\nx <- \"This is one sentence\"\nstrsplit(x, \" \")\n\n[[1]]\n[1] \"This\"     \"is\"       \"one\"      \"sentence\"\n\n\n\nx <- \"14,910\"\nstrsplit(x, \",\")\n\n[[1]]\n[1] \"14\"  \"910\"\n\n\nAs with any functions, you can compose string operations in complex ways (though it may often be considerably easier to perform multiple separate operations instead):\n\nx <- c(\"1,950\", \"2,347\")\nx\n\n[1] \"1,950\" \"2,347\"\n\n\n\nlapply(strsplit(x, \",\"), function(i) \n  paste(i, c(\"thousand\", \"dollars\"), collapse = \" and \"))\n\n[[1]]\n[1] \"1 thousand and 950 dollars\"\n\n[[2]]\n[1] \"2 thousand and 347 dollars\""
  },
  {
    "objectID": "48_StringOps.html#string-formatting",
    "href": "48_StringOps.html#string-formatting",
    "title": "20  String Operations",
    "section": "20.7 String formatting",
    "text": "20.7 String formatting\n\n20.7.1 Change case with toupper() and tolower()\n\nfeatures <- c(\"id\", \"age\", \"sex\", \"sbp\", \"dbp\", \"hct\", \"urea\", \"creatinine\")\nfeatures\n\n[1] \"id\"         \"age\"        \"sex\"        \"sbp\"        \"dbp\"       \n[6] \"hct\"        \"urea\"       \"creatinine\"\n\n\n\nfeatures_upper <- toupper(features)\nfeatures_upper\n\n[1] \"ID\"         \"AGE\"        \"SEX\"        \"SBP\"        \"DBP\"       \n[6] \"HCT\"        \"UREA\"       \"CREATININE\"\n\n\n\nfeatures_lower <- tolower(features_upper)\nfeatures_lower\n\n[1] \"id\"         \"age\"        \"sex\"        \"sbp\"        \"dbp\"       \n[6] \"hct\"        \"urea\"       \"creatinine\"\n\n\n\n\n20.7.2 abbreviate()\nabbreviate() allows to reduce elements of a character vector to short, unique abbreviations of a minimumn length (defaults to 4)\n\nx <- c(\"Emergency\", \"Cardiology\", \"Surgery\", \"Anesthesia\", \"Neurology\", \"Psychiatry\", \"Clinical Psychology\")\nabbreviate(x)\n\n          Emergency          Cardiology             Surgery          Anesthesia \n             \"Emrg\"              \"Crdl\"              \"Srgr\"              \"Anst\" \n          Neurology          Psychiatry Clinical Psychology \n             \"Nrlg\"              \"Psyc\"              \"ClnP\" \n\nabbreviate(x, minlength = 4)\n\n          Emergency          Cardiology             Surgery          Anesthesia \n             \"Emrg\"              \"Crdl\"              \"Srgr\"              \"Anst\" \n          Neurology          Psychiatry Clinical Psychology \n             \"Nrlg\"              \"Psyc\"              \"ClnP\" \n\nabbreviate(x, minlength = 5)\n\n          Emergency          Cardiology             Surgery          Anesthesia \n            \"Emrgn\"             \"Crdlg\"             \"Srgry\"             \"Ansth\" \n          Neurology          Psychiatry Clinical Psychology \n            \"Nrlgy\"             \"Psych\"             \"ClncP\""
  },
  {
    "objectID": "48_StringOps.html#pattern-matching",
    "href": "48_StringOps.html#pattern-matching",
    "title": "20  String Operations",
    "section": "20.8 Pattern matching",
    "text": "20.8 Pattern matching\nA very common task in programming is to find +/- replace string patterns in a vector of strings.\ngrep() and grepl() help find strings that contain a given pattern.\nsub() and gsub() help find and replace strings.\n\n20.8.1 grep(): Get an integer index of elements that include a pattern\n\nx <- c(\"001Age\", \"002Sex\", \"010Temp\", \"014SBP\", \"018Hct\", \"022PFratio\", \"030GCS\", \"112SBP-DBP\")\ngrep(pattern = \"SBP\", x = x)\n\n[1] 4 8\n\n\ngrep()’s value arguments which defaults to FALSE, allows returning the matched string itself (the value of the element) instead of its integer index:\n\ngrep(\"SBP\", x, value = TRUE)\n\n[1] \"014SBP\"     \"112SBP-DBP\"\n\n\n\n\n20.8.2 grepl(): Get a logical index of elements that include a pattern\ngrepl() is similar to grep(), but returns a logical index instead:\n\ngrepl(\"SBP\", x)\n\n[1] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE\n\n\n\n\n20.8.3 sub(): Find replace first match of a pattern\n\nx <- c(\"The most important variable was PF ratio. Other significant variables are listed in the supplementary information.\")\nsub(pattern = \"variable\", replacement = \"feature\", x = x)\n\n[1] \"The most important feature was PF ratio. Other significant variables are listed in the supplementary information.\"\n\n\n“First match” refers to each element of a character vector:\n\nx <- c(\"var 1, var 2\", \"var 3, var 4\")\nsub(\"var\", \"feat\", x)\n\n[1] \"feat 1, var 2\" \"feat 3, var 4\"\n\n\n\n\n20.8.4 gsub(): Find and replace all matches of a pattern\n\nx <- c(\"The most important variable was PF ratio. Other significant variables are listed in the supplementary information.\")\ngsub(pattern = \"variable\", replacement = \"feature\", x = x)\n\n[1] \"The most important feature was PF ratio. Other significant features are listed in the supplementary information.\"\n\n\n“All matches” means all matches across all elements:\n\nx <- c(\"var 1, var 2\", \"var 3, var 4\")\ngsub(\"var\", \"feat\", x)\n\n[1] \"feat 1, feat 2\" \"feat 3, feat 4\""
  },
  {
    "objectID": "48_StringOps.html#regex",
    "href": "48_StringOps.html#regex",
    "title": "20  String Operations",
    "section": "20.9 Regular expressions",
    "text": "20.9 Regular expressions\nRegular expressions allow you to perform flexible pattern matching. For example, you can look for a pattern specifically at the beginning or the end of a word, or for a variable pattern with certain characteristics.\nRegular expressions are very powerful and heavily used. They exist in multiple programming languages - with many similarities and some differences in their syntax.\nThere are many rules in defining regular expressions and take a little getting used to. You can read the R manual by typing ?base::regex.\nSome of the most important rules are liste below:\n\n20.9.1 Match a pattern at the beginning of a line/string with ^/\\\\<:\nUse the caret sign ^ in the beginning of a pattern to only match strings that begin with this pattern.\npattern 012 matches both 2nd and 3rd elements:\n\nx <- c(\"001xyz993\", \"012qwe764\", \"029aqw012\")\nx\n\n[1] \"001xyz993\" \"012qwe764\" \"029aqw012\"\n\ngrep(\"012\", x)\n\n[1] 2 3\n\n\nBy adding ^ or \\\\<, only the 2nd element matches:\n\ngrep(\"^012\", x)\n\n[1] 2\n\ngrep(\"\\\\<012\", x)\n\n[1] 2\n\n\n\n\n20.9.2 Match a pattern at the end of a line/string with $/\\\\>\nThe dollar sign $ is used at the end of a pattern to only match strings which end with this pattern:\n\nx\n\n[1] \"001xyz993\" \"012qwe764\" \"029aqw012\"\n\ngrep(\"012$\", x)\n\n[1] 3\n\ngrep(\"012\\\\>\", x)\n\n[1] 3\n\n\n\nx <- c(\"1one\", \"2one\", \"3two\", \"3three\")\ngrep(\"one$\", x)\n\n[1] 1 2\n\ngrep(\"one\\\\>\", x)\n\n[1] 1 2\n\n\n\n\n20.9.3 .: Match any character\n\ngrep(\"e.X\", c(\"eX\", \"enX\", \"ennX\", \"ennnX\", \"ennnnX\"))\n\n[1] 2\n\n\n\n\n20.9.4 +: Match preceding character one or more times:\n\ngrep(\"en+X\", c(\"eX\", \"enX\", \"ennX\", \"ennnX\", \"ennnnX\"))\n\n[1] 2 3 4 5\n\n\n\n\n20.9.5 {n}: Match preceding character n times:\n\ngrep(\"en{2}X\", c(\"eX\", \"enX\", \"ennX\", \"ennnX\", \"ennnnX\"))\n\n[1] 3\n\n\n\n\n20.9.6 {n,}: Match preceding character n or more times:\n\ngrep(\"en{2,}X\", c(\"eX\", \"enX\", \"ennX\", \"ennnX\", \"ennnnX\"))\n\n[1] 3 4 5\n\n\n\n\n20.9.7 {n,m}: Match preceding character at least n times and no more than m times:\n\ngrep(\"en{2,3}X\", c(\"eX\", \"enX\", \"ennX\", \"ennnX\", \"ennnnX\"))\n\n[1] 3 4\n\n\n\n\n20.9.8 Character classes\nYou can define a set of characters to be matched using square brackets. Any number of the characters in the set will be matched.\nFor example match and replace $ and/or @ with an underscore:\n\nx <- c(\"Feat1$alpha\", \"Feat2$gamma@5\", \"Feat9@zeta2\")\ngsub(\"[$@]\", \"_\", x)\n\n[1] \"Feat1_alpha\"   \"Feat2_gamma_5\" \"Feat9_zeta2\"  \n\n\n\n20.9.8.1 Predefined character classes\nA number of character classes are predefined. Slightly confusingly, they are themselves surrounded by brackets and to use them as a character class, you need a seconds set of brackets around them. Some of the most common ones include:\n\n[:alnum:]: alphanumeric, i.e. all letters and numbers\n[:alpha:]: all letters\n[:digit:]: all numbers\n[:lower:]: all lowercase letters\n[:upper:]: all uppercase letters\n[:punct:]: all punctuation characters (! ” # $ % & ’ ( ) * + , - . / : ; < = > ? @ [  ] ^ _ ` { | } ~.)\n[:blank:]: all spaces and tabs\n[:space:]: all spaces, tabs, newline characters, and some more\n\nLet’s look at some examples using them.\nHere we use [:digit:] to remove all numbers:\n\nx <- c(\"001Emergency\", \"010Cardiology\", \"018Neurology\", \"020Anesthesia\", \n       \"021Surgery\", \"051Psychiatry\")\nx\n\n[1] \"001Emergency\"  \"010Cardiology\" \"018Neurology\"  \"020Anesthesia\"\n[5] \"021Surgery\"    \"051Psychiatry\"\n\ngsub(\"[[:digit:]]\", \"\", x)\n\n[1] \"Emergency\"  \"Cardiology\" \"Neurology\"  \"Anesthesia\" \"Surgery\"   \n[6] \"Psychiatry\"\n\n\nWe can use [:alpha:] to remove all letters instead:\n\ngsub(\"[[:alpha:]]\", \"\", x)\n\n[1] \"001\" \"010\" \"018\" \"020\" \"021\" \"051\"\n\n\nWe can use a caret ^ in the beginning of a character class to match any character not in the character set:\n\nx <- c(\"001$Emergency\", \"010@Cardiology\", \"018*Neurology\", \"020!Anesthesia\", \n       \"021!Surgery\", \"051*Psychiatry\")\ngsub(\"[^[:alnum:]]\", \"_\", x)\n\n[1] \"001_Emergency\"  \"010_Cardiology\" \"018_Neurology\"  \"020_Anesthesia\"\n[5] \"021_Surgery\"    \"051_Psychiatry\"\n\n\n\n\n\n20.9.9 Combining character classes\nUse | to match from multiple character classes:\n\nx <- c(\"123#$%alphaBeta\")\ngsub(\"[[:digit:]|[:punct:]]\", \"\", x)\n\n[1] \"alphaBeta\"\n\n\n\n\n\nFor more information on regular expressions, start by reading the built-in documentation: ?regex\n\n\n\n\n\n20.9.10 Escaping metacharacters\nMetacharacters are characters that have a special meaning within a regular expression. They include:\n. \\ | ( ) [ { ^ $ * + ?.\nFor example, we have seen above that the period matches any character and the square brackets are used to define character classes If you want to match one of these characters itself, you must “escape” it using a double backslash. Escaping a character simply means “this is not part of a regular expression, match it as is”.\nFor example, to match a period (.) and replace it with underscores:\n\nx <- c(\"systolic.blood.pressure\", \"diastolic.blood.pressure\")\nx\n\n[1] \"systolic.blood.pressure\"  \"diastolic.blood.pressure\"\n\ngsub(\"\\\\.\", \"_\", x)\n\n[1] \"systolic_blood_pressure\"  \"diastolic_blood_pressure\"\n\n\nIf we didn’t escape the period above, it would have matched every character:\n\ngsub(\".\", \"_\", x)\n\n[1] \"_______________________\"  \"________________________\"\n\n\nAnother example, include an escaped metacharacter within a regular expression. In the example below we want to remove everything up to and including the dollar sign:\n\nx <- c(\"df$ID\", \"df$Age\")\ngsub(\".*\\\\$\", \"\", x)\n\n[1] \"ID\"  \"Age\"\n\n\nOur regular expression .*\\\\$, decomposed:\n\n.: match any character\n.*: match any character any number of times\n.*\\\\$: match any character any number of times till you find a dollar sign\n\nIf we had not escaped the $, it wouldn’t have worked:\n\ngsub(\".*$\", \"\", x)\n\n[1] \"\" \"\""
  },
  {
    "objectID": "50_DateTime.html",
    "href": "50_DateTime.html",
    "title": "21  Date and Time",
    "section": "",
    "text": "R includes builtin support for working with date +/- time data. A number of external packages further extend this support.\nThere are three builtin classes:\nBackground info: Portable Operating System Interface (POSIX) is a set of standards for maintaining compatibility among operating systems."
  },
  {
    "objectID": "50_DateTime.html#date-objects",
    "href": "50_DateTime.html#date-objects",
    "title": "21  Date and Time",
    "section": "21.1 Date objects",
    "text": "21.1 Date objects\n\n21.1.1 Character to Date: as.Date()\nYou can create a Date object from a string:\n\nx <- as.Date(\"1981-02-12\")\nx\n\n[1] \"1981-02-12\"\n\nclass(x)\n\n[1] \"Date\"\n\n\nThe tryFormats argument defines which formats are recognized.\nThe default is tryFormats = c(\"%Y-%m-%d\", \"%Y/%m/%d\"), i.e. a date of the form “2020-11-16” or “2020/11/16”\n\n\n21.1.2 Get current date & time\nGet current data:\n\ntoday <- Sys.Date()\ntoday\n\n[1] \"2022-05-10\"\n\nclass(today)\n\n[1] \"Date\"\n\n\nGet current date and time:\n\nnow <- Sys.time()\nnow\n\n[1] \"2022-05-10 16:55:48 PDT\"\n\nclass(now)\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\nGet local timezone:\n\nSys.timezone()\n\n[1] \"America/Los_Angeles\"\n\n\n\n\n21.1.3 Math on Dates\nThe reason we care about Date objects in R is because we can apply useful mathematical operations on them.\nFor example, we can substract date objects to get time intervals:\n\nstart_date <- as.Date(\"2020-09-15\")\ntime_diff <- Sys.Date() - start_date\ntime_diff\n\nTime difference of 602 days\n\nclass(time_diff)\n\n[1] \"difftime\"\n\n\nNote: While you can use the subtraction operator -, it is advised you use the difftime() function to perform subtraction on dates instead, because it allows you to specify units:\n\ntimepoint1 <- as.Date(\"2020-01-07\")\ntimepoint2 <- as.Date(\"2020-02-03\")\ndifftime(timepoint2, timepoint1, units = \"weeks\")\n\nTime difference of 3.857143 weeks\n\ndifftime(timepoint2, timepoint1, units = \"days\")\n\nTime difference of 27 days\n\ndifftime(timepoint2, timepoint1, units = \"hours\")\n\nTime difference of 648 hours\n\ndifftime(timepoint2, timepoint1, units = \"mins\")\n\nTime difference of 38880 mins\n\ndifftime(timepoint2, timepoint1, units = \"secs\")\n\nTime difference of 2332800 secs\n\n\n\n\n\nWhy is there no option for “months” or “years” in units?\n\n\nThink about it.\n\n\nBecause, unlike seconds, minutes, hours, days, and weeks, months and years do not have fixed length, i.e. literally a month or a year are not “units” of time.\n\n\nYou can always get a difference in days and divide by 365 (or 365.242.\n\n\n\n\nDOB <- as.Date(\"1969-08-04\")\nAge <- Sys.Date() - DOB\nAge\n\nTime difference of 19272 days\n\ncat(\"Age today is\", round(Age/365), \"years\")\n\nAge today is 53 years\n\n\n\n\n21.1.4 mean/median Date\n\nx <- as.Date(c(5480, 5723, 5987, 6992), origin = \"1970-01-01\")\nx\n\n[1] \"1985-01-02\" \"1985-09-02\" \"1986-05-24\" \"1989-02-22\"\n\nmean(x)\n\n[1] \"1986-07-21\"\n\nmedian(x)\n\n[1] \"1986-01-12\"\n\n\nTo check the median, we can do a mathematical operation using mmultiplication subtraction and addition, and the result is still a Date(!):\n\nx[2] + .5 * (x[3] - x[2])\n\n[1] \"1986-01-12\"\n\n\n\n\n21.1.5 Sequence of dates\nYou can create a sequence of dates using seq().\nIf an integer is passed to by, the unit is assumed to be days:\n\nstart_date <- as.Date(\"2020-09-14\")\nend_date <- as.Date(\"2020-12-07\")\nseq(from = start_date, to = end_date, by = 7)\n\n [1] \"2020-09-14\" \"2020-09-21\" \"2020-09-28\" \"2020-10-05\" \"2020-10-12\"\n [6] \"2020-10-19\" \"2020-10-26\" \"2020-11-02\" \"2020-11-09\" \"2020-11-16\"\n[11] \"2020-11-23\" \"2020-11-30\" \"2020-12-07\"\n\n\nUnlike mathematical operations like difftime() which require strict units of time, seq() can work with months and years.\nby can be one of:\n“day”, “week”, “month”, “quarter”, “year”.\nThe above is therefore equivalent to:\n\nseq(from = start_date, to = end_date, by = \"week\")\n\n [1] \"2020-09-14\" \"2020-09-21\" \"2020-09-28\" \"2020-10-05\" \"2020-10-12\"\n [6] \"2020-10-19\" \"2020-10-26\" \"2020-11-02\" \"2020-11-09\" \"2020-11-16\"\n[11] \"2020-11-23\" \"2020-11-30\" \"2020-12-07\"\n\n\nAs with numeric sequences, you can also define the length.out argument:\n\nstart_date <- as.Date(\"2020-01-20\")\nseq(from = start_date, by = \"year\", length.out = 4)\n\n[1] \"2020-01-20\" \"2021-01-20\" \"2022-01-20\" \"2023-01-20\"\n\n\nAn integer can be provided as part of character input to by:\n\nstart_date <- as.Date(\"2020-01-20\")\nend_date <- as.Date(\"2021-01-20\")\nseq(start_date, end_date, by = \"2 months\")\n\n[1] \"2020-01-20\" \"2020-03-20\" \"2020-05-20\" \"2020-07-20\" \"2020-09-20\"\n[6] \"2020-11-20\" \"2021-01-20\""
  },
  {
    "objectID": "50_DateTime.html#date-time-objects",
    "href": "50_DateTime.html#date-time-objects",
    "title": "21  Date and Time",
    "section": "21.2 Date-Time objects",
    "text": "21.2 Date-Time objects\n\n21.2.1 Character to Date-Time: as.POSIXct(), as.POSIXlt(), strptime():\n(As always, it can be very informative to look at the source code. Many of these functions call eachother internally)\nRead strptime()’s documentation for conversion specifications. These define the order and format of characters to be read as year, month, day, hour, minute, and second information.\nFor example, the ISO 8601 international standard is defined as:\n\"%Y-%m-%d %H:%M:%S\"\n\n%Y: Year with century, (0-9999 accepted) e.g. 2020\n%m: Month, 01-12, e.g. 03\n%d: Day, 01-31, e.g. 04\n%H: Hours, 00-23, e.g. 13\n%M: Minutes, 00-59, e.g. 38\n%S: Seconds, 00-61 (!) allowing for up to two leap seconds, e.g. 54\n\n\ndt <- \"2020-03-04 13:38:54\"\ndt\n\n[1] \"2020-03-04 13:38:54\"\n\nclass(dt)\n\n[1] \"character\"\n\n\nUse attributres() to see the difference between the POSIXct and POSIXlt classes:\n\ndt_posixct <- as.POSIXct(dt)\ndt_posixct\n\n[1] \"2020-03-04 13:38:54 PST\"\n\nclass(dt_posixct)\n\n[1] \"POSIXct\" \"POSIXt\" \n\nstr(dt_posixct)\n\n POSIXct[1:1], format: \"2020-03-04 13:38:54\"\n\nattributes(dt_posixct)\n\n$class\n[1] \"POSIXct\" \"POSIXt\" \n\n$tzone\n[1] \"\"\n\n\n\ndt_posixlt <- as.POSIXlt(dt)\ndt_posixlt\n\n[1] \"2020-03-04 13:38:54 PST\"\n\nclass(dt_posixlt)\n\n[1] \"POSIXlt\" \"POSIXt\" \n\nstr(dt_posixlt)\n\n POSIXlt[1:1], format: \"2020-03-04 13:38:54\"\n\ndt_posixlt$year\n\n[1] 120\n\nattributes(dt_posixlt)\n\n$names\n [1] \"sec\"    \"min\"    \"hour\"   \"mday\"   \"mon\"    \"year\"   \"wday\"   \"yday\"  \n [9] \"isdst\"  \"zone\"   \"gmtoff\"\n\n$class\n[1] \"POSIXlt\" \"POSIXt\" \n\n\nYou can compose a really large number of combination formats to match your data.\n\ndt2 <- c(\"03.04.20 01:38.54 pm\")\ndt2_posix <- as.POSIXct(dt2, format = \"%m.%d.%y %I:%M.%S %p\")\ndt2_posix\n\n[1] \"2020-03-04 13:38:54 PST\""
  },
  {
    "objectID": "50_DateTime.html#format-dates",
    "href": "50_DateTime.html#format-dates",
    "title": "21  Date and Time",
    "section": "21.3 format() Dates",
    "text": "21.3 format() Dates\nformat() operates on Date and POSIX objects to convert between representations\nDefine Date in US format:\n\ndt_us <- as.Date(\"07-04-2020\", format = \"%m-%d-%Y\")\ndt_us\n\n[1] \"2020-07-04\"\n\n\nConvert to European format:\n\ndt_eu <- format(dt_us, \"%d.%m.%y\")\ndt_eu\n\n[1] \"04.07.20\""
  },
  {
    "objectID": "50_DateTime.html#extract-partial-date-information",
    "href": "50_DateTime.html#extract-partial-date-information",
    "title": "21  Date and Time",
    "section": "21.4 Extract partial date information",
    "text": "21.4 Extract partial date information\n\nweekdays(): Get name of day of the week\nmonths(): Get name of month\nquarters(): Get quarter\njulia(): Get number of days since a specific origin\n\n\nx <- as.Date(c(18266, 18299, 18359, 18465), origin = \"1970-01-01\")\nx\n\n[1] \"2020-01-05\" \"2020-02-07\" \"2020-04-07\" \"2020-07-22\"\n\n\n\nweekdays(x)\n\n[1] \"Sunday\"    \"Friday\"    \"Tuesday\"   \"Wednesday\"\n\nmonths(x)\n\n[1] \"January\"  \"February\" \"April\"    \"July\"    \n\nquarters(x)\n\n[1] \"Q1\" \"Q1\" \"Q2\" \"Q3\"\n\njulian(x)\n\n[1] 18266 18299 18359 18465\nattr(,\"origin\")\n[1] \"1970-01-01\"\n\njulian(x, origin = as.Date(\"2020-01-01\"))\n\n[1]   4  37  97 203\nattr(,\"origin\")\n[1] \"2020-01-01\""
  },
  {
    "objectID": "50_DateTime.html#handling-dates-with-lubridate",
    "href": "50_DateTime.html#handling-dates-with-lubridate",
    "title": "21  Date and Time",
    "section": "21.5 Handling dates with lubridate",
    "text": "21.5 Handling dates with lubridate\nInstead of defining Date and/or time formats using POSIX standard abbreviations, we can let the lubridate package do some guesswork for us, which works well most of the time.\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\ndt <- c(\"2020-03-04 13:38:54\")\ndt_posix <- as_datetime(dt)\ndt_posix\n\n[1] \"2020-03-04 13:38:54 UTC\"\n\n\nNote that timezone defaults to UTC (Coordinated Universal Time) and must be set manually. PST is defined with “America/Los_Angeles” or the (officially deprecated) “US/Pacific” (tz database)\n\ndt_posix <- as_datetime(dt, tz = \"America/Los_Angeles\")\ndt_posix\n\n[1] \"2020-03-04 13:38:54 PST\"\n\n\n\ndt2_posix <- as_datetime(dt2)\ndt2_posix\n\n[1] \"2003-04-20 13:38:54 UTC\"\n\n\ndt2 got misinterpreted as year-month-day.\nFor these cases, lubridate includes a number of convenient functions to narrow down the guessing. The functions are named using all permutations of y, m, and d. The letter order signifies the order the information appears in the character you are trying to import, i.e. ymd, dmy, mdy, ydm, myd\n\ndt2 <- c(\"03.04.20 01:38.54 pm\")\ndt2_posix <- mdy_hms(dt2, tz = \"America/Los_Angeles\")\ndt2_posix\n\n[1] \"2020-03-04 13:38:54 PST\""
  },
  {
    "objectID": "52_MissingData.html",
    "href": "52_MissingData.html",
    "title": "22  Handling Missing data",
    "section": "",
    "text": "Missing data is a very common issue in statistics and data science.\nData may be missing for a variety of reasons. We often characterize the type of missingness using the following three types (Mack, Su, and Westreich 2018):"
  },
  {
    "objectID": "52_MissingData.html#check-for-missing-data",
    "href": "52_MissingData.html#check-for-missing-data",
    "title": "22  Handling Missing data",
    "section": "22.1 Check for missing data",
    "text": "22.1 Check for missing data\nYou can use your favorite base R commands to check for missing data, count NA elements by row, by column, total, etc.\nLet’s load the PimaIndiansDiabetes2 dataset from the mlbench package and make a copy of it to variable dat. Remember to check the class of a new object you didn’t create yourself with class(), check its dimensions, if applicable, with dim(), and a get a summary of its structure including data types with str():\n\ndata(\"PimaIndiansDiabetes2\", package = \"mlbench\")\ndat <- PimaIndiansDiabetes2\nclass(dat)\n\n[1] \"data.frame\"\n\ndim(dat)\n\n[1] 768   9\n\nstr(dat)\n\n'data.frame':   768 obs. of  9 variables:\n $ pregnant: num  6 1 8 1 0 5 3 10 2 8 ...\n $ glucose : num  148 85 183 89 137 116 78 115 197 125 ...\n $ pressure: num  72 66 64 66 40 74 50 NA 70 96 ...\n $ triceps : num  35 29 NA 23 35 NA 32 NA 45 NA ...\n $ insulin : num  NA NA NA 94 168 NA 88 NA 543 NA ...\n $ mass    : num  33.6 26.6 23.3 28.1 43.1 25.6 31 35.3 30.5 NA ...\n $ pedigree: num  0.627 0.351 0.672 0.167 2.288 ...\n $ age     : num  50 31 32 21 33 30 26 29 53 54 ...\n $ diabetes: Factor w/ 2 levels \"neg\",\"pos\": 2 1 2 1 2 1 2 1 2 2 ...\n\n\nCheck if there are any missing values in the data.frame with anyNA():\n\nanyNA(dat)\n\n[1] TRUE\n\n\nThe above suggests there is one or more NA values in the dataset.\nWe can create a logical index of NA values using is.na(). Remember that the output of is.na() is a logical matrix with the same dimensions as the dataset:\n\nna_index <- is.na(dat)\ndim(na_index)\n\n[1] 768   9\n\nhead(na_index)\n\n  pregnant glucose pressure triceps insulin  mass pedigree   age diabetes\n1    FALSE   FALSE    FALSE   FALSE    TRUE FALSE    FALSE FALSE    FALSE\n2    FALSE   FALSE    FALSE   FALSE    TRUE FALSE    FALSE FALSE    FALSE\n3    FALSE   FALSE    FALSE    TRUE    TRUE FALSE    FALSE FALSE    FALSE\n4    FALSE   FALSE    FALSE   FALSE   FALSE FALSE    FALSE FALSE    FALSE\n5    FALSE   FALSE    FALSE   FALSE   FALSE FALSE    FALSE FALSE    FALSE\n6    FALSE   FALSE    FALSE    TRUE    TRUE FALSE    FALSE FALSE    FALSE\n\n\nOne way to count missing values is with sum(is.na()). Remember that a logical array is coerced to an integer array for mathematical operations, where TRUE becomes 1 and FALSE becomes 0. Therefore, calling sum() on a logical index counts the number of TRUE elements (and since we are applying it on the index of NA values, it counts the number of elements with missing values):\n\nsum(is.na(dat))\n\n[1] 652\n\n\nThere are 652 NA values in total in the data.frame.\nLet’s count the number of missing values per feature, i.e. per column, using sapply()(#sapply):\n\nsapply(dat, function(i) sum(is.na(i)))\n\npregnant  glucose pressure  triceps  insulin     mass pedigree      age \n       0        5       35      227      374       11        0        0 \ndiabetes \n       0 \n\n\nThe features insulin and triceps have the most NA values.\nLet’s count the number of missing values per case (i.e. row):\n\nsapply(1:nrow(dat), function(i) sum(is.na(dat[i, ])))\n\n  [1] 1 1 2 0 0 2 0 3 0 3 2 2 2 0 0 3 0 2 0 0 0 2 2 1 0 0 2 0 0 2 1 0 0 2 1 0 2\n [38] 1 1 0 0 2 1 0 2 1 2 1 1 4 0 0 0 0 0 1 0 0 2 0 4 2 2 0 2 1 1 2 0 0 0 0 2 0\n [75] 1 2 2 1 3 1 1 4 0 1 2 0 1 0 0 1 2 0 0 2 0 0 1 0 0 0 2 2 2 0 2 0 2 0 0 0 0\n[112] 0 0 2 0 2 2 2 1 0 0 1 0 2 2 0 0 0 0 2 0 2 0 1 0 0 0 0 2 0 2 1 0 2 0 2 1 0\n[149] 2 1 0 2 0 0 2 1 0 0 0 0 1 0 0 1 2 0 1 2 2 0 2 0 2 0 0 0 2 0 2 2 2 0 1 2 2\n[186] 1 0 0 0 0 2 0 2 3 1 0 2 0 0 0 1 2 1 0 0 1 0 2 0 1 1 1 1 0 0 0 0 0 1 2 0 2\n[223] 3 0 0 0 2 1 0 0 2 0 0 2 0 2 0 1 1 2 1 0 2 0 0 1 2 0 0 1 2 2 0 1 0 1 1 1 0\n[260] 0 0 3 1 1 2 0 3 1 2 3 1 0 2 0 2 0 1 0 2 0 2 0 0 2 2 0 0 0 0 0 0 0 0 0 2 0\n[297] 0 0 0 2 3 0 0 2 2 0 0 0 0 0 1 0 0 0 1 0 0 2 0 2 0 1 1 0 1 0 0 2 0 0 1 0 3\n[334] 2 0 0 3 2 0 2 0 0 2 2 2 0 0 3 0 2 2 2 1 0 2 2 0 2 0 0 0 2 1 2 0 0 2 1 0 0\n[371] 0 1 0 0 0 0 0 0 2 0 0 1 0 0 0 0 1 1 0 0 0 2 0 0 2 0 0 1 2 1 2 2 0 1 2 0 2\n[408] 2 2 0 1 0 0 0 0 0 1 1 2 0 0 0 0 1 0 0 4 0 0 0 3 0 0 2 1 3 1 2 1 2 1 0 0 2\n[445] 1 0 0 0 0 0 0 2 0 3 0 1 2 0 0 0 0 2 0 1 2 0 0 0 3 0 1 1 1 2 2 1 0 0 0 1 0\n[482] 1 0 0 3 0 0 0 1 2 0 1 1 0 4 2 2 0 0 0 0 1 2 0 1 2 0 0 0 2 1 0 2 2 0 0 0 2\n[519] 2 0 0 0 4 2 2 1 0 0 0 2 0 2 0 3 0 3 2 2 0 0 0 0 1 0 0 0 0 0 0 1 1 0 2 0 0\n[556] 0 1 2 1 2 2 0 0 0 2 0 0 0 0 0 2 2 0 0 0 0 0 2 2 1 1 1 1 2 0 1 2 2 0 3 1 0\n[593] 2 0 0 0 2 0 2 0 1 3 1 0 3 1 0 0 0 0 0 0 0 1 0 2 2 0 1 3 0 1 2 0 2 0 2 2 2\n[630] 1 2 0 2 0 2 2 2 0 0 0 0 2 2 3 0 0 0 0 0 1 0 0 0 2 0 0 0 0 2 0 2 1 0 0 1 0\n[667] 1 1 0 0 0 1 0 0 2 2 2 2 2 0 0 1 0 2 3 0 2 1 0 0 2 2 0 0 2 0 0 3 0 2 0 1 1\n[704] 3 0 1 4 0 2 0 0 0 1 0 2 0 0 1 0 1 1 0 0 0 2 1 0 1 2 2 0 2 0 0 2 1 0 1 0 2\n[741] 0 0 0 2 0 0 1 0 0 2 2 0 1 0 1 0 1 2 2 2 0 1 2 0 1 0 2 1\n\n\nIf we wanted to get the row with the most missing values, we can use which.max():\n\nwhich.max(sapply(1:nrow(dat), function(i) sum(is.na(dat[i, ]))))\n\n[1] 50\n\n\n\nsum(is.na(dat[50, ]))\n\n[1] 4\n\n\nRow 50 has 4 missing values.\n\n22.1.1 Visualize\nIt may be helpful to visualize missing data to get a quick impression of missingness. The rtemis package includes the function mplot3_missing():\n\nlibrary(rtemis)\n\n  .:rtemis 0.91 🌊 aarch64-apple-darwin20 (64-bit)\n\n  Defaults\n  |   Theme: whitegrid\n  |    Font: Fira Sans\n  | Palette: rtCol1\n  |    Plan: multicore\n  |   Cores: 8/10 available\n\n  Resources\n  | Documentation: https://rtemis.lambdamd.org\n  |       Learn R: https://class.lambdamd.org/pdsr\n  | rtemis themes: https://egenn.lambdamd.org/software/#rtemis_themes\n  |          Cite: `citation(\"rtemis\")`\n\n  Setup\n  | Enable progress reporting: `progressr::handlers(global = TRUE)`\n\nmplot3_missing(dat)\n\n\n\n\nMissing data is shown in magenta by default. The row below the image shows total NA values per column\n\n\n22.1.2 Summarize\nGet N of missing per column:\n\nsapply(dat, function(i) sum(is.na(i)))\n\npregnant  glucose pressure  triceps  insulin     mass pedigree      age \n       0        5       35      227      374       11        0        0 \ndiabetes \n       0 \n\n\nrtemis::checkData() includes information on missing data:\n\ncheckData(dat)\n dat: A data.frame with 768 rows and 9 features\n\n  Data types________________\n  * 8 continuous features\n  * 0 integer features\n  * 1 categorical feature, which is not ordered\n  * 0 character features\n  * 0 date features\n\n  Issues____________________\n  * 0 constant features\n  * 0 duplicated cases\n  * 5 features include 'NA' values; 652 'NA' values total\n     - Max percent missing in a feature is 48.70% (insulin)\n     - Max percent missing in a case is 44.44% (case #50)\n\n  Recommendations___________\n  * Consider imputing missing values or use complete cases only"
  },
  {
    "objectID": "52_MissingData.html#handle-missing-data",
    "href": "52_MissingData.html#handle-missing-data",
    "title": "22  Handling Missing data",
    "section": "22.2 Handle missing data",
    "text": "22.2 Handle missing data\nDifferent approaches can be used to handle missing data:\n\nDo nothing - if your algorithm(s) can handle missing data (decision trees!)\nExclude data: Use complete cases only\nFill in (make up) data: Replace or Impute\n\nReplace with median/mean\nPredict missing from present\n\nSingle imputation\nMultiple imputation\n\n\n\n\n22.2.1 Do nothing\nAlgorithms like decision trees and ensemble methods that use decision trees like random forest and gradient boosting can handle missing data, depending on the particular implementation. For example, rpart::rpart() which is used by rtemis::s_CART() has no trouble with missing data in the predictors:\n\ndat.cart <- s_CART(dat)\n[2022-05-10 16:58:06 s_CART] Hello, egenn \n\n[2022-05-10 16:58:06 dataPrepare] Imbalanced classes: using Inverse Probability Weighting \n\n.:Classification Input Summary\nTraining features: 768 x 8 \n Training outcome: 768 x 1 \n Testing features: Not available\n  Testing outcome: Not available\n\n[2022-05-10 16:58:06 s_CART] Training CART... \n\n.:CART Classification Training Summary\n                   Reference \n        Estimated  neg  pos  \n              neg  426   89\n              pos   74  179\n\n                   Overall  \n      Sensitivity  0.8520 \n      Specificity  0.6679 \nBalanced Accuracy  0.7600 \n              PPV  0.8272 \n              NPV  0.7075 \n               F1  0.8394 \n         Accuracy  0.7878 \n              AUC  0.7854 \n\n  Positive Class:  neg \n\n\n\n\n[2022-05-10 16:58:06 s_CART] Run completed in 0.01 minutes (Real: 0.45; User: 0.39; System: 0.02) \n\n\n\n\n22.2.2 Use complete cases only\nR’s builtin complete.cases() function returns, as the name suggests, a logical index of cases (i.e. rows) that have no missing values, i.e. are complete.\n\ndim(dat)\n\n[1] 768   9\n\nindex_cc <- complete.cases(dat)\nclass(index_cc)\n\n[1] \"logical\"\n\nlength(index_cc)\n\n[1] 768\n\nhead(index_cc)\n\n[1] FALSE FALSE FALSE  TRUE  TRUE FALSE\n\ndat_cc <- dat[index_cc, ]\ndim(dat_cc)\n\n[1] 392   9\n\n\nWe lost 376 cases in the above example. That’s quite a few, so, for this dataset, we probably want to look at options that do not exclude cases.\n\n\n22.2.3 Replace with a fixed value\nWe can manually replace missing values with the mean or median in the case of a continuous variable, or with the mode in the case of a categorical feature.\nFor example, to replace the first feature’s missing values with the mean:\n\npressure_mean <- mean(dat$pressure, na.rm = TRUE)\ndat_im <- dat\ndat_im$pressure[is.na(dat_im$pressure)] <- pressure_mean\n\nrtemis::preprocess() can replace missing values with mean (for numeric features) and the mode (for factors) for all columns:\n\ndat_pre <- preprocess(dat, impute = TRUE, impute.type = \"meanMode\")\n[2022-05-10 16:58:06 preprocess] Imputing missing values using mean and getMode... \n[2022-05-10 16:58:06 preprocess] Done \n\n\nVerify there are no missing data by rerunning checkData():\n\ncheckData(dat_pre)\n dat_pre: A data.frame with 768 rows and 9 features\n\n  Data types________________\n  * 8 continuous features\n  * 0 integer features\n  * 1 categorical feature, which is not ordered\n  * 0 character features\n  * 0 date features\n\n  Issues____________________\n  * 0 constant features\n  * 0 duplicated cases\n  * 0 features include 'NA' values\n\n  Recommendations___________\n  * Everything looks good\n\n\nYou may want to include a “missingness” column that indicates which cases were imputed to include in your model. You can create this simply by running:\n\npressure_missing = factor(as.integer(is.na(dat$pressure)))\n\npreprocess() includes the option missingness to add indicator columns after imputation:\n\ndat_pre <- preprocess(dat, impute = TRUE, impute.type = \"meanMode\",\n                      missingness = TRUE)\n[2022-05-10 16:58:06 preprocess] Created missingness indicator for glucose... \n[2022-05-10 16:58:06 preprocess] Created missingness indicator for pressure... \n[2022-05-10 16:58:06 preprocess] Created missingness indicator for triceps... \n[2022-05-10 16:58:06 preprocess] Created missingness indicator for insulin... \n[2022-05-10 16:58:06 preprocess] Created missingness indicator for mass... \n[2022-05-10 16:58:06 preprocess] Imputing missing values using mean and getMode... \n[2022-05-10 16:58:06 preprocess] Done \n\nhead(dat_pre)\n\n  pregnant glucose pressure  triceps  insulin mass pedigree age diabetes\n1        6     148       72 35.00000 155.5482 33.6    0.627  50      pos\n2        1      85       66 29.00000 155.5482 26.6    0.351  31      neg\n3        8     183       64 29.15342 155.5482 23.3    0.672  32      pos\n4        1      89       66 23.00000  94.0000 28.1    0.167  21      neg\n5        0     137       40 35.00000 168.0000 43.1    2.288  33      pos\n6        5     116       74 29.15342 155.5482 25.6    0.201  30      neg\n  glucose_missing pressure_missing triceps_missing insulin_missing mass_missing\n1               0                0               0               1            0\n2               0                0               0               1            0\n3               0                0               1               1            0\n4               0                0               0               0            0\n5               0                0               0               0            0\n6               0                0               1               1            0\n\n\n\n22.2.3.1 Add new level “missing”\nOne option to handle missing data in categorical variables, is to introduce a new level of “missing” to the factor, instead of replacing with the mode, for example. If we bin a continuous variable to convert to categorical, the same can then also be applied.\nSince no factors have missing values in the current dataset we create a copy and replace some data with NA:\n\ndat2 <- dat\ndat2$diabetes[sample(1:NROW(dat2), 35)] <- NA\nsum(is.na(dat2$diabetes))\n\n[1] 35\n\nlevels(dat2$diabetes)\n\n[1] \"neg\" \"pos\"\n\n\nReplace NA values with new level missing:\n\ndat_pre2 <- preprocess(dat2, factorNA2missing = TRUE)\n[2022-05-10 16:58:06 preprocess] Converting NA in factors to level \"missing\"... \n[2022-05-10 16:58:06 preprocess] Done \n\nanyNA(dat_pre2$diabetes)\n\n[1] FALSE\n\nlevels(dat_pre2$diabetes)\n\n[1] \"neg\"     \"pos\"     \"missing\"\n\n\n\n\n\n22.2.4 Last observation carried forward (LOCF)\nIn longitudinal / timeseries data, we may want to replace missing values with the last observed value. This is called last observation carried forward (LOCF). As always, whether this procedure is appropriate depend the reasons for missingness. The zoo and DescTool packages provide commands to perform LOCF.\nSome simulated data. We are missing blood pressure measurements on Saturdays and Sundays:\n\ndat <- data.frame(Day = rep(c(\"Mon\", \"Tues\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"), 3),\n                  SBP = sample(105:125, 21, TRUE))\ndat$SBP[dat$Day == \"Sat\" | dat$Day == \"Sun\"] <- NA\ndat\n\n    Day SBP\n1   Mon 117\n2  Tues 118\n3   Wed 118\n4   Thu 114\n5   Fri 109\n6   Sat  NA\n7   Sun  NA\n8   Mon 109\n9  Tues 120\n10  Wed 119\n11  Thu 107\n12  Fri 114\n13  Sat  NA\n14  Sun  NA\n15  Mon 119\n16 Tues 122\n17  Wed 118\n18  Thu 110\n19  Fri 106\n20  Sat  NA\n21  Sun  NA\n\n\nThe zoo package includes the na.locf().\n\ndat$SBPlocf <- zoo::na.locf(dat$SBP)\ndat\n\n    Day SBP SBPlocf\n1   Mon 117     117\n2  Tues 118     118\n3   Wed 118     118\n4   Thu 114     114\n5   Fri 109     109\n6   Sat  NA     109\n7   Sun  NA     109\n8   Mon 109     109\n9  Tues 120     120\n10  Wed 119     119\n11  Thu 107     107\n12  Fri 114     114\n13  Sat  NA     114\n14  Sun  NA     114\n15  Mon 119     119\n16 Tues 122     122\n17  Wed 118     118\n18  Thu 110     110\n19  Fri 106     106\n20  Sat  NA     106\n21  Sun  NA     106\n\n\nSimilar functionality is included in DescTools’ LOCF() function:\n\nDescTools::LOCF(dat$SBP)\n\n [1] 117 118 118 114 109 109 109 109 120 119 107 114 114 114 119 122 118 110 106\n[20] 106 106\n\n\n\n\n22.2.5 Replace missing values with estimated values\n\n22.2.5.1 Single imputation\nYou can use non-missing data to predict missing data in an iterative procedure (Buuren and Groothuis-Oudshoorn 2010; Stekhoven and Bühlmann 2012). The missRanger package uses the optimized (and parallel-capable) package ranger (Wright and Ziegler 2015) to iteratively train random forest models for imputation.\n\nlibrary(missRanger)\ndat <- iris\nset.seed(2020)\ndat[sample(1:150, 5), 1] <- dat[sample(1:150, 22), 4] <- dat[sample(1:150, 18), 4] <- NA\ndat_rfimp <- missRanger(dat, num.trees = 100)\n\n\nMissing value imputation by random forests\n\n  Variables to impute:      Sepal.Length, Petal.Width\n  Variables used to impute: Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, Species\niter 1: ..\niter 2: ..\niter 3: ..\niter 4: ..\n\nhead(dat_rfimp)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1     5.100000         3.5          1.4         0.2  setosa\n2     4.900000         3.0          1.4         0.2  setosa\n3     4.732533         3.2          1.3         0.2  setosa\n4     4.600000         3.1          1.5         0.2  setosa\n5     5.000000         3.6          1.4         0.2  setosa\n6     5.400000         3.9          1.7         0.4  setosa\n\ncheckData(dat_rfimp)\n dat_rfimp: A data.frame with 150 rows and 5 features\n\n  Data types________________\n  * 4 continuous features\n  * 0 integer features\n  * 1 categorical feature, which is not ordered\n     - 1 unordered categorical feature has more than 2 levels\n  * 0 character features\n  * 0 date features\n\n  Issues____________________\n  * 0 constant features\n  * 1 duplicated case\n  * 0 features include 'NA' values\n\n  Recommendations___________\n  * Remove the duplicated case\n  * Check the unordered categorical feature with more than 2 levels and consider\n    if ordering would make sense\n\n\nNote: The default method for preprocess(impute = TRUE) is to use missRanger.\n\n\n22.2.5.2 Multiple imputation\nMultiple imputation creates multiple estimates of the missing data. It is more statistically valid for small datasets, especially when the goal is to get accurate estimates of a summary statistics, but may not be practical for larger datasets. It is not usually considered an option for machine learning (where duplicating cases may add bias and complexity in resampling). The package mice is a popular choice for multiple imputation in R.\n\nlibrary(mice)\ndat_mice <- mice(dat)\n\n\n\n\n\nBuuren, S van, and Karin Groothuis-Oudshoorn. 2010. “Mice: Multivariate Imputation by Chained Equations in r.” Journal of Statistical Software, 1–68.\n\n\nMack, Christina, Zhaohui Su, and Daniel Westreich. 2018. “Managing Missing Data in Patient Registries: Addendum to Registries for Evaluating Patient Outcomes: A User’s Guide, [Internet].”\n\n\nStekhoven, Daniel J, and Peter Bühlmann. 2012. “MissForest—Non-Parametric Missing Value Imputation for Mixed-Type Data.” Bioinformatics 28 (1): 112–18.\n\n\nWright, Marvin N, and Andreas Ziegler. 2015. “Ranger: A Fast Implementation of Random Forests for High Dimensional Data in c++ and r.” arXiv Preprint arXiv:1508.04409."
  },
  {
    "objectID": "54_ClassesAndOOP.html",
    "href": "54_ClassesAndOOP.html",
    "title": "23  Classes and Object-Oriented Programming",
    "section": "",
    "text": "Object-Oriented Programming (OOP) is a programming paradigm built around objects with associated data, known as attributes, and functions, known as methods.\nThere are 4 main class systems in R:\nS3 and S4 methods are part of generic functions. RC and R6 methods are part of the object, but you can (and should) write generic functions for them as well.\nThis chapter will focus on the ubiquitous S3 system. For more advanced (and real OOP) applications, we recommend looking into the R6 system."
  },
  {
    "objectID": "54_ClassesAndOOP.html#s3",
    "href": "54_ClassesAndOOP.html#s3",
    "title": "23  Classes and Object-Oriented Programming",
    "section": "23.1 S3",
    "text": "23.1 S3\nMost R objects we have been using so far are S3 objects. Data frames are some of the most common S3 objects.\nGeneric functions are functions that act differently based on the class of the input object. We have already used many of them. For example, summary() works differently on a data.frame, on a factor, or a glm object, etc.\nGeneric functions in R are saved as functionname.classname() and called automatically, based on the class of the first argument. This allows the same function, e.g. print(), summary(), c(), to have a different effect on objects of different classes. For example, the print() function applied on a data frame, will actually call print.data.frame(), while applied on a factor, it will call print.factor().\nThis means that when you type print(iris) this calls print.data.frame(iris)\nNote how the R documentation lists usage information separately for each S3 method, e.g. ## S3 method for class 'factor'\n\n23.1.1 methods()\nTo get a list of all available methods defined for a specific class,\ni.e. “What different functions can I use on this object?”\n\nmethods(class = \"data.frame\")\n\n [1] [             [[            [[<-          [<-           $<-          \n [6] aggregate     anyDuplicated anyNA         as.data.frame as.list      \n[11] as.matrix     as.vector     by            cbind         coerce       \n[16] dim           dimnames      dimnames<-    droplevels    duplicated   \n[21] edit          format        formula       head          initialize   \n[26] is.na         Math          merge         na.exclude    na.omit      \n[31] Ops           plot          print         prompt        rbind        \n[36] row.names     row.names<-   rowsum        show          slotsFromS3  \n[41] split         split<-       stack         str           subset       \n[46] summary       Summary       t             tail          transform    \n[51] type.convert  unique        unstack       within        xtfrm        \nsee '?methods' for accessing help and source code\n\n\nConversely, to get a list of all available methods for a generic function (i.e. which classes have)\n(i.e. “What objects can I use this function on?”)\n\nmethods(generic.function = \"plot\")\n\n [1] plot.acf*           plot.data.frame*    plot.decomposed.ts*\n [4] plot.default        plot.dendrogram*    plot.density*      \n [7] plot.ecdf           plot.factor*        plot.formula*      \n[10] plot.function       plot.hclust*        plot.histogram*    \n[13] plot.HoltWinters*   plot.isoreg*        plot.lm*           \n[16] plot.medpolish*     plot.mlm*           plot.ppr*          \n[19] plot.prcomp*        plot.princomp*      plot.profile.nls*  \n[22] plot.raster*        plot.spec*          plot.stepfun       \n[25] plot.stl*           plot.table*         plot.ts            \n[28] plot.tskernel*      plot.TukeyHSD*     \nsee '?methods' for accessing help and source code\n\n\n\n\n23.1.2 Defining custom S3 classes\nIt very simple to assign an object to a new class.\nThere is no formal class definition, an object is directly assigned to a class by name. An object can belong to multiple classes:\n\nx <- 1:10\nclass(x) <- c(\"specialvector\", \"numeric\")\nclass(x)\n\n[1] \"specialvector\" \"numeric\"      \n\n\nThe hierarchy of classes goes left to right, meaning that generic methods are searched for classes in the order they appear in the output of class().\nIf we print x, since there is no print method for class specialvector or for numeric, the default print.default() command is automatically called:\n\nprint(x)\n\n [1]  1  2  3  4  5  6  7  8  9 10\nattr(,\"class\")\n[1] \"specialvector\" \"numeric\"      \n\nprint.default(x)\n\n [1]  1  2  3  4  5  6  7  8  9 10\nattr(,\"class\")\n[1] \"specialvector\" \"numeric\"      \n\n\nTo create a custom print() function for out new class specialvector, we define a function named print.[classname]:\n\nprint.specialvector <- function(x, ...) {\n  cat(\"This is a special vector of length\", length(x), \"\\n\")\n  cat(\"Its mean value is\", mean(x, na.rm = TRUE), \"and its median is\", median(x, na.rm = TRUE))\n  cat(\"\\nHere are the first few elements:\\n\", head(x), \"\\n\")\n}\n\nNow, when you print an object of class specialvector, the custom print() command is invoked:\n\nx\n\nThis is a special vector of length 10 \nIts mean value is 5.5 and its median is 5.5\nHere are the first few elements:\n 1 2 3 4 5 6 \n\n\nIf needed, you can call the default or another appropriate method directly:\n\nprint.default(x)\n\n [1]  1  2  3  4  5  6  7  8  9 10\nattr(,\"class\")\n[1] \"specialvector\" \"numeric\"      \n\n\nYou can change the vector back to a regular numeric vector, or a different class, just as easily:\n\nclass(x) <- \"numeric\"\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10"
  },
  {
    "objectID": "60_DataTable.html",
    "href": "60_DataTable.html",
    "title": "24  Efficient data analysis with data.table",
    "section": "",
    "text": "The data.table package provides a modern and highly optimized version of R’s data.frame structure. It is highly memory efficient and automatically parallelizes internal operations to achieve substantial speed improvements over data.frames. The data.table package weighs in at just a few kilobytes, has zero dependencies, and maintains compatibility with R versions going as far back as possible."
  },
  {
    "objectID": "60_DataTable.html#data.table-singificantly-extends-the-power-of-data.frame",
    "href": "60_DataTable.html#data.table-singificantly-extends-the-power-of-data.frame",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.1 data.table singificantly extends the power of data.frame",
    "text": "24.1 data.table singificantly extends the power of data.frame\nSome of the ways in which a data.table differs from a data.frame:\n\nA lot more than indexing can be done within a data.table’s “frame” (dt[i, j, by]): filter, select & operate on columns, group-by operations\nAccess column names directly without quoting\nMany operations can be performed “in-place” (i.e. with no assignment)\nWorking on big data within a data.table can be orders of magnitude faster.\n\ndata.table operations remain as close as possible to data.frame operations, trying to extend rather than replace the latter’s functionality.\ndata.table includes thorough and helpful error messages that often point to a solution. This includes common mistakes new users may make when trying commands that would work on a data.frame but are different on a data.table."
  },
  {
    "objectID": "60_DataTable.html#dtinstallation",
    "href": "60_DataTable.html#dtinstallation",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.2 Installation",
    "text": "24.2 Installation\nYou can install data.table from CRAN or GitHub. Check out the data.table wiki for more info.\nTo get the latest version on CRAN:\n\ninstall.packages(\"data.table\")\n\nTo get the latest development version:\n\n# install the \"remotes\" package if you don't already have it and then\nremotes::install_github(\"Rdatatable/data.table\")\n\ndata.table also includes a built-in command to update to the latest development version:\n\ndata.table::update.dev.pkg()\n\n\n24.2.1 Load the data.table package\n\nlibrary(data.table)"
  },
  {
    "objectID": "60_DataTable.html#create-a-data.table",
    "href": "60_DataTable.html#create-a-data.table",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.3 Create a data.table",
    "text": "24.3 Create a data.table\n\n24.3.1 By assignment: data.table()\nLet’s create a data.frame and a data.table to explore side by side.\n\ndf <- data.frame(A = 1:5,\n                 B = c(1.2, 4.3, 9.7, 5.6, 8.1),\n                 C = c(\"a\", \"b\", \"b\", \"a\", \"a\"))\nclass(df)\n\n[1] \"data.frame\"\n\ndf\n\n  A   B C\n1 1 1.2 a\n2 2 4.3 b\n3 3 9.7 b\n4 4 5.6 a\n5 5 8.1 a\n\n\ndata.table() syntax is similar to data.frame() (differs in some arguments)\n\ndt <- data.table(A = 1:5,\n                 B = c(1.2, 4.3, 9.7, 5.6, 8.1),\n                 C = c(\"a\", \"b\", \"b\", \"a\", \"a\"))\ndt\n\n   A   B C\n1: 1 1.2 a\n2: 2 4.3 b\n3: 3 9.7 b\n4: 4 5.6 a\n5: 5 8.1 a\n\nclass(dt)\n\n[1] \"data.table\" \"data.frame\"\n\n\nNotice how a data.table object also inherits from data.frame. This means that if a method does not exist for data.table, the method for data.frame will be used - review classes and generic functions.\nAs part of improving efficieny, data.tables do away with row names. Instead of using rownames, you can and should add an extra column (e.g. “ID”) with the same information - this is advisable when working with data.frames as well.\nA rather convenient option is to have data.tables print each column’s class below the column name. You can pass the argument class = TRUE to print() or set the global option datatable.print.class using options()\n\noptions(datatable.print.class = TRUE)\ndt\n\n       A     B      C\n   <int> <num> <char>\n1:     1   1.2      a\n2:     2   4.3      b\n3:     3   9.7      b\n4:     4   5.6      a\n5:     5   8.1      a\n\n\nSame as with a data.frame, to automatically convert string to factors, you can use the stringsAsFactors argument (or factor() directly):\n\ndt <- data.table(A = 1:5,\n                 B = c(1.2, 4.3, 9.7, 5.6, 8.1),\n                 C = c(\"a\", \"b\", \"b\", \"a\", \"a\"),\n                 stringsAsFactors = TRUE)\ndt\n\n       A     B      C\n   <int> <num> <fctr>\n1:     1   1.2      a\n2:     2   4.3      b\n3:     3   9.7      b\n4:     4   5.6      a\n5:     5   8.1      a\n\n\n\n\n24.3.2 By coercion: as.data.table()\n\ndat <- data.frame(A = 1:5,\n                  B = c(1.2, 4.3, 9.7, 5.6, 8.1),\n                  C = c(\"a\", \"b\", \"b\", \"a\", \"a\"),\n                  stringsAsFactors = TRUE)\ndat\n\n  A   B C\n1 1 1.2 a\n2 2 4.3 b\n3 3 9.7 b\n4 4 5.6 a\n5 5 8.1 a\n\ndat2 <- as.data.table(dat)\ndat2\n\n       A     B      C\n   <int> <num> <fctr>\n1:     1   1.2      a\n2:     2   4.3      b\n3:     3   9.7      b\n4:     4   5.6      a\n5:     5   8.1      a\n\n\n\n\n24.3.3 By coercion in-place: setDT()\nsetDT converts a list or data.frame into a data.table in-place. Note: the original object itself is changed, you do not need to assign the output of setDT to a new name.\n\ndat <- data.frame(A = 1:5,\n                  B = c(1.2, 4.3, 9.7, 5.6, 8.1),\n                  C = c(\"a\", \"b\", \"b\", \"a\", \"a\"))\nclass(dat)\n\n[1] \"data.frame\"\n\nsetDT(dat)\nclass(dat)\n\n[1] \"data.table\" \"data.frame\"\n\n\nYou can similarly convert a data.table to a data.frame, in-place:\n\nsetDF(dat)\nclass(dat)\n\n[1] \"data.frame\"\n\n\n\n\n24.3.4 Read into a data.table from file with fread()\ndata.table includes the fread() function to read data from files, in a similar way as the base functions read.csv() and read.table(). It is orders of magnitude faster for very large data (e.g. thousands to millions of rows) and it can read directly from URLs and zipped files. The sep argument defines the separator (same as in read.csv() and read.table()), but when set to \"auto\" (the default) it does a great job of figuring it out by itself.\n\ndat <- fread(\"path/to/input.csv\")\ndat <- fread(\"https::/url/to/input.csv.gz\")\n\nFor its speed and convenience, fread() is recommended over read.csv()/read.table() even if you intend to work with a data.frame exclusively, in which case you can pass the argument data.table = FALSE.\n\n\n24.3.5 Write a data.table to a CSV: fwrite()\nSimilar to fread(), fwrite() can be a lot faster than write.csv()\n\nfwrite(dt, \"/path/to/text.csv\")\n\n\n\n24.3.6 Save data.table to an RDS:\nSame as any R object, you can save a data.table to disk using saveRDS(). Suppose you have read data in with fread() or coerced a dataset using as.data.table(), done some cleaning up, type conversions, data transformations, etc, this is the preferred way to save your work, so you can reload at any time.\n\nsaveRDS(dt, \"/path/to/data.rds\")"
  },
  {
    "objectID": "60_DataTable.html#display-data.table-structure-with-str",
    "href": "60_DataTable.html#display-data.table-structure-with-str",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.4 Display data.table structure with str()",
    "text": "24.4 Display data.table structure with str()\nstr() works the same (and you should keep using it!)\n\nstr(df)\n\n'data.frame':   5 obs. of  3 variables:\n $ A: int  1 2 3 4 5\n $ B: num  1.2 4.3 9.7 5.6 8.1\n $ C: chr  \"a\" \"b\" \"b\" \"a\" ...\n\n\n\nstr(dt)\n\nClasses 'data.table' and 'data.frame':  5 obs. of  3 variables:\n $ A: int  1 2 3 4 5\n $ B: num  1.2 4.3 9.7 5.6 8.1\n $ C: Factor w/ 2 levels \"a\",\"b\": 1 2 2 1 1\n - attr(*, \".internal.selfref\")=<externalptr>"
  },
  {
    "objectID": "60_DataTable.html#combine-data.tables",
    "href": "60_DataTable.html#combine-data.tables",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.5 Combine data.tables",
    "text": "24.5 Combine data.tables\ncbind() and rbind() work on data.tables the same as on data.frames:\n\ndt1 <- data.table(a = 1:5)\ndt2 <- data.table(b = 11:15)\ncbind(dt1, dt2)\n\n       a     b\n   <int> <int>\n1:     1    11\n2:     2    12\n3:     3    13\n4:     4    14\n5:     5    15\n\nrbind(dt1, dt1)\n\n        a\n    <int>\n 1:     1\n 2:     2\n 3:     3\n 4:     4\n 5:     5\n 6:     1\n 7:     2\n 8:     3\n 9:     4\n10:     5"
  },
  {
    "objectID": "60_DataTable.html#filter-rows",
    "href": "60_DataTable.html#filter-rows",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.6 Filter rows",
    "text": "24.6 Filter rows\nThere are many similarities and some notable differences in how indexing works in a data.table vs. a data.frame.\nFiltering rows with an integer or logical index is largely the same in a data.frame and a data.table, but in a data.table you can omit the comma to select all columns:\n\ndf[c(1, 3, 5), ]\n\n  A   B C\n1 1 1.2 a\n3 3 9.7 b\n5 5 8.1 a\n\ndt[c(1, 3, 5), ]\n\n       A     B      C\n   <int> <num> <fctr>\n1:     1   1.2      a\n2:     3   9.7      b\n3:     5   8.1      a\n\ndt[c(1, 3, 5)]\n\n       A     B      C\n   <int> <num> <fctr>\n1:     1   1.2      a\n2:     3   9.7      b\n3:     5   8.1      a\n\n\nUsing a variable that holds a row index, whether integer or logical:\n\nrowid <- c(1, 3, 5)\ndf[rowid, ]\n\n  A   B C\n1 1 1.2 a\n3 3 9.7 b\n5 5 8.1 a\n\ndt[rowid, ]\n\n       A     B      C\n   <int> <num> <fctr>\n1:     1   1.2      a\n2:     3   9.7      b\n3:     5   8.1      a\n\ndt[rowid]\n\n       A     B      C\n   <int> <num> <fctr>\n1:     1   1.2      a\n2:     3   9.7      b\n3:     5   8.1      a\n\n\n\nrowbn <- c(T, F, T, F, T)\ndf[rowbn, ]\n\n  A   B C\n1 1 1.2 a\n3 3 9.7 b\n5 5 8.1 a\n\ndt[rowbn, ]\n\n       A     B      C\n   <int> <num> <fctr>\n1:     1   1.2      a\n2:     3   9.7      b\n3:     5   8.1      a\n\ndt[rowbn]\n\n       A     B      C\n   <int> <num> <fctr>\n1:     1   1.2      a\n2:     3   9.7      b\n3:     5   8.1      a\n\n\n\n24.6.1 Conditional filtering\nAs a reminder, there are a few ways to conditionally filter cases in a data.frame:\n\ndf[df$A > mean(df$A) & df$B > mean(df$B), ]\n\n  A   B C\n5 5 8.1 a\n\nsubset(df, A > mean(A) & B > mean(B))\n\n  A   B C\n5 5 8.1 a\n\nwith(df, df[A > mean(A) & B > mean(B), ])\n\n  A   B C\n5 5 8.1 a\n\n\ndata.table allows you to refer to column names directly and unquoted, which makes writing filter conditions easier/more compact:\n\ndt[A > mean(A) & B > mean(B)]\n\n       A     B      C\n   <int> <num> <fctr>\n1:     5   8.1      a\n\n\nThe data.table package also includes an S3 method for subset() that works the same way as with a data.frame:\n\nsubset(dt, A > mean(A) & B > mean(B))\n\n       A     B      C\n   <int> <num> <fctr>\n1:     5   8.1      a\n\n\nAs another example, exclude cases base on missingness in a specific column:\n\nadf <- as.data.frame(sapply(1:5, function(i) rnorm(10)))\nadf |> head()\n\n          V1          V2         V3         V4          V5\n1  0.1311821 -0.41242261  0.5897420 -0.4643061  1.58410885\n2 -1.5757546 -1.46085796  0.1111828  1.5812159  0.14120671\n3 -0.2646751 -0.63027344  0.5977524 -0.3590879  0.01795485\n4 -0.3430240  0.80000771 -1.8080567  0.7472244  0.82739404\n5  1.4605731  0.09614699  1.1030527  0.5022653 -1.23512662\n6  1.0582400 -0.74200541 -1.3492387 -1.1937673 -0.77901035\n\nadf[1, 3] <- adf[3, 4] <- adf[5, 3] <- adf[7, 3] <- NA\nadt <- as.data.table(adf)\n\n\nadf[!is.na(adf$V3), ]\n\n           V1         V2         V3         V4          V5\n2  -1.5757546 -1.4608580  0.1111828  1.5812159  0.14120671\n3  -0.2646751 -0.6302734  0.5977524         NA  0.01795485\n4  -0.3430240  0.8000077 -1.8080567  0.7472244  0.82739404\n6   1.0582400 -0.7420054 -1.3492387 -1.1937673 -0.77901035\n8  -0.5440897 -0.9472814 -0.1800953 -0.6742933 -0.82677733\n9  -0.5628776  0.7821239  0.7993246 -0.2182100 -1.18673450\n10 -0.6635323 -0.4209989  1.6235166 -0.2309067  0.11117008\n\nadt[!is.na(V3)]\n\n           V1         V2         V3         V4          V5\n        <num>      <num>      <num>      <num>       <num>\n1: -1.5757546 -1.4608580  0.1111828  1.5812159  0.14120671\n2: -0.2646751 -0.6302734  0.5977524         NA  0.01795485\n3: -0.3430240  0.8000077 -1.8080567  0.7472244  0.82739404\n4:  1.0582400 -0.7420054 -1.3492387 -1.1937673 -0.77901035\n5: -0.5440897 -0.9472814 -0.1800953 -0.6742933 -0.82677733\n6: -0.5628776  0.7821239  0.7993246 -0.2182100 -1.18673450\n7: -0.6635323 -0.4209989  1.6235166 -0.2309067  0.11117008"
  },
  {
    "objectID": "60_DataTable.html#select-columns",
    "href": "60_DataTable.html#select-columns",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.7 Select columns",
    "text": "24.7 Select columns\n\n24.7.1 By position(s)\nSelecting a single column in data.table does not drop to a vector, similar to using drop = FALSE in a data.frame:\n\ndf[, 1]\n\n[1] 1 2 3 4 5\n\ndf[, 1, drop = FALSE]\n\n  A\n1 1\n2 2\n3 3\n4 4\n5 5\n\ndt[, 1]\n\n       A\n   <int>\n1:     1\n2:     2\n3:     3\n4:     4\n5:     5\n\n\nDouble bracket indexing of a single column works the same on a data.frame and a data.table, returning a vector:\n\ndf[[2]]\n\n[1] 1.2 4.3 9.7 5.6 8.1\n\ndt[[2]]\n\n[1] 1.2 4.3 9.7 5.6 8.1\n\n\nA vector of column positions returns a smaller data.table, similar to how it returns a smaller data.frame :\n\ndf[, c(1, 2)]\n\n  A   B\n1 1 1.2\n2 2 4.3\n3 3 9.7\n4 4 5.6\n5 5 8.1\n\ndt[, c(1, 2)]\n\n       A     B\n   <int> <num>\n1:     1   1.2\n2:     2   4.3\n3:     3   9.7\n4:     4   5.6\n5:     5   8.1\n\n\n\n\n24.7.2 By name(s)\nIn data.table, you access column names directly without quoting or using the $ notation:\n\ndf[, \"B\"]\n\n[1] 1.2 4.3 9.7 5.6 8.1\n\ndf$B\n\n[1] 1.2 4.3 9.7 5.6 8.1\n\ndt[, B]\n\n[1] 1.2 4.3 9.7 5.6 8.1\n\n\nBecause of the above, data.table requires a slightly different syntax to use a variable as a column index which can contain integer positions, logical index, or column names:\n\ncolid <- c(1, 2)\ncolbn <- c(F, T, T)\ncolnm <- c(\"A\", \"C\")\ndf[, colid]\n\n  A   B\n1 1 1.2\n2 2 4.3\n3 3 9.7\n4 4 5.6\n5 5 8.1\n\ndf[, colbn]\n\n    B C\n1 1.2 a\n2 4.3 b\n3 9.7 b\n4 5.6 a\n5 8.1 a\n\ndf[, colnm]\n\n  A C\n1 1 a\n2 2 b\n3 3 b\n4 4 a\n5 5 a\n\n\nTo use a variable holding a column index in a data.table, prefix it with two periods:\n\ndt[, ..colid]\n\n       A     B\n   <int> <num>\n1:     1   1.2\n2:     2   4.3\n3:     3   9.7\n4:     4   5.6\n5:     5   8.1\n\ndt[, ..colbn]\n\n       B      C\n   <num> <fctr>\n1:   1.2      a\n2:   4.3      b\n3:   9.7      b\n4:   5.6      a\n5:   8.1      a\n\ndt[, ..colnm]\n\n       A      C\n   <int> <fctr>\n1:     1      a\n2:     2      b\n3:     3      b\n4:     4      a\n5:     5      a\n\n\nIf you are familiar with the system shell:\nThink of working inside the data.table frame (i.e. within the “[…]”) like an environment. You have direct access to the variables within it. If you want to refer to variables outside the data.table, you prefix the variable name with .. similar to how you access the directory above your current working directory in the system shell:\nAlternatively, you can use the .SD special symbol together with the .SDcols argument:\n\ndt[, .SD, .SDcols = colid]\n\n       A     B\n   <int> <num>\n1:     1   1.2\n2:     2   4.3\n3:     3   9.7\n4:     4   5.6\n5:     5   8.1\n\n\nThink of .SD as a sub-data.table with columns defined by .SDcols (if SDcols is not defined, .SD refers to the entire data.table).\nThe two dots tell the data.table to not look for the variable within the data.table columns, but in the enclosing environment.\nSelecting a single column by name returns a vector:\n\ndt[, A]\n\n[1] 1 2 3 4 5\n\n\nSelecting one or more columns by name enclosed in list() or .() (which, in this case, is short for list()), always returns a data.table:\n\ndt[, .(A)]\n\n       A\n   <int>\n1:     1\n2:     2\n3:     3\n4:     4\n5:     5\n\ndt[, .(A, B)]\n\n       A     B\n   <int> <num>\n1:     1   1.2\n2:     2   4.3\n3:     3   9.7\n4:     4   5.6\n5:     5   8.1"
  },
  {
    "objectID": "60_DataTable.html#add-new-column-in-place",
    "href": "60_DataTable.html#add-new-column-in-place",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.8 Add new column in-place",
    "text": "24.8 Add new column in-place\nUse := assignment to add a new column in the existing data.table. In-place assignment means you do not have to assign the result to a variable, the existing data.table will be modified.\n\ndt[, AplusB := A + B]\ndt\n\n       A     B      C AplusB\n   <int> <num> <fctr>  <num>\n1:     1   1.2      a    2.2\n2:     2   4.3      b    6.3\n3:     3   9.7      b   12.7\n4:     4   5.6      a    9.6\n5:     5   8.1      a   13.1\n\n\nNote how dt was modified even though we did not run dt <- dt[, AplusB := A + B]"
  },
  {
    "objectID": "60_DataTable.html#add-multiple-columns-in-place",
    "href": "60_DataTable.html#add-multiple-columns-in-place",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.9 Add multiple columns in-place",
    "text": "24.9 Add multiple columns in-place\nYou can define multiple new column names using a character vector of new column names on the left of := and a list on the right.\n\ndt[, c(\"AtimesB\", \"AoverB\") := list(A*B, A/B)]\n\nWe can use lapply() since it always returns a list:\n\nvnames <- c(\"A\", \"B\")\ndt[, paste0(\"log\", vnames) := lapply(.SD, log), .SDcols = vnames]\ndt\n\n       A     B      C AplusB AtimesB    AoverB      logA      logB\n   <int> <num> <fctr>  <num>   <num>     <num>     <num>     <num>\n1:     1   1.2      a    2.2     1.2 0.8333333 0.0000000 0.1823216\n2:     2   4.3      b    6.3     8.6 0.4651163 0.6931472 1.4586150\n3:     3   9.7      b   12.7    29.1 0.3092784 1.0986123 2.2721259\n4:     4   5.6      a    9.6    22.4 0.7142857 1.3862944 1.7227666\n5:     5   8.1      a   13.1    40.5 0.6172840 1.6094379 2.0918641\n\n\nYou can also use := in a little more awkward syntax:\n\ndt[, `:=`(AminusB = A - B, AoverC = A / B)]\ndt\n\n       A     B      C AplusB AtimesB    AoverB      logA      logB AminusB\n   <int> <num> <fctr>  <num>   <num>     <num>     <num>     <num>   <num>\n1:     1   1.2      a    2.2     1.2 0.8333333 0.0000000 0.1823216    -0.2\n2:     2   4.3      b    6.3     8.6 0.4651163 0.6931472 1.4586150    -2.3\n3:     3   9.7      b   12.7    29.1 0.3092784 1.0986123 2.2721259    -6.7\n4:     4   5.6      a    9.6    22.4 0.7142857 1.3862944 1.7227666    -1.6\n5:     5   8.1      a   13.1    40.5 0.6172840 1.6094379 2.0918641    -3.1\n      AoverC\n       <num>\n1: 0.8333333\n2: 0.4651163\n3: 0.3092784\n4: 0.7142857\n5: 0.6172840"
  },
  {
    "objectID": "60_DataTable.html#convert-column-type",
    "href": "60_DataTable.html#convert-column-type",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.10 Convert column type",
    "text": "24.10 Convert column type\n\n24.10.1 Assignment by reference with :=\nUse any base R coercion function (as.*) to convert a column in-place using the := notation\n\ndt[, A := as.numeric(A)]\ndt\n\n       A     B      C AplusB AtimesB    AoverB      logA      logB AminusB\n   <num> <num> <fctr>  <num>   <num>     <num>     <num>     <num>   <num>\n1:     1   1.2      a    2.2     1.2 0.8333333 0.0000000 0.1823216    -0.2\n2:     2   4.3      b    6.3     8.6 0.4651163 0.6931472 1.4586150    -2.3\n3:     3   9.7      b   12.7    29.1 0.3092784 1.0986123 2.2721259    -6.7\n4:     4   5.6      a    9.6    22.4 0.7142857 1.3862944 1.7227666    -1.6\n5:     5   8.1      a   13.1    40.5 0.6172840 1.6094379 2.0918641    -3.1\n      AoverC\n       <num>\n1: 0.8333333\n2: 0.4651163\n3: 0.3092784\n4: 0.7142857\n5: 0.6172840\n\n\n\n\n24.10.2 Delete columns in-place with :=\nTo delete a column, use := to set it to NULL:\n\ndt[, AoverB := NULL]\ndt\n\n       A     B      C AplusB AtimesB      logA      logB AminusB    AoverC\n   <num> <num> <fctr>  <num>   <num>     <num>     <num>   <num>     <num>\n1:     1   1.2      a    2.2     1.2 0.0000000 0.1823216    -0.2 0.8333333\n2:     2   4.3      b    6.3     8.6 0.6931472 1.4586150    -2.3 0.4651163\n3:     3   9.7      b   12.7    29.1 1.0986123 2.2721259    -6.7 0.3092784\n4:     4   5.6      a    9.6    22.4 1.3862944 1.7227666    -1.6 0.7142857\n5:     5   8.1      a   13.1    40.5 1.6094379 2.0918641    -3.1 0.6172840\n\n\nDelete multiple columns\n\ndt[, c(\"logA\", \"logB\") := NULL]\n\nOr:\n\ndt[, `:=`(AplusB = NULL, AminusB = NULL)]\ndt\n\n       A     B      C AtimesB    AoverC\n   <num> <num> <fctr>   <num>     <num>\n1:     1   1.2      a     1.2 0.8333333\n2:     2   4.3      b     8.6 0.4651163\n3:     3   9.7      b    29.1 0.3092784\n4:     4   5.6      a    22.4 0.7142857\n5:     5   8.1      a    40.5 0.6172840\n\n\n\n\n24.10.3 Fast loop-able assignment with set()\ndata.table’s set() is a looop-able version of the \"= operator. Use it in a for loop to operate on multiple columns.\nSyntax: set(dt, i, j, value)\n\ndt the data.table to operate on\ni optionally define which rows to operate on. i = NULL to operate on all\nj column names or index to be assigned value\nvalue values to be assigned to j by reference\n\nAs a simple example, transform the first two columns in-place by squaring:\n\nfor (i in 1:2) {\n  set(dt, i = NULL, j = i, value = dt[[i]]^2)\n}"
  },
  {
    "objectID": "60_DataTable.html#summarize",
    "href": "60_DataTable.html#summarize",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.11 Summarize",
    "text": "24.11 Summarize\nYou can apply one or multiple summary functions on one of multiple columns. Surround the operations in list() or .() to output a new data.table holding the outputs of the operations (the input data.table remains unchanged).\n\nAsummary <- dt[, .(Amax = max(A), Amin = min(A), Asd = sd(A))]\nAsummary\n\n    Amax  Amin     Asd\n   <num> <num>   <num>\n1:    25     1 9.66954\n\n\nExample: Get sd of all numeric columns:\n\nnumid <- sapply(dt, is.numeric)\ndt_mean <- dt[, lapply(.SD, sd), .SDcols = numid]\ndt_mean\n\n         A        B  AtimesB    AoverC\n     <num>    <num>    <num>     <num>\n1: 9.66954 37.35521 15.74462 0.2060219\n\n\nIf your function returns more than one value, the output will have multiple rows:\n\ndt_range <- dt[, lapply(.SD, range), .SDcols = numid]\ndt_range\n\n       A     B AtimesB    AoverC\n   <num> <num>   <num>     <num>\n1:     1  1.44     1.2 0.3092784\n2:    25 94.09    40.5 0.8333333"
  },
  {
    "objectID": "60_DataTable.html#group-by-operations",
    "href": "60_DataTable.html#group-by-operations",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.12 Group-by operations",
    "text": "24.12 Group-by operations\nUp to now, we have learned how to use the data.table frame dat[i, j] to filter cases in i or add/remove/transform columns in-place in j. dat[i, j, by] allows to perform operations separately on groups of cases.\n\ndt <- data.table(A = 1:5,\n                 B = c(1.2, 4.3, 9.7, 5.6, 8.1),\n                 C = rnorm(5),\n                 Group = c(\"a\", \"b\", \"b\", \"a\", \"a\"))\ndt\n\n       A     B           C  Group\n   <int> <num>       <num> <char>\n1:     1   1.2 -0.01919352      a\n2:     2   4.3  1.76291989      b\n3:     3   9.7 -1.07362454      b\n4:     4   5.6 -0.40038723      a\n5:     5   8.1  0.59648264      a\n\n\n\n24.12.1 Group-by summary\nAs we’ve seen, using .() or list() in j, returns a new data.table:\n\ndt[, .(meanAbyGroup = mean(A)), by = Group]\n\n    Group meanAbyGroup\n   <char>        <num>\n1:      a     3.333333\n2:      b     2.500000\n\ndt[, list(medianBbyGroup = median(B)), by = Group]\n\n    Group medianBbyGroup\n   <char>          <num>\n1:      a            5.6\n2:      b            7.0\n\n\n\n\n24.12.2 Group-by operation and assignment\nMaking an assignment with := in j, adds a column in-place. Since here we are grouping, the same value will be assigned to all cases of the group:\n\ndt[, meanAbyGroup := mean(A), by = Group]\ndt\n\n       A     B           C  Group meanAbyGroup\n   <int> <num>       <num> <char>        <num>\n1:     1   1.2 -0.01919352      a     3.333333\n2:     2   4.3  1.76291989      b     2.500000\n3:     3   9.7 -1.07362454      b     2.500000\n4:     4   5.6 -0.40038723      a     3.333333\n5:     5   8.1  0.59648264      a     3.333333\n\n\nFor more complex operations, you may need to refer to the slice of the data.table defined by by within j. There is a special notation for this: .SD (think sub-data.table):\n\ndt[, B_DiffFromGroupMin := B - min(B), by = Group]\ndt\n\n       A     B           C  Group meanAbyGroup B_DiffFromGroupMin\n   <int> <num>       <num> <char>        <num>              <num>\n1:     1   1.2 -0.01919352      a     3.333333                0.0\n2:     2   4.3  1.76291989      b     2.500000                0.0\n3:     3   9.7 -1.07362454      b     2.500000                5.4\n4:     4   5.6 -0.40038723      a     3.333333                4.4\n5:     5   8.1  0.59648264      a     3.333333                6.9\n\n\n\n\n\nBy now, it should be clearer that the data.table frame provides a very flexible way to perform a very wide range of operations with minimal new notation."
  },
  {
    "objectID": "60_DataTable.html#apply-functions-to-columns",
    "href": "60_DataTable.html#apply-functions-to-columns",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.13 Apply functions to columns",
    "text": "24.13 Apply functions to columns\nAny function that returns a list can be used in j to return a new data.table - therefore lapply is perfect for getting summary on multiple columns:\n\ndt1 <- as.data.table(sapply(1:3, \\(i) rnorm(10)))\ndt1\n\n            V1          V2          V3\n         <num>       <num>       <num>\n 1: -0.6453819 -0.54140706  0.89180270\n 2: -1.1746275 -1.45071346  0.56509219\n 3:  0.7874612 -0.52234949 -1.43719131\n 4: -0.1689379  0.95496904 -1.07333478\n 5: -0.3129802  1.09500706 -0.73230191\n 6: -0.6912456  0.76002077  0.22109337\n 7: -0.5406528 -0.47616616  1.41270075\n 8:  0.6509229 -0.65033604 -0.59408632\n 9: -0.3552445  0.90686899  0.09407682\n10: -0.9574488  0.03100915  0.19432775\n\nsetnames(dt1, names(dt1), c(\"Alpha\", \"Beta\", \"Gamma\"))\ndt1[, lapply(.SD, mean)]\n\n        Alpha       Beta       Gamma\n        <num>      <num>       <num>\n1: -0.3408135 0.01069028 -0.04578207\n\n\nYou can specify which columns to operate on using the .SDcols argument:\n\ndt2 <- data.table(A = 1:5,\n                  B = c(1.2, 4.3, 9.7, 5.6, 8.1),\n                  C = rnorm(5),\n                  Group = c(\"a\", \"b\", \"b\", \"a\", \"a\"))\ndt2\n\n       A     B           C  Group\n   <int> <num>       <num> <char>\n1:     1   1.2  0.04655708      a\n2:     2   4.3  0.84235568      b\n3:     3   9.7 -0.57372497      b\n4:     4   5.6 -1.34500563      a\n5:     5   8.1 -0.52578439      a\n\ndt2[, lapply(.SD, mean), .SDcols = 1:2]\n\n       A     B\n   <num> <num>\n1:     3  5.78\n\n# same as\ndt2[, lapply(.SD, mean), .SDcols = c(\"A\", \"B\")]\n\n       A     B\n   <num> <num>\n1:     3  5.78\n\ncols <- c(\"A\", \"B\")\ndt2[, lapply(.SD, mean), .SDcols = cols]\n\n       A     B\n   <num> <num>\n1:     3  5.78\n\n\nYou can combine .SDcols and by:\n\ndt2[, lapply(.SD, median), .SDcols = c(\"B\", \"C\"), by = Group]\n\n    Group     B          C\n   <char> <num>      <num>\n1:      a   5.6 -0.5257844\n2:      b   7.0  0.1343154\n\n\nCreate multiple new columns from transformation of existing and store with custom prefix:\n\ndt1\n\n         Alpha        Beta       Gamma\n         <num>       <num>       <num>\n 1: -0.6453819 -0.54140706  0.89180270\n 2: -1.1746275 -1.45071346  0.56509219\n 3:  0.7874612 -0.52234949 -1.43719131\n 4: -0.1689379  0.95496904 -1.07333478\n 5: -0.3129802  1.09500706 -0.73230191\n 6: -0.6912456  0.76002077  0.22109337\n 7: -0.5406528 -0.47616616  1.41270075\n 8:  0.6509229 -0.65033604 -0.59408632\n 9: -0.3552445  0.90686899  0.09407682\n10: -0.9574488  0.03100915  0.19432775\n\ndt1[, paste0(names(dt1), \"_abs\") := lapply(.SD, abs)]\ndt1\n\n         Alpha        Beta       Gamma Alpha_abs   Beta_abs  Gamma_abs\n         <num>       <num>       <num>     <num>      <num>      <num>\n 1: -0.6453819 -0.54140706  0.89180270 0.6453819 0.54140706 0.89180270\n 2: -1.1746275 -1.45071346  0.56509219 1.1746275 1.45071346 0.56509219\n 3:  0.7874612 -0.52234949 -1.43719131 0.7874612 0.52234949 1.43719131\n 4: -0.1689379  0.95496904 -1.07333478 0.1689379 0.95496904 1.07333478\n 5: -0.3129802  1.09500706 -0.73230191 0.3129802 1.09500706 0.73230191\n 6: -0.6912456  0.76002077  0.22109337 0.6912456 0.76002077 0.22109337\n 7: -0.5406528 -0.47616616  1.41270075 0.5406528 0.47616616 1.41270075\n 8:  0.6509229 -0.65033604 -0.59408632 0.6509229 0.65033604 0.59408632\n 9: -0.3552445  0.90686899  0.09407682 0.3552445 0.90686899 0.09407682\n10: -0.9574488  0.03100915  0.19432775 0.9574488 0.03100915 0.19432775\n\n\n\ndt2\n\n       A     B           C  Group\n   <int> <num>       <num> <char>\n1:     1   1.2  0.04655708      a\n2:     2   4.3  0.84235568      b\n3:     3   9.7 -0.57372497      b\n4:     4   5.6 -1.34500563      a\n5:     5   8.1 -0.52578439      a\n\ncols <- c(\"A\", \"C\")\ndt2[, paste0(cols, \"_groupMean\") := lapply(.SD, mean), .SDcols = cols, by = Group]\ndt2\n\n       A     B           C  Group A_groupMean C_groupMean\n   <int> <num>       <num> <char>       <num>       <num>\n1:     1   1.2  0.04655708      a    3.333333  -0.6080776\n2:     2   4.3  0.84235568      b    2.500000   0.1343154\n3:     3   9.7 -0.57372497      b    2.500000   0.1343154\n4:     4   5.6 -1.34500563      a    3.333333  -0.6080776\n5:     5   8.1 -0.52578439      a    3.333333  -0.6080776"
  },
  {
    "objectID": "60_DataTable.html#row-wise-operations",
    "href": "60_DataTable.html#row-wise-operations",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.14 Row-wise operations",
    "text": "24.14 Row-wise operations\n\ndt <- data.table(a = 1:5, b = 11:15, c = 21:25, \n                 d = 31:35, e = 41:45)\ndt\n\n       a     b     c     d     e\n   <int> <int> <int> <int> <int>\n1:     1    11    21    31    41\n2:     2    12    22    32    42\n3:     3    13    23    33    43\n4:     4    14    24    34    44\n5:     5    15    25    35    45\n\n\nTo operate row-wise, we can use by = 1:nrow(dt). For example, to add a column, in-place, with row-wise sums of variables b through d:\n\ndt[, bcd.sum := sum(.SD[, b:d]), by = 1:nrow(dt)]\ndt\n\n       a     b     c     d     e bcd.sum\n   <int> <int> <int> <int> <int>   <int>\n1:     1    11    21    31    41      63\n2:     2    12    22    32    42      66\n3:     3    13    23    33    43      69\n4:     4    14    24    34    44      72\n5:     5    15    25    35    45      75"
  },
  {
    "objectID": "60_DataTable.html#melt",
    "href": "60_DataTable.html#melt",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.15 Wide <=> Long",
    "text": "24.15 Wide <=> Long\n\n24.15.1 Wide to long: melt()\n\ndt_wide <- data.table(ID = 1:4, Timepoint_A = 11:14,\n                      Timepoint_B = 21:24, Timepoint_C = 51:54)\ndt_wide\n\n      ID Timepoint_A Timepoint_B Timepoint_C\n   <int>       <int>       <int>       <int>\n1:     1          11          21          51\n2:     2          12          22          52\n3:     3          13          23          53\n4:     4          14          24          54\n\ndt_long <- melt(dt_wide, id.vars = \"ID\",\n                measure.vars = 2:4, # defaults to all non-id columns\n                variable.name = \"Timepoint\",\n                value.name = c(\"Score\"))\ndt_long\n\n       ID   Timepoint Score\n    <int>      <fctr> <int>\n 1:     1 Timepoint_A    11\n 2:     2 Timepoint_A    12\n 3:     3 Timepoint_A    13\n 4:     4 Timepoint_A    14\n 5:     1 Timepoint_B    21\n 6:     2 Timepoint_B    22\n 7:     3 Timepoint_B    23\n 8:     4 Timepoint_B    24\n 9:     1 Timepoint_C    51\n10:     2 Timepoint_C    52\n11:     3 Timepoint_C    53\n12:     4 Timepoint_C    54\n\n\n\n\n24.15.2 Long to wide: dcast()\n\ndt_long\n\n       ID   Timepoint Score\n    <int>      <fctr> <int>\n 1:     1 Timepoint_A    11\n 2:     2 Timepoint_A    12\n 3:     3 Timepoint_A    13\n 4:     4 Timepoint_A    14\n 5:     1 Timepoint_B    21\n 6:     2 Timepoint_B    22\n 7:     3 Timepoint_B    23\n 8:     4 Timepoint_B    24\n 9:     1 Timepoint_C    51\n10:     2 Timepoint_C    52\n11:     3 Timepoint_C    53\n12:     4 Timepoint_C    54\n\ndcast(dt_long, ID ~ Timepoint,\n      value.var = \"Score\")\n\n      ID Timepoint_A Timepoint_B Timepoint_C\n   <int>       <int>       <int>       <int>\n1:     1          11          21          51\n2:     2          12          22          52\n3:     3          13          23          53\n4:     4          14          24          54\n\n\n\n24.15.2.1 dcast() + aggregate\nIf your ID ~ Timepoint combination does not define a unique row in your input dataset, you need to specify an aggregate function.\nFor example, suppose you have four subjects with IDs “A”, “B”, “C”, “D” who had a couple variables measured 3 times in the AM and 3 times in the PM.\n\ndt_long2 <- data.table(ID = rep(LETTERS[1:4], each = 6),\n                      Timepoint = rep(c(\"AM\", \"PM\"), length.out = 24, each = 3),\n                      Var1 = rnorm(24, 10),\n                      Var2 = rnorm(24, 20))\n\ndt_long2[sample(24, 4), Var1 := NA]\ndt_long2[sample(24, 4), Var2 := NA]\ndt_long2\n\n        ID Timepoint      Var1     Var2\n    <char>    <char>     <num>    <num>\n 1:      A        AM 11.180429 18.83875\n 2:      A        AM        NA 19.75811\n 3:      A        AM 10.858896       NA\n 4:      A        PM  8.696999 20.67314\n 5:      A        PM        NA       NA\n 6:      A        PM 12.426356 20.00126\n 7:      B        AM  8.763709 19.83046\n 8:      B        AM  9.919536 19.57749\n 9:      B        AM  9.147396 19.40920\n10:      B        PM  9.056259 20.90345\n11:      B        PM 11.392569       NA\n12:      B        PM 10.241746 21.03389\n13:      C        AM 10.029698 20.75804\n14:      C        AM 12.481805 20.13352\n15:      C        AM  9.169376 20.60449\n16:      C        PM  8.320664 21.81184\n17:      C        PM 11.048416 20.99195\n18:      C        PM 11.660641 19.49680\n19:      D        AM  9.227025 19.06158\n20:      D        AM        NA 20.54046\n21:      D        AM 11.884037 20.68816\n22:      D        PM 10.878042       NA\n23:      D        PM 10.334032 19.21043\n24:      D        PM        NA 19.15026\n        ID Timepoint      Var1     Var2\n\n\nIf you wanted to convert the above data.table to wide format and get mean AM and PM values using the fun.aggregate argument:\n\ndcast(dt_long2,\n      ID ~ Timepoint,\n      value.var = c(\"Var1\", \"Var2\"),\n      fun.aggregate = mean, na.rm = T)\n\n       ID  Var1_AM  Var1_PM  Var2_AM  Var2_PM\n   <char>    <num>    <num>    <num>    <num>\n1:      A 11.01966 10.56168 19.29843 20.33720\n2:      B  9.27688 10.23019 19.60572 20.96867\n3:      C 10.56029 10.34324 20.49868 20.76686\n4:      D 10.55553 10.60604 20.09674 19.18035\n\n\nYou can apply multiple aggregating functions by passing a list to fun.aggregate:\n\ndcast(dt_long2,\n      ID ~ Timepoint,\n      value.var = c(\"Var1\", \"Var2\"),\n      fun.aggregate = list(mean, max, min), na.rm = T)\n\n       ID Var1_mean_AM Var1_mean_PM Var2_mean_AM Var2_mean_PM Var1_max_AM\n   <char>        <num>        <num>        <num>        <num>       <num>\n1:      A     11.01966     10.56168     19.29843     20.33720   11.180429\n2:      B      9.27688     10.23019     19.60572     20.96867    9.919536\n3:      C     10.56029     10.34324     20.49868     20.76686   12.481805\n4:      D     10.55553     10.60604     20.09674     19.18035   11.884037\n   Var1_max_PM Var2_max_AM Var2_max_PM Var1_min_AM Var1_min_PM Var2_min_AM\n         <num>       <num>       <num>       <num>       <num>       <num>\n1:    12.42636    19.75811    20.67314   10.858896    8.696999    18.83875\n2:    11.39257    19.83046    21.03389    8.763709    9.056259    19.40920\n3:    11.66064    20.75804    21.81184    9.169376    8.320664    20.13352\n4:    10.87804    20.68816    19.21043    9.227025   10.334032    19.06158\n   Var2_min_PM\n         <num>\n1:    20.00126\n2:    20.90345\n3:    19.49680\n4:    19.15026\n\n\nNote how na.rm = T was successfully applied to all aggregating functions"
  },
  {
    "objectID": "60_DataTable.html#table-joins",
    "href": "60_DataTable.html#table-joins",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.16 Table Joins",
    "text": "24.16 Table Joins\ndata.table allow you to perform table joins with the base merge() function using the same syntax as for data.frame objects or the “data.table way” using bracket notation:\n\na <- data.table(PID = c(1:9),\n                Hospital = c(\"UCSF\", \"HUP\", \"Stanford\", \n                             \"Stanford\", \"UCSF\", \"HUP\", \n                             \"HUP\", \"Stanford\", \"UCSF\"),\n                Age = c(22, 34, 41, 19, 53, 21, 63, 22, 19),\n                Sex = c(1, 1, 0, 1, 0, 0, 1, 0, 0),\n                key = \"PID\")\na\n\n     PID Hospital   Age   Sex\n   <int>   <char> <num> <num>\n1:     1     UCSF    22     1\n2:     2      HUP    34     1\n3:     3 Stanford    41     0\n4:     4 Stanford    19     1\n5:     5     UCSF    53     0\n6:     6      HUP    21     0\n7:     7      HUP    63     1\n8:     8 Stanford    22     0\n9:     9     UCSF    19     0\n\nb <- data.table(PID = c(6:12),\n                V1 = c(153, 89, 112, 228,  91, 190, 101),\n                Department = c(\"Neurology\", \"Radiology\", \"Emergency\",\n                               \"Cardiology\", \"Surgery\", \"Neurology\",\n                               \"Psychiatry\"),\n                key = \"PID\")\nb\n\n     PID    V1 Department\n   <int> <num>     <char>\n1:     6   153  Neurology\n2:     7    89  Radiology\n3:     8   112  Emergency\n4:     9   228 Cardiology\n5:    10    91    Surgery\n6:    11   190  Neurology\n7:    12   101 Psychiatry\n\n\nIn the above command we use the key argument to set PID as key. This can be performed after the data.table has been created using the setkey() command:\n\nsetkey(a, PID)\n\nMultiple keys can be set, in order, with the same setkey() command, separated by commas, e.g.:\n\nsetkey(a, PID, Hospital)\n\nKeys sort the data.table by the corresponding columns and can be used to perform left and right joins with bracket notation seen later.\n\n24.16.1 Inner\n\nmerge(a, b)\n\n     PID Hospital   Age   Sex    V1 Department\n   <int>   <char> <num> <num> <num>     <char>\n1:     6      HUP    21     0   153  Neurology\n2:     7      HUP    63     1    89  Radiology\n3:     8 Stanford    22     0   112  Emergency\n4:     9     UCSF    19     0   228 Cardiology\n\n\n\n\n24.16.2 Outer\n\nmerge(a, b, all = TRUE)\n\n      PID Hospital   Age   Sex    V1 Department\n    <int>   <char> <num> <num> <num>     <char>\n 1:     1     UCSF    22     1    NA       <NA>\n 2:     2      HUP    34     1    NA       <NA>\n 3:     3 Stanford    41     0    NA       <NA>\n 4:     4 Stanford    19     1    NA       <NA>\n 5:     5     UCSF    53     0    NA       <NA>\n 6:     6      HUP    21     0   153  Neurology\n 7:     7      HUP    63     1    89  Radiology\n 8:     8 Stanford    22     0   112  Emergency\n 9:     9     UCSF    19     0   228 Cardiology\n10:    10     <NA>    NA    NA    91    Surgery\n11:    11     <NA>    NA    NA   190  Neurology\n12:    12     <NA>    NA    NA   101 Psychiatry\n\n\n\n\n24.16.3 Left outer\nUsing merge():\n\nmerge(a, b, all.x = TRUE)\n\n     PID Hospital   Age   Sex    V1 Department\n   <int>   <char> <num> <num> <num>     <char>\n1:     1     UCSF    22     1    NA       <NA>\n2:     2      HUP    34     1    NA       <NA>\n3:     3 Stanford    41     0    NA       <NA>\n4:     4 Stanford    19     1    NA       <NA>\n5:     5     UCSF    53     0    NA       <NA>\n6:     6      HUP    21     0   153  Neurology\n7:     7      HUP    63     1    89  Radiology\n8:     8 Stanford    22     0   112  Emergency\n9:     9     UCSF    19     0   228 Cardiology\n\n\nUsing bracket notation:\n\nb[a, ]\n\n     PID    V1 Department Hospital   Age   Sex\n   <int> <num>     <char>   <char> <num> <num>\n1:     1    NA       <NA>     UCSF    22     1\n2:     2    NA       <NA>      HUP    34     1\n3:     3    NA       <NA> Stanford    41     0\n4:     4    NA       <NA> Stanford    19     1\n5:     5    NA       <NA>     UCSF    53     0\n6:     6   153  Neurology      HUP    21     0\n7:     7    89  Radiology      HUP    63     1\n8:     8   112  Emergency Stanford    22     0\n9:     9   228 Cardiology     UCSF    19     0\n\n\nIf keys were not set for a and b, you could specify the column to match on using the on argument:\n\nb[a, on = \"PID\"]\n\n     PID    V1 Department Hospital   Age   Sex\n   <int> <num>     <char>   <char> <num> <num>\n1:     1    NA       <NA>     UCSF    22     1\n2:     2    NA       <NA>      HUP    34     1\n3:     3    NA       <NA> Stanford    41     0\n4:     4    NA       <NA> Stanford    19     1\n5:     5    NA       <NA>     UCSF    53     0\n6:     6   153  Neurology      HUP    21     0\n7:     7    89  Radiology      HUP    63     1\n8:     8   112  Emergency Stanford    22     0\n9:     9   228 Cardiology     UCSF    19     0\n\n\n\n\n\nThe easy way to understand the bracket notation merges is to think that the data.table inside the bracket is used to index the data.table on the outside, therefore the resulting table will have rows dictated by the inside table’s key.\n\n\n\n\n\n24.16.4 Right outer\n\nmerge(a, b, all.y = TRUE)\n\n     PID Hospital   Age   Sex    V1 Department\n   <int>   <char> <num> <num> <num>     <char>\n1:     6      HUP    21     0   153  Neurology\n2:     7      HUP    63     1    89  Radiology\n3:     8 Stanford    22     0   112  Emergency\n4:     9     UCSF    19     0   228 Cardiology\n5:    10     <NA>    NA    NA    91    Surgery\n6:    11     <NA>    NA    NA   190  Neurology\n7:    12     <NA>    NA    NA   101 Psychiatry\n\n\nUsing bracket notation:\n\na[b, ]\n\n     PID Hospital   Age   Sex    V1 Department\n   <int>   <char> <num> <num> <num>     <char>\n1:     6      HUP    21     0   153  Neurology\n2:     7      HUP    63     1    89  Radiology\n3:     8 Stanford    22     0   112  Emergency\n4:     9     UCSF    19     0   228 Cardiology\n5:    10     <NA>    NA    NA    91    Surgery\n6:    11     <NA>    NA    NA   190  Neurology\n7:    12     <NA>    NA    NA   101 Psychiatry"
  },
  {
    "objectID": "60_DataTable.html#understanding-reference-semantics-in-data.table",
    "href": "60_DataTable.html#understanding-reference-semantics-in-data.table",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.17 Understanding reference semantics in data.table",
    "text": "24.17 Understanding reference semantics in data.table\n\n24.17.1 Get object’s location in memory with address()\nWhen you add a new column to an existing data.frame, the data.frame is copied behind the scenes - you can tell becasue its memory address (where it’s physically stored in your computer) changes:\n\ndf1 <- data.frame(alpha = 1:5, beta = 11:15)\naddress(df1)\n\n[1] \"0x105107e88\"\n\ndf1$gamma <- df1$alpha + df1$beta\naddress(df1)\n\n[1] \"0x105f21b08\"\n\n\nWhen you add a new column in a data.table in-place its address remains unchanged:\n\ndt1 <- data.table(alpha = 1:5, beta = 11:15)\naddress(dt1)\n\n[1] \"0x109ba7e00\"\n\ndt1[, gamma := alpha + beta]\naddress(dt1)\n\n[1] \"0x109ba7e00\"\n\n\n\n\n24.17.2 Reference semantics at work\nUp to now, you are likely used to working with regular R objects that behave like this:\n\ndf1 <- data.frame(a = rep(1, 5))\ndf1\n\n  a\n1 1\n2 1\n3 1\n4 1\n5 1\n\ndf2 <- df1\ndf2\n\n  a\n1 1\n2 1\n3 1\n4 1\n5 1\n\ndf2$a <- df2$a*2\ndf2\n\n  a\n1 2\n2 2\n3 2\n4 2\n5 2\n\ndf1\n\n  a\n1 1\n2 1\n3 1\n4 1\n5 1\n\naddress(df1)\n\n[1] \"0x105b7b2a8\"\n\naddress(df2)\n\n[1] \"0x105d9d660\"\n\n\ndata.table uses “reference semantics” or “pass-by-reference”. Be very careful or you might be mightily confused:\n\ndt1 <- data.table(a = rep(1, 5))\ndt1\n\n       a\n   <num>\n1:     1\n2:     1\n3:     1\n4:     1\n5:     1\n\ndt2 <- dt1\ndt2\n\n       a\n   <num>\n1:     1\n2:     1\n3:     1\n4:     1\n5:     1\n\ndt2[, a := a * 2]\ndt2\n\n       a\n   <num>\n1:     2\n2:     2\n3:     2\n4:     2\n5:     2\n\ndt1\n\n       a\n   <num>\n1:     2\n2:     2\n3:     2\n4:     2\n5:     2\n\naddress(dt1)\n\n[1] \"0x119caa600\"\n\naddress(dt2)\n\n[1] \"0x119caa600\"\n\n\n\n\n\nIf you want to create a copy of a data.table, use copy():\n\n\n\n\ndt3 <- copy(dt1)\ndt3\n\n       a\n   <num>\n1:     2\n2:     2\n3:     2\n4:     2\n5:     2\n\naddress(dt3)\n\n[1] \"0x10491ba00\"\n\ndt3[, a := a * 2]\ndt3\n\n       a\n   <num>\n1:     4\n2:     4\n3:     4\n4:     4\n5:     4\n\ndt1\n\n       a\n   <num>\n1:     2\n2:     2\n3:     2\n4:     2\n5:     2"
  },
  {
    "objectID": "60_DataTable.html#dtresources",
    "href": "60_DataTable.html#dtresources",
    "title": "24  Efficient data analysis with data.table",
    "section": "24.18 Resources",
    "text": "24.18 Resources\ndata.table GitHub data.table docs Introduction to data.table vignette"
  },
  {
    "objectID": "62_dplyr.html",
    "href": "62_dplyr.html",
    "title": "25  Introduction to dplyr",
    "section": "",
    "text": "The dplyr package offers functionality for data manipulation and is part of what is known as the tidyverse.\ndplyr’s functions are named after verbs and depend heavily on the usage of the pipe operator to build pipelines. The package offers a large number of functions in total, often with multiple versions of the same “verb”. It has undergone many major changes since its introduction, so always make sure to consult the latest documentation. Some of the welcome recent changes aim to reduce the total number of functions exported by the package.\nCore operations include:\ndplyr operates on data.frames as well as the tidyverse’s data.frame replacement, known by the bizarre name of tibble. Here, we convert iris to a tibble as an easy way to limit the number of rows printed by default in the output (without changing other Rmarkdown options, custom hooks, etc.) and as an introduction to tiblles.\nNote that dplyr masks a number of builtin functions when loaded."
  },
  {
    "objectID": "62_dplyr.html#filter",
    "href": "62_dplyr.html#filter",
    "title": "25  Introduction to dplyr",
    "section": "25.1 Filter",
    "text": "25.1 Filter\n\niris |> filter(Species == \"setosa\")\n\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# … with 40 more rows"
  },
  {
    "objectID": "62_dplyr.html#select",
    "href": "62_dplyr.html#select",
    "title": "25  Introduction to dplyr",
    "section": "25.2 Select",
    "text": "25.2 Select\n\niris |> select(\"Sepal.Length\")\n\n# A tibble: 150 × 1\n   Sepal.Length\n          <dbl>\n 1          5.1\n 2          4.9\n 3          4.7\n 4          4.6\n 5          5  \n 6          5.4\n 7          4.6\n 8          5  \n 9          4.4\n10          4.9\n# … with 140 more rows"
  },
  {
    "objectID": "62_dplyr.html#mutate",
    "href": "62_dplyr.html#mutate",
    "title": "25  Introduction to dplyr",
    "section": "25.3 Mutate",
    "text": "25.3 Mutate\n\niris |> mutate(Sepal_minus_Petal_length = Sepal.Length - Petal.Length)\n\n# A tibble: 150 × 6\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species Sepal_minus_Petal_…\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>                 <dbl>\n 1          5.1         3.5          1.4         0.2 setosa                  3.7\n 2          4.9         3            1.4         0.2 setosa                  3.5\n 3          4.7         3.2          1.3         0.2 setosa                  3.4\n 4          4.6         3.1          1.5         0.2 setosa                  3.1\n 5          5           3.6          1.4         0.2 setosa                  3.6\n 6          5.4         3.9          1.7         0.4 setosa                  3.7\n 7          4.6         3.4          1.4         0.3 setosa                  3.2\n 8          5           3.4          1.5         0.2 setosa                  3.5\n 9          4.4         2.9          1.4         0.2 setosa                  3  \n10          4.9         3.1          1.5         0.1 setosa                  3.4\n# … with 140 more rows\n\n\n\n25.3.1 Grouped\n\niris |> \n  group_by(Species) |> \n  mutate(Sepal.Length_minus_mean = Sepal.Length - mean(Sepal.Length))\n\n# A tibble: 150 × 6\n# Groups:   Species [3]\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species Sepal.Length_minus…\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>                 <dbl>\n 1          5.1         3.5          1.4         0.2 setosa              0.0940 \n 2          4.9         3            1.4         0.2 setosa             -0.106  \n 3          4.7         3.2          1.3         0.2 setosa             -0.306  \n 4          4.6         3.1          1.5         0.2 setosa             -0.406  \n 5          5           3.6          1.4         0.2 setosa             -0.00600\n 6          5.4         3.9          1.7         0.4 setosa              0.394  \n 7          4.6         3.4          1.4         0.3 setosa             -0.406  \n 8          5           3.4          1.5         0.2 setosa             -0.00600\n 9          4.4         2.9          1.4         0.2 setosa             -0.606  \n10          4.9         3.1          1.5         0.1 setosa             -0.106  \n# … with 140 more rows"
  },
  {
    "objectID": "62_dplyr.html#summarize",
    "href": "62_dplyr.html#summarize",
    "title": "25  Introduction to dplyr",
    "section": "25.4 Summarize",
    "text": "25.4 Summarize\n\n25.4.1 Single variable\n\niris |> summarize(mean(Sepal.Length))\n\n# A tibble: 1 × 1\n  `mean(Sepal.Length)`\n                 <dbl>\n1                 5.84\n\n\n\n\n25.4.2 Multiple variables\n\niris |> summarize(across(c(Sepal.Length, Petal.Length), mean))\n\n# A tibble: 1 × 2\n  Sepal.Length Petal.Length\n         <dbl>        <dbl>\n1         5.84         3.76\n\n\n\n\n25.4.3 Grouped single var\n\niris |> \n  group_by(Species) |> \n  summarize(mean(Sepal.Length))\n\n# A tibble: 3 × 2\n  Species    `mean(Sepal.Length)`\n  <fct>                     <dbl>\n1 setosa                     5.01\n2 versicolor                 5.94\n3 virginica                  6.59\n\n\n\n\n25.4.4 Grouped multivar\n\niris |> \n  group_by(Species) |> \n  summarize(across(c(Sepal.Length, Petal.Length), mean))\n\n# A tibble: 3 × 3\n  Species    Sepal.Length Petal.Length\n  <fct>             <dbl>        <dbl>\n1 setosa             5.01         1.46\n2 versicolor         5.94         4.26\n3 virginica          6.59         5.55"
  },
  {
    "objectID": "62_dplyr.html#arrange",
    "href": "62_dplyr.html#arrange",
    "title": "25  Introduction to dplyr",
    "section": "25.5 Arrange",
    "text": "25.5 Arrange\n\niris |> arrange(Sepal.Length)\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          4.3         3            1.1         0.1 setosa \n 2          4.4         2.9          1.4         0.2 setosa \n 3          4.4         3            1.3         0.2 setosa \n 4          4.4         3.2          1.3         0.2 setosa \n 5          4.5         2.3          1.3         0.3 setosa \n 6          4.6         3.1          1.5         0.2 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          4.6         3.6          1           0.2 setosa \n 9          4.6         3.2          1.4         0.2 setosa \n10          4.7         3.2          1.3         0.2 setosa \n# … with 140 more rows\n\n\n\n25.5.1 Grouped\n\niris |> \n  group_by(Species) |> \n  arrange(Sepal.Length)\n\n# A tibble: 150 × 5\n# Groups:   Species [3]\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          4.3         3            1.1         0.1 setosa \n 2          4.4         2.9          1.4         0.2 setosa \n 3          4.4         3            1.3         0.2 setosa \n 4          4.4         3.2          1.3         0.2 setosa \n 5          4.5         2.3          1.3         0.3 setosa \n 6          4.6         3.1          1.5         0.2 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          4.6         3.6          1           0.2 setosa \n 9          4.6         3.2          1.4         0.2 setosa \n10          4.7         3.2          1.3         0.2 setosa \n# … with 140 more rows"
  },
  {
    "objectID": "62_dplyr.html#specifying-multiple-variables",
    "href": "62_dplyr.html#specifying-multiple-variables",
    "title": "25  Introduction to dplyr",
    "section": "25.6 Specifying multiple variables",
    "text": "25.6 Specifying multiple variables\ndplyr includes a number of ways to identify multiple variables. The latest version of dplyr suggests using across() within dplyr functions that allow specifying columns.\nThis replaces separate functions previously used for each of filter/select/mutate/summarize/arrange that had independent functions ending with *_all(), *_at(), *_each(), *_if().\nWe’ll use summarize() to demonstrate.\n\n25.6.1 By integer column index\n\niris |> summarize(across(1:4, mean))\n\n# A tibble: 1 × 4\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n         <dbl>       <dbl>        <dbl>       <dbl>\n1         5.84        3.06         3.76        1.20\n\n\n\n\n25.6.2 By character name range\n\niris |> summarize(across(Sepal.Length:Petal.Width, mean))\n\n# A tibble: 1 × 4\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n         <dbl>       <dbl>        <dbl>       <dbl>\n1         5.84        3.06         3.76        1.20\n\n\n\n\n25.6.3 Pattern-matching\n\niris |> summarize(across(starts_with(\"Sepal\"), mean))\n\n# A tibble: 1 × 2\n  Sepal.Length Sepal.Width\n         <dbl>       <dbl>\n1         5.84        3.06\n\n\n\niris |> summarize(across(ends_with(\"Length\"), mean))\n\n# A tibble: 1 × 2\n  Sepal.Length Petal.Length\n         <dbl>        <dbl>\n1         5.84         3.76\n\n\n\n\n25.6.4 Using predicate function wrapped in where()\n\niris |> summarize(across(where(is.numeric), mean))\n\n# A tibble: 1 × 4\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n         <dbl>       <dbl>        <dbl>       <dbl>\n1         5.84        3.06         3.76        1.20\n\n\nUsing table() in this way does not output level names:\n\niris |> summarize(across(where(is.factor), table))\n\n# A tibble: 3 × 1\n  Species\n  <table>\n1 50     \n2 50     \n3 50"
  },
  {
    "objectID": "62_dplyr.html#dplyrresources",
    "href": "62_dplyr.html#dplyrresources",
    "title": "25  Introduction to dplyr",
    "section": "25.7 Resources",
    "text": "25.7 Resources\n\ndplyr website\ndplyr cheatsheet"
  },
  {
    "objectID": "70_BaseGraphics.html",
    "href": "70_BaseGraphics.html",
    "title": "26  Base Graphics",
    "section": "",
    "text": "R has powerful graphical capabilities built in to the core language. This chapter is an introduction to what is known as base graphics which is provided by the graphics built-in package. Their defaults produce minimalist plots, but they can be customized extensively. In this chapter we shall begin with default plots and demonstrate some of the more common/useful ways to customize them.\nR documentation for each of the above commands provides extensive coverage of graphical parameters. ?par gives the main documentation file for a long list of graphical parameters. These can be set either with the par() command before using any plotting command.\nLet’s create some synthetic data:"
  },
  {
    "objectID": "70_BaseGraphics.html#scatter-plot",
    "href": "70_BaseGraphics.html#scatter-plot",
    "title": "26  Base Graphics",
    "section": "26.1 Scatter plot",
    "text": "26.1 Scatter plot\nInput: 2 numeric vectors\nA 2D scatterplot displays points using two numeric vectors as X and Y coordinates.\n\nplot(x, y)\n\n\n\n\n\n26.1.1 col: point color\nSee Colors in R to learn about the different ways to define colors in R.\nSome common ways include:\n\nBy name using one of 657 names given by colors(), e.g. “red”, “magenta”, “blue”, “navy”, “cyan”\nBy RGB code\n\n\nplot(x, y, col = \"red\")\n\n\n\n\n\n\n26.1.2 bty: box type\nThere are 7 bty options: “o” “l”, “7”, “c”, “u”, or “]” and “none”. They produce a box that resembles the corresponding symbol. “none” draws no box but allows the axes to show:\n\nplot(x, y, bty = \"l\")\n\n\n\n\n\nplot(x, y, bty = \"none\")\n\n\n\n\n\n\n26.1.3 pch: point character\nThe default point character is a circle as seen above. This helps visualize overlapping points (especially for devices that do not support transparency).\nThere are 25 point characters, designated by integers 1 through 25.\nHere’s a preview of all 25 pch options. pch types 21 through 25 can be filled by a color specified by bg.\n\nplot(1:25, rep(1, 25), pch = 1:25, bg = \"blue\")\n\n\n\n\nLet’s use a solid disc:\n\nplot(x, y, bty = \"n\", pch = 16)\n\n\n\n\nWe cannot tell how many points are overlapping in the middle and therefore it’s a good idea to make the points a little transparent.\nThere are different ways to add transparency (see Colors). The easiest way is probably to use adjustcolor(). In the context of colors, alpha refers to transparency: a = 1 is opaque and a = 0 is completely transparent (therefore use a value greater than 0).\n\nplot(x, y,\n     bty = \"n\", pch = 16,\n     col = adjustcolor(\"skyblue\", alpha.f = .5))\n\n\n\n\n\n\n26.1.4 grid\nWe can add a grid behind the plot area using the panel.first argument, which accepts a graphical expression (a function that draws something), which will be evaluated before plotting the points on the graph (therefore appears behind the points).\n\nplot(x, y,\n     bty = \"n\", pch = 16,\n     col = adjustcolor(\"skyblue\", alpha.f = .5),\n     panel.first = grid(lty = 1, col = 'gray90'))\n\n\n\n\n\n\n26.1.5 main, xlab, ylab: Title and axes labels\n\nplot(x, y,\n     bty = \"n\", pch = 16,\n     col = adjustcolor(\"skyblue\", alpha.f = .5),\n     panel.first = grid(lty = 1, col = 'gray90'),\n     main = \"y vs. x\",\n     xlab = \"Variable x (xunits)\",\n     ylab = \"Variable y (yunits)\")\n\n\n\n\nNote that depending on where you intend to display the plot, you may leave the title blank and instead place it in the figure caption along with an explanation of the data (e.g. in a journal article)"
  },
  {
    "objectID": "70_BaseGraphics.html#histogram",
    "href": "70_BaseGraphics.html#histogram",
    "title": "26  Base Graphics",
    "section": "26.2 Histogram",
    "text": "26.2 Histogram\nInput: numeric vector\nA histogram displays an approximation of the distribution of a numeric vector. First the data is binned and then the number of elements that falls in each bin is counted. The histogram plot draws bars for each bin whose heights corresponds to the count of elements in the corresponding interval.\n\nhist(x)\n\n\n\n\n\n26.2.1 col: bar color\n\nhist(x, col = \"slategrey\")\n\n\n\n\n\n\n26.2.2 border: border color\nSetting border color to the same as the background gives a clean look:\n\nhist(x, col = \"slategrey\", border = \"white\")\n\n\n\n\n\n\n26.2.3 breaks: number or value of breakpoints\nThe breaks argument can be used to define the breakpoints to use for the binning of the values of the input to hist(). See the documentation in ?hist for the full range of options. An easy way to control the number of bins is to pass an integer to the breaks argument. Depending on the length of x and its distribution, it may or may not be possible to use the exact number requested, but the closest possible number will be automatically chosen.\n\nhist(x, col = \"slategrey\", border = \"white\",\n     breaks = 8)"
  },
  {
    "objectID": "70_BaseGraphics.html#density-plot",
    "href": "70_BaseGraphics.html#density-plot",
    "title": "26  Base Graphics",
    "section": "26.3 Density plot",
    "text": "26.3 Density plot\nInput: numeric vector\nA density plot is a different way to display an approximation of the distribution of a numeric vector. The density() function estimates the density of x and can be passed to plot() directly:\n\nplot(density(x))\n\n\n\n\nYou can use main = NA or main = \"\" to suppress printing a title.\n\nplot(density(x), col = \"blue\",\n     bty = \"n\",\n     main = NA)"
  },
  {
    "objectID": "70_BaseGraphics.html#barplot",
    "href": "70_BaseGraphics.html#barplot",
    "title": "26  Base Graphics",
    "section": "26.4 Barplot",
    "text": "26.4 Barplot\nInput: vector or matrix\nLet’s look at the VADeaths built-in dataset which describes death rater per 1000 population per year broken down by age range and population group.\n\n26.4.1 Single vector\nWe can plot a single column or row. Note how R automatically gets the corresponding dimension names. For this example we use the builtin VADeaths dataset, which is a matrix.\n\nbarplot(VADeaths[, 1])\n\n\n\n\n\nbarplot(VADeaths[1, ])\n\n\n\n\n\n26.4.1.1 col and border: bar fill and border color\nAs in most plotting functions, color is controlled by the col argument. border can be set to any color separately, or to NA to omit, which gives a clean look:\n\nbarplot(VADeaths[, 1],\n        col = \"aquamarine3\", border = NA)\n\n\n\n\n\n\n\n26.4.2 Matrix\nWe can draw barplots of multiple columns at the same time by passing a matrix input. The grouping on the x-axis is based on the columns. By default, data from different rows is stacked. The argument legend.text can be used to add a legend with the row labels:\n\nbarplot(VADeaths, legend.text = TRUE)\n\n\n\n\nAlternatively, we can draw groups of bars beside each other with the argument beside = TRUE:\n\nbarplot(VADeaths, beside = TRUE,\n        legend.text = TRUE, args.legend = list(x = \"topright\"))\n\n\n\n\nTo use custom colors, we pass a vector of length equal to the number of bars within each group. These will get recycled across groups, giving a consistent color coding.\nHere, we use the adjustcolor() function again to produce 5 shades of navy.\n\ncol <- sapply(seq(.2, .8, length.out = 5), function(i) adjustcolor(\"navy\", i))\nbarplot(VADeaths,\n        col = col,\n        border = NA,\n        beside = TRUE,\n        legend.text = TRUE, args.legend = list(x = \"topright\"))\n\n\n\n\n\n\n26.4.3 Formula interface"
  },
  {
    "objectID": "70_BaseGraphics.html#boxplot",
    "href": "70_BaseGraphics.html#boxplot",
    "title": "26  Base Graphics",
    "section": "26.5 Boxplot",
    "text": "26.5 Boxplot\nInput: One or more vectors of any length\nA boxplot is another way to visualize the distribution of one or more vectors. Each vector does not need to be of the same length. For example if you are plotting lab results of a patient and control group, they do not have to contain the same number of individuals.\nThere are two ways to use the boxplot() function. Either pass two separate vectors of data (whet)\nboxplot() makes it easy to plot your data from different objects. It can accept:\n\nindividual vectors\ncolumns of a matrix, columns/elements of a data.frame, elements of a list\nformula interface of the form variable ~ factor\n\n\n26.5.1 Single vector\n\na <- rnorm(500, mean = 12, sd = 2)\nboxplot(a)\n\n\n\n\n\n\n26.5.2 Anatomy of a boxplot\nA boxplot shows:\n\nthe median\nfirst and third quartiles\noutliers (defined as x < Q1 - 1.5 * IQR | x > Q3 + 1.5 * IQR)\nrange after excluding outliers\n\n\n\n\n\n\nBoxplot anatomy\n\n\n\n\nSome synthetic data:\n\nalpha <- rnorm(10)\nbeta <- rnorm(100)\ngamma <- rnorm(200, 1, 2)\ndl <- list(alpha = alpha, beta = beta, gamma = gamma)\n\n\n\n26.5.3 Multiple vectors\n\nboxplot(alpha, beta, gamma)\n\n\n\n\n\n\n26.5.4 List\n\nboxplot(dl)\n\n\n\n\n\n\n26.5.5 Matrix\nPassing a matrix to boxplot() draws one boxplot per column:\n\nmat <- sapply(seq(5), function(i) rnorm(20))\nboxplot(mat)\n\n\n\n\n\n\n26.5.6 Formula interface\nThe formula interface can be used to group any vector by a factor of the same length.\nLet’s use the built-in sleep dataset which shows the effect of two different drugs in increasing hours of sleep compared to a control group.\n\nboxplot(extra ~ group, sleep)\n\n\n\n\nThe col and border arguments work as expected. Here we define two custom colors using their hexadecimal RGB code and use the solid version for the border and a 50% transparent version for the fill. Note that we do not need two separate colors to produce an unambiguous plot since they are clearly labeled in the y-axis. It is often considered desirable/preferred to use the minimum number of different colors that is necessary. (Color coding like the following could be useful if for example data from the two groups were used on a different plot, like a scatterplot, in a multi-panel figure).\n\nborder <- c(\"#18A3AC\", \"#F48024\")\ncol <- c(adjustcolor(\"#18A3AC\", .5), adjustcolor(\"#F48024\", .5))\nboxplot(extra ~ group, sleep,\n        col = col, border = border)\n\n\n\n\n\n\n26.5.7 names: group labels\nThe x-axis group names can be defined with the names argument:\n\nboxplot(extra ~ group, sleep,\n        col = col, border = border,\n        names = c(\"Drug A\", \"Drug B\"))"
  },
  {
    "objectID": "70_BaseGraphics.html#heatmap",
    "href": "70_BaseGraphics.html#heatmap",
    "title": "26  Base Graphics",
    "section": "26.6 Heatmap",
    "text": "26.6 Heatmap\nInput: matrix\nA heatmap is a 2D matrix-like plot with x- and y-axis labels and a value in each cell. It can be used to display many different types of data. A common usage in data science is to plot the correlation matrix of a set of numerical features. In many cases, the rows and/or columns of a heatmap can be reordered based on hierarchical clustering.\n\nx <- sapply(1:20, function(i) rnorm(20))\nx_cor <- cor(x)\n\nBy default, the heatmap() function draws marginal dendrograms and rearranges rows and columns. We can prevent that by setting Rowv and Colv to NA:\n\nheatmap(x_cor, Rowv = NA, Colv = NA)\n\n\n\n\nTo allow clustering and row and column reordering, use the defaults:\n\nheatmap(x_cor)"
  },
  {
    "objectID": "70_BaseGraphics.html#mosaicplot",
    "href": "70_BaseGraphics.html#mosaicplot",
    "title": "26  Base Graphics",
    "section": "26.7 Mosaic plot",
    "text": "26.7 Mosaic plot\nMosaic plots are used to visualize contingency tables. They can be informative to look at during data exploration. They are less likely to be included in a research article where the table itself is more likely to be included.\nSynthetic data:\n\nset.seed(2021)\nCohort <- factor(sample(c(\"Control\", \"Case\"), 500, TRUE),\n                 levels = c(\"Control\", \"Case\"))\nSex <- factor(\n  sapply(seq(Cohort), \\(i) sample(c(\"Male\", \"Female\"), 1,\n                                  prob = if (Cohort[i] == \"Control\") c(1, 1) else c(2, 1))))\n\nUse mosaicplot() on the output of table():\n\nmosaicplot(table(Cohort), main = \"Cases vs. Controls\")\n\n\n\n\nWe can plot the breakdown of sexes, this time also adding color:\n\nmosaicplot(table(Sex), main = \"Males vs. Females\",\n           col = c(\"orchid\", \"skyblue\"))\n\n\n\n\nCross-tabulating is usually most informatively. We us the same color for the sexes, which will be recycled. We also remove the border for a cleaner look:\n\nmosaicplot(table(Cohort, Sex),\n           color = c(\"orchid\", \"skyblue\"),\n           border = NA,\n           main = \"Cohort x Sex\")"
  },
  {
    "objectID": "70_BaseGraphics.html#graphical-parameters",
    "href": "70_BaseGraphics.html#graphical-parameters",
    "title": "26  Base Graphics",
    "section": "26.8 Graphical parameters",
    "text": "26.8 Graphical parameters\nThe par() function allows setting or querying graphical parameters of the base graphics system. Have a look at its documentation (?par).\nSome graphical parameters can only be set with a call to par() prior to using a base plotting function. However, many parameters can also be passed using the ... construct of each base plotting function.\nSome common base graphical parameters:\n\npch: Point character\ncol: Color\ncex: Character expansion, i.e. relative size\nbty: Box type\nxlab: x-axis label\nylab: y-axis label\nmain: Main title\nmar: Plot margins\n\nYou can see what the current value of these parameters is by calling par() or directly accessing a specific parameter:\n\npar()$mar\n\n[1] 5.1 4.1 4.1 2.1\n\n\nmar sets the plot margins. It is a vector of length 4 and each number corresponds to the bottom-left-top-right margin, in that order. Use it to reduce empty white space between plots or add space if labels are getting cropped, for example.\nAlways make sure that your plotting characters, axis labels and titles are legible. You must avoid, at all costs, ever using a huge graph with tiny letters spread over an entire slide in a presentation.\n\ncex: Character expansion for the plotting characters\ncex.axis: cex for axis annotation\ncex.lab: cex for x and y labels\ncex.main: cex for main title\n\nNote: All of these can be set either with a call to par() prior to plotting or passed as arguments in a plotting command, like plot().\nThere is one important distinction: cex set with par() (which defaults to 1), sets the baseline and all other cex parameters multiply it. However, cex set within plot() still multiplies cex set with par(), but only affects the plotting character size.\n\n26.8.1 Save and reload graphical parameters\nYou can make a copy of all current graphical parameters:\n\npar_default <- par()\n\nThere are a few parameters that you cannot control, those are read-only. You can optionally exclude those since you cannot edit them anyway:\n\npar_default <- par(no.readonly = T)\n\nIf you make changes to par() to produce plots and you want to recover the parameters you saved above, you can use reload them by passing them to par():\n\npar(par_default)\n\nAlternatively, you can always restart the graphics device using dev.off() and then making a new plot.\nNote: here “device” does not refere to a physical device but software graphics interfaces that show a plot to screen or save to file.\n\ndev.off() # shuts down graphics device\n\nnull device \n          1 \n\nplot(rnorm(10))"
  },
  {
    "objectID": "70_BaseGraphics.html#multipanel-plots",
    "href": "70_BaseGraphics.html#multipanel-plots",
    "title": "26  Base Graphics",
    "section": "26.9 Multipanel plots",
    "text": "26.9 Multipanel plots\nThere are different ways to create multipanel plots, but probably the most straightforward is to use either the mfrow or the mfcol argument of par().\n\nset.seed(2020)\nx <- rnorm(500)\ny <- x^3 + rnorm(500) * 2\nz <- x^2 + rnorm(500)\n\nBoth mfrow and mfcol accept an integer vector of length 2 indicating number of rows and number of columns, respectively. With mfrow, the plots are drawn row-wise and with mfcol they are drawn column-wise. Remember to reset mfrow or mfcol back to c(1, 1)\nFor example, let’s plot a 2-by-3 panel of plots, drawn row-wise:\n\npar(mfrow = c(2, 3), mar = c(4, 4, 1, 1))\nhist(x, col = \"#052049bb\", border = \"white\", main = \"\")\nhist(y, col = \"#052049bb\", border = \"white\", main = \"\")\nhist(z, col = \"#052049bb\", border = \"white\", main = \"\")\nplot(x, y, col = \"#05204955\", pch = 16, bty = \"n\")\nplot(x, z, col = \"#05204955\", pch = 16, bty = \"n\")\nplot(y, z, col = \"#05204955\", pch = 16, bty = \"n\")\n\n\n\npar(mfrow = c(1, 1))"
  },
  {
    "objectID": "70_BaseGraphics.html#saveplots",
    "href": "70_BaseGraphics.html#saveplots",
    "title": "26  Base Graphics",
    "section": "26.10 Saving plots to file",
    "text": "26.10 Saving plots to file\nYou can save base graphics to disk using a number of different file formats. To do this, you have to:\n\nOpen a graphics device - e.g. pdf(\"path/to/xy_scatter.pdf\")\nWrite to it - e.g. plot(x, y)\nClose graphics device - dev.off()\n\nThe following commands are used to open graphical devices that will save to a file of the corresponding type:\n\nbmp(filename = \"path/to/file\", width = [in pixels], height = [in pixels])\njpeg(filename = \"path/to/file\", width = [in pixels], height = [in pixels])\npng(filename = \"path/to/file\", width = [in pixels], height = [in pixels])\ntiff(filename = \"path/to/file\", width = [in pixels], height = [in pixels])\nsvg(filename = \"path/to/file\", width = [in inches], height = [in inches]\npdf(file = \"path/to/file\", width = [in inches], height = [in inches])\n\nNotice that when writing to a vector graphics format (svg and pdf), you defined width and height in inches, not pixels. Also, you specify file instead of filename in Notice the difference when writing to PDF: you define a file instead of a filename, and width and height are in inches, not pixels.\nIt is recommended to save plots in PDF format because it handles vector graphics therefore plots will scale, and it is easy to export to other graphics formats later on if needed.\n\npdf(\"~/Desktop/plot.pdf\", width = 5, height = 5)\nplot(iris$Sepal.Length, iris$Petal.Length,\n     pch = 16,\n     col = \"#18A3AC66\",\n     cex = 1.8,\n     bty = \"n\", # also try \"l\"\n     xlab = \"Sepal Length\", ylab = \"Petal Length\")\ndev.off()"
  },
  {
    "objectID": "72_3xGraphics.html",
    "href": "72_3xGraphics.html",
    "title": "27  3x Graphics",
    "section": "",
    "text": "Visualization is central to statistics and data science. It is used to check data, explore data, and communicate results.\nR has powerful graphical capabilities built in to the core language. It contains two largely separate graphics systems: ‘base’ graphics in the graphics package, inherited from the S language, and ‘grid’ graphics in the grid package: a “rewrite of the graphics layout capabilities”. There is limited support for interaction between the two. In practice, for a given application, choose one or the other. There are no high level functions for the grid graphics system built into the base R distribution, but a few very popular packages have been built on top of it. Both graphics systems can produce beautiful, layered, high quality graphics. It is possible to build functions using either system to produce most, if not all, types of plots."
  },
  {
    "objectID": "72_3xGraphics.html#base-graphics",
    "href": "72_3xGraphics.html#base-graphics",
    "title": "27  3x Graphics",
    "section": "27.1 Base graphics",
    "text": "27.1 Base graphics\nCommon R plotting functions like plot, barplot, boxplot, heatmap, etc. are built ontop of base graphics (Murrell 2018). Their default arguments provide a minimalist output, but can be tweaked extensively. An advantage of base graphics is they are very fast and relatively easy to extend.\nThe par function allows setting or querying graphical parameters of the base graphics system. Have a look at its documentation (?par).\nSome graphical parameters can only be set with a call to par prior to using a base plotting function. However, many parameters can also be passed using the ... construct of each base plotting function.\nSome common base graphical parameters:\n\npch: Point character\ncol: Color\ncex: Character expansion, i.e. relative size\nbty: Box type\nxlab: x-axis label\nylab: y-axis label\nmain: Main title\n\nAlways make sure that your plotting characters, axis labels and titles are legible. You must avoid, at all costs, ever using a huge graph with tiny letters spread over a whole slide in a presentation.\n\ncex: Character expansion for the plotting characters\ncex.axis: cex for axis annotation\ncex.lab: cex for x and y labels\ncex.main: cex for main title\n\nNote: All of these can be set either with a call to par() prior to plotting or passed as arguments in a plotting command, like plot().\nHowever, there is one important distinction: cex set with par() (which defaults to 1), sets the baseline and all other cex parameters multiply it. However, cex set within plot() stil multiplies cex set with par(), but only affectts the plotting character size."
  },
  {
    "objectID": "72_3xGraphics.html#grid-graphics",
    "href": "72_3xGraphics.html#grid-graphics",
    "title": "27  3x Graphics",
    "section": "27.2 Grid graphics",
    "text": "27.2 Grid graphics\nThe two most popular packages built on top of the grid package are:\n\nlattice (Sarkar 2008)\nggplot2 (Wickham 2011)\n\n\n27.2.1 ggplot2\nggplot2, created by Hadley Wickham (Wickham 2011), follows the Grammar of Graphics approach of Leland Wilkinson (Wilkinson 2012) and has a very different syntax than base functions.\nThe general idea is to start by defining the data and then add and/or modify graphical elements in a stepwise manner, which allows one to build complex and layered visualizations. A simplified interface to ggplot graphics is provided in the qplot() function of ggplot2 (but you should avoid it and use learn to use the ggplot() command which is fun and much more flexible and useful to know)"
  },
  {
    "objectID": "72_3xGraphics.html#rd-party-apis",
    "href": "72_3xGraphics.html#rd-party-apis",
    "title": "27  3x Graphics",
    "section": "27.3 3rd party APIs",
    "text": "27.3 3rd party APIs\nThere are also third party libraries with R APIs that provide even more modern graphic capabilities to the R user:\n\nplotly (Sievert et al. 2017)\nrbokeh\n\nBoth build interactive plots, which can be viewed in a web browser or exported to bitmap graphics, and both also follow the grammar of graphics paradigm, and therefore follow similar syntax to ggplot2.\nThe rtemis package (Gennatas 2017) provides visualization functions built on top of base graphics (for speed and extendability) and plotly (for interactivity):\n\nmplot3 static graphics (base)\ndplot3 interactive graphics (plotly)\n\nLet’s go over the most common plot types using base graphics, rtemis (`mplot3, dplot3), ggplot2, and plotly.\nThis is meant to get you started but is barely scratching the surface. There is extensive functionality included in each plotting library and you should consult the respective documentation for details."
  },
  {
    "objectID": "72_3xGraphics.html#box-plot",
    "href": "72_3xGraphics.html#box-plot",
    "title": "27  3x Graphics",
    "section": "27.4 Box plot",
    "text": "27.4 Box plot\nLet’s create some synthetic data.\n\nset.seed(2019)\nx <- as.data.frame(matrix(rnorm(200*4), 200))\ncolnames(x) <- c(\"mango\", \"banana\", \"tangerine\", \"sugar\")\n\n\n27.4.1 base\n\nboxplot(x)\n\n\n\n\n\nboxplot(x, col = \"steelblue4\")\n\n\n\n\n\n\n27.4.2 mplot3\n\nmplot3_box(x)\n\n\n\n\n\n\n27.4.3 dplot3\n\ndplot3_box(x)\n\n\n\n\n\n\n\n27.4.4 ggplot2\nAgain, ggplot requires an explicit categorical x-axis. In this case, this means we need to convert our dataset from wide to long.\nHere we use tidyr’s pivot_longer() function, since it is part of the same cult known as the tidyverse. You can instead use the builtin reshape() function or any wide-to-long operation of your choosing, e.g. data.table’s melt().\n\nlibrary(tidyr)\nx.long <- pivot_longer(x, 1:4, \n                       names_to = \"Fruit\", \n                       values_to = \"Feature\")\nx.long\n# A tibble: 800 × 2\n   Fruit     Feature\n   <chr>       <dbl>\n 1 mango      0.739 \n 2 banana     0.721 \n 3 tangerine -1.26  \n 4 sugar     -0.0187\n 5 mango     -0.515 \n 6 banana    -0.395 \n 7 tangerine  0.258 \n 8 sugar     -0.0301\n 9 mango     -1.64  \n10 banana     0.983 \n# … with 790 more rows\n\np <- ggplot(x.long, aes(Fruit, Feature)) + geom_boxplot()\np\n\n\n\n\nAdd some color:\n\n(p <- ggplot(x.long, aes(Fruit, Feature)) +\n   geom_boxplot(fill = c(\"#44A6AC66\", \"#F4A36266\", \"#3574A766\", \"#C23A7066\"),\n                colour = c(\"#44A6ACFF\", \"#F4A362FF\", \"#3574A7FF\", \"#C23A70FF\")))\n\n\n\n\n\n\n27.4.5 plotly\nIn plotly, we can use a loop to add each column’s boxplot one at a time. In the following example, we turn off the legend, since the names also appear below each boxplot:\n\nplt <- plot_ly(type = \"box\")\nfor (i in seq_along(x)) {\n  plt <- add_trace(plt, y = x[, i], name = colnames(x)[i])\n}\nplt |> layout(showlegend = F)"
  },
  {
    "objectID": "72_3xGraphics.html#histogram",
    "href": "72_3xGraphics.html#histogram",
    "title": "27  3x Graphics",
    "section": "27.5 Histogram",
    "text": "27.5 Histogram\n\nset.seed(2020)\na <- rnorm(500)\n\n\n27.5.1 base\n\nhist(a)\n\n\n\nhist(a, col = \"#18A3AC66\")\n\n\n\nhist(a, col = \"#18A3AC99\", border = \"white\", main = \"\", breaks = 30)\n\n\n\n\n\n\n27.5.2 mplot3\n\nmplot3_x(a, \"histogram\")\n\n\n\nmplot3_x(a, \"histogram\", hist.breaks = 30)\n\n\n\n\n\n\n27.5.3 dplot3\n\ndplot3_x(a, \"hist\")\n\n\n\n\ndplot3_x(a, \"hist\", hist.n.bins = 40)\n\n\n\n\n\n\n\n27.5.4 ggplot2\n\n(p <- ggplot(mapping = aes(a)) + geom_histogram())\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n(p <- ggplot(mapping = aes(a)) + geom_histogram(binwidth = .2))\n\n\n\n(p <- ggplot(mapping = aes(a)) +\n    geom_histogram(binwidth = .2, fill = \"#18A3AC99\"))\n\n\n\n\n\n\n27.5.5 plotly\n\nplt <- plot_ly(x = a, type = \"histogram\") |> \n  layout(bargap = .1)\nplt\n\n\n\n\n\n\n27.5.5.1 Grouped\n\nmplot3_x(iris$Petal.Length, \"hist\", group = iris$Species, hist.breaks = 10)\n\n\n\n\n\ndplot3_x(iris$Sepal.Length, \"hist\", group = iris$Species)\n\n\n\n\n\nor “ridge”-mode:\n\ndplot3_x(iris$Sepal.Length, \"hist\", group = iris$Species,\n         mode = \"ridge\")\n\n\n\n\n\n\nggplot(iris, aes(x = Sepal.Length, fill = Species)) + \n  geom_histogram(binwidth = .1)"
  },
  {
    "objectID": "72_3xGraphics.html#density-plot",
    "href": "72_3xGraphics.html#density-plot",
    "title": "27  3x Graphics",
    "section": "27.6 Density plot",
    "text": "27.6 Density plot\nThere is no builtin density plot, but you can get x and y coordinates from the density function and add a polygon:\n\n27.6.1 base\n\n.density <- density(iris$Sepal.Length)\nclass(.density)\n\n[1] \"density\"\n\nplot(.density$x, .density$y,\n     type = \"l\", yaxs = \"i\")\n\n\n\nplot(.density$x, .density$y,\n     type = 'l', yaxs = \"i\",\n     bty = \"n\",\n     xlab = \"\",  ylab = \"Density\",\n     col = \"#18A3AC66\",\n     main = \"Sepal Length Density\")\npolygon(c(.density$x, rev(.density$x)), c(.density$y, rep(0, length(.density$y))),\n        col = \"#18A3AC66\", border = NA)\n\n\n\n\n\n\n27.6.2 mplot3\n\nmplot3_x(iris$Sepal.Length, 'density')\n\n\n\n\n\n\n27.6.3 dplot3\n\ndplot3_x(iris$Sepal.Length)\n\n\n\n\n\n\n\n27.6.4 ggplot2\n\nggplot(iris, aes(x = Sepal.Length)) + geom_density()\n\n\n\n\nAdd color:\n\nggplot(iris, aes(x = Sepal.Length)) + geom_density(color = \"#18A3AC66\", fill = \"#18A3AC66\")\n\n\n\n\n\n27.6.4.1 Grouped\n\nmplot3_x(iris$Sepal.Length, group = iris$Species)\n\n\n\n\n\ndplot3_x(iris$Sepal.Length, group = iris$Species)\n\n\n\n\n\n\n(ggplot(iris, aes(Sepal.Length, color = Species, fill = Species)) + \n  geom_density(alpha = .5) +\n  scale_color_manual(values = c(\"#44A6AC\", \"#F4A362\", \"#3574A7\")) +\n  scale_fill_manual(values = c(\"#44A6AC\", \"#F4A362\", \"#3574A7\")) +\n  labs(x = \"Sepal Length\", y = \"Density\"))"
  },
  {
    "objectID": "72_3xGraphics.html#barplot",
    "href": "72_3xGraphics.html#barplot",
    "title": "27  3x Graphics",
    "section": "27.7 Barplot",
    "text": "27.7 Barplot\n\nschools <- data.frame(UCSF = 4, Stanford = 7, Penn = 12)\n\n\n27.7.1 base\n\nbarplot(as.matrix(schools))\n\n\n\nbarplot(as.matrix(schools), col = \"dodgerblue3\")\n\n\n\n\n\n\n27.7.2 mplot3\n\nmplot3_bar(schools)\n\n\n\n\n\n\n27.7.3 dplot3\n\ndplot3_bar(schools)\n\n\n\n\n\n\n\n27.7.4 ggplot2\nggplot requires an explicit column in the data that define the categorical x-axis:\n\nschools.df <- data.frame(University = colnames(schools),\n                         N_schools = as.numeric(schools[1, ]))\nggplot(schools.df, aes(University, N_schools)) +\n  geom_bar(stat = \"identity\", color = \"#18A3AC\", fill = \"#18A3AC\")\n\n\n\n\n\n\n27.7.5 plotly\n\nplt <- plot_ly(x = names(schools),\n               y = unlist(schools),\n               name = \"Schools\",\n               type = \"bar\")\nplt\n\n\n\n\n\nNote that for the above to work, y needs to be a vector, and since x was a data.frame of one line, we use unlist() to convert to a vector."
  },
  {
    "objectID": "72_3xGraphics.html#scatterplot",
    "href": "72_3xGraphics.html#scatterplot",
    "title": "27  3x Graphics",
    "section": "27.8 Scatterplot",
    "text": "27.8 Scatterplot\n\n27.8.1 base\nA default base graphics plot is rather minimalist:\n\nplot(iris$Sepal.Length, iris$Petal.Length)\n\n\n\n\nBy tweaking a few parameters, we get a perhaps prettier result:\n\nplot(iris$Sepal.Length, iris$Petal.Length,\n     pch = 16,\n     col = \"#18A3AC66\",\n     cex = 1.4,\n     bty = \"n\",\n     xlab = \"Sepal Length\", ylab = \"Petal Length\")\n\n\n\n\n\n\n27.8.2 mplot3\n\nmplot3_xy(iris$Sepal.Length, iris$Petal.Length)\n\n\n\n\ndplot3 provides similar functionality to mplot3, built on top of plotly. Notice how you can interact with the plot using the mouse:\n\n\n27.8.3 dplot3\n\ndplot3_xy(iris$Sepal.Length, iris$Petal.Length)\n\n\n\n\n\n\n\n27.8.4 ggplot2\nNote: The name of the package is ggplot2, the name of the function is ggplot.\n\nggplot(iris, aes(Sepal.Length, Petal.Length)) + geom_point()\n\n\n\n\n\n\n27.8.5 plotly\n\np <- plot_ly(iris, x = ~Sepal.Length, y = ~Petal.Length) %>% \n  add_trace(type = \"scatter\", mode = \"markers\")\np\n\n\n\n\n\n\n\n27.8.6 Grouped\nIn mplot3 and dplot3, add a group argument:\n\nmplot3_xy(iris$Sepal.Length, iris$Petal.Length,\n          group = iris$Species)\n\n\n\n\n\ndplot3_xy(iris$Sepal.Length, iris$Petal.Length,\n          group = iris$Species)\n\n\n\n\n\nIn ggplot2, specify color within aes.\nggplot() plots can be assigned to an object. Print the object to view it.\n\np <- ggplot(iris, aes(Sepal.Length, Petal.Length, color = Species)) +\n  geom_point()\np\n\n\n\n\nIn plotly define the color argument:\n\np <- plot_ly(iris, x = ~Sepal.Length, y = ~Petal.Length, color = ~Species) %>% \n  add_trace(type = \"scatter\", mode = \"markers\")\np"
  },
  {
    "objectID": "72_3xGraphics.html#scatterplot-with-fit",
    "href": "72_3xGraphics.html#scatterplot-with-fit",
    "title": "27  3x Graphics",
    "section": "27.9 Scatterplot with fit",
    "text": "27.9 Scatterplot with fit\n\n27.9.1 mplot3\nIn mplot3_xy(), define the algorithm to use to fit a curve, with fit. se.fit allows plotting the standard error bar (if it can be provided by the algorithm in fit)\n\nmplot3_xy(iris$Sepal.Length, iris$Petal.Length,\n          fit = \"gam\", se.fit = T)\n\n\n\n\nPassing a group argument, automatically fits separate models:\n\nmplot3_xy(iris$Sepal.Length, iris$Petal.Length,\n          fit = \"gam\", se.fit = T,\n          group = iris$Species)\n\n\n\n\n\n\n27.9.2 dplot3\nSame syntax as mplot3_xy() above:\n\ndplot3_xy(iris$Sepal.Length, iris$Petal.Length,\n          fit = \"gam\", se.fit = T)\n\n\n\n\n\n\ndplot3_xy(iris$Sepal.Length, iris$Petal.Length,\n          fit = \"gam\", se.fit = T,\n          group = iris$Species)\n\n\n\n\n\n\n\n27.9.3 ggplot2\nIn ggplot(), add a geom_smooth:\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Length)) +\n  geom_point() +\n  geom_smooth(method = 'gam')\n\n`geom_smooth()` using formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nTo group, again, use color:\n\nggplot(iris, aes(x = Sepal.Length, y = Petal.Length, color = Species)) +\n  geom_point() +\n  geom_smooth(method = 'gam')\n\n`geom_smooth()` using formula 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n27.9.4 plotly\nIn plot_ly(), add_lines():\n\nlibrary(mgcv)\nmod.gam <- gam(Petal.Length ~ s(Sepal.Length), data = iris)\nplot_ly(iris, x = ~Sepal.Length) %>%\n  add_trace(y = ~Petal.Length, type = \"scatter\", mode = \"markers\") %>% \n  add_lines(y = mod.gam$fitted.values)\n\n\n\n\n\nTo get fit by group, you add all elements one after the other - one way would be this:\n\niris.bySpecies <- split(iris, iris$Species)\ngam.fitted <- lapply(iris.bySpecies, function(i) {\n  gam(Petal.Length ~ s(Sepal.Length), data = i)$fitted\n})\nindex <- lapply(iris.bySpecies, function(i) order(i$Sepal.Length))\ncol <- c(\"#44A6AC\", \"#F4A362\", \"#3574A7\")\n.names <- names(iris.bySpecies)\np <- plot_ly()\nfor (i in seq_along(iris.bySpecies)) {\n  p <- add_trace(p, x = ~Sepal.Length, y = ~Petal.Length, \n                 type = \"scatter\", mode = \"markers\",\n                 data = iris.bySpecies[[i]],\n                 name = .names[i],\n                 color = col[i])\n}\nfor (i in seq_along(iris.bySpecies)) {\n  p <- add_lines(p, x = iris.bySpecies[[i]]$Sepal.Length[index[[i]]],\n                 y = gam.fitted[[i]][index[[i]]], \n                 # type = \"scatter\", mode = \"markers\",\n                 data = iris.bySpecies[[i]],\n                 name = paste(.names[i], \"GAM fit\"),\n                 color = col[i])\n}\np\n\n\n\n\n\nIt’s a lot of work, and that’s why dplot3 exists."
  },
  {
    "objectID": "72_3xGraphics.html#heatmap",
    "href": "72_3xGraphics.html#heatmap",
    "title": "27  3x Graphics",
    "section": "27.10 Heatmap",
    "text": "27.10 Heatmap\nLet’s create some synthetic correlation data:\n\nset.seed(2020)\nx <- matrix(rnorm(400), 20)\nx.cor <- cor(x)\n\n\n27.10.1 base\nR has a great builtin heatmap function, which supports hierarchical clustering and plots the dendrogram in the margins by default:\n\nheatmap(x.cor)\n\n\n\n\nIt may be a little surprising that clustering is on by default. To disable row and column dendrograms, set Rowv and Colv to NA:\n\nheatmap(x.cor, Rowv = NA, Colv = NA)\n\n\n\n\n\n\n27.10.2 mplot3\nmplot3 adds a colorbar to the side of the heatmap. Notice there are 10 circles above and 10 circles below zero to represent 10% increments.\n\nmplot3_heatmap(x.cor)\n\n\n\n\n\n\n27.10.3 dplot3\n\ndplot3_heatmap(x.cor)\n\n\n\n\n\n\n\n27.10.4 ggplot2\nggplot does not have a builtin heatmap function per se, but you can use geom_tile to build one. It also needs a data frame input in long form once again:\n\nx.cor.dat <- as.data.frame(x.cor)\ncolnames(x.cor.dat) <- rownames(x.cor.dat) <- paste0(\"V\", seq(20))\ncolnames(x.cor) <- rownames(x.cor) <- paste0(\"V\", seq(20))\nx.cor.long <- data.frame(NodeA = rownames(x.cor)[row(x.cor)],\n                         NodeB = colnames(x.cor)[col(x.cor)],\n                         Weight = c(x.cor))\n(p <- ggplot(x.cor.long, aes(NodeA, NodeB, fill = Weight)) +\n    geom_tile() + coord_equal())\n\n\n\n\n\n\n\n\nGennatas, Efstathios Dimitrios. 2017. “Towards Precision Psychiatry: Gray Matter Development and Cognition in Adolescence.”\n\n\nMurrell, Paul. 2018. R Graphics. CRC Press.\n\n\nSarkar, Deepayan. 2008. Lattice: Multivariate Data Visualization with r. New York: Springer. http://lmdvr.r-forge.r-project.org.\n\n\nSievert, Carson, Chris Parmer, Toby Hocking, Scott Chamberlain, Karthik Ram, Marianne Corvellec, and Pedro Despouy. 2017. “Plotly: Create Interactive Web Graphics via ‘Plotly. Js’.” R Package Version 4 (1): 110.\n\n\nWickham, Hadley. 2011. “Ggplot2.” Wiley Interdisciplinary Reviews: Computational Statistics 3 (2): 180–85.\n\n\nWilkinson, Leland. 2012. “The Grammar of Graphics.” In Handbook of Computational Statistics, 375–414. Springer."
  },
  {
    "objectID": "74_Colors.html",
    "href": "74_Colors.html",
    "title": "28  Colors in R",
    "section": "",
    "text": "Colors in R can be defined in many different ways:"
  },
  {
    "objectID": "74_Colors.html#color-names",
    "href": "74_Colors.html#color-names",
    "title": "28  Colors in R",
    "section": "28.1 Color names",
    "text": "28.1 Color names\nThere is a long list of color names R understands, and can be listed using colors().\nThey can be passed directly as characters.\nShades of gray are provided as gray0/grey0 (white) to gray100/grey100 (black).\nAbsurdly wide PDFs with all built-in R colors, excluding the grays/greys, are available sorted alphabeticaly and sorted by increasing Red and decreasing Green and Blue values"
  },
  {
    "objectID": "74_Colors.html#hexadecimal-codes",
    "href": "74_Colors.html#hexadecimal-codes",
    "title": "28  Colors in R",
    "section": "28.2 Hexadecimal codes",
    "text": "28.2 Hexadecimal codes\nHexadecimal color codes are characters starting with the pound sign, followed by 4 pairs of hex codes representing Red, Green, Blue, and Alpha values. Since RGB values go from 0 to 255, hex goes from 00 to FF. You can convert decimal to hex using as.hexmode:\n\nas.hexmode(0)\n\n[1] \"0\"\n\nas.hexmode(127)\n\n[1] \"7f\"\n\nas.hexmode(255)\n\n[1] \"ff\"\n\n\nThe last two values for the alpha setting are optional: if not included, defaults to max (opaque)"
  },
  {
    "objectID": "74_Colors.html#rgb",
    "href": "74_Colors.html#rgb",
    "title": "28  Colors in R",
    "section": "28.3 RGB",
    "text": "28.3 RGB\n\nrgb(0, 0, 1)\n\n[1] \"#0000FF\"\n\n\nNote the default maxColorValue = 1, set to 255 to use the usual RGB range of 0 to 255:\n\nrgb(0, 0, 255, maxColorValue = 255)\n\n[1] \"#0000FF\""
  },
  {
    "objectID": "74_Colors.html#hsv",
    "href": "74_Colors.html#hsv",
    "title": "28  Colors in R",
    "section": "28.4 HSV",
    "text": "28.4 HSV\nColor can also be parameterized using the hue, saturation, and value system (HSV). Each range from 0 to 1.\nSimplistically: Hue controls the color. Saturation 1 is max color and 0 is white. Value 1 is max color and 0 is black.\n\nhsv(1, 1, 1)\n\n[1] \"#FF0000\"\n\n\nIn the following plot, the values around the polar plot represent hue. Moving inwards to the center, saturation changes from 1 to 0.\n\nmplot_hsv()\n\nWarning in x - lwidths * xpad: longer object length is not a multiple of shorter\nobject length\n\n\nWarning in x + lwidths * xpad: longer object length is not a multiple of shorter\nobject length\n\n\nWarning in y - bheights * ypad: longer object length is not a multiple of\nshorter object length\n\n\nWarning in y + theights * ypad: longer object length is not a multiple of\nshorter object length\n\n\n\n\nmplot_hsv(v = .5)\n\nWarning in x - lwidths * xpad: longer object length is not a multiple of shorter\nobject length\n\n\nWarning in x + lwidths * xpad: longer object length is not a multiple of shorter\nobject length\n\n\nWarning in y - bheights * ypad: longer object length is not a multiple of\nshorter object length\n\n\nWarning in y + theights * ypad: longer object length is not a multiple of\nshorter object length"
  },
  {
    "objectID": "80_Profiling.html",
    "href": "80_Profiling.html",
    "title": "29  Benchmarking & Profiling",
    "section": "",
    "text": "Benchmarking is the process of timing the execution of code for the purpose of comparison. For example, you can compare the execution time of a program in two different systems, e.g. a laptop and a high performance server. Another common case is to compare the performance of two different programs that produce the same output on the same computer.\nProfiling refers to timing the different steps of a program to identify bottlenecks and potential targets for optimization."
  },
  {
    "objectID": "80_Profiling.html#system.time-time-the-execution-of-an-expression",
    "href": "80_Profiling.html#system.time-time-the-execution-of-an-expression",
    "title": "29  Benchmarking & Profiling",
    "section": "29.1 system.time(): Time the execution of an expression",
    "text": "29.1 system.time(): Time the execution of an expression\nThe base package’s system.time() function allows you to measure the execution time of an R expression.\n\nsystem.time(rnorm(9999999))\n\n   user  system elapsed \n  0.281   0.009   0.290 \n\n\n“elapsed” time is real time in seconds.\n“user” and “system” are time used by the CPU on different types of tasks (see ?proc.time).\nAs always, you can pass any R expression within curly brackets:\n\nx <- rnorm(9999)\nsystem.time({\n    for (i in 2:9999) {\n      x[i]\n      x[i] <- x[i]^3\n    }\n})\n\n   user  system elapsed \n  0.002   0.000   0.001 \n\n\nYou can use replicate() to get a measure of time over multiple executions and average it:\n\nset.seed(2020)\nx <- matrix(rnorm(500000), 5000)\ny <- 12 + x[, 3] + x[, 5]^2 + x[, 7]^3 + rnorm(5000)\nfit.glm <- function(x, y) glm.fit(x, y)\n    \nfit.glm_time10 <- replicate(10, system.time(fit.glm(x, y))[[1]])\n\n\nboxplot(fit.glm_time10)"
  },
  {
    "objectID": "80_Profiling.html#compare-execution-times-with-microbenchmarkmicrobenchmark",
    "href": "80_Profiling.html#compare-execution-times-with-microbenchmarkmicrobenchmark",
    "title": "29  Benchmarking & Profiling",
    "section": "29.2 Compare execution times with microbenchmark::microbenchmark()",
    "text": "29.2 Compare execution times with microbenchmark::microbenchmark()\nThe microbenchmark package’s microbenchmark() function allows you to time the execution of multiple expressions with sub-millisecond accuracy. It will execute each command a number of times as defined by the times argument (default = 100), and output statistics of execution time per expression in nanoseconds. Using plot() on the output produces a boxplot comparing the time distributions.\n\n# install.packages(\"microbenchmark\")\nlibrary(microbenchmark)\n\n\n29.2.1 Example: loop over matrix vs. data.frame\nLet’s create xmat, a 500 by 5 matrix and, xdf a data.frame with the same data.\n\nset.seed(2021)\nxmat <- matrix(rnorm(500*5), 5)\nxdf <- as.data.frame(xmat)\n\nIf you wanted to square either of them, you would just use ^2. Here, we create a function specifically to demonstrate the difference in working on a numeric matrix vs. a data.frame by using a nested loop that replaces each element one at a time.\n\nsilly_square <- function(x) {\n  for (i in seq_len(NROW(x))) {\n    for (j in seq_len(NCOL(x))) {\n      x[i, j] <- x[i, j]^2\n    }\n  }\n}\n\n\nmat_df_sq <- microbenchmark(silly_square_mat = silly_square(xmat),\n               silly_square_df = silly_square(xdf),\n               mat_squared = xmat^2,\n               df_squared = xdf^2)\n\nWarning in microbenchmark(silly_square_mat = silly_square(xmat), silly_square_df\n= silly_square(xdf), : less accurate nanosecond times to avoid potential integer\noverflows\n\nclass(mat_df_sq)\n\n[1] \"microbenchmark\" \"data.frame\"    \n\n\nPrint microbenchmark’s output:\n\nmat_df_sq\n\nUnit: microseconds\n             expr       min         lq        mean     median         uq\n silly_square_mat   101.762   107.8505   134.46770   110.8025   116.5425\n  silly_square_df 40087.791 41718.3405 43750.75232 43158.0965 44394.7385\n      mat_squared     1.107     1.6605     2.79948     3.0340     3.5260\n       df_squared  9086.666  9933.1110 11010.92679 10485.0940 11836.9870\n       max neval\n  1997.561   100\n 64110.716   100\n     5.412   100\n 16969.039   100\n\n\nNotice how a) either operation is much faster on a matrix vs. a data.frame and b) vectorized squaring with ^2 is much faster than the nested loop as expected.\nThere is a plot() method for microbenchmark objects:\n\nplot(mat_df_sq)\n\n\n\n\n\n\n29.2.2 Example: Group means\nLet’s perform a simple mean-by-group operation and compare three different approaches. As an example, we use the flights dataset from the nycflights13 package which includes data on 336,776 flights that departed from NY area airports in 2013. The data comes as a tibble, and we create data.frame and data.table versions.th\n\nlibrary(data.table)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:data.table':\n\n    between, first, last\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(nycflights13)\nclass(flights)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\ndim(flights)\n\n[1] 336776     19\n\nflights_df <- as.data.frame(flights)\nflights_dt <- as.data.table(flights)\n\nCompare performance of the same operation using different functions:\n\nbase R aggregate() with formula input\nbase R aggregate() with list input\nbase R tapply()\n\n\nflights_aggregate_formula <- function() {\n  aggregate(arr_delay ~ carrier, \n            data = flights_df,\n            mean, na.rm = TRUE)\n}\n  \nflights_aggregate <- function() {\n  aggregate(flights_df$arr_delay, \n            by = list(flights_df$carrier), \n            mean, na.rm = TRUE)\n}\n\nflights_tapply <- function() {\n  tapply(flights_df$arr_delay, \n         flights_df$carrier, \n         mean, na.rm = TRUE)\n}\n\ngroupmean_3x <- microbenchmark(\n  aggregate_formula = flights_aggregate_formula(),\n  aggregate = flights_aggregate(),\n  tapply = flights_tapply()\n  )\n\n\ngroupmean_3x\n\nUnit: milliseconds\n              expr       min        lq     mean   median       uq       max\n aggregate_formula 44.204683 50.173791 55.97023 52.99789 57.48840 101.97487\n         aggregate 38.253287 41.948760 45.79623 44.92173 47.27468  77.83227\n            tapply  8.372077  9.325921 11.45583 10.16460 11.43785  46.09351\n neval\n   100\n   100\n   100\n\n\n\nplot(groupmean_3x)"
  },
  {
    "objectID": "80_Profiling.html#profile-a-function-with-profvis",
    "href": "80_Profiling.html#profile-a-function-with-profvis",
    "title": "29  Benchmarking & Profiling",
    "section": "29.3 Profile a function with profvis()",
    "text": "29.3 Profile a function with profvis()\nThe profvis package’s profvis() function provides an interactive output to visualize the time spent in different calls within a program.\n\nlibrary(profvis)\nprofvis({\n  hf <- read.csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00519/heart_failure_clinical_records_dataset.csv\")\n  str(hf)\n  lt5 <- which(sapply(hf, \\(i) length(unique(i))) < 5)\n  for (i in lt5) hf[, i] <- factor(hf[, i])\n  index_numeric <- which(sapply(hf, is.numeric))\n  par(mfrow = c(1, length(index_numeric)))\n  for (i in index_numeric) boxplot(hf[, i])\n  par(mfrow = c(1, 1))\n})\n\n'data.frame':   299 obs. of  13 variables:\n $ age                     : num  75 55 65 50 65 90 75 60 65 80 ...\n $ anaemia                 : int  0 0 0 1 1 1 1 1 0 1 ...\n $ creatinine_phosphokinase: int  582 7861 146 111 160 47 246 315 157 123 ...\n $ diabetes                : int  0 0 0 0 1 0 0 1 0 0 ...\n $ ejection_fraction       : int  20 38 20 20 20 40 15 60 65 35 ...\n $ high_blood_pressure     : int  1 0 0 0 0 1 0 0 0 1 ...\n $ platelets               : num  265000 263358 162000 210000 327000 ...\n $ serum_creatinine        : num  1.9 1.1 1.3 1.9 2.7 2.1 1.2 1.1 1.5 9.4 ...\n $ serum_sodium            : int  130 136 129 137 116 132 137 131 138 133 ...\n $ sex                     : int  1 1 1 1 0 1 1 1 0 1 ...\n $ smoking                 : int  0 0 1 0 0 1 0 1 0 1 ...\n $ time                    : int  4 6 7 7 8 8 10 10 10 10 ...\n $ DEATH_EVENT             : int  1 1 1 1 1 1 1 1 1 1 ..."
  },
  {
    "objectID": "82_Optimization.html",
    "href": "82_Optimization.html",
    "title": "30  Optimization",
    "section": "",
    "text": "R provides a general purpose optimization tool, optim(). You can use it to estimate parameters that minimize any defined function.\nSupervised and unsupervised learning involves defining a loss function to minimize or an objective function to minimize or maximize.\nTo learn how optim() works, let’s write a simple function that returns linear coefficients by minimizing squared error."
  },
  {
    "objectID": "82_Optimization.html#data",
    "href": "82_Optimization.html#data",
    "title": "30  Optimization",
    "section": "30.1 Data",
    "text": "30.1 Data\n\nset.seed(2020)\nx <- sapply(seq(10), function(i) rnorm(500))\ny <- 12 + 1.5 * x[, 3] + 3.2 * x[, 7] + .5 * x[, 9] + rnorm(500)"
  },
  {
    "objectID": "82_Optimization.html#glm-glm-s_glm",
    "href": "82_Optimization.html#glm-glm-s_glm",
    "title": "30  Optimization",
    "section": "30.2 GLM (glm, s_GLM)",
    "text": "30.2 GLM (glm, s_GLM)\n\nyx.glm <- glm(y ~ x)\nsummary(yx.glm)\n\n\nCall:\nglm(formula = y ~ x)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-2.38739  -0.67391   0.00312   0.65531   3.08524  \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 11.979070   0.043252 276.962   <2e-16 ***\nx1           0.061798   0.040916   1.510   0.1316    \nx2          -0.003873   0.043271  -0.090   0.9287    \nx3           1.488113   0.042476  35.034   <2e-16 ***\nx4           0.031115   0.044015   0.707   0.4800    \nx5           0.034217   0.043664   0.784   0.4336    \nx6           0.034716   0.042189   0.823   0.4110    \nx7           3.183398   0.040605  78.399   <2e-16 ***\nx8          -0.034252   0.043141  -0.794   0.4276    \nx9           0.541219   0.046550  11.627   <2e-16 ***\nx10          0.087120   0.044000   1.980   0.0483 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.9207315)\n\n    Null deviance: 7339.42  on 499  degrees of freedom\nResidual deviance:  450.24  on 489  degrees of freedom\nAIC: 1390.5\n\nNumber of Fisher Scoring iterations: 2\n\n\nOr, using rtemis:\n\nmod.glm <- s_GLM(x, y)\n\n[2022-05-10 17:28:35 s_GLM] Hello, egenn \n\n.:Regression Input Summary\nTraining features: 500 x 10 \n Training outcome: 500 x 1 \n Testing features: Not available\n  Testing outcome: Not available\n\n[2022-05-10 17:28:36 s_GLM] Training GLM... \n\n.:GLM Regression Training Summary\n    MSE = 0.90 (93.87%)\n   RMSE = 0.95 (75.23%)\n    MAE = 0.77 (74.88%)\n      r = 0.97 (p = 5.3e-304)\n   R sq = 0.94\n\n\n\n\n\n[2022-05-10 17:28:37 s_GLM] Run completed in 0.03 minutes (Real: 1.63; User: 0.43; System: 0.04) \n\n\n\nsummary(mod.glm$mod)\n\n\nCall:\nglm(formula = .formula, family = family, data = df.train, weights = .weights, \n    na.action = na.action)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-2.38739  -0.67391   0.00312   0.65531   3.08524  \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 11.979070   0.043252 276.962   <2e-16 ***\nV1           0.061798   0.040916   1.510   0.1316    \nV2          -0.003873   0.043271  -0.090   0.9287    \nV3           1.488113   0.042476  35.034   <2e-16 ***\nV4           0.031115   0.044015   0.707   0.4800    \nV5           0.034217   0.043664   0.784   0.4336    \nV6           0.034716   0.042189   0.823   0.4110    \nV7           3.183398   0.040605  78.399   <2e-16 ***\nV8          -0.034252   0.043141  -0.794   0.4276    \nV9           0.541219   0.046550  11.627   <2e-16 ***\nV10          0.087120   0.044000   1.980   0.0483 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.9207315)\n\n    Null deviance: 7339.42  on 499  degrees of freedom\nResidual deviance:  450.24  on 489  degrees of freedom\nAIC: 1390.5\n\nNumber of Fisher Scoring iterations: 2"
  },
  {
    "objectID": "82_Optimization.html#optim",
    "href": "82_Optimization.html#optim",
    "title": "30  Optimization",
    "section": "30.3 optim",
    "text": "30.3 optim\nBasic usage of optim to find values of parameters that minimize a function:\n\nDefine a list of initial parameter values\nDefine a loss function whose first argument is the above list of initial parameter values\nPass parameter list and objective function to optim\n\nIn the following example, we wrap these three steps in a function called linearcoeffs, which will output the linear coefficients that minimize squared error, given a matrix/data.frame of features x and an outcome y. We also specify the optimization method to be used (See ?base::optim for details):\n\nlinearcoeffs <- function(x, y, method = \"BFGS\") {\n  \n  # 1. List of initial parameter values\n  params <- as.list(c(mean(y), rep(0, NCOL(x))))\n  names(params) <- c(\"Intercept\", paste0(\"Coefficient\", seq(NCOL(x))))\n  \n  # 2. Loss function: first argument is parameter list\n  loss <- function(params, x, y) {\n    estimated <- c(params[[1]] + x %*% unlist(params[-1]))\n    mean((y - estimated)^2)\n  }\n  \n  # 3. optim!\n  coeffs <- optim(params, loss, x = x, y = y, method = method)\n  \n  # The values that minimize the loss function are stored in $par\n  coeffs$par\n}\n\n\ncoeffs.optim <- linearcoeffs(x, y)\nestimated.optim <- cbind(1, x) %*% coeffs.optim\nmplot3_fit(y, estimated.optim)\n\n\n\ncoeffs.glm <- mod.glm$mod$coefficients\nestimated.glm <- cbind(1, x) %*% coeffs.glm\nmplot3_fit(y, estimated.glm)\n\n\n\n\n\nmplot3_fit(coeffs.glm, coeffs.optim)"
  },
  {
    "objectID": "84_HypothesisTesting.html",
    "href": "84_HypothesisTesting.html",
    "title": "31  Common statistical tests",
    "section": "",
    "text": "R includes a large number of functions to perform statistical hypothesis testing in the built-in stats package. This chapter includes a brief overview of the syntax for some common tests along with code to produce relevant plots of your data."
  },
  {
    "objectID": "84_HypothesisTesting.html#correlation-test",
    "href": "84_HypothesisTesting.html#correlation-test",
    "title": "31  Common statistical tests",
    "section": "31.1 Correlation test",
    "text": "31.1 Correlation test\n\nset.seed(2020)\nx <- rnorm(100)\ny1 <- .1 * x + rnorm(100)\ny2 <- .3 * x + rnorm(100)\ny3 <- x + rnorm(100)/5\ny4 <- x^2 + rnorm(100)\n\nScatterplot with linear fit:\n\nplot(x, y1,\n     col = \"#00000077\",\n     pch = 16, bty = \"none\")\nabline(lm(y1 ~ x), col = \"red\", lwd = 2)\n\n\n\n\n\nplot(x, y2,\n     col = \"#00000077\",\n     pch = 16, bty = \"none\")\nabline(lm(y3 ~ x), col = \"red\", lwd = 2)\n\n\n\n\n\nplot(x, y3,\n     col = \"#00000077\",\n     pch = 16, bty = \"none\")\nabline(lm(y3 ~ x), col = \"red\", lwd = 2)\n\n\n\n\nScatterplot with a LOESS fit\n\nscatter.smooth(x, y4,\n               col = \"#00000077\",\n               pch = 16, bty = \"none\",\n               lpars = list(col = \"red\", lwd = 2))\n\n\n\n\n\ncor.test(x, y1)\n\n\n    Pearson's product-moment correlation\n\ndata:  x and y1\nt = 0.66018, df = 98, p-value = 0.5107\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.1315978  0.2595659\nsample estimates:\n       cor \n0.06654026 \n\ncor.test(x, y2)\n\n\n    Pearson's product-moment correlation\n\ndata:  x and y2\nt = 3.3854, df = 98, p-value = 0.001024\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1357938 0.4889247\nsample estimates:\n      cor \n0.3235813 \n\ncor.test(x, y3)\n\n\n    Pearson's product-moment correlation\n\ndata:  x and y3\nt = 53.689, df = 98, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9754180 0.9888352\nsample estimates:\n      cor \n0.9834225 \n\ncor.test(x, y4)\n\n\n    Pearson's product-moment correlation\n\ndata:  x and y4\nt = 0.66339, df = 98, p-value = 0.5086\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.1312793  0.2598681\nsample estimates:\n       cor \n0.06686289"
  },
  {
    "objectID": "84_HypothesisTesting.html#students-t-test",
    "href": "84_HypothesisTesting.html#students-t-test",
    "title": "31  Common statistical tests",
    "section": "31.2 Student’s t-test",
    "text": "31.2 Student’s t-test\n\nset.seed(2021)\nx0 <- rnorm(500)\nx1 <- rnorm(500, mean = .7)\n\nFor all tests of differences in means, a boxplot is a good way to visualize. It accepts individual vectors, list or data.frame of vectors, or a formula to split a vector into groups by a factor.\n\nboxplot(x0, x1,\n        col = \"#05204999\", border = \"#052049\",\n        boxwex = .3, names = c(\"x0\", \"x1\"))\n\n\n\n\n\n31.2.1 One sample t-test\n\nt.test(x0)\n\n\n    One Sample t-test\n\ndata:  x0\nt = 0.093509, df = 499, p-value = 0.9255\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.08596896  0.09456106\nsample estimates:\n  mean of x \n0.004296046 \n\n\n\nt.test(x1)\n\n\n    One Sample t-test\n\ndata:  x1\nt = 15.935, df = 499, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.6322846 0.8101311\nsample estimates:\nmean of x \n0.7212079 \n\n\n\nboxplot(extra ~ group, sleep,\n        col = \"#05204999\", border = \"#052049\",\n        boxwex = .3)\n\n\n\n\n\n\n31.2.2 Two-sample T-test\nBoth t.test() and wilcox.test() (below) either accept input as two vectors, t.test(x, y) or a formula of the form t.test(x ~ group). The paired argument allows us to define a paired test. Since the sleep dataset includes measurements on the same cases in two conditions, we set paired = TRUE.\n\nt.test(extra ~ group, sleep, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  extra by group\nt = -4.0621, df = 9, p-value = 0.002833\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -2.4598858 -0.7001142\nsample estimates:\nmean difference \n          -1.58"
  },
  {
    "objectID": "84_HypothesisTesting.html#wilcoxon-test",
    "href": "84_HypothesisTesting.html#wilcoxon-test",
    "title": "31  Common statistical tests",
    "section": "31.3 Wilcoxon test",
    "text": "31.3 Wilcoxon test\nData from R Documentation:\n\n## Hollander & Wolfe (1973), 29f.\n## Hamilton depression scale factor measurements in 9 patients with\n##  mixed anxiety and depression, taken at the first (x) and second\n##  (y) visit after initiation of a therapy (administration of a\n##  tranquilizer).\nx <- c(1.83,  0.50,  1.62,  2.48, 1.68, 1.88, 1.55, 3.06, 1.30)\ny <- c(0.878, 0.647, 0.598, 2.05, 1.06, 1.29, 1.06, 3.14, 1.29)\ndepression <- data.frame(first = x, second = y, change = y - x)\n\n\n31.3.1 One-sample Wilcoxon\n\nwilcox.test(depression$change)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  depression$change\nV = 5, p-value = 0.03906\nalternative hypothesis: true location is not equal to 0\n\n\n\n\n31.3.2 Two-sample Wilcoxon rank sum test (unpaired)\na.k.a Mann-Whitney U test a.k.a. Mann–Whitney–Wilcoxon (MWW) a.k.a. Wilcoxon–Mann–Whitney test\n\nx1 <- rnorm(500, 3, 1.5)\nx2 <- rnorm(500, 5, 2)\nwilcox.test(x1, x2)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  x1 and x2\nW = 47594, p-value < 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n\n31.3.3 Two-sample Wilcoxon signed-rank test (paired)\n\nwilcox.test(x, y, paired = TRUE)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  x and y\nV = 40, p-value = 0.03906\nalternative hypothesis: true location shift is not equal to 0\n\n\n\nwilcox.test(x, y, paired = TRUE, alternative = \"greater\")\n\n\n    Wilcoxon signed rank exact test\n\ndata:  x and y\nV = 40, p-value = 0.01953\nalternative hypothesis: true location shift is greater than 0"
  },
  {
    "objectID": "84_HypothesisTesting.html#analysis-of-variance",
    "href": "84_HypothesisTesting.html#analysis-of-variance",
    "title": "31  Common statistical tests",
    "section": "31.4 Analysis of Variance",
    "text": "31.4 Analysis of Variance\n\nset.seed(20)\nBP_drug <- data.frame(Group = factor(rep(c(\"Placebo\", \"Drug_A\", \"Drug_B\"), each = 20),\n                                     levels = c(\"Placebo\", \"Drug_A\", \"Drug_B\")),\n                  SBP = c(rnorm(20, 140, 2.2), rnorm(20, 132, 2.1), rnorm(20, 138, 2)))\n\n\nboxplot(SBP ~ Group, BP_drug,\n        col = \"#05204999\", border = \"#052049\",\n        boxwex = .3)\n\n\n\n\n\nSBP_aov <- aov(SBP ~ Group, BP_drug)\nSBP_aov\n\nCall:\n   aov(formula = SBP ~ Group, data = BP_drug)\n\nTerms:\n                   Group Residuals\nSum of Squares  728.2841  264.4843\nDeg. of Freedom        2        57\n\nResidual standard error: 2.154084\nEstimated effects may be unbalanced\n\n\n\nsummary(SBP_aov)\n\n            Df Sum Sq Mean Sq F value Pr(>F)    \nGroup        2  728.3   364.1   78.48 <2e-16 ***\nResiduals   57  264.5     4.6                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe analysis of variance p-value is highly significant, but doesn’t tell us which levels of the Group factor are significantly different from each other. The boxplot already gives us a pretty good idea, but we can follow up with a pairwise t-test\n\n31.4.1 Pot-hoc pairwise t-tests\n\npairwise.t.test(BP_drug$SBP, BP_drug$Group,\n                p.adj = \"holm\")\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  BP_drug$SBP and BP_drug$Group \n\n       Placebo Drug_A \nDrug_A 2.9e-16 -      \nDrug_B 0.065   1.7e-13\n\nP value adjustment method: holm \n\n\nThe pairwise tests suggest that the difference between Placebo and Drug_A and between Drug_a and Drug_b are highly significant, while difference between Placebo and Drub_B is not (p = 0.065)."
  },
  {
    "objectID": "84_HypothesisTesting.html#kruskal-wallis-test",
    "href": "84_HypothesisTesting.html#kruskal-wallis-test",
    "title": "31  Common statistical tests",
    "section": "31.5 Kruskal-Wallis test",
    "text": "31.5 Kruskal-Wallis test\nKruskal-Wallis rank sum test of the null that the location parameters of the distribution of x are the same in each group (sample). The alternative is that they differ in at least one. It is a generalization of the Wilcoxon test to multiple independent samples.\nFrom the R Documentation:\n\n## Hollander & Wolfe (1973), 116.\n## Mucociliary efficiency from the rate of removal of dust in normal\n##  subjects, subjects with obstructive airway disease, and subjects\n##  with asbestosis.\nx <- c(2.9, 3.0, 2.5, 2.6, 3.2) # normal subjects\ny <- c(3.8, 2.7, 4.0, 2.4)      # with obstructive airway disease\nz <- c(2.8, 3.4, 3.7, 2.2, 2.0) # with asbestosis\nkruskal.test(list(x, y, z))\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  list(x, y, z)\nKruskal-Wallis chi-squared = 0.77143, df = 2, p-value = 0.68\n\n\n\nboxplot(Ozone ~ Month, data = airquality,\n        col = \"#05204999\", border = \"#052049\",\n        boxwex = .3)\n\n\n\n\n\nkruskal.test(Ozone ~ Month, data = airquality)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  Ozone by Month\nKruskal-Wallis chi-squared = 29.267, df = 4, p-value = 6.901e-06"
  },
  {
    "objectID": "84_HypothesisTesting.html#chi-squared-test",
    "href": "84_HypothesisTesting.html#chi-squared-test",
    "title": "31  Common statistical tests",
    "section": "31.6 Chi-squared Test",
    "text": "31.6 Chi-squared Test\nPearson’s chi-squared test for count data\nSome synthetic data:\n\nset.seed(2021)\nset.seed(2021)\nCohort <- factor(sample(c(\"Control\", \"Case\"), 500, TRUE),\n                 levels = c(\"Control\", \"Case\"))\nSex <- factor(\n  sapply(seq(Cohort), \\(i) sample(c(\"Male\", \"Female\"), 1,\n                                  prob = if (Cohort[i] == \"Control\") c(1, 1) else c(2, 1))))\ndat <- data.frame(Cohort, Sex)\nhead(dat)\n\n   Cohort    Sex\n1 Control   Male\n2    Case   Male\n3    Case   Male\n4    Case Female\n5 Control Female\n6    Case   Male\n\n\nYou can lot count data using a mosaic plot, with either a table or formula input:\n\nmosaicplot(table(Cohort, Sex),\n           color = c(\"orchid\", \"skyblue\"),\n           border = NA,\n           main = \"Cohort x Sex\")\n\n\n\n\n\nmosaicplot(Cohort ~ Sex, dat,\n           color = c(\"orchid\", \"skyblue\"),\n           border = NA,\n           main = \"Cohort x Sex\")\n\n\n\n\nchisq.test() accepts either two factors, or a table:\n\ncohort_sex_chisq <- chisq.test(dat$Cohort, dat$Sex)\ncohort_sex_chisq\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  dat$Cohort and dat$Sex\nX-squared = 18.015, df = 1, p-value = 2.192e-05\n\n\n\ncohort_sex_chisq <- chisq.test(table(dat$Cohort, dat$Sex))\ncohort_sex_chisq\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table(dat$Cohort, dat$Sex)\nX-squared = 18.015, df = 1, p-value = 2.192e-05"
  },
  {
    "objectID": "84_HypothesisTesting.html#fishers-exact-test",
    "href": "84_HypothesisTesting.html#fishers-exact-test",
    "title": "31  Common statistical tests",
    "section": "31.7 Fisher’s exact test",
    "text": "31.7 Fisher’s exact test\nFisher’s exact test for count data\nWorking on the same data as above, fisher.test() also accepts either two factors or a table as input:\n\ncohort_sex_fisher <- fisher.test(dat$Cohort, dat$Sex)\ncohort_sex_fisher\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  dat$Cohort and dat$Sex\np-value = 1.512e-05\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 1.516528 3.227691\nsample estimates:\nodds ratio \n  2.207866 \n\n\n\ncohort_sex_fisher <- fisher.test(table(dat$Cohort, dat$Sex))\ncohort_sex_fisher\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  table(dat$Cohort, dat$Sex)\np-value = 1.512e-05\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 1.516528 3.227691\nsample estimates:\nodds ratio \n  2.207866"
  },
  {
    "objectID": "84_HypothesisTesting.html#f-test-to-compare-two-variances",
    "href": "84_HypothesisTesting.html#f-test-to-compare-two-variances",
    "title": "31  Common statistical tests",
    "section": "31.8 F Test to compare two variances",
    "text": "31.8 F Test to compare two variances\n\nx1 <- rnorm(500, sd = 1)\nx2 <- rnorm(400, sd = 1.5)\nvar.test(x1, x2)\n\n\n    F test to compare two variances\n\ndata:  x1 and x2\nF = 0.43354, num df = 499, denom df = 399, p-value < 2.2e-16\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.3594600 0.5218539\nsample estimates:\nratio of variances \n         0.4335363 \n\n\n\nboxplot(x1, x2,\n        col = \"#05204999\", border = \"#052049\",\n        boxwex = .3)\n\n\n\n\nFrom R Documentation:\n\nx <- rnorm(50, mean = 0, sd = 2)\ny <- rnorm(30, mean = 1, sd = 1)\nvar.test(x, y)                  # Do x and y have the same variance?\n\n\n    F test to compare two variances\n\ndata:  x and y\nF = 5.4776, num df = 49, denom df = 29, p-value = 5.817e-06\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n  2.752059 10.305597\nsample estimates:\nratio of variances \n          5.477573 \n\nvar.test(lm(x ~ 1), lm(y ~ 1))  # same\n\n\n    F test to compare two variances\n\ndata:  lm(x ~ 1) and lm(y ~ 1)\nF = 5.4776, num df = 49, denom df = 29, p-value = 5.817e-06\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n  2.752059 10.305597\nsample estimates:\nratio of variances \n          5.477573"
  },
  {
    "objectID": "84_HypothesisTesting.html#bartlett-test-of-homogeneity-of-variances",
    "href": "84_HypothesisTesting.html#bartlett-test-of-homogeneity-of-variances",
    "title": "31  Common statistical tests",
    "section": "31.9 Bartlett test of homogeneity of variances",
    "text": "31.9 Bartlett test of homogeneity of variances\nPerforms Bartlett’s test of the null that the variances in each of the groups (samples) are the same.\nFrom the R Documentation:\n\nplot(count ~ spray, data = InsectSprays)\n\n\n\n\n\nbartlett.test(InsectSprays$count, InsectSprays$spray)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  InsectSprays$count and InsectSprays$spray\nBartlett's K-squared = 25.96, df = 5, p-value = 9.085e-05\n\n\n\nbartlett.test(count ~ spray, data = InsectSprays)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  count by spray\nBartlett's K-squared = 25.96, df = 5, p-value = 9.085e-05"
  },
  {
    "objectID": "84_HypothesisTesting.html#fligner-killeen-test-of-homogeneity-of-variances",
    "href": "84_HypothesisTesting.html#fligner-killeen-test-of-homogeneity-of-variances",
    "title": "31  Common statistical tests",
    "section": "31.10 Fligner-Killeen test of homogeneity of variances",
    "text": "31.10 Fligner-Killeen test of homogeneity of variances\nPerforms a Fligner-Killeen (median) test of the null that the variances in each of the groups (samples) are the same.\n\nboxplot(count ~ spray, data = InsectSprays)\n\n\n\n# works the same if you do plot(count ~ spray, data = InsectSprays)\nfligner.test(InsectSprays$count, InsectSprays$spray)\n\n\n    Fligner-Killeen test of homogeneity of variances\n\ndata:  InsectSprays$count and InsectSprays$spray\nFligner-Killeen:med chi-squared = 14.483, df = 5, p-value = 0.01282\n\nfligner.test(count ~ spray, data = InsectSprays)\n\n\n    Fligner-Killeen test of homogeneity of variances\n\ndata:  count by spray\nFligner-Killeen:med chi-squared = 14.483, df = 5, p-value = 0.01282"
  },
  {
    "objectID": "84_HypothesisTesting.html#ansari-bradley-test",
    "href": "84_HypothesisTesting.html#ansari-bradley-test",
    "title": "31  Common statistical tests",
    "section": "31.11 Ansari-Bradley test",
    "text": "31.11 Ansari-Bradley test\nPerforms the Ansari-Bradley two-sample test for a difference in scale parameters.\n\nramsay <- c(111, 107, 100, 99, 102, 106, 109, 108, 104, 99,\n            101, 96, 97, 102, 107, 113, 116, 113, 110, 98)\njung.parekh <- c(107, 108, 106, 98, 105, 103, 110, 105, 104,\n            100, 96, 108, 103, 104, 114, 114, 113, 108, 106, 99)\nansari.test(ramsay, jung.parekh)\n\nWarning in ansari.test.default(ramsay, jung.parekh): cannot compute exact p-\nvalue with ties\n\n\n\n    Ansari-Bradley test\n\ndata:  ramsay and jung.parekh\nAB = 185.5, p-value = 0.1815\nalternative hypothesis: true ratio of scales is not equal to 1\n\n\n\nx <- rnorm(40, sd = 1.5)\ny <- rnorm(40, sd = 2.5)\nansari.test(x, y)\n\n\n    Ansari-Bradley test\n\ndata:  x and y\nAB = 963, p-value = 0.005644\nalternative hypothesis: true ratio of scales is not equal to 1"
  },
  {
    "objectID": "84_HypothesisTesting.html#mood-two-sample-test-of-scale",
    "href": "84_HypothesisTesting.html#mood-two-sample-test-of-scale",
    "title": "31  Common statistical tests",
    "section": "31.12 Mood two-sample test of scale",
    "text": "31.12 Mood two-sample test of scale\n\nmood.test(x, y)\n\n\n    Mood two-sample test of scale\n\ndata:  x and y\nZ = -2.7363, p-value = 0.006213\nalternative hypothesis: two.sided"
  },
  {
    "objectID": "84_HypothesisTesting.html#kolmogorov-smirnoff-test",
    "href": "84_HypothesisTesting.html#kolmogorov-smirnoff-test",
    "title": "31  Common statistical tests",
    "section": "31.13 Kolmogorov-Smirnoff test",
    "text": "31.13 Kolmogorov-Smirnoff test\nPerform a one- or two-sample Kolmogorov-Smirnov test Null: x and y were drawn from the same continuous distribution.\n\nx1 <- rnorm(200, 0, 1)\nx2 <- rnorm(200, -.5, 1.5)\nks.test(x1, x2)\n\n\n    Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  x1 and x2\nD = 0.35, p-value = 4.579e-11\nalternative hypothesis: two-sided"
  },
  {
    "objectID": "84_HypothesisTesting.html#shapiro-wilk-test-of-normality",
    "href": "84_HypothesisTesting.html#shapiro-wilk-test-of-normality",
    "title": "31  Common statistical tests",
    "section": "31.14 Shapiro-Wilk test of normality",
    "text": "31.14 Shapiro-Wilk test of normality\n\nset.seed(2021)\nx <- rnorm(2000)\ny1 <- .7 * x\ny2 <- x + x^3\n\n\n31.14.1 Q-Q Plot\n\nqqplot(rnorm(300), y1, pch = 16, col = \"#00000077\")\nqqline(y1, col = \"red\", lwd = 2)\n\n\n\n\n\nqqplot(rnorm(300), y2, pch = 16, col = \"#00000077\")\nqqline(y2, col = \"red\", lwd = 2)"
  },
  {
    "objectID": "84_HypothesisTesting.html#shapiro-wilk-test",
    "href": "84_HypothesisTesting.html#shapiro-wilk-test",
    "title": "31  Common statistical tests",
    "section": "31.15 Shapiro-Wilk test",
    "text": "31.15 Shapiro-Wilk test\n\nshapiro.test(y1)\n\n\n    Shapiro-Wilk normality test\n\ndata:  y1\nW = 0.99952, p-value = 0.9218\n\nshapiro.test(y2)\n\n\n    Shapiro-Wilk normality test\n\ndata:  y2\nW = 0.72936, p-value < 2.2e-16"
  },
  {
    "objectID": "84_HypothesisTesting.html#statsresources",
    "href": "84_HypothesisTesting.html#statsresources",
    "title": "31  Common statistical tests",
    "section": "31.16 Resources",
    "text": "31.16 Resources\n\nRegression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models"
  },
  {
    "objectID": "85_GLM.html",
    "href": "85_GLM.html",
    "title": "32  Introduction to the GLM",
    "section": "",
    "text": ".:rtemis 0.91 🌊 aarch64-apple-darwin20 (64-bit)\n\n  Defaults\n  |   Theme: whitegrid\n  |    Font: Fira Sans\n  | Palette: rtCol1\n  |    Plan: multicore\n  |   Cores: 8/10 available\n\n  Resources\n  | Documentation: https://rtemis.lambdamd.org\n  |       Learn R: https://class.lambdamd.org/pdsr\n  | rtemis themes: https://egenn.lambdamd.org/software/#rtemis_themes\n  |          Cite: `citation(\"rtemis\")`\n\n  Setup\n  | Enable progress reporting: `progressr::handlers(global = TRUE)`"
  },
  {
    "objectID": "85_GLM.html#generalized-linear-model-glm",
    "href": "85_GLM.html#generalized-linear-model-glm",
    "title": "32  Introduction to the GLM",
    "section": "32.1 Generalized Linear Model (GLM)",
    "text": "32.1 Generalized Linear Model (GLM)\nThe Generalized Linear Model is one of the most popular and important models in statistics.\nIt fits a model of the form:\n\\[y = β_0 + β_1 x_1 + β_2 x_2 + ... β_n x_n + ε\\]\nwhere:\n\n\\(y\\) is the dependent variable, i.e. outcome of interest\n\\(x_1\\) to \\(x_n\\) are the independent variables, a.k.a. covariates, a.k.a. predictors\n\\(β_0\\) is the intercept\n\\(β_1\\) to \\(β_n\\) are the coefficients\n\\(ε\\) is the error\n\nIn matrix notation:\n\\[y = Χβ + ε\\]\nLet’s look at an example using the GLM for regression. We’ll use the diabetes dataset from the mlbench package as an example.\n\ndata(PimaIndiansDiabetes2, package = 'mlbench')\nstr(PimaIndiansDiabetes2)\n\n'data.frame':   768 obs. of  9 variables:\n $ pregnant: num  6 1 8 1 0 5 3 10 2 8 ...\n $ glucose : num  148 85 183 89 137 116 78 115 197 125 ...\n $ pressure: num  72 66 64 66 40 74 50 NA 70 96 ...\n $ triceps : num  35 29 NA 23 35 NA 32 NA 45 NA ...\n $ insulin : num  NA NA NA 94 168 NA 88 NA 543 NA ...\n $ mass    : num  33.6 26.6 23.3 28.1 43.1 25.6 31 35.3 30.5 NA ...\n $ pedigree: num  0.627 0.351 0.672 0.167 2.288 ...\n $ age     : num  50 31 32 21 33 30 26 29 53 54 ...\n $ diabetes: Factor w/ 2 levels \"neg\",\"pos\": 2 1 2 1 2 1 2 1 2 2 ...\n\n\nWe fit a model predicting glucose level from all other covariates:\n\nmod <- glm(glucose ~ ., family = \"gaussian\", data = PimaIndiansDiabetes2)\nmod\n\n\nCall:  glm(formula = glucose ~ ., family = \"gaussian\", data = PimaIndiansDiabetes2)\n\nCoefficients:\n(Intercept)     pregnant     pressure      triceps      insulin         mass  \n    75.5742      -0.2168       0.1826       0.0277       0.1174      -0.0896  \n   pedigree          age  diabetespos  \n     0.1955       0.3700      21.6502  \n\nDegrees of Freedom: 391 Total (i.e. Null);  383 Residual\n  (376 observations deleted due to missingness)\nNull Deviance:      372400 \nResidual Deviance: 192000   AIC: 3561\n\nclass(mod)\n\n[1] \"glm\" \"lm\" \n\n\nThe glm() function accepts a formula that defines the model.\nThe formula hp ~ . means “regress hp on all other variables”. The family argument defines we are performing regression and the data argument points to the data frame where the covariates used in the formula are found.\nFor a gaussian output, we can also use the lm() function. There are minor differences in the output created, but the model is the same:\n\nmod_lm <- lm(glucose ~ ., data = PimaIndiansDiabetes2)\nmod_lm\n\n\nCall:\nlm(formula = glucose ~ ., data = PimaIndiansDiabetes2)\n\nCoefficients:\n(Intercept)     pregnant     pressure      triceps      insulin         mass  \n    75.5742      -0.2168       0.1826       0.0277       0.1174      -0.0896  \n   pedigree          age  diabetespos  \n     0.1955       0.3700      21.6502  \n\nclass(mod_lm)\n\n[1] \"lm\"\n\n\n\n32.1.1 Summary\nGet summary of the model using summary():\n\nsummary(mod)\n\n\nCall:\nglm(formula = glucose ~ ., family = \"gaussian\", data = PimaIndiansDiabetes2)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-73.274  -14.043   -2.433   13.004   78.112  \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 75.57419    8.08972   9.342  < 2e-16 ***\npregnant    -0.21685    0.48754  -0.445   0.6567    \npressure     0.18262    0.10014   1.824   0.0690 .  \ntriceps      0.02770    0.14665   0.189   0.8503    \ninsulin      0.11740    0.01027  11.433  < 2e-16 ***\nmass        -0.08960    0.22836  -0.392   0.6950    \npedigree     0.19555    3.40567   0.057   0.9542    \nage          0.36997    0.16183   2.286   0.0228 *  \ndiabetespos 21.65020    2.75623   7.855 4.07e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 501.4034)\n\n    Null deviance: 372384  on 391  degrees of freedom\nResidual deviance: 192037  on 383  degrees of freedom\n  (376 observations deleted due to missingness)\nAIC: 3560.6\n\nNumber of Fisher Scoring iterations: 2\n\n\nNote how R prints stars next to covariates whose p-values falls within certain limits, described right below the table of estimates.\nAlso notice that categorical variables of n levels get n-1 separate coefficients; the first level is considered the baseline. Therefore, make sure to set up factors appropriately before modeling to ensure the correct level is used as baseline.\n\n\n32.1.2 Coefficients\n\ncoefficients(mod)\n\n(Intercept)    pregnant    pressure     triceps     insulin        mass \n75.57418905 -0.21684536  0.18261621  0.02769953  0.11739871 -0.08959822 \n   pedigree         age diabetespos \n 0.19554841  0.36997121 21.65020282 \n\n# or\nmod$coefficients\n\n(Intercept)    pregnant    pressure     triceps     insulin        mass \n75.57418905 -0.21684536  0.18261621  0.02769953  0.11739871 -0.08959822 \n   pedigree         age diabetespos \n 0.19554841  0.36997121 21.65020282 \n\n\n\n\n32.1.3 Fitted values\n\nfitted(mod) |> head()\n\n       4        5        7        9       14       15 \n104.3669 134.0163 123.8123 191.4744 227.1301 147.0313 \n\n# or\nmod$fitted.values |> head()\n\n       4        5        7        9       14       15 \n104.3669 134.0163 123.8123 191.4744 227.1301 147.0313 \n\n\n\n\n32.1.4 Residuals\n\nresiduals(mod) |> head()\n\n         4          5          7          9         14         15 \n-15.366923   2.983712 -45.812340   5.525562 -38.130138  18.968718 \n\n# or\nmod$residuals |> head()\n\n         4          5          7          9         14         15 \n-15.366923   2.983712 -45.812340   5.525562 -38.130138  18.968718 \n\n\n\n\n32.1.5 p-values\nTo extract the p-values of the intercept and each coefficient, we use coef() on summary(). The final (4th) column lists the p-values:\n\ncoef(summary(mod))\n\n               Estimate Std. Error     t value     Pr(>|t|)\n(Intercept) 75.57418905 8.08972421  9.34199821 7.916237e-19\npregnant    -0.21684536 0.48753958 -0.44477489 6.567337e-01\npressure     0.18261621 0.10014453  1.82352656 6.900293e-02\ntriceps      0.02769953 0.14664887  0.18888336 8.502843e-01\ninsulin      0.11739871 0.01026851 11.43288871 3.047384e-26\nmass        -0.08959822 0.22835576 -0.39236241 6.950087e-01\npedigree     0.19554841 3.40566786  0.05741852 9.542418e-01\nage          0.36997121 0.16183342  2.28612371 2.279239e-02\ndiabetespos 21.65020282 2.75623039  7.85500474 4.071061e-14\n\n\n\n\n32.1.6 Plot linear fit\nYou use lines() to add a line on top of a scatterplot drawn with plot().\nlines() accepts x and y vectors of coordinates:\n\nset.seed(2020)\nx <- rnorm(500)\ny <- .73 * x + .5 * rnorm(500)\nxy.fit <- lm(y~x)$fitted\nplot(x, y, pch = 16, col = \"#18A3AC99\")\nlines(x, xy.fit, col = \"#178CCB\", lwd = 2)\n\n\n\n\nIn rtemis, you can use argument fit to use any supported algorithm (see modSelect()) to estimate the fit:\n\nmplot3_xy(x, y, fit = \"glm\")\n\n\n\n\n\n\n32.1.7 Logistic Regression\nFor logistic regression, i.e. classification, you can use glm() with family = binomial\nUsing the same dataset, let’s predict diabetes status:\n\ndiabetes_mod <- glm(diabetes ~ ., PimaIndiansDiabetes2, \n                    family = \"binomial\")\ndiabetes_mod\n\n\nCall:  glm(formula = diabetes ~ ., family = \"binomial\", data = PimaIndiansDiabetes2)\n\nCoefficients:\n(Intercept)     pregnant      glucose     pressure      triceps      insulin  \n -1.004e+01    8.216e-02    3.827e-02   -1.420e-03    1.122e-02   -8.253e-04  \n       mass     pedigree          age  \n  7.054e-02    1.141e+00    3.395e-02  \n\nDegrees of Freedom: 391 Total (i.e. Null);  383 Residual\n  (376 observations deleted due to missingness)\nNull Deviance:      498.1 \nResidual Deviance: 344  AIC: 362\n\n\n\nsummary(diabetes_mod)\n\n\nCall:\nglm(formula = diabetes ~ ., family = \"binomial\", data = PimaIndiansDiabetes2)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.7823  -0.6603  -0.3642   0.6409   2.5612  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.004e+01  1.218e+00  -8.246  < 2e-16 ***\npregnant     8.216e-02  5.543e-02   1.482  0.13825    \nglucose      3.827e-02  5.768e-03   6.635 3.24e-11 ***\npressure    -1.420e-03  1.183e-02  -0.120  0.90446    \ntriceps      1.122e-02  1.708e-02   0.657  0.51128    \ninsulin     -8.253e-04  1.306e-03  -0.632  0.52757    \nmass         7.054e-02  2.734e-02   2.580  0.00989 ** \npedigree     1.141e+00  4.274e-01   2.669  0.00760 ** \nage          3.395e-02  1.838e-02   1.847  0.06474 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 498.10  on 391  degrees of freedom\nResidual deviance: 344.02  on 383  degrees of freedom\n  (376 observations deleted due to missingness)\nAIC: 362.02\n\nNumber of Fisher Scoring iterations: 5"
  },
  {
    "objectID": "85_GLM.html#mass-univariate-analysis",
    "href": "85_GLM.html#mass-univariate-analysis",
    "title": "32  Introduction to the GLM",
    "section": "32.2 Mass-univariate analysis",
    "text": "32.2 Mass-univariate analysis\nThere are many cases where we have a large number of predictors and, along with any other number of tests or models, we may want to regress our outcome of interest on each covariate, one at a time.\nLet’s create some synthetic data with 1000 cases and 100 covariates\nThe outcome is generated using just 4 of those 100 covariates and has added noise.\n\nset.seed(2020)\nn_col <- 100\nn_row <- 1000\nx <- as.data.frame(lapply(seq(n_col), function(i) rnorm(n_row)),\n                   col.names = paste0(\"Feature_\", seq(n_col)))\ndim(x)\n\n[1] 1000  100\n\ny <- .7 + x[, 10] + .3 * x[, 20] + 1.3 * x[, 30] + x[, 50] + rnorm(500)\n\nLet’s fit a linear model regressing y on each column of x using lm:\n\nmod.xy.massuni <- lapply(seq(x), function(i) lm(y ~ x[, i]))\nlength(mod.xy.massuni)\n\n[1] 100\n\nnames(mod.xy.massuni) <- paste0(\"mod\", seq(x))\n\nTo extract p-values for each model, we must find where exactly to look.\nLet’s look into the first model:\n\n(ms1 <- summary(mod.xy.massuni$mod1))\n\n\nCall:\nlm(formula = y ~ x[, i])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.5402 -1.4881 -0.0618  1.4968  5.8152 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.61800    0.06878   8.985   <2e-16 ***\nx[, i]       0.08346    0.06634   1.258    0.209    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.174 on 998 degrees of freedom\nMultiple R-squared:  0.001584,  Adjusted R-squared:  0.0005831 \nF-statistic: 1.583 on 1 and 998 DF,  p-value: 0.2086\n\nms1$coefficients\n\n              Estimate Std. Error  t value     Pr(>|t|)\n(Intercept) 0.61800326 0.06878142 8.985032 1.266204e-18\nx[, i]      0.08346393 0.06634074 1.258110 2.086464e-01\n\n\nThe p-values for each feature is stored in row 1, column 4 fo the coefficients matrix. Let’s extract all of them:\n\nmod.xy.massuni.pvals <- sapply(mod.xy.massuni, function(i) summary(i)$coefficients[2, 4])\n\nLet’s see which variable are significant at the 0.05:\n\nwhich(mod.xy.massuni.pvals < .05)\n\n mod5 mod10 mod12 mod20 mod28 mod30 mod42 mod50 mod61 mod65 mod72 mod82 mod85 \n    5    10    12    20    28    30    42    50    61    65    72    82    85 \nmod91 mod94 mod99 \n   91    94    99 \n\n\n…and which are significant at the 0.01 level:\n\nwhich(mod.xy.massuni.pvals < .01)\n\nmod10 mod20 mod28 mod30 mod50 \n   10    20    28    30    50"
  },
  {
    "objectID": "85_GLM.html#correction-for-multiple-comparisons",
    "href": "85_GLM.html#correction-for-multiple-comparisons",
    "title": "32  Introduction to the GLM",
    "section": "32.3 Correction for multiple comparisons",
    "text": "32.3 Correction for multiple comparisons\nWe’ve performed a large number of tests and before reporting the results, we need to control for multiple comparisons.\nTo do that, we use R’s p.adjust() function. It adjusts a vector of p-values to account for multiple comparisons using one of multiple methods. The default, and recommended, is the Holm method. It ensures that FWER < α, i.e. controls the family-wise error rate, a.k.a. the probability of making one or more false discoveries (Type I errors)\n\nmod.xy.massuni.pvals.holm_adjusted <- p.adjust(mod.xy.massuni.pvals)\n\nNow, let’s see which features’ p-values survive the magical .05 threshold:\n\nwhich(mod.xy.massuni.pvals.holm_adjusted < .05)\n\nmod10 mod20 mod30 mod50 \n   10    20    30    50 \n\n\nThese are indeed the correct features (not surprisingly, still reassuringly)."
  },
  {
    "objectID": "85_GLM.html#glmresources",
    "href": "85_GLM.html#glmresources",
    "title": "32  Introduction to the GLM",
    "section": "32.4 Resources",
    "text": "32.4 Resources\n\nRegression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models"
  },
  {
    "objectID": "86_Resampling.html",
    "href": "86_Resampling.html",
    "title": "33  Resampling",
    "section": "",
    "text": "Resampling refers to a collection of techniques for selecting cases from a sample. It is central to many machine learning algorithms and pipelines. The two core uses of resampling in predictive modeling / machinbe learning are model selection (a.k.a. tuning) and model assessment. By convention, we use the terms training and validation set when refering to model selection, and training and testing set when refering to model assessment. The terminology is unfortunately not intuitive and has led to much confusion. Some people reverse the terms, but we use the terms training, validation, and testing as they are used in the Elements of Statistical Learning (p. 222, Second edition, 12th printing)."
  },
  {
    "objectID": "86_Resampling.html#model-selection-and-assessment",
    "href": "86_Resampling.html#model-selection-and-assessment",
    "title": "33  Resampling",
    "section": "33.1 Model Selection and Assessment",
    "text": "33.1 Model Selection and Assessment\n\nModel Selection aka Hyperparameter tuning\n\nResamples of the training set are drawn creating multiple training and validation sets. For each resample, a combination of hyperparameters is used to train a model. The mean validation-set error across resamples is calculated. The combination of hyperparameters with the minimum loss on average across validation-set resamples is selected to train the full training sample.\n\nModel assessment\n\nResamples of the full sample are drawn, resulting into multiple training - testing sets. A model is trained on each training set and its performance assessed on the corresponding test set. Model performance is averaged across all test sets.\nNested resampling or nested crossvalidation is the procedure where 1. and 2. are nested so that hyperparameter tuning (resampling of the training set) is performed within each of multiple training resamples and performance is tested in each corresponding test set. [elevate] performs automatic nested resampling and is one of the core supervised learning functions in rtemis.\n\n\n\n\n\n10-fold crossvalidation"
  },
  {
    "objectID": "86_Resampling.html#the-resample-function",
    "href": "86_Resampling.html#the-resample-function",
    "title": "33  Resampling",
    "section": "33.2 The resample function",
    "text": "33.2 The resample function\nThe resample() function is responsible for all resampling in rtemis.\n\nx <- rnorm(500)\nres <- resample(x)\n.:Resampling Parameters\n    n.resamples: 10 \n      resampler: strat.sub \n   stratify.var: y \n        train.p: 0.75 \n   strat.n.bins: 4 \n[2022-05-10 21:14:20 resample] Created 10 stratified subsamples \n\nclass(res)\n\n[1] \"resample\" \"list\"    \n\n\nIt outputs a list which is an S3 object of class resample, with print and plot methods.\n\nres\n.:rtemis resample object \n              N: 10 \n           type: strat.sub \n        train.p: 0.75 \n   strat.n.bins: 4 \n\nplot(res)\n\n\n\n\nThe teal-colored lines represent the training cases selected for each resample, the white are testing cases (held out).\nresample() supports 5 types of resampling:\n\nk-fold crossvalidation (Stratified)\n\nYou split the cases into k sets (folds). Each set is used once as the validation or testing set. This means each cases is left out exactly once and there is no overlap between different validation/test sets. In rtemis, the folds are also stratified by default on the outcome unless otherwise chosen. Stratification tries to maintain the full sample’s distribution in both training and left-out sets. This is crucial for non-normally distributed continuous outcomes or imbalanced datasets. 10 is a common value for k, called 10-fold. Note that the size of the training and left-out sets depends on the sample size.\n\nres.10fold <- resample(x, 10, \"kfold\")\n.:Resampling Parameters\n    n.resamples: 10 \n      resampler: kfold \n   stratify.var: y \n   strat.n.bins: 4 \n[2022-05-10 21:14:22 resample] Created 10 independent folds \n\n\n\nStratified subsampling\n\nDraw n.resamples stratified samples from the data given a certain probability (train.p) that each case belongs to the training set. Since you are randomly sampling from the full sample each time, there will be overlap in the test set cases, but you control the training-to-testing ratio and number of resamples independently, unlike in k-fold resampling.\n\nres.25ss <- resample(x, 25, \"strat.sub\")\n.:Resampling Parameters\n    n.resamples: 25 \n      resampler: strat.sub \n   stratify.var: y \n        train.p: 0.75 \n   strat.n.bins: 4 \n[2022-05-10 21:14:22 resample] Created 25 stratified subsamples \n\n\n\nBootstrap\n\nThe bootstrap: random sampling with replacement. Since cases are replicated, you should use bootstrap as the outer resampler if you will also have inner resampling for tuning, since the same case may end up in both training and validation sets.\n\nres.100boot <- resample(x, 100, \"bootstrap\")\n.:Resampling Parameters\n   n.resamples: 100 \n     resampler: bootstrap \n[2022-05-10 21:14:22 resample] Created 100 bootstrap resamples \n\n\n\nStratified Bootstrap\n\nThis is stratified subsampling with random replication of cases to match the length of the original sample. Same as the bootstrap, do not use if you will be further resampling each resample.\n\nres.100sboot <- resample(x, 100, \"strat.boot\")\n.:Resampling Parameters\n     n.resamples: 100 \n       resampler: strat.boot \n    stratify.var: y \n         train.p: 0.75 \n    strat.n.bins: 4 \n   target.length: 500 \n[2022-05-10 21:14:22 resample] Created 100 stratified bootstraps \n\n\n\nLeave-One-Out-Crossvalidation (LOOCV)\n\nThis is k-fold crossvalidation where \\(k = N\\), where \\(N\\) is number of data points/cases in the whole sample. It has been included for experimentation and completenes, but it is not recommended either for model selection or assessment over the other resampling methods.\n\nres.loocv <- resample(x, resampler = \"loocv\")\n.:Resampling Parameters\n   n.resamples: 500 \n     resampler: loocv \n[2022-05-10 21:14:22 resample] Created 500 independent folds (LOOCV)"
  },
  {
    "objectID": "86_Resampling.html#example-stratified-vs-random-sampling-in-a-binomial-distribution",
    "href": "86_Resampling.html#example-stratified-vs-random-sampling-in-a-binomial-distribution",
    "title": "33  Resampling",
    "section": "33.3 Example: Stratified vs random sampling in a binomial distribution",
    "text": "33.3 Example: Stratified vs random sampling in a binomial distribution\nImagine y is the outcome of interest where events occur with a probability of .1 - a common scenario in many fields.\n\nset.seed(2020)\nx <- rbinom(100, 1, .1)\nmplot3_x(x)\n\n\n\nfreq <- table(x)\nprob <- freq[2] / sum(freq)\nres.nonstrat <- lapply(seq(10), function(i) sample(seq(x), .75*length(x)))\nres.strat <- resample(x)\n.:Resampling Parameters\n    n.resamples: 10 \n      resampler: strat.sub \n   stratify.var: y \n        train.p: 0.75 \n   strat.n.bins: 4 \n[2022-05-10 21:14:22 strat.sub] Using max n bins possible = 2 \n[2022-05-10 21:14:22 resample] Created 10 stratified subsamples \n\nprob.nonstrat <- sapply(seq(10), function(i) {\n  freq <- table(x[res.nonstrat[[i]]])\n  freq[2]/sum(freq)\n})\nprob.strat <- sapply(seq(10), function(i) {\n  freq <- table(x[res.strat[[i]]])\n  freq[2]/sum(freq)\n})\nprob.nonstrat\n\n         1          1          1          1          1          1          1 \n0.09333333 0.08000000 0.08000000 0.06666667 0.06666667 0.10666667 0.10666667 \n         1          1          1 \n0.10666667 0.09333333 0.08000000 \n\nsd(prob.nonstrat)\n\n[1] 0.0156505\n\nprob.strat\n\n         1          1          1          1          1          1          1 \n0.08108108 0.08108108 0.08108108 0.08108108 0.08108108 0.08108108 0.08108108 \n         1          1          1 \n0.08108108 0.08108108 0.08108108 \n\nsd(prob.strat)\n\n[1] 0\n\n\nAs expected, the random sampling resulted in different event probability in each resample, whereas stratified subsampling maintained a constant probability across resamples."
  },
  {
    "objectID": "87_Supervised.html",
    "href": "87_Supervised.html",
    "title": "34  Supervised Learning",
    "section": "",
    "text": "This is a very brief introduction to machine learning using the rtemis package. rtemis includes functions for:"
  },
  {
    "objectID": "87_Supervised.html#installation",
    "href": "87_Supervised.html#installation",
    "title": "34  Supervised Learning",
    "section": "34.1 Installation",
    "text": "34.1 Installation\ninstall the remotes package, if you don’t have it:\n\ninstall.packages(\"remotes\")\n\nInstall rtemis:\n\nremotes::install_github(\"egenn/rtemis\")\n\nrtemis uses a large number of packages under the hood. Since you would not need to use all of them, they are not installed by default. Each time an rtemis function is called, a dependency check is run and a message is printed if any packages need to be installed.\nFor this short tutorial, start by installing ranger, if it is not already installed:\n\ninstall.packages(\"ranger\")\n\nLoad rtemis:\n\nlibrary(rtemis)\n\n  .:rtemis 0.91 🌊 aarch64-apple-darwin20 (64-bit)\n\n  Defaults\n  |   Theme: whitegrid\n  |    Font: Fira Sans\n  | Palette: rtCol1\n  |    Plan: multicore\n  |   Cores: 8/10 available\n\n  Resources\n  | Documentation: https://rtemis.lambdamd.org\n  |       Learn R: https://class.lambdamd.org/pdsr\n  | rtemis themes: https://egenn.lambdamd.org/software/#rtemis_themes\n  |          Cite: `citation(\"rtemis\")`\n\n  Setup\n  | Enable progress reporting: `progressr::handlers(global = TRUE)`"
  },
  {
    "objectID": "87_Supervised.html#data-input-for-supervised-learning",
    "href": "87_Supervised.html#data-input-for-supervised-learning",
    "title": "34  Supervised Learning",
    "section": "34.2 Data Input for Supervised Learning",
    "text": "34.2 Data Input for Supervised Learning\nAll rtemis supervised learning functions begin with s. (“supervised”).\nThey accept the same first four arguments:\nx, y, x.test, y.test\nbut are flexible and allow you to also provide combined (x, y) and (x.test, y.test) data frames, as explained below.\n\n34.2.1 Scenario 1 (x.train, y.train, x.test, y.test)\nIn the most straightforward case, provide each featureset and outcome individually:\n\nx: Training set features\ny: Training set outcome\nx.test: Testing set features (Optional)\ny.test: Testing set outcome (Optional)\n\n\nx <- rnormmat(200, 10, seed = 2019)\nw <- rnorm(10)\ny <- x %*% w + rnorm(200)\nres <- resample(y, seed = 2020)\n.:Resampling Parameters\n    n.resamples: 10 \n      resampler: strat.sub \n   stratify.var: y \n        train.p: 0.75 \n   strat.n.bins: 4 \n[2022-05-10 18:10:21 resample] Created 10 stratified subsamples \n\nx.train <- x[res$Subsample_1, ]\nx.test <- x[-res$Subsample_1, ]\ny.train <- y[res$Subsample_1]\ny.test <- y[-res$Subsample_1]\n\n\nmod_glm <- s_GLM(x.train, y.train, x.test, y.test)\n[2022-05-10 18:10:21 s_GLM] Hello, egenn \n\n.:Regression Input Summary\nTraining features: 147 x 10 \n Training outcome: 147 x 1 \n Testing features: 53 x 10 \n  Testing outcome: 53 x 1 \n\n[2022-05-10 18:10:21 s_GLM] Training GLM... \n\n.:GLM Regression Training Summary\n    MSE = 0.84 (91.88%)\n   RMSE = 0.92 (71.51%)\n    MAE = 0.75 (69.80%)\n      r = 0.96 (p = 5.9e-81)\n   R sq = 0.92\n\n.:GLM Regression Testing Summary\n    MSE = 1.22 (89.03%)\n   RMSE = 1.10 (66.88%)\n    MAE = 0.90 (66.66%)\n      r = 0.94 (p = 2.5e-26)\n   R sq = 0.89\n\n\n\n\n[2022-05-10 18:10:22 s_GLM] Run completed in 0.03 minutes (Real: 1.62; User: 0.43; System: 0.04) \n\n\n\n\n34.2.2 Scenario 2: (x.train, x.test)\nYou can provide training and testing sets as a single data.frame each, where the last column is the outcome. Now x is the full training data and y the full testing data:\n\nx: data.frame(x.train, y.train)\ny: data.frame(x.test, y.test)\n\n\nx <- rnormmat(200, 10, seed = 2019)\nw <- rnorm(10)\ny <- x %*% w + rnorm(200)\ndat <- data.frame(x, y)\nres <- resample(dat, seed = 2020)\n[2022-05-10 18:10:22 resample] Input contains more than one columns; will stratify on last \n.:Resampling Parameters\n    n.resamples: 10 \n      resampler: strat.sub \n   stratify.var: y \n        train.p: 0.75 \n   strat.n.bins: 4 \n[2022-05-10 18:10:22 resample] Created 10 stratified subsamples \n\ndat_train <- dat[res$Subsample_1, ]\ndat_test <- dat[-res$Subsample_1, ]\n\n\nmod_glm <- s_GLM(dat_train, dat_test)\n[2022-05-10 18:10:22 s_GLM] Hello, egenn \n\n.:Regression Input Summary\nTraining features: 147 x 10 \n Training outcome: 147 x 1 \n Testing features: 53 x 10 \n  Testing outcome: 53 x 1 \n\n[2022-05-10 18:10:22 s_GLM] Training GLM... \n\n.:GLM Regression Training Summary\n    MSE = 0.84 (91.88%)\n   RMSE = 0.92 (71.51%)\n    MAE = 0.75 (69.80%)\n      r = 0.96 (p = 5.9e-81)\n   R sq = 0.92\n\n.:GLM Regression Testing Summary\n    MSE = 1.22 (89.03%)\n   RMSE = 1.10 (66.88%)\n    MAE = 0.90 (66.66%)\n      r = 0.94 (p = 2.5e-26)\n   R sq = 0.89\n\n\n\n\n[2022-05-10 18:10:22 s_GLM] Run completed in 4.8e-04 minutes (Real: 0.03; User: 0.02; System: 4e-03) \n\n\nThe dataPrepare() function will check data dimensions and determine whether data was input as separate feature and outcome sets or combined and ensure the correct number of cases and features was provided.\nIn either scenario, Regression will be performed if the outcome is numeric and Classification if the outcome is a factor."
  },
  {
    "objectID": "87_Supervised.html#regression",
    "href": "87_Supervised.html#regression",
    "title": "34  Supervised Learning",
    "section": "34.3 Regression",
    "text": "34.3 Regression\n\n34.3.1 Check Data with checkData()\n\nx <- rnormmat(500, 50, seed = 2019)\nw <- rnorm(50)\ny <- x %*% w + rnorm(500)\ndat <- data.frame(x, y)\nres <- resample(dat)\n[2022-05-10 18:10:23 resample] Input contains more than one columns; will stratify on last \n.:Resampling Parameters\n    n.resamples: 10 \n      resampler: strat.sub \n   stratify.var: y \n        train.p: 0.75 \n   strat.n.bins: 4 \n[2022-05-10 18:10:23 resample] Created 10 stratified subsamples \n\ndat_train <- dat[res$Subsample_1, ]\ndat_test <- dat[-res$Subsample_1, ]\n\n\ncheckData(x)\n x: A matrix with 500 rows and 50 features\n\n  Data types________________\n  * 50 continuous features\n  * 0 integer features\n  * 0 categorical features\n  * 0 character features\n  * 0 date features\n\n  Issues____________________\n  * 0 constant features\n  * 0 duplicated cases\n  * 0 features include 'NA' values\n\n  Recommendations___________\n  * Everything looks good\n\n\n\n\n34.3.2 Single Model\n\nmod <- s_GLM(dat_train, dat_test)\n[2022-05-10 18:10:23 s_GLM] Hello, egenn \n\n.:Regression Input Summary\nTraining features: 374 x 50 \n Training outcome: 374 x 1 \n Testing features: 126 x 50 \n  Testing outcome: 126 x 1 \n\n[2022-05-10 18:10:23 s_GLM] Training GLM... \n\n.:GLM Regression Training Summary\n    MSE = 1.02 (97.81%)\n   RMSE = 1.01 (85.18%)\n    MAE = 0.81 (84.62%)\n      r = 0.99 (p = 1.3e-310)\n   R sq = 0.98\n\n.:GLM Regression Testing Summary\n    MSE = 0.98 (97.85%)\n   RMSE = 0.99 (85.35%)\n    MAE = 0.76 (85.57%)\n      r = 0.99 (p = 2.7e-105)\n   R sq = 0.98\n\n\n\n\n[2022-05-10 18:10:23 s_GLM] Run completed in 7.8e-04 minutes (Real: 0.05; User: 0.03; System: 0.01) \n\n\n\n\n34.3.3 Crossvalidated Model\n\nmod <- elevate(dat, mod = \"glm\")\n[2022-05-10 18:10:23 elevate] Hello, egenn \n\n.:Regression Input Summary\nTraining features: 500 x 50 \n Training outcome: 500 x 1 \n[2022-05-10 18:10:23 elevate] Using future framework \n[2022-05-10 18:10:23 resLearn] Outer resampling plan set to sequential \n[2022-05-10 18:10:23 resLearn] Training Generalized Linear Model on 10 stratified subsamples... \n\n.:elevate GLM\nN repeats = 1 \nN resamples = 10 \nResampler = strat.sub \nMean MSE of 10 resamples in each repeat = 1.22\nMean MSE reduction in each repeat =  97.50%\n\n\n\n\n\n[2022-05-10 18:10:23 elevate] Run completed in 0.01 minutes (Real: 0.39; User: 0.33; System: 0.02) \n\n\nUse the describe() function to get a summary in (plain) English:\n\nmod$describe()\n\nRegression was performed using Generalized Linear Model. Model generalizability was assessed using 10 stratified subsamples. The mean R-squared across all testing set resamples was 0.97.\n\n\n\nmod$plot()"
  },
  {
    "objectID": "87_Supervised.html#classification",
    "href": "87_Supervised.html#classification",
    "title": "34  Supervised Learning",
    "section": "34.4 Classification",
    "text": "34.4 Classification\n\n34.4.1 Check Data\n\ndata(Sonar, package = 'mlbench')\ncheckData(Sonar)\n Sonar: A data.frame with 208 rows and 61 features\n\n  Data types________________\n  * 60 continuous features\n  * 0 integer features\n  * 1 categorical feature, which is not ordered\n  * 0 character features\n  * 0 date features\n\n  Issues____________________\n  * 0 constant features\n  * 0 duplicated cases\n  * 0 features include 'NA' values\n\n  Recommendations___________\n  * Everything looks good\n\nres <- resample(Sonar)\n[2022-05-10 18:10:24 resample] Input contains more than one columns; will stratify on last \n.:Resampling Parameters\n    n.resamples: 10 \n      resampler: strat.sub \n   stratify.var: y \n        train.p: 0.75 \n   strat.n.bins: 4 \n[2022-05-10 18:10:24 strat.sub] Using max n bins possible = 2 \n[2022-05-10 18:10:24 resample] Created 10 stratified subsamples \n\nsonar_train <- Sonar[res$Subsample_1, ]\nsonar_test <- Sonar[-res$Subsample_1, ]\n\n\n\n34.4.2 Single model\n\nmod <- s_RANGER(sonar_train, sonar_test)\n[2022-05-10 18:10:24 s_RANGER] Hello, egenn \n\n[2022-05-10 18:10:24 dataPrepare] Imbalanced classes: using Inverse Probability Weighting \n\n.:Classification Input Summary\nTraining features: 155 x 60 \n Training outcome: 155 x 1 \n Testing features: 53 x 60 \n  Testing outcome: 53 x 1 \n\n.:Parameters\n   n.trees: 1000 \n      mtry: NULL \n\n[2022-05-10 18:10:24 s_RANGER] Training Random Forest (ranger) Classification with 1000 trees... \n\n.:RANGER Classification Training Summary\n                   Reference \n        Estimated  M   R   \n                M  83   0\n                R   0  72\n\n                   Overall  \n      Sensitivity  1      \n      Specificity  1      \nBalanced Accuracy  1      \n              PPV  1      \n              NPV  1      \n               F1  1      \n         Accuracy  1      \n              AUC  1      \n\n  Positive Class:  M \n\n.:RANGER Classification Testing Summary\n                   Reference \n        Estimated  M   R   \n                M  25  11\n                R   3  14\n\n                   Overall  \n      Sensitivity  0.8929 \n      Specificity  0.5600 \nBalanced Accuracy  0.7264 \n              PPV  0.6944 \n              NPV  0.8235 \n               F1  0.7812 \n         Accuracy  0.7358 \n              AUC  0.8643 \n\n  Positive Class:  M \n\n\n\n\n[2022-05-10 18:10:24 s_RANGER] Run completed in 2.3e-03 minutes (Real: 0.14; User: 0.20; System: 0.05) \n\n\n\n\n34.4.3 Crossvalidated Model\n\nmod <- elevate(Sonar)\n[2022-05-10 18:10:24 elevate] Hello, egenn \n\n.:Classification Input Summary\nTraining features: 208 x 60 \n Training outcome: 208 x 1 \n[2022-05-10 18:10:24 elevate] Using future framework \n[2022-05-10 18:10:24 resLearn] Outer resampling plan set to sequential \n[2022-05-10 18:10:24 resLearn] Training Random Forest (ranger) on 10 stratified subsamples... \n\n.:elevate RANGER\nN repeats = 1 \nN resamples = 10 \nResampler = strat.sub \nMean Balanced Accuracy of 10 test sets in each repeat = 0.83\n\n\n\n\n[2022-05-10 18:10:25 elevate] Run completed in 0.02 minutes (Real: 1.18; User: 2.06; System: 0.40) \n\n\n\nmod$describe()\n\nClassification was performed using Random Forest (ranger). Model generalizability was assessed using 10 stratified subsamples. The mean Balanced Accuracy across all testing set resamples was 0.83.\n\n\n\nmod$plot()\n\n\n\n\n\nmod$plotROC()\n\n\n\n\n\nmod$plotPR()\n\n\n\n\n\n\n34.4.4 Evaluation of a binary classifier\n\n\n\n\n\nEvaluation metrics for a binary classifier"
  },
  {
    "objectID": "87_Supervised.html#understanding-overfitting",
    "href": "87_Supervised.html#understanding-overfitting",
    "title": "34  Supervised Learning",
    "section": "34.5 Understanding Overfitting",
    "text": "34.5 Understanding Overfitting\nOverfitting occurs when a model fits noise in the outcome. To make this clear, consider the following example:\nAssume a random variable x:\n\nset.seed(2020)\nx <- sort(rnorm(500))\n\nand a data-generating function fn():\n\nfn <- function(x) 12 + x^5\n\nThe true y is therefore equal to fn(x):\n\ny_true <- fn(x)\n\nHowever, assume y is recorded with some noise, in this case gaussian:\n\ny_noise <- fn(x) + rnorm(500, sd = sd(y_true))\n\nWe plot:\n\nmplot3_xy(x, list(y_noise = y_noise, y_true = y_true))\n\n\n\n\nWe want to find a model that best approximates y_true, but we only know y_noise.\nA maximally overfitted model would model noise perfectly:\n\nmplot3_xy(x, list(Overfitted_model = y_noise, Ideal_model = y_true), type = \"l\")\n\n\n\n\nAn example of an SVM set up to overfit heavily:\n\nmplot3_xy(x, y_noise, fit = \"svm\",\n          fit.params = list(kernel = \"radial\", cost = 100, gamma = 100))\n\n\n\n\nAn example of a good approximation of fn using a GAM with penalized splines:\n\nmplot3_xy(x, y_noise, fit = \"gam\")"
  },
  {
    "objectID": "87_Supervised.html#rtemis-documentation",
    "href": "87_Supervised.html#rtemis-documentation",
    "title": "34  Supervised Learning",
    "section": "34.6 rtemis Documentation",
    "text": "34.6 rtemis Documentation\nFor more information on using rtemis, see the rtemis online documentation and vignettes"
  },
  {
    "objectID": "88_Unsupervised.html",
    "href": "88_Unsupervised.html",
    "title": "35  Unsupervised Learning",
    "section": "",
    "text": "Unsupervised learning aims to learn relationships within a dataset without focusing at a particular outcome. You will often hear of unsupervised learning being performed on unlabeled data. To be clear, it means it does not use the labels to guide learning - whether labels are available or not. You might, for example, perform unsupervised learning ahead of supervised learning as we shall see later. Unsupervised learning includes a number of approaches, most of which can be divided into two categories:\nIn rtemis, clustering algorithms begin with c_ and decomposition/dimensionality reduction algorithms begin with d_"
  },
  {
    "objectID": "88_Unsupervised.html#decomposition",
    "href": "88_Unsupervised.html#decomposition",
    "title": "35  Unsupervised Learning",
    "section": "35.1 Decomposition / Dimensionality Reduction",
    "text": "35.1 Decomposition / Dimensionality Reduction\nUse decomSelect() to get a listing of available decomposition algorithms:\n\ndecomSelect()\n\n.:decomSelect\nrtemis supports the following decomposition algorithms:\n\n    Name                                   Description\n     CUR                      CUR Matrix Approximation\n   H2OAE                               H2O Autoencoder\n H2OGLRM                H2O Generalized Low-Rank Model\n     ICA                Independent Component Analysis\n  ISOMAP                                        ISOMAP\n    KPCA           Kernel Principal Component Analysis\n     LLE                      Locally Linear Embedding\n     MDS                      Multidimensional Scaling\n     NMF             Non-negative Matrix Factorization\n     PCA                  Principal Component Analysis\n    SPCA           Sparse Principal Component Analysis\n     SVD                  Singular Value Decomposition\n    TSNE   t-distributed Stochastic Neighbor Embedding\n    UMAP Uniform Manifold Approximation and Projection\n\n\nWe can further divide decomposition algorithms into linear (e.g. PCA, ICA, NMF) and nonlinear dimensionality reduction, (also called manifold learning, like LLE and tSNE).\n\n35.1.1 Principal Component Analysic (PCA)\n\nx <- iris[, 1:4]\niris_PCA <- d_PCA(x)\n\n[2022-05-10 18:13:50 d_PCA] Hello, egenn \n[2022-05-10 18:13:50 d_PCA] ||| Input has dimensions 150 rows by 4 columns, \n[2022-05-10 18:13:50 d_PCA]     interpreted as 150 cases with 4 features. \n[2022-05-10 18:13:50 d_PCA] Performing Principal Component Analysis... \n[2022-05-10 18:13:50 d_PCA] Run completed in 4e-04 minutes (Real: 0.02; User: 0.01; System: 1e-03) \n\nmplot3_xy(iris_PCA$projections.train[, 1], \n          iris_PCA$projections.train[, 2], \n          group = iris$Species,\n          xlab = \"1st PCA component\", \n          ylab = \"2nd PCA component\", \n          main = \"PCA on iris\")\n\n\n\n\n\n\n35.1.2 Independent Component Analysis (ICA)\n\niris_ICA <- d_ICA(x, k = 2)\n\n[2022-05-10 18:13:51 d_ICA] Hello, egenn \n[2022-05-10 18:13:51 d_ICA] ||| Input has dimensions 150 rows by 4 columns, \n[2022-05-10 18:13:51 d_ICA]     interpreted as 150 cases with 4 features. \n[2022-05-10 18:13:51 d_ICA] Running Independent Component Analysis... \n[2022-05-10 18:13:51 d_ICA] Run completed in 4.2e-04 minutes (Real: 0.02; User: 3e-03; System: 1e-03) \n\nmplot3_xy(iris_ICA$projections.train[, 1], \n          iris_ICA$projections.train[, 2], \n          group = iris$Species,\n          xlab = \"1st ICA component\", \n          ylab = \"2nd ICA component\", \n          main = \"ICA on iris\")\n\n\n\n\n\n\n35.1.3 Non-negative Matrix Factorization (NMF)\n\niris_NMF <- d_NMF(x, k = 2)\n\n[2022-05-10 18:13:51 d_NMF] Hello, egenn \n[2022-05-10 18:13:52 d_NMF] ||| Input has dimensions 150 rows by 4 columns, \n[2022-05-10 18:13:52 d_NMF]     interpreted as 150 cases with 4 features. \n[2022-05-10 18:13:52 d_NMF] Running Non-negative Matrix Factorization... \n[2022-05-10 18:13:53 d_NMF] Run completed in 0.02 minutes (Real: 1.22; User: 1.02; System: 0.05) \n\nmplot3_xy(iris_NMF$projections.train[, 1], \n          iris_NMF$projections.train[, 2], \n          group = iris$Species,\n          xlab = \"1st NMF component\", \n          ylab = \"2nd NMF component\", \n          main = \"NMF on iris\")"
  },
  {
    "objectID": "88_Unsupervised.html#clustering",
    "href": "88_Unsupervised.html#clustering",
    "title": "35  Unsupervised Learning",
    "section": "35.2 Clustering",
    "text": "35.2 Clustering\nUse clustSelect() to get a listing of available clustering algorithms:\n\nclustSelect()\n\n.:clustSelect\nrtemis supports the following clustering algorithms:\n\n      Name                                                 Description\n    CMEANS                                    Fuzzy C-means Clustering\n    DBSCAN Density-based spatial clustering of applications with noise\n       EMC                         Expectation Maximization Clustering\n    HARDCL                                   Hard Competitive Learning\n    HOPACH     Hierarchical Ordered Partitioning And Collapsing Hybrid\n H2OKMEANS                                      H2O K-Means Clustering\n    KMEANS                                          K-Means Clustering\n MEANSHIFT                                       Mean Shift Clustering\n      NGAS                                       Neural Gas Clustering\n       PAM                                 Partitioning Around Medoids\n      PAMK               Partitioning Around Medoids with k estimation\n      SPEC                                         Spectral Clustering\n\n\nLet’s cluster iris and we shall also use an NMF decomposition as we saw above to project to 2 dimensions.\nWe’ll use two of the most popular clustering algorithms, K-means and PAM, aka K-medoids.\n\nx <- iris[, 1:4]\niris_NMF <- d_NMF(x, k = 2)\n\n[2022-05-10 18:13:53 d_NMF] Hello, egenn \n[2022-05-10 18:13:53 d_NMF] ||| Input has dimensions 150 rows by 4 columns, \n[2022-05-10 18:13:53 d_NMF]     interpreted as 150 cases with 4 features. \n[2022-05-10 18:13:53 d_NMF] Running Non-negative Matrix Factorization... \n[2022-05-10 18:13:53 d_NMF] Run completed in 4.5e-03 minutes (Real: 0.27; User: 0.26; System: 0.01) \n\n\n\n35.2.1 K-Means\n\niris.KMEANS <- c_KMEANS(x, k = 3)\n\n[2022-05-10 18:13:53 c_KMEANS] Hello, egenn \n[2022-05-10 18:13:53 c_KMEANS] Performing K-means Clustering with k = 3... \n[2022-05-10 18:13:53 c_KMEANS] Run completed in 1.8e-03 minutes (Real: 0.11; User: 0.09; System: 0.01) \n\nmplot3_xy(iris_NMF$projections.train[, 1], iris_NMF$projections.train[, 2],\n          group = iris.KMEANS$clusters.train,\n          xlab = \"1st NMF component\", \n          ylab = \"2nd NMF component\", \n          main = \"KMEANS on iris\")\n\n\n\n\n\n\n35.2.2 Partitioning Around Medoids with k estimation (PAMK)\n\niris_PAMK <- c_PAMK(x, krange = 3:10)\n\n[2022-05-10 18:13:53 c_PAMK] Hello, egenn \n[2022-05-10 18:13:54 c_PAMK] Partitioning Around Medoids... \n[2022-05-10 18:13:54 c_PAMK] Estimated optimal number of clusters: 3 \n[2022-05-10 18:13:54 c_PAMK] Run completed in 0.01 minutes (Real: 0.49; User: 0.38; System: 0.02) \n\nmplot3_xy(iris_NMF$projections.train[, 1], iris_NMF$projections.train[, 2],\n          group = iris_PAMK$clusters.train,\n          xlab = \"1st NMF component\", \n          ylab = \"2nd NMF component\", \n          main = \"PAMK on iris\")"
  },
  {
    "objectID": "94_GitHubIntro.html",
    "href": "94_GitHubIntro.html",
    "title": "36  Git & GitHub: the basics",
    "section": "",
    "text": "git is famously powerful and notoriously complex. This is a very brief introduction to a very small subset of git’s functionality. Multiple online resources can help you delve into git in considerably more depth.\nFirst, some important definitions:"
  },
  {
    "objectID": "94_GitHubIntro.html#installing-git",
    "href": "94_GitHubIntro.html#installing-git",
    "title": "36  Git & GitHub: the basics",
    "section": "36.1 Installing git",
    "text": "36.1 Installing git\nCheck if you system already includes an installation of git. If not you can download it from the official git website"
  },
  {
    "objectID": "94_GitHubIntro.html#basic-git-usage",
    "href": "94_GitHubIntro.html#basic-git-usage",
    "title": "36  Git & GitHub: the basics",
    "section": "36.2 Basic git usage",
    "text": "36.2 Basic git usage\nIn the system terminal, all git commanda begin with git and are followed by a command name:\n\n36.2.1 Cloning (“Downloading”)\nDownload a repository to your computer for the first time. Replace “user” with the username and “repo” with the repository name.\n\ngit clone https://github.com/user/repo.git\n\nThis will clone the remote repository to a folder name ‘repo’. You can optionally provide a different folder name after the URL.\nTo update a previously cloned repository:\n\ngit pull\n\n\n\n36.2.2 Pushing (“Uploading”)\nGet info on local changes to repository:\n\ngit status\n\nWorking locally, stage new or modified files:\n\ngit add /path/to/file\n\nStill working locally, commit changes with an informative message:\n\ngit commit -m Fixed this or added that\n\n(Note that the previous steps did not require an internet connection - this one does) Push one or multiple commits to remote repository:\n\ngit push\n\n\n\n36.2.3 Collaborating\nThe main way of contributing to a project is by a) making a new “branch” of the repository, b) making your edits, and c) either merging to master yourself or requesting your edits be merged by the owner/s of the repository. This allows multiple people to work on the codebase without getting in each other’s way.\n\n\n36.2.4 Branching and merging\nScenario: you are working on your own project, hosted on its own repository. You want to develop a new feature, which may take some time to code and test before you make it part of your official project code.\n* Create a new branch, e.g. devel * Work in your new branch until all testing is successful * Merge back to master branch\nAlways from your system terminal, from within a directory in your repository: Create a new branch:\n\ngit branch devel\n\nSwitch to your new branch:\n\ngit checkout devel\n\nWork on your code, using git add/commit/push as per usual.\nWhen you are done testing and are happy to merge back to master:\n\ngit checkout master\ngit merge devel\ngit push\n\nAll the commits performed while you were working in the devel branch will be included in that last git push from master.\n\n\n36.2.5 Pull request\nScenario: You are contributing to a repository along with other collaborators. You want to suggest a new feature is added to the code:\n\nCreate a new branch, e.g. mynewfeature\nWork in new branch until you are ready happy to share and testing is complete\nGo on to the repository website, select your branch and perform a “Pull request” asking that the changes in your mynewfeature branch are merged into master\nThe repository owner/s will review the request and can merge"
  },
  {
    "objectID": "94_GitHubIntro.html#gists",
    "href": "94_GitHubIntro.html#gists",
    "title": "36  Git & GitHub: the basics",
    "section": "36.3 Gists",
    "text": "36.3 Gists\nGitHub also offers a very convenient pastebin-like service called Gist, which lets you quickly and easily share code snippets.\nTo share some R code using a gist:\n\nVist the gist site.\nWrite in/copy-paste some code\nAdd a name including a .R suffix at the top left of the entry box\nCopy-paste the URL to share with others"
  },
  {
    "objectID": "94_GitHubIntro.html#gitresources",
    "href": "94_GitHubIntro.html#gitresources",
    "title": "36  Git & GitHub: the basics",
    "section": "36.4 Git Resources",
    "text": "36.4 Git Resources\nGit and GitHub are very powerful and flexible, with a great deal of functionality. Some resources to learn (a great deal) more:\n\nGit cheat sheet\nGitHub guides # Pro Git Book by Scott Chacon and Ben Straub"
  },
  {
    "objectID": "94_GitHubIntro.html#git-and-github-for-open-and-reproducible-science",
    "href": "94_GitHubIntro.html#git-and-github-for-open-and-reproducible-science",
    "title": "36  Git & GitHub: the basics",
    "section": "36.5 Git and GitHub for open and reproducible science",
    "text": "36.5 Git and GitHub for open and reproducible science\nIt is recommended to create a new GitHub repository for each new research project. It may be worthwhile creating a new repository when it’s time to publish a paper, to include all final working code that should accompany the publication (and e.g. exclude all trial-and-error, testing, etc. code). As Always, make sure to follow journal requirements for reporting data deposition (includes code) and accessibility."
  },
  {
    "objectID": "94_GitHubIntro.html#applications-with-builtin-git-support",
    "href": "94_GitHubIntro.html#applications-with-builtin-git-support",
    "title": "36  Git & GitHub: the basics",
    "section": "36.6 Applications with builtin git support",
    "text": "36.6 Applications with builtin git support\nMany applications support git, and allow you to pull / add / commit / push and more directly from the app using their GUI.\nA couple of interest for the R user:\n\nRStudio Out trusty IDE has a Git panel enabled when a project is in a directory that’s part of a git repository\nThe Atom editor GitHub’s own feature-packed text editor is naturally built around git and GitHub support. It offers its own package manager with access to a large and growing ecosystem of packages. Packages are available that transform Atom to a very capable and customizable IDE."
  },
  {
    "objectID": "95_TerminalIntro.html",
    "href": "95_TerminalIntro.html",
    "title": "37  Introduction to the system shell",
    "section": "",
    "text": "This is a very brief introduction to some of the most commonly used shell commands.\nA shell is a command line interface allowing access to an operating system’s services. Multiple different shells exist. The most popular is probably bash, which is the default in most Linux installations. In MacOS, the default shell switched form bash to zsh in 2019 with the release of Catalina. In Windows, various shells are available through the Windows Subsystem for Linux.\nThe commands listed here will work similarly in all/most shells."
  },
  {
    "objectID": "95_TerminalIntro.html#common-shell-commands",
    "href": "95_TerminalIntro.html#common-shell-commands",
    "title": "37  Introduction to the system shell",
    "section": "37.1 Common shell commands",
    "text": "37.1 Common shell commands\nThe first thing to look for in a new environment is the help system. In the shell, this is accessed with man:\n\nman: Print the manual pages\n\n\nman man\n\n\npwd: Print working directory (the directory you are currently in)\n\n\npwd\n\n\ncd: Set working directory to /path/to/dir\n\n\ncd /path/to/dir\n\n\nmv: Move file from /current/dir/ to /new/dir\n\n\nmv /current/dir/file /new/dir\n\n\nmv: Rename file to newfilename\n\n\nmv /current/dir/file /current/dir/newfilename\n\n\ncp: Make a copy of file from currentPath into altPath\n\n\ncp /currentPath/file /altPath/file\n\n\nmkdir: Create a new directory named ‘newdir’\n\n\nmkdir /path/to/newdir\n\n\nrmdir: Remove (i.e. delete) uselessFile\n\n\n\n\n\nrm: Remove (i.e. delete) uselessFile\n\n\nrm /path/to/uselessFile\n\n\ncat: Print contents of file to the console\n\n\ncat /path/to/file\n\n\nuname: Get system information\n\n\nuname -a\n\n\nwhoami: When you forget the basics\n\n\nwhoami"
  },
  {
    "objectID": "95_TerminalIntro.html#running-system-commands-within-r",
    "href": "95_TerminalIntro.html#running-system-commands-within-r",
    "title": "37  Introduction to the system shell",
    "section": "37.2 Running system commands within R",
    "text": "37.2 Running system commands within R\nYou can execute any system command within R using the system() command:\n\nsystem(\"uname -a\")"
  },
  {
    "objectID": "98_Resources.html",
    "href": "98_Resources.html",
    "title": "38  Resources",
    "section": "",
    "text": "The R Manuals include a number of resources, including:\n\nIntroduction to R\nCRAN task views offer curated lists of packages by topic"
  },
  {
    "objectID": "98_Resources.html#r-markdown",
    "href": "98_Resources.html#r-markdown",
    "title": "38  Resources",
    "section": "38.2 R markdown",
    "text": "38.2 R markdown\n\nR Markdown: The Definitive Guide by Yihui Xie, J. J. Allaire, Garrett Grolemund\nbookdown: Authoring Books and Technical Documents with R Markdown: how to make websites like this one you are on right now"
  },
  {
    "objectID": "98_Resources.html#documentation",
    "href": "98_Resources.html#documentation",
    "title": "38  Resources",
    "section": "38.3 Documentation",
    "text": "38.3 Documentation\n\nDocumentation with roxygen2"
  },
  {
    "objectID": "98_Resources.html#r-for-data-science",
    "href": "98_Resources.html#r-for-data-science",
    "title": "38  Resources",
    "section": "38.4 R for data science",
    "text": "38.4 R for data science\n\nR Programming for Data Science by Roger D. Peng, based mostly on base R, and also covers the basics of dplyr.\nR for Data Science by Hadley Wickham & Garrett Grolemund, based on the tidyverse\nData wrangling, exploration, and analysis with R"
  },
  {
    "objectID": "98_Resources.html#graphics",
    "href": "98_Resources.html#graphics",
    "title": "38  Resources",
    "section": "38.5 Graphics",
    "text": "38.5 Graphics\n\n38.5.1 ggplot2\n\nggplot2\n\n\n\n38.5.2 Plotly\n\nPlotly R API\nInteractive web-based data visualization with R, plotly, and shiny"
  },
  {
    "objectID": "98_Resources.html#advanced-r",
    "href": "98_Resources.html#advanced-r",
    "title": "38  Resources",
    "section": "38.6 Advanced R",
    "text": "38.6 Advanced R\n\nEfficient R Programming by Colin Gillespie & Robin Lovelace\nHigh performance functions with Rcpp"
  },
  {
    "objectID": "98_Resources.html#git-and-github",
    "href": "98_Resources.html#git-and-github",
    "title": "38  Resources",
    "section": "38.7 Git and GitHub",
    "text": "38.7 Git and GitHub\n\nGitHub guides\nPro Git Book by Scott Chacon and Ben Straub"
  },
  {
    "objectID": "98_Resources.html#machine-learning",
    "href": "98_Resources.html#machine-learning",
    "title": "38  Resources",
    "section": "38.8 Machine Learning",
    "text": "38.8 Machine Learning\n\nAn Introduction to Statistical Learning offers an accessible view of core learning algorithms, without being math-heavy.\nElements of Statistical Learning offers a deeper and more extensive view on learning algorithms.\nMachine Learning with rtemis"
  },
  {
    "objectID": "98_Resources.html#getting-help",
    "href": "98_Resources.html#getting-help",
    "title": "38  Resources",
    "section": "38.9 Getting help",
    "text": "38.9 Getting help\nStack Overflow is a massively popular Q&A site for programmers, part of the wider Stack Exchange network. Many R-related web searches will bring up posts in Stack Overflow. You can view all questions tagged with “r”.\nWhen posting a question in any setting, it is strongly recommended to provide a minimal reproducible example (MRE). Stack Overflow provides guidelines on how to create an MRE."
  },
  {
    "objectID": "200_CrashCourse.html#introduction-format",
    "href": "200_CrashCourse.html#introduction-format",
    "title": "DCR Intro to R",
    "section": "Introduction & Format",
    "text": "Introduction & Format\nThis is a brief introduction to the R programming language for health data science. It covers basic commands to allow you to read in data, perform common manipulations, plot data and run common tests.\nR is a programming language developed specifically for statistical computing and graphics.\nIt is often mis-characterized as a “statistical package”, similar to SPSS, for example, but as a full programming language it has far more extensive functionality.\nFor a more thorough coverage of the topic, see the Programming for Data Science in R book. Links to book chapters will be provided throughout these notes for those interested in reading up further into particular topics.\nR can be used to perform most, if not all, operations in statistics, data science, machine learning. This is not a crash course in statistics, data science, or machine learning, but an introduction to the language itself.\nIf you have any questions, write them down and make sure to ask them during each section’s Q&A."
  },
  {
    "objectID": "200_CrashCourse.html#introduction-to-programming",
    "href": "200_CrashCourse.html#introduction-to-programming",
    "title": "DCR Intro to R",
    "section": "Introduction to Programming",
    "text": "Introduction to Programming\nEveryone can code\nEveryone can learn how to program. Whether it takes you a minute or a little longer to learn how to write the code neessary to perform a certain, you can master it. Do not worry about comparing yourself to others.\nAs with more or less everything, a) motivation is key and b) you get good with practice.\nYou are here because, presumably, you have important questions to answet using data.\nKnowing even a little code can give you the power to work with your own data without depending fully on someone else. At the same time, it makes collaborating with other scientistics, clinicians, statisticians, etc. much more effective & efficient.\nYou don’t learn to code by reading books or slides, you learn to code by doing. Ideally, work on data you are interested in, trying to asnwet questions you care about.\nLearning to code can be exciting and frustrating. It’s similar to learning to play an instrument - like the guitar. At first, it may seem unnatural and annoying, but you can get better at it rather quickly and it’s very rewarding and satisfying.\nIt is important to be able to read & write code.\nCoding needs logic & creativity\nProgramming is based on logic and learning to code helps structure your thoughts, your experiments, your reasoning.\nProgramming languages are human constructs. They developed to answer important needs. They develop with time, as needs evolve. Many design choices are explained historically, a few may be more arbitrary.\nEverything is a series of simple, manageable steps\nRemember this: the shortest and simplest piece of code up to the longest and most complex is made of a sequence of relatively simple steps.\nAlways be clear about what you want to achieve first, then break it down step-by-step in code. Each step is relatively easy to figure out and check that it is working as desired. A common mistake is to write multiple steps in one go, and then have a hard time figuring out where an error occurs or why.\nTo get from A to B using code there are virtually always multiple different paths. That can be confusing or perhaps frustrating, but it is also exciting. Programming to the uninitiated may seem a a rigid exercise but it is highly creative. Remember that there objective and subjective differences to consider when designing a code to take you from A to B. Suppose you have two approaches that have the same input and produce the same output. An objective difference would be how fast each completes the task and how many lines of code or number of function calls it requires. A subjective difference would be the programming style / syntax used / whether the code is “elegant” - a pretty broad topic.\nErrors happen and they are not all the same\nErrors in code happen all the time, it is part of the process. But, not all errors are the same, far from it. One crucial difference is coding errors that:\n\nstop execution of code and produce an error message. This is the best case scenario because it can’t go unnoticed.\ndo not stop execution of code but produce a warning. These warnings are too often ignored. They may be serios or trivial, but must be investigated.\ndo not stop execution and produce no warnings. This is the worst kind of error since it is silent. These are very common and the only way to recognize them are to check the output.\n\nDetails matter (a lot)\nA lot of beginner and non-beginner mistakes occur because a variable or function name is misspelled.\nALWAYS, ALWAYS, ALWAYS READ ERROR AND WARNING MESSAGES. The all-caps is because this is a) essential and b) far too often ignored.\nAlways check yourself\nRemember: the most important thing is to ensure you produce correct results at each step. Don’t place writing smart or elegant code above writing correct code. Spend time reviewing your code. Ideally, if possible, have one or more other people review your code.\nDocument everything\nMake a habit from the very beginning to always use comments in your code to explai what you are trying to achieve and why. You will often need to revisit your code after some time has passed. Life will be very hard if it’s not clear what is happening and why.\nProgramming is largely a team sport. A lot of code is written collaboratively or is used by people other than the author. Again, comprehensive documentation is super important.\nHelp is at your fingertips\nWhether you are just starting out or you are a seasoned programmer, you have many sources of information to help you troubleshoot or learn new skills.\n\nUse the built-in documentation! Builtin help files, written by the code author, are almost always the best place to start. Their quality will vary, but they are often sufficient to learn how to use a function properly.\nProgramming is largely an online activity. All documentation and source code (for open source projects) is available online. Most errors or difficulties you encounter have been encountered many times before by others. A very large number of Q&A sites, blogs, forums are a web search away. Copy-pasting an error message into a search engine will often result in multiple hits."
  },
  {
    "objectID": "200_CrashCourse.html#the-r-language",
    "href": "200_CrashCourse.html#the-r-language",
    "title": "DCR Intro to R",
    "section": "The R language",
    "text": "The R language\n\nThe S statistical programming language was developed in 1976 at Bell Labs by John Chambers and others “to turn ideas into software, quickly and faithfully”.\nR is an open source implementation of S developed by Ross Ihaka and Robert Gentleman at the University of Auckland, New Zealand; initial version released in 1995.\nSupported by the R Foundation for Statistical Computing, developed by the R Core Team and the contributions of many others.\nOfficial part of the Free Software Foundation’s GNU project available under GNU GPL v2.\nLatest version 4.1.3 released 2022-03-10 (as of writing)"
  },
  {
    "objectID": "200_CrashCourse.html#free-open-source-software",
    "href": "200_CrashCourse.html#free-open-source-software",
    "title": "DCR Intro to R",
    "section": "Free Open Source Software",
    "text": "Free Open Source Software\nFree Open Source Software (FOSS) is software that is “free” and “open source” - what does that really mean?\nWhat is Free Software?\n\n“Free software is software that gives you, the user, the freedom to share, study and modify it. We call this free software because the user is free.”\n\n— Free Software Foundation\n\n\nWhat is Open Source Software\n\n“Open source software is software with source code that anyone can inspect, modify, and enhance.”\n\n— opensource.com\n\n\nWhy is FOSS important?\nThere are many advantage to FOSS, in general. Some of those, that are highly relevant in biomedical research and clinical applications include the promotion of inclusion, transparency, and trustworthiness."
  },
  {
    "objectID": "200_CrashCourse.html#rstudio-integrated-development-environment",
    "href": "200_CrashCourse.html#rstudio-integrated-development-environment",
    "title": "DCR Intro to R",
    "section": "RStudio Integrated Development Environment",
    "text": "RStudio Integrated Development Environment\nRStudio offers a popular, feature-full Integrated Development Environment (IDE) for R.\nMore advanced users can use Visual Studio Code with the R Extension for a similar R IDE experience together with all the extra functionality an convenience of VS Code."
  },
  {
    "objectID": "200_CrashCourse.html#the-r-core-language-package-ecosystem",
    "href": "200_CrashCourse.html#the-r-core-language-package-ecosystem",
    "title": "DCR Intro to R",
    "section": "The R core language & package ecosystem",
    "text": "The R core language & package ecosystem\nR boasts extensive quantitative and statistical functionality in the base system.\nThis functionality is extended through a vast ecosystem of external packages.\n\nCRAN: The Comprehensive R Archive Network (https://cran.r-project.org/): 19001 packages\nBioconductor: Bioinformatics-related packages and more (https://www.bioconductor.org/): 2083+ packages\nGitHub: The largest source code host (>200M repositories; https://github.com/): Likely hosts most of the above and quite a few more. Also hosts a copy of the entire CRAN."
  },
  {
    "objectID": "200_CrashCourse.html#reading-in-data",
    "href": "200_CrashCourse.html#reading-in-data",
    "title": "DCR Intro to R",
    "section": "Reading in Data",
    "text": "Reading in Data\nWe shall use a heart failure dataset as an example. It is freely available at the UCI repository: “https://archive.ics.uci.edu/ml/machine-learning-databases/00519/heart_failure_clinical_records_dataset.csv”\n\nCSV\n\ndat <- read.csv(\"~/icloud/Data/UCI/heart_failure_clinical_records_dataset.csv\")\n\nThe head() function prints the first few lines of an object:\n\nhead(dat)\n\n  age anaemia creatinine_phosphokinase diabetes ejection_fraction\n1  75       0                      582        0                20\n2  55       0                     7861        0                38\n3  65       0                      146        0                20\n4  50       1                      111        0                20\n5  65       1                      160        1                20\n6  90       1                       47        0                40\n  high_blood_pressure platelets serum_creatinine serum_sodium sex smoking time\n1                   1    265000              1.9          130   1       0    4\n2                   0    263358              1.1          136   1       0    6\n3                   0    162000              1.3          129   1       1    7\n4                   0    210000              1.9          137   1       0    7\n5                   0    327000              2.7          116   0       0    8\n6                   1    204000              2.1          132   1       1    8\n  DEATH_EVENT\n1           1\n2           1\n3           1\n4           1\n5           1\n6           1\n\n\nThe read.csv() function read the contents of the CSV file into an R object known as a data.frame. This is essentially a table like a spreadsheet, where each row represents a case (e.g. a subject, patient, etc.) and each columnn represents a variable (e.g. Patient ID, Age, Sex, Dx, etc.)\n\n\nXLSX\n\ndat_too <- openxlsx::read.xlsx(\"~/icloud/Data/UCI/heart_failure_clinical_records_dataset.xlsx\")\n\n\nhead(dat_too)\n\n  age anaemia creatinine_phosphokinase diabetes ejection_fraction\n1  75       0                      582        0                20\n2  55       0                     7861        0                38\n3  65       0                      146        0                20\n4  50       1                      111        0                20\n5  65       1                      160        1                20\n6  90       1                       47        0                40\n  high_blood_pressure platelets serum_creatinine serum_sodium sex smoking time\n1                   1    265000              1.9          130   1       0    4\n2                   0    263358              1.1          136   1       0    6\n3                   0    162000              1.3          129   1       1    7\n4                   0    210000              1.9          137   1       0    7\n5                   0    327000              2.7          116   0       0    8\n6                   1    204000              2.1          132   1       1    8\n  DEATH_EVENT\n1           1\n2           1\n3           1\n4           1\n5           1\n6           1"
  },
  {
    "objectID": "200_CrashCourse.html#inspect-summarize-data",
    "href": "200_CrashCourse.html#inspect-summarize-data",
    "title": "DCR Intro to R",
    "section": "Inspect & summarize data",
    "text": "Inspect & summarize data\nGet data dimensions:\n\ndim(dat)\n\n[1] 299  13\n\n\nLook at the data structure, including data types:\n\nstr(dat)\n\n'data.frame':   299 obs. of  13 variables:\n $ age                     : num  75 55 65 50 65 90 75 60 65 80 ...\n $ anaemia                 : int  0 0 0 1 1 1 1 1 0 1 ...\n $ creatinine_phosphokinase: int  582 7861 146 111 160 47 246 315 157 123 ...\n $ diabetes                : int  0 0 0 0 1 0 0 1 0 0 ...\n $ ejection_fraction       : int  20 38 20 20 20 40 15 60 65 35 ...\n $ high_blood_pressure     : int  1 0 0 0 0 1 0 0 0 1 ...\n $ platelets               : num  265000 263358 162000 210000 327000 ...\n $ serum_creatinine        : num  1.9 1.1 1.3 1.9 2.7 2.1 1.2 1.1 1.5 9.4 ...\n $ serum_sodium            : int  130 136 129 137 116 132 137 131 138 133 ...\n $ sex                     : int  1 1 1 1 0 1 1 1 0 1 ...\n $ smoking                 : int  0 0 1 0 0 1 0 1 0 1 ...\n $ time                    : int  4 6 7 7 8 8 10 10 10 10 ...\n $ DEATH_EVENT             : int  1 1 1 1 1 1 1 1 1 1 ...\n\n\nGet summary of dataset:\n\nsummary(dat)\n\n      age           anaemia       creatinine_phosphokinase    diabetes     \n Min.   :40.00   Min.   :0.0000   Min.   :  23.0           Min.   :0.0000  \n 1st Qu.:51.00   1st Qu.:0.0000   1st Qu.: 116.5           1st Qu.:0.0000  \n Median :60.00   Median :0.0000   Median : 250.0           Median :0.0000  \n Mean   :60.83   Mean   :0.4314   Mean   : 581.8           Mean   :0.4181  \n 3rd Qu.:70.00   3rd Qu.:1.0000   3rd Qu.: 582.0           3rd Qu.:1.0000  \n Max.   :95.00   Max.   :1.0000   Max.   :7861.0           Max.   :1.0000  \n ejection_fraction high_blood_pressure   platelets      serum_creatinine\n Min.   :14.00     Min.   :0.0000      Min.   : 25100   Min.   :0.500   \n 1st Qu.:30.00     1st Qu.:0.0000      1st Qu.:212500   1st Qu.:0.900   \n Median :38.00     Median :0.0000      Median :262000   Median :1.100   \n Mean   :38.08     Mean   :0.3512      Mean   :263358   Mean   :1.394   \n 3rd Qu.:45.00     3rd Qu.:1.0000      3rd Qu.:303500   3rd Qu.:1.400   \n Max.   :80.00     Max.   :1.0000      Max.   :850000   Max.   :9.400   \n  serum_sodium        sex            smoking            time      \n Min.   :113.0   Min.   :0.0000   Min.   :0.0000   Min.   :  4.0  \n 1st Qu.:134.0   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.: 73.0  \n Median :137.0   Median :1.0000   Median :0.0000   Median :115.0  \n Mean   :136.6   Mean   :0.6488   Mean   :0.3211   Mean   :130.3  \n 3rd Qu.:140.0   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:203.0  \n Max.   :148.0   Max.   :1.0000   Max.   :1.0000   Max.   :285.0  \n  DEATH_EVENT    \n Min.   :0.0000  \n 1st Qu.:0.0000  \n Median :0.0000  \n Mean   :0.3211  \n 3rd Qu.:1.0000  \n Max.   :1.0000"
  },
  {
    "objectID": "200_CrashCourse.html#data-types",
    "href": "200_CrashCourse.html#data-types",
    "title": "DCR Intro to R",
    "section": "Data types",
    "text": "Data types\nA vector in R is a collection of items of the same type (e.g. numbers or characters) of any length, including 1 (i.e. there is no distinction between a scalar and a vector).\nData types in R are essentially different types of vectors.\nR includes a number of builtin data types. Some of the most common are:\n\nnumeric (e.g. 1.2, 5.9, 11.4)\ncharacter (e.g. “SF”, “SD”)\nlogical (e.g. “TRUE”, “FALSE”)\n\nTo create a new vector you can use the assignment operator <- or =.\n\na <- 4\n\nYou can print the contents of an object just by typing its name in the console:\n\na\n\n[1] 4\n\n\nis the same as:\n\nprint(a)\n\n[1] 4\n\n\nA comment beging with #. Anything placed after this will not be executed. Use comments to document every step in your code.\nUse c() to combine multiple values:\n\nb <- c(3, 5, 7)\n\n\nb\n\n[1] 3 5 7\n\n\nTo create a character vector, use single or double quotes around each element:\n\ndept <- c(\"ED\", \"Neuro\", \"Peds\")\n\n\ndept\n\n[1] \"ED\"    \"Neuro\" \"Peds\""
  },
  {
    "objectID": "200_CrashCourse.html#data-structures",
    "href": "200_CrashCourse.html#data-structures",
    "title": "DCR Intro to R",
    "section": "Data Structures",
    "text": "Data Structures\nR includes multiple different data structures. Think of a data structure as a container that holds one or more vectors of data.\nThe data.frame is one of the most common data structures for statistics, because it can hold vectors of different kinds, e.g. numeric, categorical, and character.\n\n\n\nRead more about data structures\n\n\n\n\nFactors\nFactors in R are used to store categorical variables and therefore have many important uses in statistics / data science / machine learning.\nLet’s convert binary categorical variables in our dataset to factors:\n\ndat$anaemia <- factor(dat$anaemia)\ndat$diabetes <- factor(dat$diabetes)\ndat$high_blood_pressure <- factor(dat$high_blood_pressure)\ndat$sex <- factor(dat$sex)\ndat$smoking <- factor(dat$smoking)\ndat$DEATH_EVENT <- factor(dat$DEATH_EVENT)\n\n\n\n\nRead more about factors"
  },
  {
    "objectID": "200_CrashCourse.html#working-with-data.frames",
    "href": "200_CrashCourse.html#working-with-data.frames",
    "title": "DCR Intro to R",
    "section": "Working with data.frames",
    "text": "Working with data.frames\nOne way to select a column of a data.frame by name, is to use the $ notation. Note, we use head() to avoid printing the entire variable.\n\nhead(dat$age)\n\n[1] 75 55 65 50 65 90"
  },
  {
    "objectID": "200_CrashCourse.html#functions-in-r",
    "href": "200_CrashCourse.html#functions-in-r",
    "title": "DCR Intro to R",
    "section": "Functions in R",
    "text": "Functions in R\nR includes a very large number of functions in the base language, which allow you to do a whole lot of data cleaning & manipulation, plotting, and modeling.\nA function is called by typing its name, followed by a parenthesis with or without arguments.\nFor example, to get the mean of the b vector from above:\n\nmean(b)\n\n[1] 5\n\n\n\n\n\nLearn how to write your own functions"
  },
  {
    "objectID": "200_CrashCourse.html#summarize-data",
    "href": "200_CrashCourse.html#summarize-data",
    "title": "DCR Intro to R",
    "section": "Summarize data",
    "text": "Summarize data\nA lot of statistical functionality is built in to the language. You can easily get summary statistics of variables using functions like mean(), median(), range(), max(), min().\n\nContinuous variables\n\nmean(dat$age)\n\n[1] 60.83389\n\nmedian(dat$age)\n\n[1] 60\n\nmin(dat$age)\n\n[1] 40\n\nmax(dat$age)\n\n[1] 95\n\nsummary(dat$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  40.00   51.00   60.00   60.83   70.00   95.00 \n\n\n\n\nCategorical variables\nThe table() function gives you the counts for each level/unique value of a variable:\n\ntable(dat$sex)\n\n\n  0   1 \n105 194 \n\ntable(dat$smoking)\n\n\n  0   1 \n203  96"
  },
  {
    "objectID": "200_CrashCourse.html#plots",
    "href": "200_CrashCourse.html#plots",
    "title": "DCR Intro to R",
    "section": "Plots",
    "text": "Plots\nR has powerful and extensive support for graphics built in to the core language.\nHere, we look at how to produce some common and important plot types:\n\nHistogram\nDraw a histogram using hist(x)\n\nhist(dat$age, col = \"lightseagreen\")\n\n\n\n\n\n\nBoxplot\nDraw a boxplot using boxplot(x)\n\nboxplot(dat$ejection_fraction, col = \"lightseagreen\")\n\n\n\n\nYou can use a simple formula notation to draw boxplots grouped by a categorical variable using ~ symbol:continuous variable ~ grouping variable\n\nboxplot(dat$serum_sodium ~ dat$smoking, col = \"lightseagreen\")\n\n\n\n\n\n\nScatter plot\nDraw a scatter plot using plot(x, y)\n\nplot(dat$age, dat$serum_sodium, col = \"lightseagreen\")\n\n\n\n\n\nplot(dat$age, dat$serum_sodium, col = \"lightseagreen\")"
  },
  {
    "objectID": "200_CrashCourse.html#hypothesis-testing",
    "href": "200_CrashCourse.html#hypothesis-testing",
    "title": "DCR Intro to R",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nT-test\nAre the means of two groups significantly different? We use a simple formula notation as with the boxplot above to split values by group:\n\nt.test(dat$serum_sodium ~ dat$sex)\n\n\n    Welch Two Sample t-test\n\ndata:  dat$serum_sodium by dat$sex\nt = 0.45176, df = 184.61, p-value = 0.652\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.8565791  1.3653665\nsample estimates:\nmean in group 0 mean in group 1 \n       136.7905        136.5361 \n\n\n\n\nChi-squared test\nTest for association between two categorical variables:\n\nchisq.test(dat$smoking, dat$DEATH_EVENT)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  dat$smoking and dat$DEATH_EVENT\nX-squared = 0.0073315, df = 1, p-value = 0.9318\n\n\n\nsmoking_sex <- chisq.test(dat$smoking, dat$sex)\nsmoking_sex\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  dat$smoking and dat$sex\nX-squared = 57.463, df = 1, p-value = 3.444e-14\n\n\nYou can print the observed frequencies:\n\nsmoking_sex$observed\n\n           dat$sex\ndat$smoking   0   1\n          0 101 102\n          1   4  92\n\n\nand the expected frequencies:\n\nsmoking_sex$expected\n\n           dat$sex\ndat$smoking        0         1\n          0 71.28763 131.71237\n          1 33.71237  62.28763"
  },
  {
    "objectID": "200_CrashCourse.html#saving-data",
    "href": "200_CrashCourse.html#saving-data",
    "title": "DCR Intro to R",
    "section": "Saving data",
    "text": "Saving data\n\nCSV\nYou can write R objects to CSV file using `write.csv()’. These can be read directly into any program or language that can handle data.\n\nwrite.csv(dat, \"~/Data/dat.csv\")\n\n\n\nRDS\nYou can also directly save any R object as an “RDS” file. These can be read into R. The advantage is that they are compressed and therefore may take a lot less space, and will maintain any type conversion you have performed.\n\nsaveRDS(dat, \"~/Data/dat.rds\")"
  },
  {
    "objectID": "200_CrashCourse.html#builtin-documentation",
    "href": "200_CrashCourse.html#builtin-documentation",
    "title": "DCR Intro to R",
    "section": "Builtin Documentation",
    "text": "Builtin Documentation\nAfter you’ve successfully installed R and RStudio, one of the first things to know is how to access and search the builtin documentation.\n\nGet help on a specific item\nIf you know the name of what you’re looking for (an R function most commonly, but possibly also the name of a dataset, or a package itself), just type ? followed by the name of said function, dataset, etc. in the R prompt:\n\n?sample\n\nIn RStudio, the above example will bring up the documentation for the sample function in the dedicated “Help” window, commonly situated at the bottom right (but can be moved by the user freely). If you are running R directly at the system shell, the same information is printed directly at the console.\nTry running the above example on your system.\n\n\nSearch the docs\nIf you do not know the name of what you are looking for, you can use double question marks, ??, followed by your query (this is short for the help.search command that provides a number of arguments you can look up using ?help.search):\n\n??bootstrap"
  },
  {
    "objectID": "99_References.html",
    "href": "99_References.html",
    "title": "References",
    "section": "",
    "text": "Bengtsson, Henrik. 2019. matrixStats: Functions That Apply to Rows\nand Columns of Matrices (and to Vectors). https://CRAN.R-project.org/package=matrixStats.\n\n\nBuuren, S van, and Karin Groothuis-Oudshoorn. 2010. “Mice:\nMultivariate Imputation by Chained Equations in r.” Journal\nof Statistical Software, 1–68.\n\n\nChambers, John M. 1998. Programming with Data: A Guide to the s\nLanguage. Springer Science & Business Media.\n\n\nGennatas, Efstathios Dimitrios. 2017. “Towards Precision\nPsychiatry: Gray Matter Development and Cognition in\nAdolescence.”\n\n\nMack, Christina, Zhaohui Su, and Daniel Westreich. 2018. “Managing\nMissing Data in Patient Registries: Addendum to Registries for\nEvaluating Patient Outcomes: A User’s Guide, [Internet].”\n\n\nMurrell, Paul. 2018. R Graphics. CRC Press.\n\n\nSarkar, Deepayan. 2008. Lattice: Multivariate Data Visualization\nwith r. New York: Springer. http://lmdvr.r-forge.r-project.org.\n\n\nSievert, Carson, Chris Parmer, Toby Hocking, Scott Chamberlain, Karthik\nRam, Marianne Corvellec, and Pedro Despouy. 2017. “Plotly: Create\nInteractive Web Graphics via ‘Plotly. Js’.” R\nPackage Version 4 (1): 110.\n\n\nStekhoven, Daniel J, and Peter Bühlmann. 2012.\n“MissForest—Non-Parametric Missing Value Imputation for Mixed-Type\nData.” Bioinformatics 28 (1): 112–18.\n\n\nWickham, Hadley. 2011. “Ggplot2.” Wiley\nInterdisciplinary Reviews: Computational Statistics 3 (2): 180–85.\n\n\nWilkinson, Leland. 2012. “The Grammar of Graphics.” In\nHandbook of Computational Statistics, 375–414. Springer.\n\n\nWright, Marvin N, and Andreas Ziegler. 2015. “Ranger: A Fast\nImplementation of Random Forests for High Dimensional Data in c++ and\nr.” arXiv Preprint arXiv:1508.04409."
  },
  {
    "objectID": "01_Preface.html",
    "href": "01_Preface.html",
    "title": "1  Preface",
    "section": "",
    "text": "Throughout this book you will see boxes with R code followed by its output, if any. The code (or input) is decorated with a teal border on the left to separate it from its output, like in the following example:\n\nx <- rnorm(200)\nx[1:20]\n\n [1]  1.43890051  0.30798483  0.70450976 -0.60107735 -0.36263414  1.22274118\n [7]  0.92326612  1.83287150 -0.83819416 -0.60649768  0.28315816 -2.15783263\n[13] -0.51911617  0.30228461  2.09433337  0.20119295  0.15636645 -0.65842281\n[19]  0.01900776  0.02839434\n\n\nNotice that R adds numbers in brackets in the beginning of each row. This happens when R prints the contents of a vector. The number is the integer index of the first element in that row. Therefore, the first one is always [1] and the number of the subsequent rows depends on how many elements fit in each line. If the output is a single element, it will still have [1] in front of it.\nAlso notice that if we enclose the assignment operation of a variable in parentheses, this prints the resulting value of the variable. Therefore, this:\n\n(y <- 4)\n\n[1] 4\n\n\nis equivalent to:\n\ny <- 4\ny\n\n[1] 4\n\n\nCurrently, this site uses Fira Code to display source code, which supports multiple character ligatures that make code prettier / easier to read.\n\n\n\n\n\nNote ligated versions of some common character combinations as they should appear in this site\n\n\n\n\nNote that if you mouse over the input code box, a clickable “Copy to clipboard” appears on the top right of the box allowing you to copy paste into an R session or file.\nLastly, you will see the following informational boxes at times:\n\n\n\nNote\n\n\n\n\n\n\nTip\n\n\n\n\n\n\nImportant\n\n\n\n\n\n\nWarning\n\n\n\n\n\n\nCaution\n\n\n\nThis book was created using Quarto, ported from the previous version which used bookdown."
  },
  {
    "objectID": "10_Indexing.html",
    "href": "10_Indexing.html",
    "title": "8  Indexing",
    "section": "",
    "text": "An index is used to select elements of a vector, matrix, array, list or data frame.\nYou can select (or exclude) one or multiple elements at a time.\nThere are two general ways to identify which elements you want to select:\nAn index can be one of two types:\nLogical indexes are usually created as the output of a logical operation, e.g. an elemntwise comparison.\nThe main indexing operator in R is the square bracket [.\nInteger indexing in R is 1-based, meaning the first item of a vector is in position 1.\n(If you are wondering why we have to mention this, know that many programming languages use 0-based indexing where the first element is in the 0th position, the second in the 1st, and the nth in the n-1 position)\nTo understand indexing make sure you are clear on the basic R data structures (vectors, matrices, lists, data.frames)\nIndexing can be used to extract values from an object or to replace values in an object."
  },
  {
    "objectID": "10_Indexing.html#indexvectors",
    "href": "10_Indexing.html#indexvectors",
    "title": "8  Indexing",
    "section": "8.1 Vectors",
    "text": "8.1 Vectors\nStart with a simple vector:\n\nx <- 15:24\nx\n\n [1] 15 16 17 18 19 20 21 22 23 24\n\n\n\n8.1.1 Integer Index\nGet the 5th element of a vector:\n\nx[5]\n\n[1] 19\n\n\nGet elements 6 through 9 of the same vector:\n\nx[6:9]\n\n[1] 20 21 22 23\n\n\n\n\n8.1.2 Logical Index\nSelect elements with value greater than 19 (logical index):\n\nx[x > 19]\n\n[1] 20 21 22 23 24\n\n\nNote that an integer index can be used to repeat elements:\n\nx[c(1, 1, 1, 4)]\n\n[1] 15 15 15 18\n\n\n\n\n8.1.3 Extract vs. Replace\n\nx <- c(24, 32, 41, 37, 999, 999, 999)\n\nIndexing allows you to access specific elements, for example to perform calculation on them.\nGet the mean of elements 2:5:\n\nmean(x[1:4])\n\n[1] 33.5\n\n\nYou can combine indexing with assignment to replace elements of an object.\nReplace elements 1:4 with their log:\n\nx[1:4] <- log(x[1:4])\nx\n\n[1]   3.178054   3.465736   3.713572   3.610918 999.000000 999.000000 999.000000\n\n\nReplace elements that are equal to 999 with NA:\n\nx[x == 999] <- NA\nx\n\n[1] 3.178054 3.465736 3.713572 3.610918       NA       NA       NA"
  },
  {
    "objectID": "10_Indexing.html#indexmatrices",
    "href": "10_Indexing.html#indexmatrices",
    "title": "8  Indexing",
    "section": "8.2 Matrices",
    "text": "8.2 Matrices\nReminder:\n\nA matrix is a 2D vector and contains elements of the same type (numeric, integer, character).\nA data frame is a 2D list and each column can contain a different data type.\n\nTo index a 2D structure, whether a matrix or data frame, we use the form [row, column]\nThe following indexing operations are therefore the same whether applied on a matrix or a data frame.\n\nmat <- matrix(21:60, 10)\ncolnames(mat) <- paste0(\"Feature_\", seq(ncol(mat)))\nrownames(mat) <- paste0(\"Row_\", seq(nrow(mat)))\nmat\n\n       Feature_1 Feature_2 Feature_3 Feature_4\nRow_1         21        31        41        51\nRow_2         22        32        42        52\nRow_3         23        33        43        53\nRow_4         24        34        44        54\nRow_5         25        35        45        55\nRow_6         26        36        46        56\nRow_7         27        37        47        57\nRow_8         28        38        48        58\nRow_9         29        39        49        59\nRow_10        30        40        50        60\n\ndf <- as.data.frame(mat)\ndf\n\n       Feature_1 Feature_2 Feature_3 Feature_4\nRow_1         21        31        41        51\nRow_2         22        32        42        52\nRow_3         23        33        43        53\nRow_4         24        34        44        54\nRow_5         25        35        45        55\nRow_6         26        36        46        56\nRow_7         27        37        47        57\nRow_8         28        38        48        58\nRow_9         29        39        49        59\nRow_10        30        40        50        60\n\n\nTo get the contents of the fifth row, second column:\n\nmat[5, 2]\n\n[1] 35\n\ndf[5, 2]\n\n[1] 35\n\n\nWe show the following on matrices, but they work just the same on data.frames.\nIf you want to select an entire row or an entire column, you leave the row or column index blank, but must use a comma:\nGet the first row:\n\nmat[1, ]\n\nFeature_1 Feature_2 Feature_3 Feature_4 \n       21        31        41        51 \n\n\nGet the second column:\n\nmat[, 2]\n\n Row_1  Row_2  Row_3  Row_4  Row_5  Row_6  Row_7  Row_8  Row_9 Row_10 \n    31     32     33     34     35     36     37     38     39     40 \n\n\nNote that colnames and rownames where added to the matrix above for convenience - if they are absent, there are no labels above each element.\nYou can define ranges for both rows and columns:\n\nmat[6:7, 2:4]\n\n      Feature_2 Feature_3 Feature_4\nRow_6        36        46        56\nRow_7        37        47        57\n\n\nYou can return rows and/or columns reversed if desired:\n\nmat[7:6, 4:2]\n\n      Feature_4 Feature_3 Feature_2\nRow_7        57        47        37\nRow_6        56        46        36\n\n\nYou can use vectors to specify any combination of rows and columns.\nGet rows 2, 4, and 7 of columns 1, 4, and 3:\n\nmat[c(2, 4, 7), c(1, 4, 3)]\n\n      Feature_1 Feature_4 Feature_3\nRow_2        22        52        42\nRow_4        24        54        44\nRow_7        27        57        47\n\n\nSince a matrix is a vector with 2 dimensions, you can also index the underlying vector directly. Regardless of whether a matrix was created by row or by column (default), the data is stored and acceesed by column. You can see that by converting the matrix to a 1D vector:\n\nas.vector(mat)\n\n [1] 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45\n[26] 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60\n\n\nsame as:\n\nc(mat)\n\n [1] 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45\n[26] 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60\n\n\nFor example, ‘mat’ has 10 rows and 4 columns, therefore the 11th element is in row 1 column 2\n\nmat[11]\n\n[1] 31\n\n\nis the same as:\n\nmat[1, 2]\n\n[1] 31\n\n\nThis only works with matrices, not data.frames.\n\n8.2.1 Matrix of indexes\nThis is quite less common, but potentially useful. It allows you to specify a series of individual [i, j] indexes, i.e. is a way to select multiple individual non-contiguous elements\n\nidm <- matrix(c(2, 4, 7, 4, 3, 1), 3)\nidm\n\n     [,1] [,2]\n[1,]    2    4\n[2,]    4    3\n[3,]    7    1\n\n\nAn n-by-2 matrix can be used to index as a length n vector of [row, colum] indexes. Therefore, the above matrix, will return elements [2, 4], [4, 3], [7, 1]:\n\nmat[idm]\n\n[1] 52 44 27\n\n\n\n\n8.2.2 Logical index\nSelect all rows with values greater than 15 on the second column:\nThe logical index for this operation is:\n\nmat[, 2] > 15\n\n Row_1  Row_2  Row_3  Row_4  Row_5  Row_6  Row_7  Row_8  Row_9 Row_10 \n  TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE   TRUE \n\n\nIt can be used directly to index the matrix:\n\nmat[mat[, 2] > 15, ]\n\n       Feature_1 Feature_2 Feature_3 Feature_4\nRow_1         21        31        41        51\nRow_2         22        32        42        52\nRow_3         23        33        43        53\nRow_4         24        34        44        54\nRow_5         25        35        45        55\nRow_6         26        36        46        56\nRow_7         27        37        47        57\nRow_8         28        38        48        58\nRow_9         29        39        49        59\nRow_10        30        40        50        60\n\n\nIndexing a matrix or a data.frame can return either a smaller matrix/data.frame or a vector.\nIn general, objects in R are returned in their most simple form unless otherwise specified. This means that if you extract a column or a row, you get a vector:\nGet the third column:\n\nmat[, 3]\n\n Row_1  Row_2  Row_3  Row_4  Row_5  Row_6  Row_7  Row_8  Row_9 Row_10 \n    41     42     43     44     45     46     47     48     49     50 \n\nclass(mat[, 3])\n\n[1] \"integer\"\n\n\nYou can specify drop = FALSE to stop R from dropping the unused dimension and return a matrix or data.frame of a single column:\n\nmat[, 3, drop = FALSE]\n\n       Feature_3\nRow_1         41\nRow_2         42\nRow_3         43\nRow_4         44\nRow_5         45\nRow_6         46\nRow_7         47\nRow_8         48\nRow_9         49\nRow_10        50\n\ndf[, 3, drop = FALSE]\n\n       Feature_3\nRow_1         41\nRow_2         42\nRow_3         43\nRow_4         44\nRow_5         45\nRow_6         46\nRow_7         47\nRow_8         48\nRow_9         49\nRow_10        50\n\n\nCheck it is still a matrix or data.frame:\n\nclass(mat[, 3, drop = FALSE])\n\n[1] \"matrix\" \"array\" \n\nclass(df[, 3, drop = FALSE])\n\n[1] \"data.frame\""
  },
  {
    "objectID": "10_Indexing.html#indexlists",
    "href": "10_Indexing.html#indexlists",
    "title": "8  Indexing",
    "section": "8.3 Lists",
    "text": "8.3 Lists\nReminder: A list can contain elements of different class and of different length:\n\nx <- list(one = 1:4,\n           two = sample(seq(0, 100, .1), 10),\n           three = c(\"mango\", \"banana\", \"tangerine\"),\n           four = median)\nx\n\n$one\n[1] 1 2 3 4\n\n$two\n [1] 35.3 98.9  0.3 41.6 35.9 96.7 94.2  3.7 38.0 77.8\n\n$three\n[1] \"mango\"     \"banana\"    \"tangerine\"\n\n$four\nfunction (x, na.rm = FALSE, ...) \nUseMethod(\"median\")\n<bytecode: 0x13caf3f98>\n<environment: namespace:stats>\n\n\nYou can access a list element with:\n\n$ followed by name of the element (therefore only works if elements are named)\nusing double brackets [[ with either name or integer index\n\nTo access the third element:\n\nx$three\n\n[1] \"mango\"     \"banana\"    \"tangerine\"\n\n\nsame as:\n\nx[[3]]\n\n[1] \"mango\"     \"banana\"    \"tangerine\"\n\n\nsame as:\n\nx[[\"three\"]]\n\n[1] \"mango\"     \"banana\"    \"tangerine\"\n\n\nTo access an element with a name or integer index stored in a variable, only the bracket notation works. Therefore, programmatically, you would always use double brackets to access different elements:\n\nidi <- 3\nidc <- \"three\"\nx[[idi]]\n\n[1] \"mango\"     \"banana\"    \"tangerine\"\n\nx[[idc]]\n\n[1] \"mango\"     \"banana\"    \"tangerine\"\n\n\n$ or [[ return an element.\nIn contrast, single bracket [ indexing of a list returns a pruned list:\n\nx[[idi]]\n\n[1] \"mango\"     \"banana\"    \"tangerine\"\n\nclass(x[[idi]])\n\n[1] \"character\"\n\n\nvs.\n\nx[idi]\n\n$three\n[1] \"mango\"     \"banana\"    \"tangerine\"\n\nclass(x[idi])\n\n[1] \"list\"\n\n\nExtract multiple list elements with single brackets, as expected:\n\nx[2:3]\n\n$two\n [1] 35.3 98.9  0.3 41.6 35.9 96.7 94.2  3.7 38.0 77.8\n\n$three\n[1] \"mango\"     \"banana\"    \"tangerine\"\n\nclass(x[2:3])\n\n[1] \"list\"\n\n\nBeware (confusing) recursive indexing.\n(This is probably rarely used).\nUnlike in the single brackets example above, where you can use a colon to specify a range of elements to index, colon notation within double brackets accesses elements recursively at the given position.\nFor example, the following extracts the 3rd element of the 2nd element of the list:\n\nx[[2:3]]\n\n[1] 0.3\n\n\nYou can convert a list to one lone vector containing all the individual components of the original list using unlist(). Notice how names are automatically created based on the original structure:\n\nx <- list(alpha = sample(seq(100), 10),\n          beta = sample(seq(100), 10),\n          gamma = sample(seq(100), 10))\nx\n\n$alpha\n [1]  3 10 46 23 80 95 36 27 38 11\n\n$beta\n [1] 81 46 51 61 25 13  5 30 79  4\n\n$gamma\n [1]  7 37 99 93 75 76 23 65 94  5\n\nunlist(x)\n\n alpha1  alpha2  alpha3  alpha4  alpha5  alpha6  alpha7  alpha8  alpha9 alpha10 \n      3      10      46      23      80      95      36      27      38      11 \n  beta1   beta2   beta3   beta4   beta5   beta6   beta7   beta8   beta9  beta10 \n     81      46      51      61      25      13       5      30      79       4 \n gamma1  gamma2  gamma3  gamma4  gamma5  gamma6  gamma7  gamma8  gamma9 gamma10 \n      7      37      99      93      75      76      23      65      94       5 \n\n\nIf you want to drop the names, you can wrap the above in unname():\n\nunname(unlist(x))\n\n [1]  3 10 46 23 80 95 36 27 38 11 81 46 51 61 25 13  5 30 79  4  7 37 99 93 75\n[26] 76 23 65 94  5\n\n\n\n8.3.1 Logical index\nWe can use a logical index on a list with single bracket:\n\nx[c(T, F, T, F)]\n\n$alpha\n [1]  3 10 46 23 80 95 36 27 38 11\n\n$gamma\n [1]  7 37 99 93 75 76 23 65 94  5"
  },
  {
    "objectID": "10_Indexing.html#indexdfs",
    "href": "10_Indexing.html#indexdfs",
    "title": "8  Indexing",
    "section": "8.4 Data frames",
    "text": "8.4 Data frames\nWe’ve already seen above that a data frame can be indexed in many ways similar to a matrix, i.e. by defining rows and columns. At the same time, we know that a data frame is a rectangular list. Like a list, its elements are vectors of any type (integer, double, character, factor, and more) but, unlike a list, they have to be of the same length. A data frame can also be indexed the same way as a list and similar to list indexing, notice that some methods return a smaller data frame, while others return vectors.\n\n\n\nYou can index a data frame using all the ways you can index a list and all the ways you can index a matrix.\n\n\n\nLet’s create a simple data frame:\n\nx <- data.frame(Feat_1 = 21:25,\n                Feat_2 = rnorm(5),\n                Feat_3 = paste0(\"rnd_\", sample(seq(100), 5)))\nx\n\n  Feat_1     Feat_2 Feat_3\n1     21  0.5096966 rnd_24\n2     22  0.2328109 rnd_89\n3     23  0.8612651  rnd_4\n4     24 -0.2623318 rnd_52\n5     25 -1.3491615 rnd_92\n\n\n\n8.4.1 Extract column(s)\nJust like in a list, using the $ operator or double bracket [[ returns an element, i.e. a vector:\n\nx$Feat_2\n\n[1]  0.5096966  0.2328109  0.8612651 -0.2623318 -1.3491615\n\nclass(x$Feat_2)\n\n[1] \"numeric\"\n\n\n\nx[[2]]\n\n[1]  0.5096966  0.2328109  0.8612651 -0.2623318 -1.3491615\n\nclass(x[[2]])\n\n[1] \"numeric\"\n\n\nAccessing a column by name with square brackets, returns a single-column data.frame:\n\nx[\"Feat_2\"]\n\n      Feat_2\n1  0.5096966\n2  0.2328109\n3  0.8612651\n4 -0.2623318\n5 -1.3491615\n\nclass(x[\"Feat_2\"])\n\n[1] \"data.frame\"\n\n\nAccessing a column by [row, column] either by position or name, return a vector by default:\n\nx[, 2]\n\n[1]  0.5096966  0.2328109  0.8612651 -0.2623318 -1.3491615\n\nclass(x[, 2])\n\n[1] \"numeric\"\n\nx[, \"Feat_2\"]\n\n[1]  0.5096966  0.2328109  0.8612651 -0.2623318 -1.3491615\n\nclass(x[, \"Feat_2\"])\n\n[1] \"numeric\"\n\n\nAs we saw earlier, we can specify drop = FALSE to return a data.frame:\n\nclass(x[, 2, drop = FALSE])\n\n[1] \"data.frame\"\n\nclass(x[, \"Feat_2\", drop = FALSE])\n\n[1] \"data.frame\"\n\n\nAs in lists, all indexing and slicing operations, with the exception of the $ notation, work with a variable holding either a column name of or an integer location:\n\nidi <- 2\nidc <- \"Feat_2\"\nx[idi]\n\n      Feat_2\n1  0.5096966\n2  0.2328109\n3  0.8612651\n4 -0.2623318\n5 -1.3491615\n\nx[idc]\n\n      Feat_2\n1  0.5096966\n2  0.2328109\n3  0.8612651\n4 -0.2623318\n5 -1.3491615\n\nx[[idi]]\n\n[1]  0.5096966  0.2328109  0.8612651 -0.2623318 -1.3491615\n\nx[[idc]]\n\n[1]  0.5096966  0.2328109  0.8612651 -0.2623318 -1.3491615\n\nx[, idi]\n\n[1]  0.5096966  0.2328109  0.8612651 -0.2623318 -1.3491615\n\nx[, idc]\n\n[1]  0.5096966  0.2328109  0.8612651 -0.2623318 -1.3491615\n\nx[, idi, drop = F]\n\n      Feat_2\n1  0.5096966\n2  0.2328109\n3  0.8612651\n4 -0.2623318\n5 -1.3491615\n\nx[, idc, drop = F]\n\n      Feat_2\n1  0.5096966\n2  0.2328109\n3  0.8612651\n4 -0.2623318\n5 -1.3491615\n\n\nExtracting multiple columns returns a data frame:\n\nx[, 2:3]\n\n      Feat_2 Feat_3\n1  0.5096966 rnd_24\n2  0.2328109 rnd_89\n3  0.8612651  rnd_4\n4 -0.2623318 rnd_52\n5 -1.3491615 rnd_92\n\nclass(x[, 2:3])\n\n[1] \"data.frame\"\n\n\n\n\n8.4.2 Extract rows\nUnlike indexing a row of a matrix, indexing a row of a data.frame returns a single-row data.frame, since it contains multiple columns of potentially different types:\n\nx[1, ]\n\n  Feat_1    Feat_2 Feat_3\n1     21 0.5096966 rnd_24\n\nclass(x[1, ])\n\n[1] \"data.frame\"\n\n\nConvert into a list using c():\n\nc(x[1, ])\n\n$Feat_1\n[1] 21\n\n$Feat_2\n[1] 0.5096966\n\n$Feat_3\n[1] \"rnd_24\"\n\nclass(c(x[1, ]))\n\n[1] \"list\"\n\n\nConvert into a (named) vector using unlist():\n\nunlist(x[1, ])\n\n             Feat_1              Feat_2              Feat_3 \n               \"21\" \"0.509696614600682\"            \"rnd_24\" \n\nclass(unlist(x[1, ]))\n\n[1] \"character\"\n\n\n\n\n8.4.3 Logical index\n\nx[x$Feat_1 > 22, ]\n\n  Feat_1     Feat_2 Feat_3\n3     23  0.8612651  rnd_4\n4     24 -0.2623318 rnd_52\n5     25 -1.3491615 rnd_92"
  },
  {
    "objectID": "10_Indexing.html#logical---integer-indexing",
    "href": "10_Indexing.html#logical---integer-indexing",
    "title": "8  Indexing",
    "section": "8.5 Logical <-> Integer indexing",
    "text": "8.5 Logical <-> Integer indexing\nWe have seen that there are two types of indexes/indices: integer and logical.\n\n\n\n\n\nA logical index needs to be of the same dimensions as the object it is indexing (unless you really want to recycle values - see chapter on vectorization): you are specifying whether to include or exclude each element\n\n\n\n\nAn integer index will be shorter than the object it is indexing: you are specifying which subset of elements to include (or with a - in front, which elements to exclude)\n\n\n\n\n\nIt’s easy to convert between the two types.\nFor example, start with a sequence of integers:\n\nx <- 21:30\nx\n\n [1] 21 22 23 24 25 26 27 28 29 30\n\n\nLet’s create a logical index based on two inequalities:\n\nlogical_index <- x > 23 & x < 28\nlogical_index\n\n [1] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n\n\n\n8.5.1 Logical to integer index with which():\n\n\n\nThe common mistake is to attempt to convert a logical index to an integer index using as.integer(). This results in a vector of 1’s and 0’s, NOT an integer index. which() converts a logical index to an integer index.\n\n\n\nwhich() literally gives the position of all TRUE elements in a vector, thus converting a logical to an integer index:\n\ninteger_index <- which(logical_index)\ninteger_index\n\n[1] 4 5 6 7\n\n\ni.e. positions 4, 5, 6, 7 of the logical_index are TRUE\n\n\n\nA logical and an integer index are equivalent if they select the exact same elements\n\n\n\nLet’s check than when used to index x, they both return the same result:\n\nx[logical_index]\n\n[1] 24 25 26 27\n\nx[integer_index]\n\n[1] 24 25 26 27\n\nall(x[logical_index] == x[integer_index])\n\n[1] TRUE\n\n\n\n\n8.5.2 Integer to logical index\nOn the other hand, if we want to convert an integer index to a logical index, we can begin with a logical vector of the same length or dimension as the object we want to index with all FALSE values:\n\nlogical_index_too <- vector(length = length(x))\nlogical_index_too\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nAnd use the integer index to replace the corresponding elements to TRUE:\n\nlogical_index_too[integer_index] <- TRUE\nlogical_index_too\n\n [1] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n\n\nThis, of course, is the same as the logical index we started with.\n\nall(logical_index == logical_index_too)\n\n[1] TRUE"
  },
  {
    "objectID": "10_Indexing.html#exclude-cases-using-an-index",
    "href": "10_Indexing.html#exclude-cases-using-an-index",
    "title": "8  Indexing",
    "section": "8.6 Exclude cases using an index",
    "text": "8.6 Exclude cases using an index\nVery often, we want to use an index, whether logical or integer, to exclude cases instead of to select cases.\nTo do that with a logical integer, we simply use an exclamation point in front of the index to negate each element (convert each TRUE to FALSE and each FALSE to TRUE):\n\nlogical_index\n\n [1] FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n\n!logical_index\n\n [1]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE\n\n\n\nx[!logical_index]\n\n[1] 21 22 23 28 29 30\n\n\nTo exclude elements using an integer index, R allows you to use negative indexing:\n\nx[-integer_index]\n\n[1] 21 22 23 28 29 30\n\n\n\n\n\nTo get the complement of an index, you negate a logical index (!logical_index) or you subtract an integer index (-integer_index):"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PDSR",
    "section": "",
    "text": "Welcome to PDSR!\nThis book is aimed as an introductory- to intermediate-level R programming learning resource.\nIt is the online book for UCSF Biostat 213/4.\nEDG\nUCSF LCM,\nSan Francisco, CA, June 2022"
  },
  {
    "objectID": "15_InputOutput.html#r-datasets",
    "href": "15_InputOutput.html#r-datasets",
    "title": "10  Data Input/Output",
    "section": "10.1 R datasets",
    "text": "10.1 R datasets\n\n10.1.1 Datasets included with R (in package ‘datasets’)\nList built-in datasets with data() and no arguments:\n\ndata()\n\nThese built-in datasets are normally readily available in the R console (because the datasets package is automatically loaded)\nYou can check if this is the case using search()\n\nsearch()\n\n [1] \".GlobalEnv\"        \"tools:quarto\"      \"package:stats\"    \n [4] \"package:graphics\"  \"package:grDevices\" \"package:utils\"    \n [7] \"package:datasets\"  \"package:methods\"   \"Autoloads\"        \n[10] \"package:base\"     \n\n\n\n\n10.1.2 Datasets included with other packages\nList a dataset included with some R package:\n\ndata(package = \"glmnet\")\ndata(package = \"MASS\")\ndata(package = \"mlbench\")\n\nLoad a dataset from some R package:\n\ndata(Sonar, package = \"mlbench\")\n\nNote: quotes around “Sonar” in the data() command above are optional."
  },
  {
    "objectID": "15_InputOutput.html#system-commands",
    "href": "15_InputOutput.html#system-commands",
    "title": "10  Data Input/Output",
    "section": "10.2 System commands",
    "text": "10.2 System commands\nGet working directory with getwd()\n\ngetwd()\n\nYou can set a different working directory with setwd()\nList files in current directory:\n\ndir()\n\nYou can execute a command of you operating system (OS) -i.e. MacOS, Linux, Windows- from within R using the system() function:\n\nsystem(\"uname -a\")\n\nNote: See issue here"
  },
  {
    "objectID": "15_InputOutput.html#data-io",
    "href": "15_InputOutput.html#data-io",
    "title": "10  Data Input/Output",
    "section": "10.3 Data I/O",
    "text": "10.3 Data I/O\n\n\n\n\n\nCommon Data Input/Output commands in R\n\n\n\n\n\n10.3.1 Read local CSV\nread.table() is the core function that reads data from formatted text files in R, where cases correspond to lines and variables to columns. Its many arguments allow to read different formats.\nread.csv() is an alias for read.table() that defaults to commas as separators and dots for decimal points. (Run read.csv in the console to print its source read the documentation with ?read.table).\nSome important arguments for read.table() listed here with their default values for read.csv():\n\nsep = \",\": Character that separate entries. Default is a comma; use “ for tab-separated files (default setting in read.delim())\ndec = \".\": Character for the decimal point. Default is a dot; in some cases where a comma is used as the decimal point, the entry separator sep may be a semicolon (default setting in read.csv2())\nna.strings = \"NA\": Character vector of strings to be coded as “NA”\ncolClasses = NA: Either a character vector defining each column’s type (e.g. c(“character”, “numeric”, “numeric”) recycled as necessary or a named vector defining specific columns’ types (e.g. c(ICD9 = “character”, Sex = “factor”, SBP = “numeric”, DOB = “Date”)). Unspecified columns automatically determined. Note: Set a column to “NULL” (with quotes) to exclude column.\n\n\nmen <-  read.csv(\"../Data/pone.0204161.s001.csv\")\n\n\n\n10.3.2 Read data from the web\nread.csv() can directly read an online file. In the second example below, we also define that missing data is coded with ? using the na.strings argument:\n\nparkinsons <- read.csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data\")\n\nsleep <- read.csv(\"https://www.openml.org/data/get_csv/53273/sleep.arff\",\n                  na.strings = \"?\")\n\nThe above files are read from two very popular online data repositories. Confusingly, neither file ends in .csv, but they both work with read.csv(). Always look at the plain text file first to determine if it can work with read.table() /read.csv() and what settings to use.\n\n\n10.3.3 Read zipped data from the web\n\n10.3.3.1 using gzcon() and csv.read()\nread.table() /read.csv() also accepts a “connection” as input.\nHere we define a connection to a zipped file by nesting gzcon() and url():\n\ncon <- gzcon(url(\"https://github.com/EpistasisLab/pmlb/raw/master/datasets/breast_cancer_wisconsin/breast_cancer_wisconsin.tsv.gz\"),\n             text = TRUE)\n\nWe read the connection and specify the file is tab-separated, or call read.delim():\n\nbcw <- read.csv(con, header = TRUE, sep = \"\\t\")\n\n#same as\nbcw <- read.delim(con, header = TRUE)\n\n\n\n10.3.3.2 using data.table’s fread()\nYou can also use data.table’s fread(), which will directly handle zipped files:\n\nlibrary(data.table)\nbcw2 <- fread(\"https://github.com/EpistasisLab/penn-ml-benchmarks/raw/master/datasets/classification/breast-cancer-wisconsin/breast-cancer-wisconsin.tsv.gz\")\n\nIf you want to stick to using data frames, set the argument data.table to FALSE:\n\nbcw2 <- fread(\"https://github.com/EpistasisLab/penn-ml-benchmarks/raw/master/datasets/classification/breast-cancer-wisconsin/breast-cancer-wisconsin.tsv.gz\",\n              data.table = FALSE)\n\n\n\n\n10.3.4 Write to CSV\nUse the write.csv() function to write an R object (usually data frame or matrix) to a CSV file. Setting row.names = FALSE is usually a good idea. (Instead of storing data in rownames, it’s usually best to create a new column.)\n\nwrite.csv(iris, \"../Data/iris.csv\", row.names = FALSE)\n\nNote that in this case we did not need to save row names (which are just integers 1 to 150 and would add a useless extra column in the output)\n\n\n10.3.5 Read .xslx using openxlsx::read.xlsx()\nAs an example, we can read the csv we saved earlier into Excel and then save it as a .xlsx file.\n\niris.path <- normalizePath(\"../Data/iris.xlsx\")\niris2 <- openxlsx::read.xlsx(iris.path)\n\nNote: openxlsx::read.xlsx() does not work with a relative path like \"./Data/iris.xlsc\". Therefore we used the normalizePath() function to give us the full path of the file without having to type it out.\nCheck that the data is still identical:\n\nall(iris == iris2)\n\n\n\n10.3.6 Write an R object to RDS\nYou can write any R object directly to file so that you can recover it at any time, share it, etc. Remember that since a list can contain any number of objects of any type, you can save any collection of objects as an RDS file. For multiple objects, see also the save.image() command below.\n\nsaveRDS(iris, \"iris.rds\")\n\nTo load an object saved in an rds file, assign it to an object using readRDS():\n\niris_fromFile <- readRDS(\"iris.rds\")\nall(iris == iris_fromFile)\n\n\n\n10.3.7 Write multiple R objects to RData file using save()\n\nmat1 <- sapply(seq_len(10), function(i) rnorm(500))\nmat2 <- sapply(seq_len(10), function(i) rnorm(500))\nsave(mat1, mat2, file = \"./mat.RData\")\n\nNote: we will learn how to use sapply() later under “Loop functions”\nTo load the variables in the .RData file you saved, use the load() command:\n\nload(\"./Rmd/mat.RData\")\n\nNote that load() adds the objects to your workspace using with their original names. You do not assign them to a new object, unlike with the readRDS() call above.\n\n\n10.3.8 Write your entire workspace to a RData image using save.image()\nYou can save your entire workspace to a RData file using the save.image() function.\n\nsave.image(\"workspace_10_05_2020.RData\")\n\nSame as above, to re-load the workspace saved in the .RData file, use the load() command:\n\nload(\"workspace_10_05_2020.RData\")"
  },
  {
    "objectID": "44_Reshaping.html",
    "href": "44_Reshaping.html",
    "title": "18  Reshaping",
    "section": "",
    "text": "Wide and Long data format example. Take a moment to notice how the wide table on the left with 3 cases (3 IDs) and 3 variables gets converted from a 3 x 4 table to a 9 x 3 long table on the right. The values (outlined in magenta) are present once in each table: on the wide table they form an ID x Variable matrix, while on the long they are stacked on a single column. The IDs have to be repeated on the long table, once for each variable and there is a new ‘Variable’ column to provide the information present in the wide table’s column names.\nA wide dataset contains only a single row per case (e.g. patient), while a long dataset can contain multiple rows per case (e.g. for multiple timepoints). We want to be able to reshape from one form to the other because different programs (e.g. statistical models, visualization) may expect data in one of the other format for different applications (e.g. longitudinal modeling or grouped visualizations)."
  },
  {
    "objectID": "44_Reshaping.html#wide-to-long",
    "href": "44_Reshaping.html#wide-to-long",
    "title": "18  Reshaping",
    "section": "18.1 Wide to Long",
    "text": "18.1 Wide to Long\nLet’s create an example data frame:\n\ndat_wide <- data.frame(ID = c(1, 2, 3),\n                       mango = c(1.1, 2.1, 3.1),\n                       banana = c(1.2, 2.2, 3.2),\n                       tangerine = c(1.3, 2.3, 3.3),\n                       Group = c(\"a\", \"b\", \"b\"))\ndat_wide\n\n  ID mango banana tangerine Group\n1  1   1.1    1.2       1.3     a\n2  2   2.1    2.2       2.3     b\n3  3   3.1    3.2       3.3     b\n\n\n\n18.1.1 base\nThe reshape() function is probably one of the more complicated builtin functions because its documentation is not entirely clear, especially if you’re not used to the jargon and specifically with regards to which arguments refer to the input vs. output data frame. Use the following figure as a guide to understand reshape()’s syntax. You can use it as a reference when building your own reshape() command by following steps 1 through 5:\n\n\n\n\n\nreshape() syntax for Wide to Long transformation.\n\n\n\n\n\ndat_wide2long <- reshape(# Data in wide format\n                         data = dat_wide,\n                         # The column name that defines case ID\n                         idvar = \"ID\",\n                         # The columns whose values we want to keep\n                         varying = list(2:4),\n                         # The name of the new column which will contain all \n                         # the values from the columns above\n                         v.names = \"Score\",\n                         # The values/names, of length = (N columns in \"varying\"), \n                         #that will be recycled to indicate which column from the \n                         #wide dataset each row corresponds to\n                         times = c(colnames(dat_wide)[2:4]),\n                         # The name of the new column created to hold the values \n                         # defined by \"times\"\n                         timevar = \"Fruit\",                  \n                         direction = \"long\")\ndat_wide2long\n\n            ID Group     Fruit Score\n1.mango      1     a     mango   1.1\n2.mango      2     b     mango   2.1\n3.mango      3     b     mango   3.1\n1.banana     1     a    banana   1.2\n2.banana     2     b    banana   2.2\n3.banana     3     b    banana   3.2\n1.tangerine  1     a tangerine   1.3\n2.tangerine  2     b tangerine   2.3\n3.tangerine  3     b tangerine   3.3\n\n\nYou can also define varying with a character vector:\nvarying = list(c(\"mango\", \"banana\",\"tangerine\")\nExplore the resulting data frame’s attributes:\n\nattributes(dat_wide2long)\n\n$row.names\n[1] \"1.mango\"     \"2.mango\"     \"3.mango\"     \"1.banana\"    \"2.banana\"   \n[6] \"3.banana\"    \"1.tangerine\" \"2.tangerine\" \"3.tangerine\"\n\n$names\n[1] \"ID\"    \"Group\" \"Fruit\" \"Score\"\n\n$class\n[1] \"data.frame\"\n\n$reshapeLong\n$reshapeLong$varying\n$reshapeLong$varying[[1]]\n[1] \"mango\"     \"banana\"    \"tangerine\"\n\n\n$reshapeLong$v.names\n[1] \"Score\"\n\n$reshapeLong$idvar\n[1] \"ID\"\n\n$reshapeLong$timevar\n[1] \"Fruit\"\n\n\nThese attributes are present if and only if a long data.frame was created from a wide data.frame as above. In this case, reshaping back to wide format is as easy as calling reshape() on the previously converted data.frame with no arguments:\n\ndat_wideagain <- reshape(dat_wide2long)\ndat_wideagain\n\n        ID Group mango banana tangerine\n1.mango  1     a   1.1    1.2       1.3\n2.mango  2     b   2.1    2.2       2.3\n3.mango  3     b   3.1    3.2       3.3\n\n\nNote that the reverse does not work, you need to specify the wide to long reshaping normally.\n\n\n18.1.2 tidyr\n\ndat_wide2long_tv <- pivot_longer(dat_wide,\n                           cols = 2:4,\n                           names_to = \"Fruit\",\n                           values_to = \"Score\")\ndat_wide2long_tv\n\n# A tibble: 9 × 4\n     ID Group Fruit     Score\n  <dbl> <chr> <chr>     <dbl>\n1     1 a     mango       1.1\n2     1 a     banana      1.2\n3     1 a     tangerine   1.3\n4     2 b     mango       2.1\n5     2 b     banana      2.2\n6     2 b     tangerine   2.3\n7     3 b     mango       3.1\n8     3 b     banana      3.2\n9     3 b     tangerine   3.3\n\n\n\n\n18.1.3 data.table\n\ndat_wide_dt <- as.data.table(dat_wide)\ndat_wide2long_dt <- melt(dat_wide_dt,\n                         id.vars = c(1, 5),\n                         measure.vars = 2:4,\n                         variable.name = \"Fruit\",\n                         value.name = \"Score\")\nsetorder(dat_wide2long_dt, \"ID\")\ndat_wide2long_dt\n\n   ID Group     Fruit Score\n1:  1     a     mango   1.1\n2:  1     a    banana   1.2\n3:  1     a tangerine   1.3\n4:  2     b     mango   2.1\n5:  2     b    banana   2.2\n6:  2     b tangerine   2.3\n7:  3     b     mango   3.1\n8:  3     b    banana   3.2\n9:  3     b tangerine   3.3"
  },
  {
    "objectID": "44_Reshaping.html#long-to-wide",
    "href": "44_Reshaping.html#long-to-wide",
    "title": "18  Reshaping",
    "section": "18.2 Long to Wide",
    "text": "18.2 Long to Wide\nLet’s recreate the same long dataset:\n\ndat_long <- data.frame(ID = c(1, 2, 3, 1, 2, 3, 1, 2, 3),\n                       Fruit = c(\"mango\", \"mango\", \"mango\", \n                                 \"banana\", \"banana\", \"banana\", \n                                 \"tangerine\", \"tangerine\", \"tangerine\"),\n                       Score = c(1.1, 2.1, 3.1, 1.2, 2.2, 3.2, 1.3, 2.3, 3.3),\n                       Group = c(\"a\", \"b\", \"b\", \"a\", \"b\", \"b\", \"a\", \"b\", \"b\"))\ndat_long\n\n  ID     Fruit Score Group\n1  1     mango   1.1     a\n2  2     mango   2.1     b\n3  3     mango   3.1     b\n4  1    banana   1.2     a\n5  2    banana   2.2     b\n6  3    banana   3.2     b\n7  1 tangerine   1.3     a\n8  2 tangerine   2.3     b\n9  3 tangerine   3.3     b\n\n\n\n18.2.1 base\nUsing base reshape() for long-to-wide transformation is simpler than wide-to-long:\n\n\n\n\n\nreshape() syntax for Long to Wide transformation.\n\n\n\n\n\ndat_long2wide <- reshape(dat_long,\n                         idvar = \"ID\",\n                         timevar = \"Fruit\",\n                         v.names = \"Score\",\n                         direction = \"wide\")\n# Optionally rename columns\ncolnames(dat_long2wide) <- gsub(\"Score.\", \"\", colnames(dat_long2wide))\ndat_long2wide\n\n  ID Group mango banana tangerine\n1  1     a   1.1    1.2       1.3\n2  2     b   2.1    2.2       2.3\n3  3     b   3.1    3.2       3.3\n\n\n\n\n18.2.2 tidyr\n\ndat_long2wide_tv <- pivot_wider(dat_long,\n                                id_cols = c(\"ID\", \"Group\"),\n                                names_from = \"Fruit\",\n                                values_from = \"Score\")\ndat_long2wide_tv\n\n# A tibble: 3 × 5\n     ID Group mango banana tangerine\n  <dbl> <chr> <dbl>  <dbl>     <dbl>\n1     1 a       1.1    1.2       1.3\n2     2 b       2.1    2.2       2.3\n3     3 b       3.1    3.2       3.3\n\n\n\n\n18.2.3 data.table\ndata.table’s long to wide procedure is defined with a convenient formula notation:\n\ndat_long_dt <- as.data.table(dat_long)\ndat_long2wide_dt <- dcast(dat_long_dt,\n                          ID + Group ~ Fruit,\n                          value.var = \"Score\")\ndat_long2wide_dt\n\n   ID Group banana mango tangerine\n1:  1     a    1.2   1.1       1.3\n2:  2     b    2.2   2.1       2.3\n3:  3     b    3.2   3.1       3.3"
  },
  {
    "objectID": "42_TableJoins.html",
    "href": "42_TableJoins.html",
    "title": "17  Table joins",
    "section": "",
    "text": "We often have data from separate sources that we want to combine into a single data.frame. Table joins allow you to specify how to perform such a merge.\nScenario: You have received two tables with clinical data. Each table contains a column with a unique identifier (ID) plus a number of variables which are unique to each table. You want to merge them into one big table so that for each ID you have all available variables. You want to make sure that the same ID number (e.g. 108) corresponds to the same case in both datasets, but not all IDs needs to be present in both datasets.\nLet’s make up some synthetic data:\nThere are four main types of join operations:"
  },
  {
    "objectID": "42_TableJoins.html#merge",
    "href": "42_TableJoins.html#merge",
    "title": "17  Table joins",
    "section": "17.1 merge()",
    "text": "17.1 merge()\nThe merge() command in R is used to perform table joins.\nSyntax:\nmerge(x, y, by)\nwhere x and y and the two data.frames to join, and by is the column name of the ID variable used to identify rows. If the two datasets’ ID column has a different name, e.g. “PatientID” in one and “PID” in the other, you can either rename one of them, or probably best, use the following syntax:\nmerge(x, y, by.x, by.y)\nwhere by.x should be the name of the ID column for the x dataset and by.y should be the name of the ID column for the y dataset.\nIf you do not specify by or by.x and by.y arguments, merge() defaults to using the intersection of column names of the two input datasets. Look at the merge()’s documentation:\nby = intersect(names(x), names(y))\nIn our example datasets above, this works as expected and identifies “PID” as the common column:\n\nintersect(names(a), names(b))\n\n[1] \"PID\""
  },
  {
    "objectID": "42_TableJoins.html#inner-join",
    "href": "42_TableJoins.html#inner-join",
    "title": "17  Table joins",
    "section": "17.2 Inner join",
    "text": "17.2 Inner join\nThe default arguments of merge() perform an inner join:\n\n(ab.inner <- merge(a, b))\n\n  PID Hospital Age Sex  V1 Department\n1 106      HUP  21   0 153  Neurology\n2 107      HUP  63   1  89  Radiology\n3 108 Stanford  22   0 112  Emergency\n4 109     UCSF  19   0 228 Cardiology\n\n# same as\n(ab.inner <- merge(a, b, by = \"PID\"))\n\n  PID Hospital Age Sex  V1 Department\n1 106      HUP  21   0 153  Neurology\n2 107      HUP  63   1  89  Radiology\n3 108 Stanford  22   0 112  Emergency\n4 109     UCSF  19   0 228 Cardiology\n\n# same as\n(ab.inner <- merge(a, b, all = FALSE))\n\n  PID Hospital Age Sex  V1 Department\n1 106      HUP  21   0 153  Neurology\n2 107      HUP  63   1  89  Radiology\n3 108 Stanford  22   0 112  Emergency\n4 109     UCSF  19   0 228 Cardiology\n\n\nNote that the resulting table only contains cases found in both datasets, i.e. IDs 106 through 109"
  },
  {
    "objectID": "42_TableJoins.html#outer-join",
    "href": "42_TableJoins.html#outer-join",
    "title": "17  Table joins",
    "section": "17.3 Outer join",
    "text": "17.3 Outer join\nYou can perform an outer join by specifying all = TRUE:\n\n(ab.outer <- merge(a, b, all = TRUE))\n\n   PID Hospital Age Sex  V1 Department\n1  101     UCSF  22   1  NA       <NA>\n2  102      HUP  34   1  NA       <NA>\n3  103 Stanford  41   0  NA       <NA>\n4  104 Stanford  19   1  NA       <NA>\n5  105     UCSF  53   0  NA       <NA>\n6  106      HUP  21   0 153  Neurology\n7  107      HUP  63   1  89  Radiology\n8  108 Stanford  22   0 112  Emergency\n9  109     UCSF  19   0 228 Cardiology\n10 110     <NA>  NA  NA  91    Surgery\n11 111     <NA>  NA  NA 190  Neurology\n12 112     <NA>  NA  NA 101 Psychiatry\n\n(ab.outer <- merge(a, b, by = \"PID\", all = TRUE))\n\n   PID Hospital Age Sex  V1 Department\n1  101     UCSF  22   1  NA       <NA>\n2  102      HUP  34   1  NA       <NA>\n3  103 Stanford  41   0  NA       <NA>\n4  104 Stanford  19   1  NA       <NA>\n5  105     UCSF  53   0  NA       <NA>\n6  106      HUP  21   0 153  Neurology\n7  107      HUP  63   1  89  Radiology\n8  108 Stanford  22   0 112  Emergency\n9  109     UCSF  19   0 228 Cardiology\n10 110     <NA>  NA  NA  91    Surgery\n11 111     <NA>  NA  NA 190  Neurology\n12 112     <NA>  NA  NA 101 Psychiatry\n\n\nNote that the resulting data frame contains all cases found in either dataset and missing values are represented with NA."
  },
  {
    "objectID": "42_TableJoins.html#left-outer-join",
    "href": "42_TableJoins.html#left-outer-join",
    "title": "17  Table joins",
    "section": "17.4 Left outer join",
    "text": "17.4 Left outer join\nYou can perform a left outer join by specifying all.x = TRUE:\n\n(ab.leftOuter <- merge(a, b, all.x = TRUE))\n\n  PID Hospital Age Sex  V1 Department\n1 101     UCSF  22   1  NA       <NA>\n2 102      HUP  34   1  NA       <NA>\n3 103 Stanford  41   0  NA       <NA>\n4 104 Stanford  19   1  NA       <NA>\n5 105     UCSF  53   0  NA       <NA>\n6 106      HUP  21   0 153  Neurology\n7 107      HUP  63   1  89  Radiology\n8 108 Stanford  22   0 112  Emergency\n9 109     UCSF  19   0 228 Cardiology\n\n\nNote that the resulting data frame contains all cases present in the left input dataset (i.e. the one defined first in the arguments) only."
  },
  {
    "objectID": "42_TableJoins.html#right-outer-join",
    "href": "42_TableJoins.html#right-outer-join",
    "title": "17  Table joins",
    "section": "17.5 Right outer join",
    "text": "17.5 Right outer join\nYou can perform a right outer join by specifying all.y = TRUE:\n\n(ab.rightOuter <- merge(a, b, all.y = TRUE))\n\n  PID Hospital Age Sex  V1 Department\n1 106      HUP  21   0 153  Neurology\n2 107      HUP  63   1  89  Radiology\n3 108 Stanford  22   0 112  Emergency\n4 109     UCSF  19   0 228 Cardiology\n5 110     <NA>  NA  NA  91    Surgery\n6 111     <NA>  NA  NA 190  Neurology\n7 112     <NA>  NA  NA 101 Psychiatry\n\n\nNote how the resulting data frame contains all cases present in the right input dataset (i.e. the one defined seecond in the arguments) only."
  },
  {
    "objectID": "42_TableJoins.html#specifying-columns",
    "href": "42_TableJoins.html#specifying-columns",
    "title": "17  Table joins",
    "section": "17.6 Specifying columns",
    "text": "17.6 Specifying columns\nAs mentioned above, if the ID columns in the two data.frames to be merged do not have the same name, you can specify them directly:\n\na <- data.frame(PID = c(101:109),\n                Hospital = c(\"UCSF\", \"HUP\", \"Stanford\",\n                             \"Stanford\", \"UCSF\", \"HUP\", \n                             \"HUP\", \"Stanford\", \"UCSF\"),\n                Age = c(22, 34, 41, 19, 53, 21, 63, 22, 19),\n                Sex = c(1, 1, 0, 1, 0, 0, 1, 0, 0))\n\nb <- data.frame(PatientID = c(106:112),\n                 V1 = c(153, 89, 112, 228,  91, 190, 101),\n                 Department = c(\"Neurology\", \"Radiology\",\n                                \"Emergency\", \"Cardiology\",\n                                \"Surgery\", \"Neurology\", \"Psychiatry\"))\n\n\nmerge(a, b, by.x = \"PID\", by.y = \"PatientID\")\n\n  PID Hospital Age Sex  V1 Department\n1 106      HUP  21   0 153  Neurology\n2 107      HUP  63   1  89  Radiology\n3 108 Stanford  22   0 112  Emergency\n4 109     UCSF  19   0 228 Cardiology"
  },
  {
    "objectID": "08_DataStructures.html",
    "href": "08_DataStructures.html",
    "title": "6  Data Structures",
    "section": "",
    "text": "There are 5 main data structures in R:\nHomogeneous vs. hetereogeneous refers to the kind of data types (integer, double, character, logical, factor, etc.) that a structure can hold. This means a matrix can hold only numbers or only characters, but a data frame can hold different types in different columns. That is why data frames are very popular data structure for statistical work."
  },
  {
    "objectID": "08_DataStructures.html#initialize---coerce---test-structures",
    "href": "08_DataStructures.html#initialize---coerce---test-structures",
    "title": "6  Data Structures",
    "section": "6.1 Initialize - coerce - test (structures)",
    "text": "6.1 Initialize - coerce - test (structures)\nThe following summary table lists the functions to initialize, coerce (=convert), and test the core data structures, which are shown in more detail in the following paragraphs:\n\n\n\nInitialize\nCoerce\nTest\n\n\n\n\nvector(n)\nas.vector(x)\nis.vector(x)\n\n\nmatrix(n)\nas.matrix(x)\nis.matrix(x)\n\n\narray(n)\nas.array(x)\nis.array(x)\n\n\nlist(n)\nas.list(x)\nis.list(x)\n\n\ndata.frame(n)\nas.data.frame(x)\nis.data.frame(x)"
  },
  {
    "objectID": "08_DataStructures.html#vectors",
    "href": "08_DataStructures.html#vectors",
    "title": "6  Data Structures",
    "section": "6.2 Vectors",
    "text": "6.2 Vectors\nA vector is the basic structure that contains data in R. Other structures that contain data are made up of one or more vectors.\n\n(x <- c(1, 3, 5, 7))\n\n[1] 1 3 5 7\n\nclass(x)\n\n[1] \"numeric\"\n\ntypeof(x)\n\n[1] \"double\"\n\n\nA vector has length() but no dim():\n\nlength(x)\n\n[1] 4\n\ndim(x)\n\nNULL\n\n\n\n(x2 <- 1:10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n(x3 <- rnorm(10))\n\n [1] -2.1572940  1.4080765 -0.4920152  0.1903149  0.3528404 -1.3746255\n [7]  1.1590791  0.5890638  0.2475779 -1.2081195\n\n(x4 <- seq(0, 1, .1))\n\n [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n\nseq(10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n(x5 <- sample(seq(100), 20))\n\n [1] 24 10 26 60 49 57 35 78 11 82 87 81 47 67 13 12 97 39 48 61\n\n\n\n6.2.1 Generating sequences with seq()\n\nfrom, to, by\n\n\nseq(1, 10, .5)\n\n [1]  1.0  1.5  2.0  2.5  3.0  3.5  4.0  4.5  5.0  5.5  6.0  6.5  7.0  7.5  8.0\n[16]  8.5  9.0  9.5 10.0\n\n\n\n1:n\n\n\n(seq(12))\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12\n\n# or \n(seq_len(12))\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12\n\n# is same as\n1:12\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12\n\n\n\nAlong the length of another object\n\n\nseq_along(iris)\n\n[1] 1 2 3 4 5\n\n1:ncol(iris)\n\n[1] 1 2 3 4 5\n\n\n\nfrom, to with length n\n\n\nseq(-5, 12, length.out = 11)\n\n [1] -5.0 -3.3 -1.6  0.1  1.8  3.5  5.2  6.9  8.6 10.3 12.0\n\n\n\n\n6.2.2 Initializing a vector\n\nx <- vector(length = 10)\nx <- vector(\"numeric\", 10)\nx <- vector(\"list\", 10)"
  },
  {
    "objectID": "08_DataStructures.html#matrices",
    "href": "08_DataStructures.html#matrices",
    "title": "6  Data Structures",
    "section": "6.3 Matrices",
    "text": "6.3 Matrices\nA matrix is a vector with 2 dimensions.\nTo create a matrix, you pass a vector to the matrix() command and specify number of rows using nrow and/or number of columns using ncol:\n\nx <- matrix(sample(seq(1000), 30),\n            nrow = 10, ncol = 3)\nx\n\n      [,1] [,2] [,3]\n [1,]  857  830  199\n [2,]  361  278  121\n [3,]   45   43  856\n [4,]   62  846  735\n [5,]  696  966  927\n [6,]  400  312  309\n [7,]  404  548  537\n [8,]  855  854  677\n [9,]  903  150   14\n[10,]  392  999  740\n\nclass(x)\n\n[1] \"matrix\" \"array\" \n\n\n\n\n\nA matrix has length (length(x)) equal to the number of all (i, j) elements or nrow * ncol (if i is the row index and j is the column index) and dimensions (dim(x)) as expected:\n\n\n\n\nlength(x)\n\n[1] 30\n\ndim(x)\n\n[1] 10  3\n\nnrow(x)\n\n[1] 10\n\nncol(x)\n\n[1] 3\n\n\n\n6.3.1 Construct by row or by column\nBy default, vectors are constructed by column (byrow = FALSE)\n\nx <- matrix(1:20, nrow = 10, ncol = 2, byrow = FALSE)\nx\n\n      [,1] [,2]\n [1,]    1   11\n [2,]    2   12\n [3,]    3   13\n [4,]    4   14\n [5,]    5   15\n [6,]    6   16\n [7,]    7   17\n [8,]    8   18\n [9,]    9   19\n[10,]   10   20\n\n\n\nx <- matrix(1:20, nrow = 10, ncol = 2, byrow = TRUE)\nx\n\n      [,1] [,2]\n [1,]    1    2\n [2,]    3    4\n [3,]    5    6\n [4,]    7    8\n [5,]    9   10\n [6,]   11   12\n [7,]   13   14\n [8,]   15   16\n [9,]   17   18\n[10,]   19   20\n\n\n\n\n6.3.2 Initialize a matrix\n\n(x <- matrix(NA, nrow = 6, ncol = 4))\n\n     [,1] [,2] [,3] [,4]\n[1,]   NA   NA   NA   NA\n[2,]   NA   NA   NA   NA\n[3,]   NA   NA   NA   NA\n[4,]   NA   NA   NA   NA\n[5,]   NA   NA   NA   NA\n[6,]   NA   NA   NA   NA\n\n(x <- matrix(0, nrow = 6, ncol = 4))\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    0    0    0\n[2,]    0    0    0    0\n[3,]    0    0    0    0\n[4,]    0    0    0    0\n[5,]    0    0    0    0\n[6,]    0    0    0    0\n\n\n\n\n6.3.3 Bind vectors by column or by row\nUse cbind (“column-bind”) to convert a set of input vectors to columns of a matrix. The vectors must be of the same length:\n\nx <- cbind(1:10, 11:20, 41:50)\nx\n\n      [,1] [,2] [,3]\n [1,]    1   11   41\n [2,]    2   12   42\n [3,]    3   13   43\n [4,]    4   14   44\n [5,]    5   15   45\n [6,]    6   16   46\n [7,]    7   17   47\n [8,]    8   18   48\n [9,]    9   19   49\n[10,]   10   20   50\n\nclass(x)\n\n[1] \"matrix\" \"array\" \n\n\nSimilarly, you can use rbind (“row-bind”) to convert a set of input vectors to rows of a matrix. The vectors again must be of the same length:\n\nx <- rbind(1:10, 11:20, 41:50)\nx\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    1    2    3    4    5    6    7    8    9    10\n[2,]   11   12   13   14   15   16   17   18   19    20\n[3,]   41   42   43   44   45   46   47   48   49    50\n\nclass(x)\n\n[1] \"matrix\" \"array\" \n\n\n\n\n6.3.4 Combine matrices\ncbind() and rbind() can be used to combine two or more matrices together - or vector and matrices:\n\ncbind(matrix(1, 5, 2), matrix(2, 5, 4))\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    1    1    2    2    2    2\n[2,]    1    1    2    2    2    2\n[3,]    1    1    2    2    2    2\n[4,]    1    1    2    2    2    2\n[5,]    1    1    2    2    2    2"
  },
  {
    "objectID": "08_DataStructures.html#arrays",
    "href": "08_DataStructures.html#arrays",
    "title": "6  Data Structures",
    "section": "6.4 Arrays",
    "text": "6.4 Arrays\nArrays are vectors with dimensions.\nYou can have 1D, 2D or any-D, i.e. ND arrays.\n\n6.4.1 1D array\nA 1D array is just like a vector but of class array and with dim(x) equal to length(x) (remember, vectors have only length(x) and undefined dim(x)):\n\nx <- 1:10\nxa <- array(1:10, dim = 10)\nclass(x)\n\n[1] \"integer\"\n\nis.vector(x)\n\n[1] TRUE\n\nlength(x)\n\n[1] 10\n\ndim(x)\n\nNULL\n\nclass(xa)\n\n[1] \"array\"\n\nis.vector(xa)\n\n[1] FALSE\n\nlength(xa)\n\n[1] 10\n\ndim(xa)\n\n[1] 10\n\n\nIt is quite unlikely you will need to use a 1D array instead of a vector.\n\n\n6.4.2 2D array\nA 2D array is a matrix:\n\nx <- array(1:40, dim = c(10, 4))\nclass(x)\n\n[1] \"matrix\" \"array\" \n\ndim(x)\n\n[1] 10  4\n\n\n\n\n6.4.3 ND array\nYou can build an N-dimensional array:\n\n(x <- array(1:60, dim = c(5, 4, 3)))\n\n, , 1\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    6   11   16\n[2,]    2    7   12   17\n[3,]    3    8   13   18\n[4,]    4    9   14   19\n[5,]    5   10   15   20\n\n, , 2\n\n     [,1] [,2] [,3] [,4]\n[1,]   21   26   31   36\n[2,]   22   27   32   37\n[3,]   23   28   33   38\n[4,]   24   29   34   39\n[5,]   25   30   35   40\n\n, , 3\n\n     [,1] [,2] [,3] [,4]\n[1,]   41   46   51   56\n[2,]   42   47   52   57\n[3,]   43   48   53   58\n[4,]   44   49   54   59\n[5,]   45   50   55   60\n\nclass(x)\n\n[1] \"array\"\n\n\nYou can provide names for each dimensions using the dimnames argument. It accepts a list where each elements is a character vector of legth equal to the dimension length. Using the same example as above, we pass three character vector of length 5, 4, and 3 to match the length of the dimensions:\n\nx <- array(1:60,\n            dim = c(5, 4, 3),\n            dimnames = list(letters[1:5],\n                            c(\"alpha\", \"beta\", \"gamma\", \"delta\"),\n                            c(\"x\", \"y\", \"z\")))\n\n3D arrays can be used to represent color images. Here, just for fun, we use rasterImage to show how you would visualize such an image:\n\nx <- array(sample(1:255, 432, TRUE), dim = c(12, 12, 3))\npar(\"pty\")\n\n[1] \"m\"\n\npar(pty = \"s\")\nplot(NULL, NULL,\n     xlim = c(0, 100), ylim = c(0, 100),\n     axes = F, ann = F, pty = \"s\")\nrasterImage(x/255, 0, 0, 100, 100)"
  },
  {
    "objectID": "08_DataStructures.html#lists",
    "href": "08_DataStructures.html#lists",
    "title": "6  Data Structures",
    "section": "6.5 Lists",
    "text": "6.5 Lists\nTo define a list, we use list() to pass any number of objects.\nIf these objects are passed as named arguments, the names will rename as element names:\n\nx <- list(one = 1:4,\n          two = sample(seq(0, 100, .1), 10),\n          three = c(\"mango\", \"banana\", \"tangerine\"),\n          four = median)\nclass(x)\n\n[1] \"list\"\n\nstr(x)\n\nList of 4\n $ one  : int [1:4] 1 2 3 4\n $ two  : num [1:10] 46 73.2 94.3 42.4 78.6 50.5 73.1 12 86.2 13.5\n $ three: chr [1:3] \"mango\" \"banana\" \"tangerine\"\n $ four :function (x, na.rm = FALSE, ...)  \n\n\n\n6.5.1 Nested lists\nSince each element can be any object at all, it is simple to build a nested list:\n\nx <- list(alpha = letters[sample(26, 4)],\n          beta = sample(12),\n          gamma = list(i = rnorm(10),\n                       j = runif(10),\n                       j = seq(0, 1000, length.out = 10)))\nx\n\n$alpha\n[1] \"p\" \"m\" \"o\" \"g\"\n\n$beta\n [1]  4 11  6  1 10  8  7  9 12  5  2  3\n\n$gamma\n$gamma$i\n [1] -0.92331776  0.71985256 -0.07201912  1.00296921 -0.29721180  0.87690497\n [7]  0.57970859 -1.03098624  1.46187517  1.17093238\n\n$gamma$j\n [1] 0.48195793 0.02273958 0.29249130 0.88741885 0.08562779 0.07454521\n [7] 0.42302021 0.99943692 0.02163897 0.61607215\n\n$gamma$j\n [1]    0.0000  111.1111  222.2222  333.3333  444.4444  555.5556  666.6667\n [8]  777.7778  888.8889 1000.0000\n\n\n\n\n6.5.2 Initialize a list\n\nx <- vector(\"list\", 4)\nx\n\n[[1]]\nNULL\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\n[[4]]\nNULL\n\n\n\n\n6.5.3 Combine lists\nYou can combine lists with c() (just like vectors):\n\nl1 <- list(q = 11:14, r = letters[11:14])\nl2 <- list(s = LETTERS[21:24], t = 100:97)\n(x <- c(l1, l2))\n\n$q\n[1] 11 12 13 14\n\n$r\n[1] \"k\" \"l\" \"m\" \"n\"\n\n$s\n[1] \"U\" \"V\" \"W\" \"X\"\n\n$t\n[1] 100  99  98  97\n\nlength(x)\n\n[1] 4\n\n\n\n\n6.5.4 Mixing types with c()\nIt’s best to use c() to either combine elements of the same type into a vector, or to combine lists. Otherwise you must inspect the outcome to be certain it was as intended.\nAs we’ve seen, if all arguments passed to c() are of a single type, you get a vector of that type:\n\n(x <- c(12.9, 94.67, 23.74, 46.901))\n\n[1] 12.900 94.670 23.740 46.901\n\nclass(x)\n\n[1] \"numeric\"\n\n\nIf arguments passed to c() are a mix of numeric and character, they all get coerced to character.\n\n(x <- c(23.54, \"mango\", \"banana\", 75))\n\n[1] \"23.54\"  \"mango\"  \"banana\" \"75\"    \n\nclass(x)\n\n[1] \"character\"\n\n\nIf you pass more types of objects (which cannot be coerced to character) you get a list, since it is the only structure that can support all of them together:\n\n(x <- c(42, mean, \"potatoes\"))\n\n[[1]]\n[1] 42\n\n[[2]]\nfunction (x, ...) \nUseMethod(\"mean\")\n<bytecode: 0x148ca9480>\n<environment: namespace:base>\n\n[[3]]\n[1] \"potatoes\"\n\nclass(x)\n\n[1] \"list\"\n\n\n\n\n\nOther than concatenating vectors of the same type or lists into a larger list, it probably best to avoid using c() and directly constructing the object you want using, e.g. list()."
  },
  {
    "objectID": "08_DataStructures.html#dataframestruc",
    "href": "08_DataStructures.html#dataframestruc",
    "title": "6  Data Structures",
    "section": "6.6 Data frames",
    "text": "6.6 Data frames\n\n\n\nA data frames is a special type of list where each element has the same length and forms a column, resulting in a 2D structure. Unlike matrices, each column can contain a different data type.\n\n\n\n\nx <- data.frame(Feat_1 = 1:5,\n                Feat_2 = rnorm(5),\n                Feat_3 = paste0(\"rnd_\", sample(seq(100), 5)))\nx\n\n  Feat_1    Feat_2 Feat_3\n1      1 0.5909129 rnd_11\n2      2 1.1436751 rnd_65\n3      3 0.4820415 rnd_92\n4      4 1.4752170 rnd_49\n5      5 0.7837454 rnd_47\n\nclass(x)\n\n[1] \"data.frame\"\n\nstr(x)\n\n'data.frame':   5 obs. of  3 variables:\n $ Feat_1: int  1 2 3 4 5\n $ Feat_2: num  0.591 1.144 0.482 1.475 0.784\n $ Feat_3: chr  \"rnd_11\" \"rnd_65\" \"rnd_92\" \"rnd_49\" ...\n\nclass(x$Feat_1)\n\n[1] \"integer\"\n\n\n\nmat <- matrix(1:100, 10)\nlength(mat)\n\n[1] 100\n\ndf <- as.data.frame(mat)\nlength(df)\n\n[1] 10"
  },
  {
    "objectID": "08_DataStructures.html#attributes",
    "href": "08_DataStructures.html#attributes",
    "title": "6  Data Structures",
    "section": "6.7 Attributes",
    "text": "6.7 Attributes\nR objects may have some builtin attributes but you can add arbitrary attributes to any R object. These are used to store additional information, sometimes called metadata.\n\n6.7.1 Print all attributes\nTo print an object’s attributes, use attributes:\n\nattributes(iris)\n\n$names\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"     \n\n$class\n[1] \"data.frame\"\n\n$row.names\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n[109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n[127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n[145] 145 146 147 148 149 150\n\n\nThis returns a named list. In this case we got names, class, and row.names of the iris data frame.\n\n\n6.7.2 Get or set specific attributes\nYou can assign new attributes using attr:\n\n(x <- c(1:10))\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nattr(x, \"name\") <- \"Very special vector\"\n\nPrinting the vector after adding a new attribute, prints the attribute name and value underneath the vector itself:\n\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\nattr(,\"name\")\n[1] \"Very special vector\"\n\n\nOur trusty str function will print attributes as well\n\nstr(x)\n\n int [1:10] 1 2 3 4 5 6 7 8 9 10\n - attr(*, \"name\")= chr \"Very special vector\"\n\n\n\n6.7.2.1 A matrix is a vector - a closer look\nLet’s see how a matrix is literally just a vector with assigned dimensions.\nStart with a vector of length 20:\n\nx <- 1:20\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\n\n\nThe vector has no attributes - yet:\n\nattributes(x)\n\nNULL\n\n\nTo convert to a matrix, we would normally pass our vector to the matrix() function and define number of rows and/or columns:\n\nxm <- matrix(x, 5)\nxm\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    6   11   16\n[2,]    2    7   12   17\n[3,]    3    8   13   18\n[4,]    4    9   14   19\n[5,]    5   10   15   20\n\nattributes(xm)\n\n$dim\n[1] 5 4\n\n\nJust for demonstration, let’s instead directly add a dimension attribute to our vector:\n\nattr(x, \"dim\") <- c(5, 4)\nx\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    6   11   16\n[2,]    2    7   12   17\n[3,]    3    8   13   18\n[4,]    4    9   14   19\n[5,]    5   10   15   20\n\nclass(x)\n\n[1] \"matrix\" \"array\" \n\n\nJust like that, we have a matrix.\n\n\n\n6.7.3 Common builtin attributes\nVectors can have named elements. A new vector has no names, but you can add them:\n\nx <- rnorm(10)\nnames(x)\n\nNULL\n\nnames(x) <- paste0(\"Value\", seq(x))\nx\n\n    Value1     Value2     Value3     Value4     Value5     Value6     Value7 \n 0.2207668 -1.5308554  0.9757982  0.9845394 -0.8417019 -0.1565539 -1.0194172 \n    Value8     Value9    Value10 \n 0.7681391 -1.6175819  0.9917961 \n\n\nMatrices and data frames can have column names (colnames) and row names (rownames):\n\nx <- matrix(1:15, 5)\ncolnames(x)\n\nNULL\n\nrownames(x)\n\nNULL\n\ncolnames(x) <- paste0(\"Feature\", seq(3))\nrownames(x) <- paste0(\"Case\", seq(5))\nx\n\n      Feature1 Feature2 Feature3\nCase1        1        6       11\nCase2        2        7       12\nCase3        3        8       13\nCase4        4        9       14\nCase5        5       10       15\n\n\nLists are vectors so they have names. These can be defined when a list is created using the name-value pairs or added/changed at any time.\n\nx <- list(HospitalName = \"CaliforniaGeneral\",\n          ParticipatingDepartments = c(\"Neurology\", \"Psychiatry\", \"Neurosurgery\"),\n          PatientIDs = 1001:1253)\nnames(x)\n\n[1] \"HospitalName\"             \"ParticipatingDepartments\"\n[3] \"PatientIDs\"              \n\n\nAdd/Change names:\n\nnames(x) <- c(\"Hospital\", \"Departments\", \"PIDs\")\nx\n\n$Hospital\n[1] \"CaliforniaGeneral\"\n\n$Departments\n[1] \"Neurology\"    \"Psychiatry\"   \"Neurosurgery\"\n\n$PIDs\n  [1] 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015\n [16] 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030\n [31] 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045\n [46] 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060\n [61] 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075\n [76] 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090\n [91] 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105\n[106] 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120\n[121] 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135\n[136] 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150\n[151] 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165\n[166] 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180\n[181] 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195\n[196] 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210\n[211] 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225\n[226] 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240\n[241] 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253\n\n\nRemember that data a frame is a special type of list. Therefore in data frames colnames and names are equivalent:\n\ncolnames(iris)\n\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"     \n\nnames(iris)\n\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"     \n\n\nNote: As we saw, matrices have colnames and rownames. Using names on a matrix will assign names to individual elements, as if it was a long vector - this is not usually useful."
  },
  {
    "objectID": "04_Packages.html#r",
    "href": "04_Packages.html#r",
    "title": "3  R, IDEs, Packages, Docs",
    "section": "3.1 R",
    "text": "3.1 R\nThis book was compiled using R version 4.2.1 (2022-06-23).\nMake sure you have the latest version by visiting the R project website\nIt’s a good idea to keep a log of the version of R and installed packages when beginning a new project. An easy way to do this is to save the output of sessionInfo():\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Monterey 12.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.5.4 compiler_4.2.1    magrittr_2.0.3    fastmap_1.1.0    \n [5] cli_3.3.0         tools_4.2.1       htmltools_0.5.2   stringi_1.7.6    \n [9] rmarkdown_2.14    knitr_1.39        stringr_1.4.0     xfun_0.31        \n[13] digest_0.6.29     jsonlite_1.8.0    rlang_1.0.2       evaluate_0.15"
  },
  {
    "objectID": "04_Packages.html#ides",
    "href": "04_Packages.html#ides",
    "title": "3  R, IDEs, Packages, Docs",
    "section": "3.2 IDEs",
    "text": "3.2 IDEs\nAn Integrated Development Environment (IDE) is a software application that offers extensive functionality for programmers, including ability to read, write, and execute code, develop and test software packages, etc.\nIDEs that support R usually also allow viewing plots or launching web applications within the same environment. An IDE can make working in R easier, more productive, and, importantly, more fun.\n\n3.2.1 RStudio\nRStudio is a very popular Integrated Development Environment (IDE) for R. This is the recommended environment for beginners. Make sure to keep your installation up-to-date; new features are added often.\nIt is recommended to set up a new RStudio project for each data project:\nRStudio projects allows you to organize your work. Each project keeps track of your workspace, open source files, working directory, and history.\nTo create a new RStudio Project click on File > New Project… from the main menu or the “Create a project” icon (second from top-left usually) in the RStudio toolbar.\n\n\n3.2.2 VS Code\nVisual Studio Code, a.k.a. VS Code is a source code editor and one of the most popular IDEs across different languages. The VS Code marketplace includes a very large number of extensions.\nThe vscode-R extension allows using VS Code as an R IDE. To use it, install the languageserver package:\ninstall.packages(\"languageserver\")\nThe httpgd graphics device is recommended. Install it using:\nremotes::install_github(\"nx10/httpgd\")\nand enable it in the extension settings (“Plot: Use httpgd”)\nThe Remote - SSH extension allows using a local VS Code installation (e.g. on your laptop) and executing code (R, Python, etc.) on a remote server on which you have SSH access."
  },
  {
    "objectID": "04_Packages.html#r-packages",
    "href": "04_Packages.html#r-packages",
    "title": "3  R, IDEs, Packages, Docs",
    "section": "3.3 R packages",
    "text": "3.3 R packages\n\n3.3.1 CRAN\nThe Comprehensive R Archive Network (CRAN) is the official R package repository and currently hosts 16271 packages (as of 2020-09-13). To install a package from CRAN, use the builtin install.packages command:\n\ninstall.packages('glmnet')\n\n\n3.3.1.1 Check for outdated packages\n\nold.packages()\n\n\n\n3.3.1.2 Update installed packages\nIf you don’t set ask = FALSE, you will have to accept each package update separately.\n\nupdate.packages(ask = FALSE)\n\n\n\n\n3.3.2 GitHub\nGitHub contains a large number of R packages, some of which also exist in CRAN, but the GitHub version may be updated a lot more frequently. To install from GitHub, you need to have the remotes package from CRAN first:\n\ninstall.packages(\"remotes\")\n\n\nremotes::install_github(\"user/repo\")\n\nNote: Running remotes::install_github(\"user/repo\") will not reinstall a previously installed package, unless it has been updated.\n\n\n3.3.3 Bioconductor\nBioconductor is a repository which includes tools for the analysis and comprehension of high-throughput genomic data, among others. To install package from Bioconductor, first install the BiocManager package from CRAN:\n\ninstall.packages(\"BiocManager\")\n\nand then use that similar to the builtin install.packages:\n\nBiocManager::install(\"packageName\")\n\n\n\n3.3.4 Installed packages\nList all R packages installed on your system with installed.packages() (the following block has not been run to prevent a very long output)\n\ninstalled.packages()\n\nList attached packages with search():\n\nsearch()\n\n [1] \".GlobalEnv\"        \"tools:quarto\"      \"package:stats\"    \n [4] \"package:graphics\"  \"package:grDevices\" \"package:utils\"    \n [7] \"package:datasets\"  \"package:methods\"   \"Autoloads\"        \n[10] \"package:base\"     \n\n\nList attached packages with their system path:\n\nsearchpaths()\n\n [1] \".GlobalEnv\"                                                                    \n [2] \"tools:quarto\"                                                                  \n [3] \"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/stats\"    \n [4] \"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/graphics\" \n [5] \"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/grDevices\"\n [6] \"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/utils\"    \n [7] \"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/datasets\" \n [8] \"/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library/methods\"  \n [9] \"Autoloads\"                                                                     \n[10] \"/Library/Frameworks/R.framework/Resources/library/base\"                        \n\n\n\n\n3.3.5 Dependencies\nMost R packages, whether in CRAN, Bioconductor, or GitHub, themselves rely on other packages to run. These are called dependencies. Many of these dependencies get installed automatically when you call install.packages() or remotes::install_github(), etc. This depends largely on whether they are essential for the new package to work. Some packages, especially if they provide a large number of functions that may not all be used by all users, may make some dependencies optional. In that cases, if you try to execute a specific function that depends on uninstalled packages you may get a warning or error or some type of message indicating that you need to install further packages."
  },
  {
    "objectID": "04_Packages.html#builtin-documentation",
    "href": "04_Packages.html#builtin-documentation",
    "title": "3  R, IDEs, Packages, Docs",
    "section": "3.4 Builtin Documentation",
    "text": "3.4 Builtin Documentation\nAfter you’ve successfully installed R and RStudio, one of the first things to know is how to access and search the builtin documentation.\n\n3.4.1 Get help on a specific item\nIf you know the name of what you’re looking for (an R function most commonly, but possibly also the name of a dataset, or a package itself), just type ? followed by the name of said function, dataset, etc. in the R prompt:\n\n?sample\n\nIn RStudio, the above example will bring up the documentation for the sample function in the dedicated “Help” window, commonly situated at the bottom right (but can be moved by the user freely). If you are running R directly at the system shell, the same information is printed directly at the console.\nTry running the above example on your system.\n\n\n3.4.2 Search the docs\nIf you do not know the name of what you are looking for, you can use double question marks, ??, followed by your query (this is short for the help.search command that provides a number of arguments you can look up using ?help.search):\n\n??bootstrap"
  },
  {
    "objectID": "26_Aggregate.html",
    "href": "26_Aggregate.html",
    "title": "13  Aggregate",
    "section": "",
    "text": "aggregate() is a powerful way to apply functions on splits of your data. It can replicate functionality of the *apply() family, but can be more flexible. This may come with a performance penalty, only noticeable with big data, in which case it is recommended to use data.table for fast group-by data summarization.\naggregate() can work either with a formula notation or directly on data.frames and vectors. We show how to perform each operation below with either approach. The formula interface might be easier to work with interactivly on the console. While you can code with the formula interface, the regular approach is a lot more straightforward to do so.\nFor this example, we get the penguin data:\nSee example below for 1 or multiple variables by 1 or more groups using either the formula interface, or working directly on objects with $ indexing or using with():"
  },
  {
    "objectID": "26_Aggregate.html#single-variable-by-single-group",
    "href": "26_Aggregate.html#single-variable-by-single-group",
    "title": "13  Aggregate",
    "section": "13.1 Single variable by single group",
    "text": "13.1 Single variable by single group\nNote that the formula method defaults to na.action = na.omit\nUsing the formula interface:\n\naggregate(bill_length_mm ~ species,\n          penguins, mean)\n\n    species bill_length_mm\n1    Adelie       38.79139\n2 Chinstrap       48.83382\n3    Gentoo       47.50488\n\n\nDirectly working with vectors:\n\naggregate(penguins$bill_length_mm,\n          by = list(penguins$species),\n          mean, na.rm = T)\n\n    Group.1        x\n1    Adelie 38.79139\n2 Chinstrap 48.83382\n3    Gentoo 47.50488\n\n\nUsing with():\n\nwith(penguins,\n     aggregate(bill_length_mm,\n               by = list(species),\n               mean, na.rm = TRUE))\n\n    Group.1        x\n1    Adelie 38.79139\n2 Chinstrap 48.83382\n3    Gentoo 47.50488"
  },
  {
    "objectID": "26_Aggregate.html#multiple-variables-by-single-group",
    "href": "26_Aggregate.html#multiple-variables-by-single-group",
    "title": "13  Aggregate",
    "section": "13.2 Multiple variables by single group",
    "text": "13.2 Multiple variables by single group\n\naggregate(cbind(bill_length_mm, flipper_length_mm) ~ species,\n          penguins, mean)\n\n    species bill_length_mm flipper_length_mm\n1    Adelie       38.79139          189.9536\n2 Chinstrap       48.83382          195.8235\n3    Gentoo       47.50488          217.1870\n\n\n\naggregate(penguins[, c(\"bill_length_mm\", \"flipper_length_mm\")],\n          by = list(penguins$species),\n          mean, na.rm = TRUE)\n\n    Group.1 bill_length_mm flipper_length_mm\n1    Adelie       38.79139          189.9536\n2 Chinstrap       48.83382          195.8235\n3    Gentoo       47.50488          217.1870\n\n\n\nwith(penguins,\n     aggregate(cbind(bill_length_mm, flipper_length_mm),\n               by = list(species),\n               mean, na.rm = TRUE))\n\n    Group.1 bill_length_mm flipper_length_mm\n1    Adelie       38.79139          189.9536\n2 Chinstrap       48.83382          195.8235\n3    Gentoo       47.50488          217.1870"
  },
  {
    "objectID": "26_Aggregate.html#single-variable-by-multiple-groups",
    "href": "26_Aggregate.html#single-variable-by-multiple-groups",
    "title": "13  Aggregate",
    "section": "13.3 Single variable by multiple groups",
    "text": "13.3 Single variable by multiple groups\n\naggregate(bill_length_mm ~ species + island, penguins, mean)\n\n    species    island bill_length_mm\n1    Adelie    Biscoe       38.97500\n2    Gentoo    Biscoe       47.50488\n3    Adelie     Dream       38.50179\n4 Chinstrap     Dream       48.83382\n5    Adelie Torgersen       38.95098\n\n\n\naggregate(penguins$bill_length_mm,\n          by = list(penguins$species, penguins$island),\n          mean, na.rm = TRUE)\n\n    Group.1   Group.2        x\n1    Adelie    Biscoe 38.97500\n2    Gentoo    Biscoe 47.50488\n3    Adelie     Dream 38.50179\n4 Chinstrap     Dream 48.83382\n5    Adelie Torgersen 38.95098\n\n\n\nwith(penguins,\n     aggregate(bill_length_mm,\n               by = list(species, island),\n               mean, na.rm = TRUE))\n\n    Group.1   Group.2        x\n1    Adelie    Biscoe 38.97500\n2    Gentoo    Biscoe 47.50488\n3    Adelie     Dream 38.50179\n4 Chinstrap     Dream 48.83382\n5    Adelie Torgersen 38.95098"
  },
  {
    "objectID": "26_Aggregate.html#multiple-variables-by-multiple-groups",
    "href": "26_Aggregate.html#multiple-variables-by-multiple-groups",
    "title": "13  Aggregate",
    "section": "13.4 Multiple variables by multiple groups",
    "text": "13.4 Multiple variables by multiple groups\n\naggregate(cbind(bill_length_mm, flipper_length_mm) ~ species + island,\n          penguins, mean)\n\n    species    island bill_length_mm flipper_length_mm\n1    Adelie    Biscoe       38.97500          188.7955\n2    Gentoo    Biscoe       47.50488          217.1870\n3    Adelie     Dream       38.50179          189.7321\n4 Chinstrap     Dream       48.83382          195.8235\n5    Adelie Torgersen       38.95098          191.1961\n\n\n\naggregate(penguins[, c(\"bill_length_mm\", \"flipper_length_mm\")],\n          by = list(penguins$species, penguins$island),\n          mean, na.rm = TRUE)\n\n    Group.1   Group.2 bill_length_mm flipper_length_mm\n1    Adelie    Biscoe       38.97500          188.7955\n2    Gentoo    Biscoe       47.50488          217.1870\n3    Adelie     Dream       38.50179          189.7321\n4 Chinstrap     Dream       48.83382          195.8235\n5    Adelie Torgersen       38.95098          191.1961\n\n\n\nwith(penguins,\n     aggregate(cbind(bill_length_mm, flipper_length_mm),\n               by = list(species, island),\n               mean, na.rm = TRUE))\n\n    Group.1   Group.2 bill_length_mm flipper_length_mm\n1    Adelie    Biscoe       38.97500          188.7955\n2    Gentoo    Biscoe       47.50488          217.1870\n3    Adelie     Dream       38.50179          189.7321\n4 Chinstrap     Dream       48.83382          195.8235\n5    Adelie Torgersen       38.95098          191.1961"
  },
  {
    "objectID": "26_Aggregate.html#see-also",
    "href": "26_Aggregate.html#see-also",
    "title": "13  Aggregate",
    "section": "13.5 See also",
    "text": "13.5 See also\ntapply() for an alternative methods of applying function on subsets of a single variable (probably faster)"
  },
  {
    "objectID": "24_Summarize.html",
    "href": "24_Summarize.html",
    "title": "12  Summarizing Data",
    "section": "",
    "text": "Let’s read in a dataset from OpenML:"
  },
  {
    "objectID": "24_Summarize.html#get-summary-of-an-r-object-with-summary",
    "href": "24_Summarize.html#get-summary-of-an-r-object-with-summary",
    "title": "12  Summarizing Data",
    "section": "12.1 Get summary of an R object with summary()",
    "text": "12.1 Get summary of an R object with summary()\nR includes summary() methods for a number of different objects.\n\nsummary(heart)\n\n      age            sex             chest_pain           trestbps    \n Min.   :28.00   Length:294         Length:294         Min.   : 92.0  \n 1st Qu.:42.00   Class :character   Class :character   1st Qu.:120.0  \n Median :49.00   Mode  :character   Mode  :character   Median :130.0  \n Mean   :47.83                                         Mean   :132.6  \n 3rd Qu.:54.00                                         3rd Qu.:140.0  \n Max.   :66.00                                         Max.   :200.0  \n                                                       NA's   :1      \n      chol           fbs              restecg             thalach     \n Min.   : 85.0   Length:294         Length:294         Min.   : 82.0  \n 1st Qu.:209.0   Class :character   Class :character   1st Qu.:122.0  \n Median :243.0   Mode  :character   Mode  :character   Median :140.0  \n Mean   :250.8                                         Mean   :139.1  \n 3rd Qu.:282.5                                         3rd Qu.:155.0  \n Max.   :603.0                                         Max.   :190.0  \n NA's   :23                                            NA's   :1      \n    exang              oldpeak          slope                 ca     \n Length:294         Min.   :0.0000   Length:294         Min.   :0    \n Class :character   1st Qu.:0.0000   Class :character   1st Qu.:0    \n Mode  :character   Median :0.0000   Mode  :character   Median :0    \n                    Mean   :0.5861                      Mean   :0    \n                    3rd Qu.:1.0000                      3rd Qu.:0    \n                    Max.   :5.0000                      Max.   :0    \n                                                        NA's   :291  \n     thal               num           \n Length:294         Length:294        \n Class :character   Class :character  \n Mode  :character   Mode  :character"
  },
  {
    "objectID": "24_Summarize.html#fast-builtin-column-and-row-operations",
    "href": "24_Summarize.html#fast-builtin-column-and-row-operations",
    "title": "12  Summarizing Data",
    "section": "12.2 Fast builtin column and row operations",
    "text": "12.2 Fast builtin column and row operations\nWe saw in Loop Functions how we can apply functions on rows, columns, or other subsets of our data. R has optimized builtin functions for some very common operations, with self-explanatory names:\n\ncolSums(): column sums\nrowSums(): row sums\ncolMeans(): column means\nrowMeans(): row means\n\n\na <- matrix(1:20, 5)\na\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    6   11   16\n[2,]    2    7   12   17\n[3,]    3    8   13   18\n[4,]    4    9   14   19\n[5,]    5   10   15   20\n\n\n\ncolSums(a)\n\n[1] 15 40 65 90\n\n# same as\napply(a, 2, sum)\n\n[1] 15 40 65 90\n\n\n\nrowSums(a)\n\n[1] 34 38 42 46 50\n\n# same as\napply(a, 1, sum)\n\n[1] 34 38 42 46 50\n\n\n\ncolMeans(a)\n\n[1]  3  8 13 18\n\n# same as\napply(a, 2, mean)\n\n[1]  3  8 13 18\n\n\n\nrowMeans(a)\n\n[1]  8.5  9.5 10.5 11.5 12.5\n\n# same as\napply(a, 1, mean)\n\n[1]  8.5  9.5 10.5 11.5 12.5"
  },
  {
    "objectID": "24_Summarize.html#optimized-matrix-operations-with-matrixstats",
    "href": "24_Summarize.html#optimized-matrix-operations-with-matrixstats",
    "title": "12  Summarizing Data",
    "section": "12.3 Optimized matrix operations with matrixStats",
    "text": "12.3 Optimized matrix operations with matrixStats\nWhile the builtin operations above are already optimized and faster than the equivalent calls, the matrixStats package (Bengtsson 2019) offers a number of futher optimized matrix operations, including drop-in replacements of the above. These should be prefered when dealing with bigger data:\n\nlibrary(matrixStats)\ncolSums2(a)\n\n[1] 15 40 65 90\n\nrowSums2(a)\n\n[1] 34 38 42 46 50\n\ncolMeans2(a)\n\n[1]  3  8 13 18\n\nrowMeans2(a)\n\n[1]  8.5  9.5 10.5 11.5 12.5\n\n\nNote: matrixStats provides replacement functions named almost identically to their base counterpart - so they are easy to find - but are different - so they don’t mask the base functions (this is important and good software design)."
  },
  {
    "objectID": "24_Summarize.html#see-alos",
    "href": "24_Summarize.html#see-alos",
    "title": "12  Summarizing Data",
    "section": "12.4 See alos",
    "text": "12.4 See alos\naggregate() for grouped summary statistics\n\n\n\n\nBengtsson, Henrik. 2019. matrixStats: Functions That Apply to Rows and Columns of Matrices (and to Vectors). https://CRAN.R-project.org/package=matrixStats."
  },
  {
    "objectID": "30_Functions.html",
    "href": "30_Functions.html",
    "title": "14  Functions",
    "section": "",
    "text": "Writing functions is a core part of programming.\nWhen should you write a function?\nWhenever you find yourself repeating pieces of code.\nWhy is it important?\nWriting functions helps reduce the total amount of code, which increases efficiency, reduces the chance of error, and can make code more readable.\nFunctions in R are “first-class objects”.\nThis means they can be passed in and out of other functions or objects like any other R structure.\nFor example, you can use a command like apply(mat, 2, mean)\nFunctions in R are for the most part like mathematical functions: they have one or more inputs and one output. The inputs are known as the function arguments. If you want to return multiple outputs, you can return a list containing any number of R objects."
  },
  {
    "objectID": "30_Functions.html#simple-functions",
    "href": "30_Functions.html#simple-functions",
    "title": "14  Functions",
    "section": "14.1 Simple functions",
    "text": "14.1 Simple functions\nLet’s start with a very simple function: single argument with no default value:\n\nsquare <- function(x) {\n  x^2\n}\n\nsquare(3)\n\n[1] 9\n\n\nNotice above that x^2 is automatically returned by the function. It is the same as explicitly returning it with return():\n\nsquare <- function(x) {\n  out <- x^2\n  return(out)\n}\n\nsquare(4)\n\n[1] 16\n\n\nwhich is the same as:\n\nsquare <- function(x) {\n  out <- x^2\n  out\n}\n\nsquare(5)\n\n[1] 25\n\n\nA function returns either:\n\nan object passed to return()\nthe value of the last expression within the function definition such as out or x^2 above.\n\nreturn() is a way to end evaluation early:\n\nsquare.pos <- function(x) {\n  if (x > 0) {\n    return(x^2)\n  } else {\n    x\n  }\n  cat(\"The input was left unchanged\\n\")\n}\n\nx <- sample(-10:10, 1)\nx\n\n[1] 6\n\nsquare.pos(x)\n\n[1] 36\n\n\nMultiple arguments, with and without defaults:\n\nraise <- function(x, power = 2) {\n  x^power\n}\n\nx <- sample(10, 1)\nx\n\n[1] 9\n\nraise(x)\n\n[1] 81\n\nraise(x, power = 3)\n\n[1] 729\n\nraise(x, 3)\n\n[1] 729"
  },
  {
    "objectID": "30_Functions.html#argument-matching",
    "href": "30_Functions.html#argument-matching",
    "title": "14  Functions",
    "section": "14.2 Argument matching",
    "text": "14.2 Argument matching\nR will match unambiguous abbreviations of arguments:\n\nfn <- function(alpha = 2, beta = 3, gamma = 4) {\n  alpha * beta + gamma\n}\nfn(g = 2)\n\n[1] 8"
  },
  {
    "objectID": "30_Functions.html#arguments-with-prescribed-set-of-allowed-values",
    "href": "30_Functions.html#arguments-with-prescribed-set-of-allowed-values",
    "title": "14  Functions",
    "section": "14.3 Arguments with prescribed set of allowed values",
    "text": "14.3 Arguments with prescribed set of allowed values\nYou can match specific values for an argument using match.arg():\n\nmyfn <- function(type = c(\"alpha\", \"beta\", \"gamma\")) {\n  type <- match.arg(type)\n  cat(\"You have selected type '\", type, \"'\\n\", sep = \"\")\n}\n\nmyfn(\"a\")\n\nYou have selected type 'alpha'\n\nmyfn(\"b\")\n\nYou have selected type 'beta'\n\nmyfn(\"g\")\n\nYou have selected type 'gamma'\n\nmyfn(\"d\")\n\nError in match.arg(type): 'arg' should be one of \"alpha\", \"beta\", \"gamma\"\n\n\nAbove you see that partial matching using match.arg() was able to identify a valid option, and when there was no match, an informative error was printed.\nPartial matching is also automatically done on the argument names themselves, but it’s important to avoid depending on that.\n\nadsr <- function(attack = 100,\n                 decay = 250,\n                 sustain = 40,\n                 release = 1000) {\n  cat(\"Attack time:\", attack, \"ms\\n\",\n      \"Decay time:\", decay, \"ms\\n\",\n      \"Sustain level:\", sustain, \"\\n\",\n      \"Release time:\", release, \"ms\\n\")\n}\n\nadsr(50, s = 100, r = 500)\n\nAttack time: 50 ms\n Decay time: 250 ms\n Sustain level: 100 \n Release time: 500 ms"
  },
  {
    "objectID": "30_Functions.html#passing-extra-arguments-to-another-function-with-the-...-argument",
    "href": "30_Functions.html#passing-extra-arguments-to-another-function-with-the-...-argument",
    "title": "14  Functions",
    "section": "14.4 Passing extra arguments to another function with the ... argument",
    "text": "14.4 Passing extra arguments to another function with the ... argument\nMany functions include a ... argument at the end. Any arguments not otherwise matched are collected there. A common use for this is to pass them to another function:\n\ncplot <- function(x, y,\n                  cex = 1.5,\n                  pch = 16,\n                  col = \"#18A3AC\",\n                  bty = \"n\", ...) {\n  plot(x, y, cex = cex, pch = pch, col = col, bty = bty, ...)\n                  }\n\n... is also used for variable number of iputs, often as the first argument of a function. For example, look at the documentation of c, cat, cbind, rbind, paste\nNote: Any arguments after the ..., must be named fully, i.e. will not be partially matched."
  },
  {
    "objectID": "30_Functions.html#return-multiple-objects",
    "href": "30_Functions.html#return-multiple-objects",
    "title": "14  Functions",
    "section": "14.5 Return multiple objects",
    "text": "14.5 Return multiple objects\nR function can only return a single object. This is not much of a problem because you can simply put any collection of objects into a list and return it:\n\nlfn <- function(x, fn = square) {\n  xfn <- fn(x)\n  \n  list(x = x,\n       xfn = xfn,\n       fn = fn)\n}\n\nlfn(3)\n\n$x\n[1] 3\n\n$xfn\n[1] 9\n\n$fn\nfunction(x) {\n  out <- x^2\n  out\n}\n<bytecode: 0x10a2c3f10>"
  },
  {
    "objectID": "30_Functions.html#warnings-and-errors",
    "href": "30_Functions.html#warnings-and-errors",
    "title": "14  Functions",
    "section": "14.6 Warnings and errors",
    "text": "14.6 Warnings and errors\nYou can use warning(\"some warning message\") at any point inside a function to produce a warning message during execution. The message gets printed to the R console, but function execution is not stopped.\nOn the other hand, you can use stop(\"some error message\") to print an error message to console and stop function execution.\nThe following function (el10) calculates:\n\\[ e^{log_{10}(x)} \\]\n\nel10 <- function(x) {\n  exp(log10(x))\n}\n\nwhich is not defined for negative x. In this case, we could let R give a warning when it tries to compute log10(x):\n\nval1 <- el10(-3)\n\nWarning in el10(-3): NaNs produced\n\n\nWe could instead produce our own warning message:\n\nel10 <- function(x) {\n  if (x < 0) warning(\"x must be positive\")\n  exp(log10(x))\n}\nval2 <- el10(-3)\n\nWarning in el10(-3): x must be positive\n\n\nWarning in el10(-3): NaNs produced\n\nval2\n\n[1] NaN\n\n\nAs you see, the output (NaN) still gets returned.\nAlternatively, we can use stop() to end function execution:\n\nel10 <- function(x) {\n  if (x < 0) stop(\"x must be positive\")\n  exp(log10(x))\n}\nval3 <- el10(-3)\n\nError in el10(-3): x must be positive\n\n\nNote how, in this case, function evalutation is stopped and no value is returned."
  },
  {
    "objectID": "30_Functions.html#scoping",
    "href": "30_Functions.html#scoping",
    "title": "14  Functions",
    "section": "14.7 Scoping",
    "text": "14.7 Scoping\nFunctions exist in their own environment, i.e. contain their own variable definitions.\n\nx <- 3\ny <- 4\nfn <- function(x, y) {\n  x <- 10*x\n  y <- 20*y\n  cat(\"Inside the function, x = \", x, \" and y = \", y, \"\\n\")\n}\n\nfn(x, y)\n\nInside the function, x =  30  and y =  80 \n\ncat(\"Outside the function, x = \", x, \" and y = \", y, \"\\n\")\n\nOutside the function, x =  3  and y =  4 \n\n\nHowever, if a variable is referenced within a function but no local definition exists, the interpreter will look for the variable at the parent directory. It is best ensure all objects needed within a function are specified as arguments and passed appropriately when the function is called.\nIn the following example, x is only defined outside the function definition, but referenced within it.\n\nx <- 21\n\nitfn <- function(y, lr = 1) {\n  x + lr * y\n}\n\nitfn(3)\n\n[1] 24\n\n\n\n14.7.1 function vs. for loop\nLet’s z-score the built-in mtcars dataset once with a for loop and once with a custom function. This links back to the example seen earlier in the for loop section. In practice, this would be performed with the scale() command:\nWithin the for loop, we are assigning columns directly to the object initialized before the loop. In the following example, we use print(environment()) to print the environment outside and inside the loop function to show that it is the same. This is purely for demonstration:\n\n# initialize new object 'mtcars_z'\nmtcars_z <- mtcars\ncat(\"environment outside for loop is: \")\n\nenvironment outside for loop is: \n\nprint(environment())\n\n<environment: R_GlobalEnv>\n\n# z-score one column at a time in a for loop\nfor (i in 1:ncol(mtcars)) {\n  mtcars_z[, i] <- (mtcars[, i] - mean(mtcars[, i])) / sd(mtcars[, i])\n  cat(\"environment inside for loop is: \")\n  print(environment())\n}\n\nenvironment inside for loop is: <environment: R_GlobalEnv>\nenvironment inside for loop is: <environment: R_GlobalEnv>\nenvironment inside for loop is: <environment: R_GlobalEnv>\nenvironment inside for loop is: <environment: R_GlobalEnv>\nenvironment inside for loop is: <environment: R_GlobalEnv>\nenvironment inside for loop is: <environment: R_GlobalEnv>\nenvironment inside for loop is: <environment: R_GlobalEnv>\nenvironment inside for loop is: <environment: R_GlobalEnv>\nenvironment inside for loop is: <environment: R_GlobalEnv>\nenvironment inside for loop is: <environment: R_GlobalEnv>\nenvironment inside for loop is: <environment: R_GlobalEnv>\n\n\nIn contrast, all operations remain local within a function and the output must be returned:\n\nztransform <- function(x) {\n  cat(\"environment inside function body is: \")\n  print(environment())\n  z <- as.data.frame(sapply(mtcars, function(i) (i - mean(i))/sd(i)))\n  rownames(z) <- rownames(x)\n  z\n}\nmtcars_z2 <- ztransform(mtcars)\n\nenvironment inside function body is: <environment: 0x109b7a820>\n\ncat(\"environment outside function body is: \")\n\nenvironment outside function body is: \n\nprint(environment())\n\n<environment: R_GlobalEnv>\n\n\nNotice how the environment outside and inside the loop function is the same, it is the Global environemnt, but the environment within the function is different. That is why any objects created or changed within a function must be returned if we want to make them available."
  },
  {
    "objectID": "30_Functions.html#pipe",
    "href": "30_Functions.html#pipe",
    "title": "14  Functions",
    "section": "14.8 The pipe operator",
    "text": "14.8 The pipe operator\n\n\n\n\n\nIllustration of pipes in R\n\n\n\n\nA pipe operator was first introduced to R by the magrittr package with the %>% symbol. Note that a number of other packages that endorse the use of pipes export the pipe operator as well.\nStarting with R version 4.1, a native pipe operator is included with the |> symbol.\nA pipe allows writing f(x) as x |> f() (native pipe) or x %>% f (magrittr).\nNote that the native pipe requires parentheses, but magrittr works with or without them.\nA pipe is often used to:\n\navoid multiple temporary assignments in a multistep procedure, or\nas an alternative to nesting functions.\n\nSome packages and developers promote its use, others discourage it. You should try and see if/when it suits your needs.\nThe following:\n\nx <- f1(x)\nx <- f2(x)\nx <- f3(x)\n\nis equivalent to:\n\nx <- f3(f2(f1(x)))\n\nis equivalent to:\n\nx <- x |> f1() |> f2() |> f3()\n\n\niris[, -5] |>\n  split(iris$Species) |>\n  lapply(function(i) sapply(i, mean))\n\n$setosa\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n       5.006        3.428        1.462        0.246 \n\n$versicolor\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n       5.936        2.770        4.260        1.326 \n\n$virginica\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n       6.588        2.974        5.552        2.026 \n\n\nPipes are used extensively in the tidyverse packages and many other third-party packages.\nYou can learn more about the magrittr pipe operator in the vignette\n\n\n\nIn RStudio the keyboard shortcut for the pipe operator is Shift-Command-M (MacOS) or Ctrl-Shift-M (Windows)\n\n\n\n\n14.8.1 Differences between native pipe and magrittr\n\nnative pipe requires () after function name, magrittr works with or without them\n\n\nx <- rnorm(300)\n\n\nx |> mean()\n\n[1] -0.05065302\n\n\nbut this would fail:\n\nx |> mean\n\nwhile either works in magrittr\n\nlibrary(magrittr)\nx %>% mean()\n\n[1] -0.05065302\n\n\n\nx %>% mean\n\n[1] -0.05065302\n\n\n\nnative pipe by design only pipes its LHS to the first unnamed argument on the RHS. magrittr allows using a period . to pipe to any position on the RHS. The native pipe workaround is using an anonymous function (can use the new shorter syntax \\(x) instead of function(x))\n\ne.g.: Find the position of “r” in the latin alphabet\nIn this example, we want to pass the LHS to the second argument of grep().\nUsing native pipe, we name the first argument pattern and the LHS is passed to the first unnamed argument, i.e. the second (which is x, the character vector where matches are looked for)\n\nletters |> grep(pattern = \"r\")\n\n[1] 18\n\n\nwith magrittr you can use the dot notation to specify where to pipe into:\n\nletters %>% grep(\"r\", .)\n\n[1] 18\n\n\nFor demonstration, here’s the slightly involved way you would achieve this with an anonymous function and the native pipe. This may make sense for more complex calls.\n\nletters |> {\\(x) grep(\"r\", x)}()\n\n[1] 18"
  }
]